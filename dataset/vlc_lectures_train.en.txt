17 en Grobelnik Marko - Analyse the portugese statistical office website??? 
20 en Information Society Readyness Nagovor ministra
22 en Speciation analysis using ICPMS 
26 en Bioinformatic, Structural Biology and Structure Based Ligand Design in Drug discovery During the lecture we will be shown the principles on which the modern development and drug discovery with emphasis on optimization of compounds with the use of three dimensional structures of the receptor. Also the use of bioinformatics will be shown and computer supported planning of ligands together with the implications of the understanding of detailed interactions between the protein and the ligand in the transfiguration of the enzyme inhibitors into medicines in the context of modern challenges.
27 en Comparisson Of Technologies For Connecting Business Processes Among Enterprises 
30 en Pogodbe - Slovenija v 5. in 6. okvirnem programu EU 
31 en Comprehension Assistance Meets Machine Translation 
36 en Safety and Risks in Nanotechnology 
41 en Decision Support for Everyone: Olap in Ms Excel 
42 en Organic Functionalization of Carbon Nanotubes 
43 en History of NANONET-Styria 
52 en Adaptation of a cllasical rule-learning algorhytm for subgroup discovery and application of this modified algorhytm on real life domain 
56 en Poliamids: BASF chains of added value on the chemical products market 
63 en Telework in Slovenia 
64 en Adapting Proactive Mobile Agents to Dynamically Reconfigurable Networks – Report On Current Research 
65 en Design Thinking as a Problem Solving Tool in technology and life Design thinking is a process for practical, creative resolution of problems or issues.n n The stages of this process are suggested as:n n Define | Research | Ideate | Prototype | Choose | Implement | Learnn n Within these seven steps, problems can be framed, the right questions can be asked, more ideas can be created, and the best answers can be chosen. The steps aren't linear; they can occur simultaneously or be repeated.n n Although design is always subject to personal taste, design thinkers share a common set of values that drive innovation: these values are mainly creativity, ambidextrous thinking, teamwork, end-user focus, curiosity.n n There is considerable academic interest in understanding design thinking or design cognition, including an ongoing series of symposia on 'research in design thinking'.
66 en Social Constructivism as a Philosophy of Mathematics 
67 en Learning User Profiles in Intelligent Systems 
68 en Kernel-Based Methods for Pattern Recognition 
71 en Lexical Semantics in the Age of the Semantic Web 
78 en Ovweview of network analysis 
79 en Symmetry in the works of M. C. Escher Introductory References about Tessellations (tilings) and Sources for Illustrations: [[http://www.geom.uiuc.edu/software/tilings/TilingBibliography.html|link]]
80 en Succint Data Structures 
82 en The Sol-Eu-Net Project: Data mining Lessons Learned 
85 en Describing Decision Support, Darta Mining, and Text/Web Mining Studies in SolEuNet 
86 en Time-series analysis of UK traffic accident data 
90 en Web User Behavior Characterization: Techniques, Applications And Research Directions 
92 en Image categorization 
94 en Developing Software Across Time Zones: An Exploratory Empirical Study 
96 en Science Policy in Slovenia Nagovor dr?avnega sekretarja
98 en Discovering interesting rules from financial data 
99 en Functional 4D visualisation of proteoliosys: the influence of the micro environment 
101 en Introduction, welcome, conference presentation Nagovor direktorja
102 en Qualitative Clustering of Short Time-Series: A Case Study of Firms Reputation Data 
121 en Parallel Manipulators in Biomechanics and Robotics In the lecture we will be shown the newest discoveries on the field of parallel manipulators. Parallel manipulators are mechanisms which contain more closed, parallel, cinematic loops, where only few are driven by motors. The attention will be focused on the influence of different parameters; air at the joints, constructional defects, cinematic singularities and isotropy, the activity of the manipulator especially from the biomechanics and robotic point of view.
123 en Monodispersed particles in technologies and medicine In the last decades we witnessed the development of different techniques for the manufacturing of monodispersed systems with particle size from few micrometers to few nanometers. These particles are of different sizes and forms, their chemical structure can be complex or simple. At the beginning the focus of research of these systems was in the search of different physical and chemical properties, which can be strongly dependent from the size of the particles and their morphology. Later on the interest was pointed towards their use in the making of materials with specific and repeated properties. This use is specially accelerated by today’s trends in miniaturizing in technology and medicine.  They enable a large progress in the transmission of laboratory techniques of preparative arrangements in monodispersed colloid systems in the economy.  The lecturer will focus on the role of monodispersive colloid systems in the manufacturing of cheramics and pigments, photography, chemical polishing, and especially in the making of monodispersive medicines. He will show how colloid particles are useful in the transmission of medicines with delayed effect on the in advance chosen part of the human body. They are also used in diagnostics, especially in x-ray examinations. Their use is different because of the differences in the properties of nano and micro dispersions.
134 en Surface plasmon resonance imagery and single molecule approaches of proteo-nucleic complex self-assembly The presentation will illustrate the use of use surface plasmon resonance imagery and of single molecule approaches based on quantum dot labelling for the analysis of the dynamic and structure of proteo-nucleic assemblies. Semi-synthetic complexes (PDAs), encompassing a protein part embedding electro active redox cofactors and a nucleic acid part allowing highly controlled self-organisation of the protein domains, were designed and constructed. This approach allows the building of large and fully defined chain of electron transport centres with tuneable spatial organization. Association with quantum dot offering single molecule monitoring power opens the route to the use of self-assembled bio molecules as molecular wire for bioelectronics.
136 en In-situ Fabrication, Manipulation and Property Measurements of Single Nanotubes and Nanowires with Near Atomic Resolution Carbon nanotube (CNT) and nanowire materials are important building
137 en Self-assembly of Nematic colloids Nematic colloids are dispersions of micrometric particles in nematic liquid crystals which show unusual properties of self-assembly into regular geometrical patterns. The reason for this type of behavior is the pressure of the liquid crystal among the particles. These pressures are called structures, and are the consequence of the orientation of the liquid crystals and the deformation of the structure of the crystal near the colloid particles. \\ It has been known for quite a while that the nematic colloids are unifying into one dimensional chain. We also know about two dimensional structures on the border of liquid crystals and isotropic liquids. Latest research has shown progress in the understanding of the nature of self-assembly of nematic colloids; they have shown that nematic colloids can spontaneously make also regular two dimensional structures which are very tightly bounded. \\ With these facts a new path has been laid for the self-assembly of colloids in 3 dimensional crystals. There are also interesting possibilities in the manufacturing of materials with the self arranging of nanoparticles.
139 en Liquid Crystal Elastomers Rubber elasticity is a unique feature of polymer networks or elastomers formed from long polymer chains connected to one another by crosslinkages. An implanting of rod-like mesogenic monomer units into the network chains can induce the liquid crystalline state of the elastomers. The interplay between network chain conformation and liquid crystalline phase creates material with remarkable properties. Detailed investigations proved a direct coupling between chain conformation and anisotropic state of order of the liquid crystalline phase. Introducing an anisotropic chain conformation a priori by a suitable synthesis concept, networks can be obtained that are macroscopic uniformly ordered (liquid single crystal elastomers, LSCE). Depending on the liquid crystalline phase structure, LSCE exhibit exceptional properties, e.g. nematic LSCE change their macroscopic dimensions when the state of order is modified by external stimuli. This effect can be applied to novel thermo- and opto-mechanical actuators.
140 en Spectroscopy This week we are hosting Stan Terras, the representative of the Ranishw Company from the UK. He is an expert on the field of spectroscopy. He will introduce the latest results with the Raman spectrometer and with the AFM/SEM that are used in nanotechnology.
142 en Cambridge fenomenon of new technologies transfer into the economy Dr. Smeets will introduce the experiences of the university town of Cambridge, one of the most important European centers for the transfer of new technologies into the economy. He will focus on the development of the "Cambridge phenomenon of new technologies transfer into the economy" and the conditions that influenced the success of this concept. On the basis of the above listed facts he will discuss the mechanisms for achieving this phenomenon. The lecture is being organized by the Slovenian Technology Park in Ljubljana in cooperation with the Jozef Stefan Institute and the British Embassy in Slovenia.
164 en How is protein synthesis catalyzed? The Cassiopeia synchrotron stations for protein crystallography at MAX-II. 
165 en The simulation of structures in modern materials with the theory of density functional calculations With the theory of density functional calculations we can study the connection between the crystal structure, electronic structure and the properties connected with these. With the comparison of complete energies of different structures we can establish the relative stability of individual structures. With the minimizing of inter-atomic powers we can optimize the position of atoms (ions) in the crystal. With this method we can accurately calculate also the phonic structure in the super cell which contains 60-100 atoms. As an example of use of TGF I will introduce the study of Y2Nb2O7, which can crystallize in the pyrochlore structure and at the same time works as an isolator.
167 en Laser Manipulation of Atoms and Spins Two of the most important themes of modern magnetism are the making of magnetic structures and the switching of magnetization on ultra fast time scale. With the help of the interaction between photons and electrical states in atoms we can achieve both. Nanofabrication with the help of atomic optics can provide a tool for the production of magnetic nanostructures, where femtoseconds laser shots can provide coherent control of magnetization on ultra light picoseconds timescale.
169 en Bioinformatic, Structural Biology and Structure Based Ligand Design in Drug discovery During the lecture we will be shown the principles on which the modern development and drug discovery with emphasis on optimization of compounds with the use of three dimensional structures of the receptor. Also the use of bioinformatics will be shown and computer supported planning of ligands together with the implications of the understanding of detailed interactions between the protein and the ligand in the transfiguration of the enzyme inhibitors into medicines in the context of modern challenges.
170 en Enzyme synthesis of new dna nanostructures We have developed a new technique for the enzyme synthesis of equally long micrometric structures made of double chains - polyG-polyC, triple chains poly(dG)-poly(dG)-poly(dC), and quadruple chains 4G. We were studying the structure of molecules and the mechanism of their synthesis With the use of absorbed and CD spectroscopy, FRET and new highly dissoluble microscopy’s on atomic force (AFM).
171 en Architecture for humananoid robots: emulation of biological processes To achieve better understanding of the processing of information with humans, which is processing in the brain, we can help with the construction and study of robotic systems which are human like. Three views to this topic will be presented in this lecture; behaviorist, integration of systems, the organization of flow and processing of information. The suggested architecture is based on a decentralized and cooperative processing of information. These processes can be serial or parallel, with forward and backward links. From here a complex network between interlinked and dependent process elements can be established. On the basis of the sensory input and output movements, the system generates more complex operations and decisions. The results of the research which will be presented during the lecture are based on the research in many other disciplines: humanoid robotics for integration and implementation of the system, neuroscience for the organization of the flux in the processing of information and nevropsychology for the evaluation of the systems behavior.
176 en Sustainability concept of energy, water and environmental systems This review first introduces the historical background for the sustainability concept development. Special reference is given to the energy resource depletion and its forecast. In the assessment of global energy resources attention is focused in on the resource consumption and its relevancy to the future demand. The recent assessment of sustainability is reflecting the normative and strategic dimension of sustainability. Special attention is devoted to the most recent development of the concept of sustainability science. A new field of sustainability science is emerging that seeks to understand the fundamental character of interactions between nature and society. With a view toward promoting research necessary to achieve such advances, an initial set of core questions for sustainability science was proposed.n;n://The real lecture (English part) starts at around 4:40 within video.//
179 en Expression Profiling of Developmental Processes in the Social Amoeba Dictyostelium 
198 en Mining Relational Model Trees Multi-Relational Data Mining (MRDM) refers to the process of discovering implicit, previously unknown and potentially useful information from data scattered in multiple tables of a relational database. MRDM is necessary to face the substantial complexity added to data mining tasks when properties of units of analysis to be investigated are potentially affected by attributes of related units of analysis eventually of different types and naturally modeled to yield as many tables as the number of object types. Regression is a fundamental task in MRDM where the goal is to examine samples of past experience with known continuous answers (response) and generalize future cases throughan inductive process. Following the mainstream of MRDM research, Mr-SMOTI resorts to the structural approach in order to recursively partition data stored in a tightly-coupled database and build a multi-relational model tree that captures the linear dependence between the response variable and one or more explanatory variables of both the reference objects and task-relevant objects. The model tree is top-down induced by choosing, at each step, either to partition the training space (split nodes) or to introduce a regression variable in the linear models to be associated with the leaves (regression nodes). Internal regression nodes contribute to the definition of multiple models and capture global effects, while straight-line regressions with leaves capture only local effects. The tight-coupling with the database makes the knowledge on data structures (e.g., foreign keys) available free of charge to guide the search in the multi-relational pattern space.
201 en Semantic Web Usage Mining – Overview and Case Studies In this tutorial we will review fundamentals of web usage mining - theory, case studies and related topics. Web usage mining is a topic which became in the late 90ties one of the first profitable areas of data mining and which was necessity for the succesful e-commerce companies to understand better their customers, their behaviour and to optimize the e-services accordingly. In this tutorial lecture we will show several case studies which show approaches, techniques and results coming out of this area.
203 en KDD - CUP 2004 Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? \\ In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be "repaired" so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. \\ During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.
204 en Some Operations on the Lattice of Equivalence Relations Equivalence relations have played a fundamental role through the History of Mathematics. These special relations are so omnipresent in everyday life that we often forget about their proactive existence. Though, according to some authors, still much is unknown about them. In this talk, we will studdy independence and permutability, on the context of a equivalence relations lattice of a given set, refering to the interpretation of these mathematical concepts in the Information Theory point of view. We, then, present of equivalence pairs of finite type, due to B.Jónsson, and introduce the operation star defined within equivalences, rewrithing M.-L. Dubreil important theorem, describing permutable equivalences.
205 en Predicting Rare Extreme Values - recent developments Predicting rare extreme values of a continuous variable is of key importance in several important real world applications (e.g. finance, ecology, etc.). In this seminar we start by presenting the problem and its motivation and then go through a series of existing approaches to the problem, highlighting their main limitations. As a result of this analysis we then describe a series of recent developments of our work in this area that has lead to the introduction of the notions of cost and benefit surfaces. We present these two new notions and their intuition in the context of handling regression problems with differentiated importance of observations (as it is the case of predicting rare extreme values). We formalize both notions and explain how they can be used in the context of our target applications. We finish by providing some initial results on the use of these notions in the context of evaluating models in tasks related to the prediction of rare extreme values of a continuous variable.
212 en Telematic management of Diabetes: current European experiences The recent advances of Information and Communication Technologies (IT) have transformed telemedicine into a mature field, so that the research results collected over years can be nowadays translated into clinical practice. Diabetes management represents a sort of natural field for the application of new distributed model of care, that fully exploits currently available telemedicine solutions. For example, the US Health Care Financing Administration (HCFA) has recently funded the IDEATel project with a $28 million grant; IDEATel is the largest telemedicine effort ever funded by the US federal government. Also in Europe there are several projects that are testing innovative IT-based services for supporting diabetes management. Within the V framework research programme of the European Commission, about 40 projects have been devoted to home-care telemedicine applications: 8 of them dealt with the application of IT to the management of Diabetes.  The talk will be divided into two parts: in the first part, it will be given an introduction on the current status of the European research on telemedicine applications in Diabetes care; in the second part, it will be presented the M2DM project, together with the some results obtained in its clinical evaluation.
217 en Differential Evolution and Particle Swarm Optimization in Partitional Clustering In recent years, many partitional clustering algorithms based on genetic algorithms (GA) have been proposed to tackle the problem of finding the optimal partition of a data set. Surprisingly, very few studies considered alternative stochastic search heuristics other than GAs or simulated annealing. Two promising algorithms for numerical optimization, which are hardly known outside the heuristic search field, are particle swarm optimisation (PSO) and differential evolution (DE). In this study, we compared the performance of GAs with PSO and DE for a medoid evolution approach to clustering. Moreover, we compared these results with the nominal classification, k-means and random search (RS) as a lower bound. Our results show that DE is clearly and consistently superior compared to GAs and PSO for hard clustering problems, both in respect to precision as well as robustness (reproducibility) of the results. Only for trivial problems all algorithms can obtain comparable results. Apart from superior performance, DE is very easy to implement and requires hardly any parameter tuning compared to substantial tuning for GAs and PSOs. Our study shows that DE rather than GAs should receive primary attention in partitional cluster algorithms.
226 en Comparison of information retrieval techniques: Latent semantic indexing (LSI) and Concept indexing (CI) Information retrieval in the vector space model is based on literal matching of terms in the documents and the queries. The model is implemented by creating the term-document matrix, which is formed on the base of frequencies of terms in documents. Literal matching of terms does not necessarily retrieve all relevant documents. Synonymy (multiple words having the same meaning) and polysemy (words having multiple meaning) are two major obstacles for efficient information retrieval. Latent semantic indexing (LSI) and concept indexing (CI) are information retrieval techniques embedded in the vector space model, which address the problem of synonymy and polysemy. The method of LSI is an information retrieval technique using a low-rank singular value decomposition (SVD) of the term-document matrix. Although the LSI method has empirical success, it suffers from the lack of interpretation for the low-rank approximation and, consequently, the lack of controls for accomplishing specific tasks in information retrieval. The method of CI uses centroids of clusters or so-called concept decomposition (CD) for lowering the rank of the term-document matrix. Here we compare SVD/LSI and CD/CI in terms of matrix approximations and precision of information retrieval.
230 en Can diagrammatic reasoning be automated Theorems in automated theorem proving are usually proved by formal logical proofs. However, there is a subset of problems which humans can prove by the use of geometric operations on diagrams, so called diagrammatic proofs. Insight is often more clearly perceived in these proofs than in the corresponding algebraic proofs; they capture an intuitive notion of truthfulness that humans find easy to see and understand. We are investigating and automating such diagrammatic reasoning about mathematical theorems. Concrete, rather than general diagrams are used to prove particular concrete instances of the universally quantified theorem. The diagrammatic proof is captured by the use of geometric operations on the diagram. These operations are the "inference steps" of the proof. \\ An abstracted schematic proof of the universally quantified theorem is induced from these proof instances. The constructive omega-rule provides the mathematical basis for this step from schematic proofs to theoremhood. In this way we avoid the difficulty of treating a general case in a diagram. One method of confirming that the abstraction of the schematic proof from the proof instances is sound is proving the correctness of schematic proofs in the meta-theory of diagrams. These ideas have been implemented in the system, called DIAMOND, which is presented here.
240 en Distance based learning on relational algebra representations We will present a general framework based on concepts of relationalnalgebra for distance based learning over relational schemata. Thenadvantage of the proposed framework is that it requires nontransformation of the representation of data that come in the form ofnrelational databases. It is directly applicable to any relationalndatabase without the need of type and mode definitions and conversionsnto logic programming as it is the case with most relational learningnsystems based on Inductive Logic Programming.nnOur framework builds on the notions of tuples of relations and sets ofntuples. We show how exploiting these elementary building blocks ournlearning examples are represented via tree like structures. In order tondefine distances between relational examples we will explore twonavenues. Both of them are based on the definition of simple operators onntuples and sets of tuples which are subsequently combined in order tonprovide a global operator on the full relational structure. The firstnapproach is based on the use of classical distances over tuples and setsnof tuples and the second one on the definition of kernels.nnThe user of the system has at his disposal a number of possible distancenoperators from which he can choose, or alternatively, to what amounts tonsomething like model selection, can let the system perform the selectionnautomatically. Some results on well known relational datasets will benpresented.
244 en FP6-IST - behind the scenes The presentation will focus on future research supported by the European Union in general and within the field of Networked Organisations in particular. A possible outlook on future research in this area will be shown, given its evolution during the past decade. The presentation will cover an overview of past research in the area, as well as a synopsis of the challenges researchers are currently dealing with. Furthermore, an insight will be offered in the EC's work on the remaining part of FP6 and Work Programme 2005-2006. Finally, some preliminary ideas towards the next Framework Programme (FP7) will be presented regarding its contents, constituency, and procedures.
249 en Object Identification by Statistical Methods Numerical data fusion or merging of overlapping data files becomes a hard problem if no global unique identifying keys exist in the corresponding data sets. Typical examples are the linkage of address files supplied from different sources for commercial purposes - a money making area-, the merging of special offers in various media (cf. duplicate detection), or an administrative record census (ARC) as planed in Germany, where several autonomous, heterogeneous registers are to be merged. We present a three-step procedure consisting of the steps conversion of attributes, comparison of values of a pair of objects, and classification ('matching problem') of pairs either as "same" or "matched and "not same" or "not matched". We pay special attention to the quality and the efficiency of the methodology. We briefly discuss questions like correctness and completeness as well as pre-selection techniques like 'blocking' to reduce the computational complexity of pairwise comparisons. The approach is illustrated on data from carefully composed benchmark data sets. We assume some basic knowledge in computer science and classification (supervised learning).
250 en Fast, Exact Nearest Neighbor in Arbitrary Dimensions with a Cover Tree Given only a metric between points, how quickly can the nearest neighbor of a point be found? In the worst case, this time is O(n). When these points happen to obey a dimensionality constraint, more speed is possible. The "cover tree" is O(n) space datastructure which allows us to answer queries in O(log(n)) time given a fixed intrinsic dimensionality. It is also a very practical algorithm yielding speedups between a factor of 1 and 1000 on all datasets tested. This speedup has direct implications for several learning algorithms, simulations, and some systems
267 en Which Supervised Learning Method Works Best for What? An Empirical Comparison of Learning Methods and Metrics Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? \\ In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be "repaired" so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. \\ During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.
268 en The Dynamics of Viral Marketing We present an analysis of a person-to-person recommendation network,n consisting of 4 million people who made 16 million recommendationsn on half a million products. We observed the propagation ofn recommendations and the cascade sizes, which can be explained by an stochastic model. We then established how the recommendation networkn grows over time and how effective it is from the viewpoint of then sender and receiver of the recommendations. While on averagen recommendations are not very effective at inducing purchases and don not spread very far, there are product and pricing categories forn which viral marketing seems to be very effective.n n This is a joint work with Lada Adamic and Bernardo Huberman from HP Labs.n
270 en Object-Oriented Natural Language RequirementsSpecification Recent advances in software technology such as the development of** the Unified Modeling Language (UML) **have not reduced the need for betterrequirements specification. Natural language remains the method ofchoice for producing such documents.  These informal specifications mustbe turned into more formal designs on the way to a completeimplementation. These formal requirements are necessary not only for therapid prototyping of the evolving software systems but also to provide astandard reference model upon which all successive implementationsshould be constructed.  This presentation describes on-going research inautomating the construction of software systems from a natural language requirements specification, building upon**the theories of Two-LevelGrammar (TLG), object-oriented design and the Vienna Development Methodfor formal specification.**
277 en Simulating the Evolution of Language using Multi-Agent Systems While we have a good understanding of how life evolves, we have little idea of how evolution occurs in cultural systems. Of particular interest in this area is the development of language. As spoken words leave no physical remains, we are unable to find historical evidence of this development. \\ Therefore an alternative approach is needed. The approach many have taken is simulation. By conducting simulations, we can test hypotheses of different models of the evolution of communication, comparing simulated results with those we see in language today.  Multi-agent systems lend themselves naturally to this approach with each entity in the simulation being modelled as a different agent. This ensures the 'mind' of each entity can be kept private from all others and data can only be transferred amongst entities by language acts.  I will present an overview of the most important approaches and results in this field followed by an outline of the research that I am currently engaged in, which seeks to address the act of communication in an artificial-life environment within an altruistic framework..
281 en Learning predictive clustering rules Predictive clustering is based on ideas from two machine learning subareas, predictive modeling and clustering. Methods for predictive clustering enable us to construct models for predicting multiple target variables, which are normally simpler and more comprehensible than the corresponding collection of models, each predicting a single variable. To this end, predictive clustering has been restricted to decision tree methods. Our goal is to extend this approach to methods for learning rules. We have developed a generalized version of the covering algorithm that enables learning of ordered or unordered rules, on single or multiple target classification or regression domains. Performance of the new method compares favorably to existing methods. Comparison of single target and multiple target prediction models shows that multiple target models offer comparable performance and drastically lower complexity than the corresponding collections of single target models.
294 en WWW Challenges - Publishing, Searching, and Browsing on the Web Web is a highly distributed and dynamic environment that poses a number of challenges to designers and users of Web services and applications. In our research we promote a user centric approach to studying Web related issues and designing solutions that would improve the user's experience. We show how an exploratory study of users' behaviour led to new opportunities for modifying and improving Web publishing practices and browser features. We present the findings from quantitative and qualitative analyses of the users' logs and the impact they had on the prototype design. We point to the depth of problem understanding that has been achieved through the combined approach of statistical analysis of user logs and user interviews.
299 en Subgroup discovery experiments in functional genomics Functional genomics is a typical scientific discovery domain characterized by a very large number of attributes (genes) relative to the number of examples (observations). The danger of data overfitting is crucial in such domains. To avoid this pitfall and achieve predictor robustness, state-of-art approaches construct complex classifiers that combine relatively weak contributions of up to thousands of genes (attributes) to classify a disease. The complexity of such classifiers limits their transparency and consequently the biological insight they can provide. The goal of this study is to apply to this domain the methodology of constructing simple yet robust logic-based classifiers amenable to direct expert interpretation. The approach is based on the subgroup discovery rule learning methodology, enhanced by methods of restricting the hypothesis search space by exploiting the relevancy of features that enter the rule construction process as well as their combinations that form the rules. A multi-class functional genomics problem of classifying fourteen cancer types based on more than 16000 gene expression values is used to illustrate the methodology. Some of the discovered rules allow for novel biological interpretations.
301 en Privilege Management using X.509 Attribute Certificates (upravljanje z digitalnimi pooblastili) 
309 en Searching the Web by Discovering and Clustering Related Terms The amount of information on the web is growing so fast that it is becoming more and more difficult for classical search engines to find relevant information. Indeed, due to the frenetic increase of webpages written in different languages and sometimes in mis-interpreted languages, the degree of ambiguity of the human language has been constantly evolving to levels unseen so far. However, people still query the systems with no more than 2 words on average. As a consequence, new information retrieval systems need to be proposed to decrease the level of ambiguity of the queries. Such systems usually make use of query expansion techniques to solve this problem. In this talk, I will present a system based on the automatic discovery of terms that are related to the query as a means of helping the user to search for relevant information. This technique can be classified within Interactive Query Expansion systems. However, unlike other systems, we use Web Mining Techniques to discover related terms based on different features such as association measures, document similarity, document relevance, etc. In the second part of my talk, I will present the future extensions of our retrieval systems based on the automatic discovery of relations between related terms. So, by using agglomerative clustering techniques and an auto-fed WebWarehouse, we hope to be able to propose less ambiguous query expansion terms than in present systems where the user needs to sort out the terms he is interested in. ; [[http://webspy.di.ubi.pt/|Web spider]] : Web Spider is a system that returns all related terms and links from a given URL and a given query. : The Spider has been developped using C5.0 machine learning algorithm.
330 en Constructive Adaptive User Interfaces and Active Mining We propose a method to locate relations and constraints between a music score and its impressions, by which we show that machine learning techniques may provide a powerful tool for composing music and analyzing human feelings.  We examine its generality by modifying some arrangements to provide the subjects with a specified impression.  This paper introduces some user interfaces, which are capable of predicting feelings and creating new objects based on seed structures, such as spectrums and their transition for sounds that have been extracted and are perceived as favorable by the test subject.  We would like to discuss a role of such interfaces in the Active Mining project.
331 en Towards a formal understanding of object database query languages In this seminar I'll cover work I have done whilst in Ljubljana. I've been studying various proposals for query languages for object databases, in particular the language OQL proposed by the ODMG.  I've developed a type system and operational semantics for this language, and have proved some correctness properties. There are some interesting complications with this language, which justify the need for formality.  I'll cover this material and try to explain the formal techniques that I have used - type systems and operational semantics. (These are dominant themes in programming language research.) This seminar should be interesting to people who use database query languages and are interested in proposals for future-generation databases.  I'll try to keep the talk self-contained, so hopefully people wont need to know operational semantics before coming!
332 en Information Dynamics in a Networked World The shift of communication to the internet, in particular to email, weblogs (blogs), and online communities, presents an opportunity to study the information dynamics of social networks on a large scale. Blogs, now numbering in the millions, are web pages updated using blogging software that makes it easy for authors to share new content online in the form of time-stamped posts.  One can track how a piece of information spreads by observing when it appears on different blogs. The exact route the information takes is not obvious, since most blog authors will not explicitly identify the source of the information when they write about it. Likely routes can be inferred, however, by analyzing timing information, blogs' past entries, and the explicit network of blogs linking to one another through blogrolls or posts. While one can gain insights from observing how information passes from one individual to another, one can also analyze networks to see how easily one can actively navigate them to locate needed information or individuals.  One test of the navigability of a network is the classic small world experiment, where subjects attempt to reach a target individual through their chain of acquaintances. Examining an email network within an organization reveals how individuals are capable of routing messages locally, even though their knowledge of the organization's global social network is limited.
337 en A short Tutorial on Semantic Web The availability of electronically stored information increased drastically through the development of the World Wide Web. Currently the WWW contains more than a billion documents, but support for accessing and precessing information is limited. Most information is only presentable but not understandable by computers. Tim Berners-Lee envisioned the Semantic Web that aims at providing automated access to information due to machine-processable semantics of data. Ontologies formalize a shared understanding of a domain and therefore play a crucial role for communication among human beings and software agents.  We will present the underlying ideas of the Semantic Web and will shortly introduce ontologies as the backbone of the Semantic Web. Further we will show how much effort is necessary to setup the Semantic Web and how tools can support this process. Additionally Web and Data Mining techniques can be used to bootstrap the Semantic Web. The idea of Semantic Web Mining is to improve the results of Web Mining by exploiting the new semantic structures in the web.
342 en Challenges in the Computational Discovery of Explanatory Scientific Models The growing amount of scientific data has led to the increased use of computational discovery methods to understand and interpret them. However, most work has relied on knowledge-lean techniques like clustering and classification learning, which produce descriptive rather than explanatory models, and it has utilized formalisms developed in AI or statistics, so that results seldom make contact with current theories or scientific notations. In this talk, I present a new approach to computational discovery that encodes explanatory scientific models as sets of quantitative processes, simulates these models' behavior over time, incorporates background knowledge to constrain model construction, and induces these models from time-series data in a robust manner. I illustrate this framework on data and models from Earth science and microbiology, two domains in which explanatory process accounts occur frequently. In closing, I describe our progress toward an interactive software environment for the construction, evaluation, and revision of such explanatory scientific models.  This talk describes joint work with Kevin Arrigo, Stephen Bay, Lonnie Chrisman, Dileep George, Andrew Pohorille, Javier Sanchez, Dan Shapiro, and Jeff Shrager.
360 en Spooky Stuff in Metric Space Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? \\ In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be "repaired" so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. \\ During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.
361 en Model Compression Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? \\ In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be "repaired" so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. \\ During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.
370 en Direct Control of Dynamic Systems with Evolutionary Algorithms Evolutionary algorithms for optimization of dynamic problems have recently received increasing attention; though, moderately few investigations have been carried out on real-world problems. From an optimization point of view, online control is a particularly interesting class of dynamic problems, because of the interactions between the controller and the controlled system. In this seminar, we introduce the technique called direct control with evolutionary algorithms and various aspects related to this approach. The aspects are exemplified in an empirical study of greenhouse control..
371 en Subgroup discovery and rule evaluation in ROC space Traditionally, machine learning has focussed on induction of classification and prediction rules. More recently, non-predictive or descriptive induction is gaining substantial interest of machine learning researchers. Two major trends in descriptive induction are association rule learning and subgroup discovery. In this seminar we present our recent work in descriptive induction.  We also argue that accuracy is not always an appropriate evaluation measure in the descriptive induction framework, and propose quality measures designed for subgroup evaluation in ROC space. After a brief presentation of the APRIORI-C and SD-algorithm, we give a detailed presentation of the CN2-SD algorithm, which includes a new -- weighted -- covering algorithm, a new search heuristic (weighted relative accuracy), probabilistic classification of instances, and a new measure for evaluating the results of subgroup discovery (area under ROC curve).  The presented work was done in collaboration with V. Jovanoski (APRIORI-C), D. Gamberger (SD-algorithm), B. Kavsek and L. Todorovski (CN2-SD algorithm).  Our research was supported by the Slovenian Ministry of Education, Science and Sport, the IST-1999-11495 project Data Mining and Decision Support for Business Competitiveness: A European Virtual Enterprise, and the British Council project Partnership in Science PSP-18.
410 en A Tutorial Introduction to Stochastic Differential Equations: Continuous-time Gaussian Markov Processes 
411 en Demonstrations 
412 en An SMO-like algorithm for Kernel Conditional Random Fields 
413 en Projection and Projectability 
414 en Using features of probability distributions to achieve covariate shift 
415 en Semantic text features from small world graphs We present a set of methods for creating a semantic representation from a collection of textual documents. Given a document collection we use a simple algorithm to connect the documents into a tree or a graph. Using the imposed topology we define a feature and document similarity measures. We use the kernel alignment to compare the quality of various similarity measures. Results show that the document similarity defined over the topology gives better alignment than standard cosine similarity measure on a bag of words document representation.
416 en The Empirical Bayes Estimation of an Instantaneous Spike Rate with a Gaussian Process Prior 
417 en Optimal Support Vector Selection for Kernel Perceptrons 
418 en Coevolution in EC 1 It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
419 en Special Session: Projects in Multimodal Interaction - VACE 
420 en Some aspects of Latent Structure Analysis Latent structure models involve real, potentially observable variables and latent, unobservable variables. Depending on the nature of these variables, whether they be discrete or continuous, the framework includes various particular types of model, such as factor analysis, latent class analysis, latent trait analysis, latent profile models, mixtures of factor analysers, state-space models and others. The simplest scenario, of a single discrete latent variable, includes finite mixture models, hidden Markov chain models and hidden Markov random field models. The talk will give an overview of the application of maximum likelihood and Bayesian approaches to the estimation of parameters within these models, emphasising especially the fact that computational complexity varies greatly among the different scenarios. In the case of a single discrete latent variable, the issue of assessing its cardinality will be discussed, in the context of questions such as the appropriate number of mixture components to be included in a mixture model, or, in the interests of parsimony, the minimum plausible cardinality of such a latent variable. Techniques such as the EM algorithm, Markov chain Monte Carlo methods and variational approximations will be featured in the talk.
421 en XML structure mapping A key problem for automating the processing of semi-structured resources is the format heterogeneity among data sources. For dealing with heterogeneous semi-structured data, the correspondence between the different formats has to be established. The multiplicity and the rapid growth of information sources have motivated researchers to develop machine learning technologies for helping to automate those transformations.
422 en TecnoVision-ROBIN: benchmarking object retrieval algorithms Technovision is a recent program of the French Ministry of Research and Technology that will fund evaluation projects in the area of computer vision. Many vision algorithms have been proposed in the past, but comparing their performance has been difficult owing to the lack of common datasets. Technovision aims to correct this by funding the creation of large, representative image datasets. ROBIN is a Technovision proposal covering the evaluation of object retrieval algorithms
423 en Fast SVM Approximations for Object Detection state-of-the-art accuracies in object detection. However, for real time applications, standard SVMs are usually too slow. In this work, we propose a method for approximating an SVM detector in terms of a small number of separable nonlinear filters. We are building on work of Romdhani et al. (ICCV 2001), where an SVM face detector was approximated using the so-called reduced set algorithm and evaluated in a cascade. However, when using plain gray values as features, we found it more effective to reduce the high computational cost for the pixel-wise comparisons, rather than focusing on sparsity of the detectors alone. In our approach, we constrain the reduced set optimization to a class of nonlinear convolution filters which can be evaluated more efficiently (i.e. O(w+h) instead of O(wh), where w and h are the patch dimensions, respectively). We demonstrate a prototype of our system which runs in real time on a standard PC.
424 en Going beyond bag-of-words: dealing with a text as a graph of triples 
425 en Dimensionality Reduction in Gaussian Process Models 
426 en Convergence of MDL and Bayesian Methods We introduce a complexity measure which we call KL-complexity. Based on this concept, we present a general information exponential inequality that measures the statistical complexity of some deterministic and randomized estimators. We show that simple and clean finite sample convergence bounds can be obtained from this approach. In particular, we are able to improve some classical results concerning the convergence of MDL density estimation and Bayesian posterior distributions
427 en Fast Learning Rates for Support Vector Machines We establish learning rates to the Bayes risk for support vector machines with hinge loss (L1-SVM's). Since a theorem of Devroye states that no learning algorithm can learn with a uniform rate to the Bayes risk for all probability distributions we have to restrict the class of considered distributions: in order to obtain fast rates we assume a noise condition recently proposed by Tsybakov and an approximation condition in terms of the distribution and the reproducing kernel Hilbert space used by the L1-SVM. For Gaussian RBF kernels with varying widths we propose a geometric noise assumption on the distribution which ensures the approximation condition. This geometric assumption is not in terms of smoothness but describes the concentration of the marginal distribution near the decision boundary. In particular we are able to describe nontrivial classes of distributions for which L1-SVM's using a Gaussian kernel can learn with almost linear rate.
428 en Universal Coding/Prediction and Statistical (In)consistency of Bayesian inference Part of this talk is based on results of A. Barron (1986) and recent joint work with J. Langford (2004). We introduce the information-theoretic concepts of universal coding and prediction. Under weak conditions on the prior, Bayesian sequential prediction is universal. This means that a code based on the Bayesian predictive distribution allows one to substantially compress data. We give a simple proof of the fact that universality implies consistency of the Bayesian posterior. It follows that Bayesian inconsistency in nonparametric settings (a la Diaconis & Freedman) can only occur if priors are used that do not allow for data compression. This gives a frequentist rationale for Rissanen's Minimum Description Length Principle. We also show that under misspecification, the Bayesian predictions can substantially outperform the predictions of the best distribution in the model. Ironically, this implies that the Bayesian posterior can become *inconsistent*: in some sense good predictive performance implies inconsistency!
429 en Learning to Reconstruct 3D Human Pose and Motion from Silhouettes We will describe our ongoing work on learning-based methods for recovering 3D human body pose and motion from single images and from monocular image sequences. The methods work directly with raw image observations and require neither an explicit 3D body model nor a prior labelling of body parts in the image. Instead, they recover the body pose or motion by direct nonlinear regression against shape descriptors extracted automatically from image silhouettes or contours.
431 en Object categorization with SVM: kernels for local features We will focus on object categorization. The basic idea is to combine the nice invariance propreties of local features with the robustness of SVM's and the ability to control generalization in this framework.
432 en Probabilistic account for multi-view stereo This paper describes a method for dense depth reconstruction from wide-baseline images. In a wide-baseline setting an inherent difficulty which complicates the stereo correspondence problem is self-occlusion. Also, we have to consider the possibility that image pixels in different images, which are projections of the same point in the scene, will have different colour values due to non-Lambertian effects or discretization errors. We propose a Bayesian approach to tackle these problems. In this framework, the images are regarded as noisy measurements of an underlying 'true' image-function. Also, the image data is considered incomplete, in the sense that we do not know which pixels from a particular image are occluded in the other images. We describe an EM-algorithm, which iterates between estimating values for all hidden quantities, and optimising the current depth estimates. The algorithm has few free parameters, displays a stable convergence behaviour and generates accurate depth estimates.
433 en Special Session: Projects in Multimodal Interaction - AMI 
434 en Linear Projections and Gaussian Process Reconstructions 
435 en Auxillary Variational Information Maximization for Dimensionality Reduction Mutual Information (MI) is a long studied measure of in- formation content, and many attempts to apply it to feature extraction and stochastic coding have been made. However, in general MI is com- putationally intractable to compute, and most previous studies redefine the criterion in forms of approximations. Recently we described proper- ties of a simple lower bound on MI [2], and discussed its links to some of the popular dimensionality reduction techniques. Here we introduce a richer family of the auxiliary variational bounds on MI, which gener- alize our previous approximations. Our specific focus then is on apply- ing the bound to extracting informative lower-dimensional projections in the presence of irreducible Gaussian noise. We show that our method produces significantly tighter bounds on MI compared with the as-if Gaussian approximation [7]. We also show that learning projections to multinomial auxiliary spaces may facilitate reconstructions of the sources from noisy lower-dimensional representations.
436 en Latent Semantic Variable Models In the context of information retrieval and natural language processing, latent variable models are quite useful in modeling and discovering hidden structure that often leads to "semantic" data representations. This talk will provide an overview of the most popular approaches and discuss the range of possible applications for such models, including language modeling, ad hoc retrieval, text categorization and collaborative filtering.
437 en Real-Time Collaborative Environments 
438 en Speaker Localization: introduction to system evaluation 
439 en Searching Speech: A Research Agenda 
440 en The “FAME” Interactive Space This paper describes the “FAME” multi-modal demonstrator, which integrates multiple communication modes – vision, speech and object manipulation – by combining the physical and virtual worlds to provide support for multi-cultural or multi-lingual communication and problem solving. The major challenges are automatic perception of human actions and understanding of dialogs between people from different cultural or linguistic backgrounds. The system acts as an information butler, which demonstrates context awareness using computer vision, speech and dialog modeling. The integrated computerenhanced human-to-human communication has been publicly demonstrated at the FORUM2004 in Barcelona and at IST2004 in The Hague. Specifically, the “Interactive Space” described features an “Augmented Table” for multi-cultural interaction, which allows several users at the same time to perform multi-modal, cross-lingual document retrieval of audio-visual documents previously recorded by an “Intelligent Cameraman” during a week-long seminar.
441 en EC update: Information on IST Call 5 and FP7 
442 en TNO submission 
443 en Describing and Discovering Language Resources 
444 en Hierarchical Multi-Stream Posterior Based Speech Recognition System In this paper, we present initial results towards boosting posterior based speech recognition systems by estimating more informative posteriors using multiple streams of features and taking into account acoustic context (e.g., as available in the whole utterance), as well as possible prior information (such as topological constraints). These posteriors are estimated based on \state gamma posterior" de—nition (typically used in standard HMMs training) extended to the case of multi-stream HMMs.This approach provides a new, principled, theoretical framework for hierarchical estimation/use of posteriors, multi-stream feature combination, and integrating appropriate context and prior knowledge in posterior estimates. In the present work, we used the resulting gamma posteriors as features for a standard HMM/GMM layer. On the OGI Digits database and on a reduced vocabulary version (1000 words) of the DARPA Conversational Telephone Speech-to-text (CTS) task, this resulted in signi—cant performance improvement, compared to the stateof- the-art Tandem systems.
445 en Dynamic Bayesian Networks for Multimodal Interaction Dynamic Bayesian networks (DBNs) offer a natural upgrade path beyond classical hidden Markov models and become especially relevant when temporal data contains higher order structure, multiple modalities or multi-person interaction. We describe several instantiations of dynamic Bayesian networks that are useful for modeling temporal phenomena spanning audio, video and haptic channels in single, two-person and multi-person activity. These models include input-output hidden Markov models, switched Kalman filters and, most generally, dynamical systems trees (DSTs). These models are used to learn audio-video interaction in social activities, video interaction in multi-person game playing and haptic-video interaction in robotic laparoscopy. Model parameters are estimated from data in an unsupervised setting using generalized expectation maximization methods. Subsequently, these models can predict, synthesize and classify various types of rich multimodal human activity. Experiments in gesture interaction, audio-video conversation, football game playing and surgical drill evaluation are shown.
447 en Gender issues in user interfaces Most areas of the computing sciences consider themselves to be either gender neutral or they aim at acknowledging physiologically-based differences between women and men. While the first standpoint draws on the traditional ideal of science as a rational, objective and value-free project, the second one does not refer to gender, but to sex differences – a general tendency that is supported by recent interpretations of brain research in popular media.
448 en Physical Substrates All life forms rely on information processing to maintain their highly organised state. Macromolecules and supramolecular structures are key to the special properties that set living systems apart from dead matter. The course will adopt an engineering perspective to introduce the molecular biology (proteins, RNA, DNA) and the physics (thermodynamics, kinetics, dynamics) required for understanding the operation of the molecularmachinery at work in living cells. On this basis the role andthe processing of information at the molecular level will be discussed - covering topics such as noise, molecular motors, conformational switching and intracellular networks - leading to decision making in cells (chemotaxis, development). Throughout the course the potential transfer of concepts from nature to artificial systems will be explored (robustness, self-repair, nano-engineering, molecular computing).
451 en Random Walks All life forms rely on information processing to maintain their highly organised state. Macromolecules and supramolecular structures are key to the special properties that set living systems apart from dead matter. The course will adopt an engineering perspective to introduce the molecular biology (proteins, RNA, DNA) and the physics (thermodynamics, kinetics, dynamics) required for understanding the operation of the molecularmachinery at work in living cells. On this basis the role andthe processing of information at the molecular level will be discussed - covering topics such as noise, molecular motors, conformational switching and intracellular networks - leading to decision making in cells (chemotaxis, development). Throughout the course the potential transfer of concepts from nature to artificial systems will be explored (robustness, self-repair, nano-engineering, molecular computing).
453 en Genomic Repeat Visualisation Using Suffix Arrays Repeat analysis is an important technique for understanding the structure of genomic sequences. Here we present a visualisation for describing the repeat character of a sequence, the repeat-score plot. This visualisation allows the identification of all repeats within a sequence.
456 en Modelling Intra-Speaker Variability for Improved Speaker Recognition In this paper we present a speaker recognition algorithm that models explicitly intra-speaker inter-session variability. Such variability, which is caused by channel, noise and temporary speaker characteristics (mood, fatigue, etc.), is not modeled explicitly by the state-of-the-art speaker recognition algorithms. We define a session-space in which each session (either train or test spoken utterance) is a vector. We then calculate a rotation of the session-space for which the estimated intra-speaker subspace is trivially isolated and can be modeled explicitly. Due to the high dimensionality of the session-space, it is impossible to use standard orthogonalization methods. We therefore used QR factorization based on Givens rotations to calculate the projection. On the NIST-2004 evaluation corpus, recognition error rate was reduced by 23% compared to the classic GMM state-of-the-art algorithm.
457 en On Numerical Characterization of DNA, Proteins, Proteomics Maps and Proteome from their Graphical Representations We will outline calculation of a selection of mathematical invariants that can be extracted from matrices associated with various graphical representations of DNA, Proteins, Proteomics Maps and Proteome. In the case with proteome maps one can construct a zigzag line connecting ordered spots in a map, one may construct the partial order graph, one may construct the cluster graph, one may construct the nearest neighbor graph or the sequential neighbor graph, and one may construct the minimal spanning tree.
460 en Online Learning and Bregman Divergences L 1: Introduction to Online Learning (Predicting as good as the best expert, Predicting as good as the best linear combination of experts, Additive versus multiplicative family of updates)\\ L 2: Bregman divergences and Loss bounds (Introduction to Bregman divergences, Relative loss bounds for the linear case, Nonlinear case & matching losses, Duality and relation to exponential families)\\ L 3: Extensions, interpretations, applications (Online to Batch Conversions, Prior information on the weight vector, Some applications)
461 en On Social networks with an overview of graph drawing with demo of a system Pajek Network = Graph + Data. The data can be measured or computed/derived from the network. The graph drawing is already well established field with its own conference http://www.gd2005.org/ (started in 1992). In ’traditional’ graph drawing the goal is to produce ’the best’ layout of given graph. SNA (Social Network Analysis) is a part of data analysis. Its goal is to get insight into the structure and characteristics of given network.
462 en Kernel Methods In this short course I will discuss exponential families, density estimation, and conditional estimators such as Gaussian Process classification, regression, and conditional random fields. The key point is that I will be providing a unified view of these estimation methods. In the second part I will discuss how moment matching techniques in Hilbert space can be used to design two-sample tests and independence tests in statistics. I will describe the basic principles and show how they can be used to correct covariate shift, select features, or merge databases.
464 en Unifying Divergence Minimization and Statistical Inference via Convex Duality We unify divergence minimization and statistical inference by means of convex duality. In the process of doing so, we prove that the dual of approximate maximum entropy estimation is maximum a posteriori estimation. Moreover, our treatment leads to stability and convergence bounds for many statistical learning problems.
465 en Identifying Temporal Patterns and Key Players in Document Collections We consider the problem of analyzing the development of a document collection over time without requiring meaningful citation data. Given a collection of timestamped documents, we formulate and explore the following two questions. First, what are the main topics and how do these topics develop over time? Second, to gain insight into the dynamics driving this development, what are the documents and who are the authors that are most influential in this process? Unlike prior work in citation analysis, we propose methods addressing these questions without requiring the availability of citation data. The methods use only the text of the documents as input. Consequentially, they are applicable to a much wider range of document collections (email, blogs, etc.), most of which lack meaningful citation data. We evaluate our methods on the proceedings of the Neural Information Processing Systems (NIPS) conference. Even with the preliminary methods that we implemented, the results show that the methods are effective and that addressing the questions based on the text alone is feasible. In fact, the text-based methods sometimes even identify influential papers that are missed by citation analysis.
466 en Facial expression recognition and emotion recognition from speech The presentation tackles the problem of recognizing the emotions based on video and audio data analysis. A fully automatic facial expression recognition system is based on three components: face detection, facial characteristic point extraction and classification. Face detection is employed by boosting simple rectangle Haar-like features that give a decent representation of the face. These features also allow the differentiation between a face and a non-face. The boosting algorithm is combined with an Evolutionary Search to speed up the overall search time. Facial characteristic points (FCP) are extracted from the detected faces. The same technique applied on faces is utilized for this purpose. Additionally, FCP extraction using corner detection methods and brightness distribution has also been considered. Finally, after retrieving the required FCPs the emotion of the facial expression can be determined.
467 en Visualization of text document corpus From the automated text processing point of view, natural language is very redundant in the sense that many different words share a common or similar meaning. For computer this can be hard to understand without some background knowledge. Latent Semantic Indexing (LSI) is a technique that helps in extracting some of this background knowledge from corpus of text documents.
468 en Text Visualisation Tutorial 
469 en Discrete PCA Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic indexing, genotype inference with admixture, non-negative matrix factorization, latent Dirichlet allocation, multinomial PCA, and Gamma-Poisson models. Statistical methodologies for developing algorithms are equally as varied, although this talk will focus on the Bayesian framework. The most well published application is genetype inference, but text analysis is now increasingly seeing use because the algorithms cope with very large sparse matrices. This talk will present the general model, a discrete version of both PCA and ICA, present alternative representations, and several algorithms (mean field and Gibbs).
471 en Machine Learning Reductions There are several different classification problems commonly encountered in real world applications such as 'importance weighted classification', 'cost sensitive classification', 'reinforcement learning', 'regression' and others. Many of these problems can be related to each other by simple machines (reductions) that transform problems of one type into problems of another type. Finding a reduction from your problem to a more common problem allows the reuse of simple learning algorithms to solve relatively complex problems. It also induces an organization on learning problems — problems that can be easily reduced to each other are 'nearby' and problems which can not be so reduced are not close.
473 en Coevolution in EC 2 It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
474 en Targeted PDF Learning 
475 en In search of Non-Gaussian Components of a High-Dimensional Distribution In high dimensional data analysis, finding non-Gaussian components is an important preprocessing step for efficient information processing. This article proposes a new linear method to identify the non- Gaussian subspace within a very general semi-parametric framework. Our proposed method NGCA (Non-Gaussian Component Analysis) is essentially based on the theoretical fact that, via an arbitrary nonlinear function, a vector which approximately belongs to the low dimensional non-Gaussian subspace can be constructed. Since different nonlinear functions yield different directions, one can obtain an approximate subspace from a set of different nonlinear functions. PCA is then applied to identify the non-Gaussian subspace. A numerical study demonstrates the usefulness of our method.
476 en How to Teach Support Vector Machine to Learn Vector Outputs 
477 en Structured Linear Models Over the last five years, we have been able to extend the theory of linear classifiers to structure prediction problems, combining the benefits of discriminative learning and of structured probabilistic models like hidden Markov models. I will review these models and their learning algorithms, and exemplify their use in text processing, with a focus on information extraction from biomedical text.
478 en Machine Learning for Sequential Data: A Comparative Study with Applications to Natural Language Processing 
487 en Information Retrieval and Text Mining This four hour course will provide an overview of applications of machine learning and statistics to problems in information retrieval and text mining. More specifically, it will cover tasks like document categorization, concept-based information retrieval, question-answering, topic detection and document clustering, information extraction, and recommender systems. The emphasis is on showing how machine learning techniques can help to automatically organize content and to provide efficient access to information in textual form.
489 en Statistical Learning Theory 
491 en Top-down vs. bottom-up methods for hierarchical classification We deal with hierarchical classification in the general case when an instance could be associated with multiple and/or partial paths in a given taxonomy. We approach the problem from different perspectives: ”top-down vs. bottom-up”, but also ”on-line vs. batch” and ”theoretical vs. experimental”. In this talk, we briefly present our recent research experience on this subject matter.
492 en Exponential Families in Feature Space In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
496 en Bioinformatics 
498 en Learning with Kernels The course will cover the basics of Support Vector Machines and related kernel methods. # Kernel and Feature Spaces # Large Margin Classification # Basic Ideas of Learning Theory # Support Vector Machines # Other Kernel Algorithms
502 en Evolution of Complexity It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
505 en Probabilistic Non-Linear Principal Component Analysis with Gaussian Process Latent Variables It is known that Principal Component Analysis has an underlying probabilistic representation based on a latent variable model. Principal component analysis (PCA) is recovered when the latent variables are integrated out and the parameters of the model are optimised by maximum likelihood. It is less well known that the dual approach of integrating out the parameters and optimising with respect to the latent variables also leads to PCA. The marginalised likelihood in this case takes the form of Gaussian process mappings, with linear Covariance functions, from a latent space to an observed space, which we refer to as a Gaussian Process Latent Variable Model (GPLVM). This dual probabilistic PCA is still a linear latent variable model, but by looking beyond the inner product kernel as a for a covariance function we can develop a non-linear probabilistic PCA.
506 en Introduction 
507 en Research Problems in applaying RL in Interactive Systems Towards a Taxonomy... 
508 en Special Session: Projects in Multimodal Interaction - CHIL 
512 en The exploration and exploitation tradeoff: Strategy learning and queries 
513 en Overview of Results Pump Priming Project 
517 en Exponential Families in Feature Space In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
519 en Kernel Methods for Higher Order Image Statistics The conditions under which natural vision systems evolved show statistical regularities determined both by the environment and by the actions of the organism. Many aspects of biological vision can be understood as evolutionary adaptations to these regularities. This is demonstrated by the recent sucess in explaining properties of retinal and cortical neurons from the statistics of natural images. At the same time, we observe an increasing interest in statistical modeling techniques in the computer vision community. Here, the motivation comes from the need for powerful image models in image processing tasks such as super-resolution or denoising. In the literature, the statistical analysis of natural images has mainly been done with linear techniques such as Principal Component Analysis (PCA) or Fourier analysis. These techniques capture only the second-order statistics of an image ensemble. A large part of the interesting image structure, however, is contained in the higher-order statistics. Unfortunately, the estimation of these statistics involves a huge number of terms which makes their explicit computation for images infeasible in practice. Kernel methods provide an implicit access to higher-order statistics that avoids this combinatorial explosion. In the course, we start with an overview of existing approaches to image statistics. The need to go beyond the usual linear, second-order techniques will lead us to the classical higher-order statistics such as Wiener series, higher-order cumulants and spectra. We will see that the exponential number of terms involved in these statistics prevents them from being applied to images. This motivates the introduction of kernel techniques. Here, we will discuss two approaches: 1. The Wiener series can be estimated implicitly via polynomial kernel regression. We will use this technique to decompose an image into components that are characterized by pixel interactions of a given order. 2. Kernel PCA of image patches provides a powerful image model that takes higher-order statistics into account. We will show applications of this model to various image processing tasks.
520 en Exponential Families in Feature Space - Part 6 In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
521 en Exponential Families in Feature Space - Part 5 In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
522 en Evolutionary Algorithms It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
523 en Uncertainty, Dynamics and Multimodal interaction 
524 en Audio-Visual Processing in Meetings: Seven Questions and Current AMI Answers 
525 en Elie: A two-level boundary classification approach to adaptive information extraction 
526 en Amilcare 
527 en Pascal Challenge on Evaluating Machine Learning for Information Extraction: Goals, Results and Conclusions 
528 en Selective Sampling for Information Extraction with a Committee of Classifiers 
529 en Template Sampling for Leveraging Domain Knowledge in Information Extraction 
531 en Rectifaying Fluctuations All life forms rely on information processing to maintain their highly organised state. Macromolecules and supramolecular structures are key to the special properties that set living systems apart from dead matter. The course will adopt an engineering perspective to introduce the molecular biology (proteins, RNA, DNA) and the physics (thermodynamics, kinetics, dynamics) required for understanding the operation of the molecularmachinery at work in living cells. On this basis the role andthe processing of information at the molecular level will be discussed - covering topics such as noise, molecular motors, conformational switching and intracellular networks - leading to decision making in cells (chemotaxis, development). Throughout the course the potential transfer of concepts from nature to artificial systems will be explored (robustness, self-repair, nano-engineering, molecular computing).
532 en Visualisation of Cost Landscapes in Combinatorial Optimisation Problems Understanding the structure of the cost landscape of optimisation problems is important for algorithm design. In this talk, I discuss one approach based on Barrier Trees which captures the structure of the local minima and barriers between minima for search spaces with up to 10^12 states. This structure allows visualisation of heuristic search strategies. Furthermore, the visualisation can be used to construct a model of the problem which captures many of the relevant features of the real problem, but with a vastly reduced number of states. The model can be used for investigating optimal heuristic strategies.
533 en Time Distance – A New Generic Approach for Analysis and Visualiziation of Time Related Data The art of handling different views of data is crucial for discovering the relevant patterns and for providing a broader framework for policy analysis. The new generic time distance approach (with associated novel statistical measure S-time-distance) offers a new view of data that is exceptionally easy to understand and communicate, and it allows for developing and exploring new hypotheses and perspectives.
534 en Regret to the Best vs. Regret to the Average We study regret minimization algorithms, focusing not only on their regret to the best expert, but also on their regret to the average of all experts and to the worst expert. We show that any algorithm that achieves only O(pT) regret to the best expert must, in the worst case, suffer regret (pT) to the average, and that for a wide class of update rules that includes many existing no-regret algorithms (such as weighted majority, exponential weights, follow the perturbed leader, and others), the product of the regret to the best and the regret to the average is (T). We describe and analyze a new algorithm, based on the exponential weights algorithm, that achieves cumulative regret only O(pT log(T)) to the best expert and has a constant regret to the average (with no dependence on either T or N). We also give a simple algorithm whose payoff is always better (or equal) to the worst expert and has regret of O(pT) to the best expert.
535 en Use of variance estimation in the multi-armed bandit problem An important aspect of most decision making problems concerns the appropriate balance between exploitation (acting optimally according to the partial knowledge acquired so far) and exploration of the environment (acting sub-optimally in order to refine the current knowledge and improve future decisions). A typical example of this so-called exploration versus exploitation dilemma is the multi-armed bandit problem, for which many strategies have been developed. Here we investigate policies based the choice of the arm having the highest upper-confidence bound, where the bound takes into account the empirical variance of the different arms. Such an algorithm was found earlier to outperform its peers in a series of numerical experiments. The main contribution of this paper is the theoretical investigation of this algorithm. Our contribution here is twofold. First, we prove that with probability at least 1 ? B, the regret after n plays of a variant of the UCB algorithm (called B-UCB) is upper-bounded by a constant, that scales linearly with log(1/B), but which is independent from n. We also analyse a variant which is closer to the algorithm suggested earlier. We prove a logarithmic bound on the expected regret of this algorithm and argue that the bound scales favourably with the variance of the suboptimal arms.
536 en Finite horizon exploration for path integral control problems We have recently developed a path integral method for solving a class of non-linear stochastic control problems in the continuous domain [1, 2]. Path integral (PI) control can be applied for timedependent finite-horizon tasks (motor control, coordination between agents) and static tasks (which behave similar to discounted reward reinforcement learning). In this control formalism, the cost-togo or value function can be solved explicitly as a function of the environment and rewards (as a path integral). Thus, for PI control one does not need to solve the Bellman equation. The computation of the path integral can also be complex, but one can use methods and concepts from statistical physics, such as Monte Carlo sampling or the Laplace approximation to obtain efficient approximations. One can also generalize this control formalism to multiple agents that jointly solve a task. In this case the agents need to coordinate their actions not only through time, but also among each other. It was recently shown that the problem can be mapped on a graphical model inference problem and can be solved using the junction tree algorithm. Exact control solutions can be computed for instance with hundreds of agents, depending on the complexity of the cost function [3].
538 en Evidence Integration in Bioinformatics Biologists frequently use databases; for example, when a biologist encounters some unfamiliar proteins, s/he will use databases to get a preliminary idea of what is known about them. The databases can be often interpreted as lists of assertions. An example is a protein-protein interaction database: each entry is a pair of proteins that are asserted to interact, along with the supporting evidence. Often a candidate for inclusion in such a database can be supported in a variety of fundamentally different ways. A methodological challenge is how to effectively combine these different sources of evidence to make accurate aggregate predictions. Ideas from machine learning are useful for this. I will describe some of the special properties of problems like this, and relevant methods from machine learning, including algorithms based on bayesian networks, boosting and SVMs.
539 en Semi-supervised Learning, Manifold Methods 
541 en Empirical Comparisons of Learning Methods & Case Studies Decision trees may be intelligible, but can they cut the mustard? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes a margin much like SVMs, but can boosting compete with SVMs? And is it better to boost weak models, as theory suggests, or to boost stronger models? Bagging is much easier than boosting, so how well does bagging stack up against boosting? Bagging is supposed to be best with low bias high variance methods like decision trees, so if we bag lower variance models like neural nets are they as good as bagged trees? What happens if we do bagging with steroids, i.e. switch to random forests? And what about old friends like k-nearest neighbor — should they just be put out to pasture? In this lecture I'll compare the performance of a variety of popular machine learning methods on nine performance criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even? Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy?, and Probabilistic Calibration. I'll show that while no one learning method does it all, it is possible to "repair" some of them so that they do well on all metrics. I'll then describe NACHOS, a new ensemble method that does even better by by building on top of these other learning methods. Finally, I'll discuss how the nine performance metrics relate to each other, and look at a few case-studies to show why it is important to use the right metric for each problem.
542 en Trees for Regression and Classification Tree models are widely used for regression and classification problems, with interpretability and ease of implementation being among their chief attributes. Despite the widespread use tree models, a comprehensive theoretical analysis of their performance has only begun to emerge in recent years.  This lecture provides an overview of tree modeling theory and methods, with an emphasis on risk bounds, oracle inequalities, approximation theory, and rates of convergence, in a variety of contexts. Special attention is devoted to decision trees and wavelet-based regression methods, two of the most well-known examples of tree models. The choice of loss function (squared error, absolute error, 0/1 error) plays a pivotal role in both theory and methods.  In particular, optimal tree selection rules vary dramatically depending on the loss function employed. Despite these differences, suitable tree-based models coupled with appropriate selection rules can provide fast algorithms and near-minimax optimal performance in a very broad range of regression and classification problems. Examples from image reconstruction and pattern classification will demonstrate the effectiveness of trees in practice.
543 en Boosting Boosting is a general method for producing a very accurate classification rule by combining rough and moderately inaccurate "rules of thumb." While rooted in a theoretical framework of machine learning, boosting has been found to perform quite well empirically. This tutorial will introduce the boosting algorithm AdaBoost?, and explain the underlying theory of boosting, including explanations that have been given as to why boosting often does not suffer from overfitting, as well as some of the myriad other theoretical points of view that have been taken on this algorithm. Some recent applications and extensions of boosting will also be described.
544 en Energy-based models & Learning for Invariant Image Recognition 
545 en Algorithms for Learning and their Estimates We will try to give an elementary account of bounds for "regularized least squares" that reflects our current knowledge. The framework for the discussion is that of Reproducing Kernel Hilbert Spaces, with a regression point of view. As a corollary of these ideas we will see some estimates for the binary classification problem. The talk will be based on joint work with Felipe Cucker and Ding-Xuan? Zhou.
546 en Online Learning with Kernels Online learning is concerned with the task of making decisions on-the-fly as observations are received. We describe and analyze several online learning tasks through the same algorithmic prism. We start with online binary classification and show how to build simple yet efficient and effective online algorithms that incorporate kernel functions. We describe how to analyze the algorithms in the mistake bound model for both separable and inseparable settings. We then describe numerous generalizations of online learning with kernels to other, often more complex, problems. Specifically, we discuss learning algorithms for uniclass prediction, regression, multiclass problems, and sequence prediction. We conclude with discussion on implications to batch learning and generalization. Based on joint works with Koby Crammer, Ofer Dekel, Vineet Gupta, Joseph Keshet, Andrew Ng, Shai Shalev-Shwartz?, Lavi Shpigelman.
557 en Optimisation It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
558 en The Sparse Grid Method The sparse grid method is a special discretization technique, which allows to cope with the curse of dimensionality to some extent. It is based on a hierarchical basis and a sparse tensor product decompositon. Sparse grids have been successfully used to solve partial differential equations in the past and, more recently, have been shown to be competitive for learning problems as well. The lecture will provide a general introduction to the major properties of sparse grids and present the sparse grid combination technique for classification and regression.
563 en Nonparametric Bayesian Models in Machine Learning Bayesian methods make it possible to handle uncertainty in a principled manner, sidestep the problem of overfitting, and incorporate domain knowledge. However, most parametric models are too limited to adequately model complex real-world problems. Thus, interest has shifted to nonparametric models which can capture much richer and more complex probability distributions. This talk will review some of the core nonparametric tools for regression and classification (Gaussian processes; GPs) and density estimation (Dirichlet process mixtures). We will then focus on extensions of these basic tools (such as mixtures of GPs, warped GPs, and GPs for ordinal regression) and approximation methods which allow efficient inference in these models (such as expectation propagation; EP).
564 en Applications of Bayesian Sensitivity and Uncertainty Analysis to the Statistical Analysis of Computer Simulators for Carbon Dynamics Uncertainties about the dynamics of carbon in forest ecosystems have a major impact on defining and verifying policies, as is evident from the difficulties in ratifying the Kyoto protocol. Quantifying and reducing this uncertainty requires the combination of mathematical models for ecological processes, and earth observation data, within a unifying statistical framework. The Centre for Terrestrial Carbon Dynamics (CTCD) is developing several computer codes to simulate the relevant processes at different spatial and temporal scales. Inputs to theses codes at a given site describe the characteristics of the vegetation grown there. Soil and climate data are also used to drive the model. My talk will illustrate the use of efficient Bayesian tools both in the development of these codes and in their use for prediction and uncertainty reduction. The first step is to build an emulator of the computer code. The emulator is a statistical representation of the code output based on a Gaussian process prior model. From this we can derive inferences about a range of sensitivity and uncertainty measures: Sensitivity analysis is performed to find out the level of influence each input or group of inputs have on the output. This can lead to efficiency gains by revealing inactive inputs. Examination of the expected response curve of the output as a function of individual inputs has also uncovered a number of coding errors. Uncertainty analysis is employed to assess the uncertainty in the prediction resulting from the various uncertain input conditions. It also tells us where to concentrate research effort in reducing uncertainties in inputs if we want to reduce the total uncertainty in the output. Conventional approaches to sensitivity analysis and uncertainty analysis involve Monte Carlo sampling of code outputs. This is highly inefficient and is not feasible for complex models. Bayesian methods can reduce the required number of simulator runs by several orders of magnitude. I will also mention some extensions to the methodology that are being developed to handle the dynamic and multivariate nature of the CTCD vegetation models.
565 en Accessing Multimodal Meeting Data: Systems, Problems and Possibilities As the amount of multimodal meetings data being recorded increases, so does the need for sophisticated mechanisms for accessing this data. This process is complicated by the different informational needs of users, as well as the range of data collected from meetings. This paper examines the current state of the art in meeting browsers. We examine both systems specifically designed for browsing multimodal meetings data and those designed to browse data collected from different environments, for example broadcast news and lectures. As a result of this analysis, we highlight potential directions for future research - semantic access, filtered presentation, limited display environments, browser evaluation and user requirements capture.
566 en EU research initiatives in multimodal interaction An overview of recent EU research initiatives in the area of multimodal interfaces and natural interactivity with examples of funded projects. Plans for the remainder of FP6 and an outline of the roadmap to FP7 (2007-11) will be presented.
567 en Confidence Measures in Speech Recognition A confidence measure (CM) is a number between 0 and 1 that is applied to speech recognition output. A CM gives an indication of how confident we are that the unit to which it has been applied (e.g. a phrase, word, phone) is correct. Confidence measures are extremely useful in any speech application that involves a dialogue, because they can guide the system towards a more intelligent dialogue that is faster and less frustrating for the user.
568 en Browsing Recorded Meetings With Ferret Finding elements of interest within a recorded meeting is time-consuming. We describe work in progress on the Ferret meeting browser, which aims to support this process by displaying many types of data. These include media, transcripts and processing results, such as speaker segmentations. Users interact with these visualizations to observe and control synchronized playback of the recorded meeting.
569 en Machine Learning, Uncertain Information, and the Inevitability of Negative `Probabilities' //`The only difference between a probabilistic classical world and the equations of the quantum world is that somehow or other it appears as if the probabilities would have to go negative ... that's the fundamental problem. I don't know the answer to it, but I wanted to explain that if I try my best to make the equations look as near as possible to what would be imitable by a classical probabilistic computer, I get into trouble'// n These are the words of Richard Feynman in a famous keynote talk on Simulating Physics with Computers. He was pointing out that we have to face an intrinsic conceptual difficulty if we want to understand the world through mimicking its behaviour with computational systems. Actually, we do not have to go as esoteric as quantum physics. We see some of the same issues in Machine Learning and inference from probabilistic estimators in data-driven modelling. And in the same way that Feynman did not know the resolution to his problem, we are only just starting to become aware of some of our own problems in machine intelligence. The principled approach to Machine Intelligence that we have now come to accept is through a probabilistic viewpoint. The Bayesian view of inference is a subjective one and our knowledge of the universe derives from observation. But I will argue that the use of Machine Learning to represent or simulate the universe only allows generically non-positive probabilities! Of course, we can fudge some of the more uncomfortable aspects that some of these issues raise, but it still should make us think about whether we have got the correct working framework. In this talk I want to question parts of our working machinery we use in Machine Learning. At its heart I want to challenge the assumption that probabilities have to be positive. I want to give several arguments, descriptive and formal, to indicate why the use of positive probabilities is an ideal which is both overly restrictive and unrealisable. Indeed I will argue that the use of non-positive `probabilities' is both inevitable and natural. To do this I will need to use some old mathematical ideas from classical statistics and some more modern ideas from information theory. I will use some simple examples and proofs from Machine Learning applied to regression and classification tasks, and draw parallels with some basic quantum theory ideas. The core of the argument is that in modelling the universe through Machine Learning, we are obliged to make inferences based on finite and hence typically less-than-complete information. We can never know everything about a situation, and this gives us our link between quantum mechanics and statistical inference through machine learning. I will try to make a case that inference through any finite data-driven computation leads to this apparent problem with `probabilities'. So the issue is not just connected with quantum mechanics, but is a more generic problem related to trying to simulate even classical probabilities by Machine Learning ideas. If we have enough time, I will also discuss the consequences of this for information measures such as Entropy, and make the case for Fisher Information being a more appropriate measure for our state of knowledge about a system instead.
570 en Probabilistic user interfaces Gaussian process priors have recently been applied to control problems. The GPs bring advantages in their representation of model prediction uncertainty, and because the derivative of a Gaussian process is a Gaussian process, they can also incorporate derivative information, and analytically provide the uncertainty of model derivatives. This can be used to bring a natural regularization of control effort, resulting in appropriately cautious control. It can also be used to provide a ‘quickened’ display, which takes account of model uncertainty. I will describe our work in ambiguous or probabilistic user interfaces, and demonstrate some of the techniques we have developed for combining probabilistic models with continuous control and for providing feedback of system uncertainty to the user.
572 en Unsupervised Learning with Kernels 
575 en Adaptive Behaviour and Emergence Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
576 en Evolution of Sex It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
577 en Complexity: Scale and Connectivity Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
578 en Learning Word Weighting Schema 
580 en Searching for People in the Personal Work Space 
581 en Measuring the Quality of Multi-document Cluster Headlines 
582 en Engineered Complexity Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
583 en Iterative Algorithms for Collaborative Filtering with Mixture Models 
584 en Ranking in Folksonomies 
585 en Bootstrapping Ontology Evolution with Multimedia Information Extraction 
586 en Proactive Information Retrieval by User Modeling from Eye Tracking 
587 en Convex transduction with the normalized cut 
588 en Gaussian Process Basics How on earth can a plain old Gaussian distribution be useful for sophisticated regression and machine learning tasks?
589 en Quasi-Random Resamplings, with Applications to Rule Extraction, Cross-Validation and (Su-)Bagging 
590 en Visual Categorization with Bags of Keypoints We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Naive Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying several semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.
592 en The NIST Meeting Room Phase II Corpus 
594 en Unified Loss Function and Estimating Function Based Learning Current applications in genomics and epidemiology concern high dimensional (and, possibly, time-dependent) data structures, and the questions of interest correspond typically with high dimensional parameters of interest. In such problems it is typically not possible to a priory pose a model allowing estimation at a parametric rate, and thereby requiring estimators of non-pathwise differentiable parameters. We will present a general loss based estimation procedure, which is grounded by theory (e.g., minimax adaptive), and generalizes existing estimation problems. An application of this methodology yields data adaptive algorithms for conditional mean estimation, conditional hazard/density estimation based on censored and uncensored data. In addition, we present a general estimating function based estimation procedure for pathwise and non-pathwise differentiable parameters. Both methodologies involve loss based and estimating function based cross-validation as a tool to select among candidate estimators of the parameter of interest. We illustrate the methodology with some applications in genomics and epidemiology.
595 en How classifieres can be use to solve any reasonable loss 
596 en Penalized empirical risk minimization in the estimation of thresholds 
597 en Generalization Error under Covariate Shift Input-Dependent Estimation of Generalization Error under Covariate Shift 
599 en Faster Rates via Active Learning Traditional sampling and statistical learning theories deal with data collection processes that are completely independent of the target function to be estimated, aside from possible a priori specifications reflective of assumed properties of the target. We refer to such processes as passive learning methods. Alternatively, one can envision sequential, adaptive data collection procedures that use information gleaned from previous observations to guide the process. We refer to such feedback-driven processes as active learning methods. While there have been many successful practical applications of active learning, there is scant theoretical evidence to support the effectiveness of active over passive learning. This talk covers some of the most encouraging theoretical results to date, and focuses on new results regarding the capabilities of active methods for learning (nonparametric) smooth and piecewise smooth functions. Significantly faster rates of error convergence are achieved by active learning compared to passive learning in cases involving functions whose complexity is highly concentrated within small regions its domain (e.g., functions that are smoothly varying apart from highly localized abrupt changes such as jumps or edges). This is joint work with Rui Castro and Rebecca Willett. Please see our on-line technical report for further details: http://www.ece.wisc.edu/~nowak/ECE-05-03.pdf
600 en Nonparametric Tests between Distributions Reproducing Kernel Hilbert Spaces have been mainly used for estimation. Distributional tests in this area were mainly concerned with tests for independence of random variables. We give concentration of measure bounds for the latter using an easy to compute criterion between spaces of observations. In addition, we show that a similar criterion can be used easily for the purpose of testing the identity between two distributions. In both cases, we prove necessary and sufficient conditions for the tests.
601 en The Limit of One-Class SVM In this talk, I will present an analysis of the asymptotic behaviour of the One-Class support vector machine (SVM), a popular algorithm for outlier detection. I will show that One-Class SVM asymptotically estimates a truncated version of the density of the distribution generating the data, in the case where the Gaussian kernel is used with a well-calibrated decreasing bandwidth parameter, and the regularization parameter involved in the algorithm is held fixed as the training sample size goes to infinity.A long version of this work can be found at www.lri.fr/~vert/Publi/regularizeGaussianKernel.ps , in which extensions to the 2-class case and to more general convex loss functions are considered.
602 en On-line learning competitive with reproducing kernel Hilbert spaces In this talk I will describe a new technique for designing competitive on-line prediction algorithms and proving loss bounds for them. The goal of such algorithms is to perform almost as well as the best decision rules in a wide benchmark class, with no assumptions made about the way the observations are generated. However, standard algorithms in this area can only deal with finite-dimensional (often countable) benchmark classes. The new technique gives similar results for decision rules ranging over infinite-dimensional function spaces. It is based on a recent game-theoretic approach to the foundations of probability and, more specifically, on recent results about defensive forecasting. Given the probabilities produced by a defensive forecasting algorithm, which are known to be well calibrated and to have good resolution in the long run, the expected loss minimization principle is used to find a suitable prediction.
605 en Foundations of Learning 
609 en Learning Causal Graphical Models with Latent Variables 
610 en Letter-to-Phoneme Conversion Challenge and Discussion 
612 en Improved Fast Gauss Transform 
613 en Large Scale Genomic Sequence Support Vector Machines 
614 en Output kernel tree In this paper, we generalize tree-based methods to the prediction in structured output space. The extension is based on a kernelization of the algorithm that allows one to grow trees as soon as a kernel can be defined on the output space. The resulting algorithm, called output kernel trees (OK3), generalizes classification and regression trees in a principled way.
615 en First order logic for learning user models in the semantic web: why do we need it? In this paper we claim that learning in a first order logic framework is crucial for the future of user modeling applications in the context of the Semantic Web (SW in the remaining of the paper). We first present some works that have currently been done for designing first order logic based languages, for reasoning in the SW. In the context of user modeling in the SW, it would then be relevant to use such languages to model user’s behaviors and preferences. We show that discovering knowledge in the context of such languages could be done using multi-relational data mining that has already provided efficient prototypes. Nevertheless, some work remains to be done in order to use them in that context and we give some directions for that purpose.
616 en Organisational Resources in Biological System All life forms rely on information processing to maintain their highly organised state. Macromolecules and supramolecular structures are key to the special properties that set living systems apart from dead matter. The course will adopt an engineering perspective to introduce the molecular biology (proteins, RNA, DNA) and the physics (thermodynamics, kinetics, dynamics) required for understanding the operation of the molecularmachinery at work in living cells. On this basis the role andthe processing of information at the molecular level will be discussed - covering topics such as noise, molecular motors, conformational switching and intracellular networks - leading to decision making in cells (chemotaxis, development). Throughout the course the potential transfer of concepts from nature to artificial systems will be explored (robustness, self-repair, nano-engineering, molecular computing).
619 en Round table: Gender Issues 
620 en How to Predict Sequences with Bayes, MDL and Experts 
621 en Large-scale parallel implementations of SVMs 
622 en Working Set Selection Using the Second Order Information for SVMs 
623 en Implementing SVM in an RDBMS: Improved Scalability and Usability 
624 en Online Learning with a Memory Harness 
629 en Ranking Individuals by Group Comparisons We discuss the problem of ranking individuals from their group competition results. Many real-world problems are of this type. For example, ranking players from team games is important in some sports. In machine learning, this is closely related to multi-class classification and probability estimates. We propose new models for estimating individuals' abilities, and hence rankings of individuals. We develop easy and effective solution procedures. Experiments on real bridge records and multi-class classification demonstrate the viability of the proposed models.
630 en Protein Subcellular Localization Prediction Based on Compartment-Specific Biological Features Prediction of subcellular localization of proteins is important for genome annotation, protein function prediction, and drug discovery. We present a prediction method for Gram-negative bacteria that uses ten one-versus-one support vector machine (SVM) classifiers, where compartment-specific biological features are selected as input to each SVM classifier. The final prediction of localization sites is determined by inte-grating the results from ten binary classifiers using a combination of majority votes and a probabilistic method. The overall accuracy reaches 91.4%, which is 1.6% better than the state-of-the-art system, in a ten-fold cross-validation evaluation on a bench-mark data set. We demonstrate that feature selection guided by biological knowledge and insights in one-versus-one SVM classifiers can lead to a significant improvement in the prediction performance. Our model is also used to produce highly accurate prediction of 92.8% overall accuracy for proteins of dual localizations.
632 en Kernel Methods in Computational Biology Many problems in computational biology and chemistry can be formalized as classical statistical problems, e.g., pattern recognition, regression or dimension reduction, with the caveat that the data are often not vectors. Indeed objects such as gene sequences, small molecules, protein 3D structures or phylogenetic trees, to name just a few, have particular structures which contain relevant information for the statistical problem but can hardly be encoded into finite-dimensional vector representations. Kernel methods are a class of algorithms well suited for such problems. Indeed they extend the applicability of many statistical methods initially designed for vectors to virtually any type of data, without the need for explicit vectorization of the data. The price to pay for this extension to non-vectors is the need to define a positive definite kernel between the objects, formally equivalent to an implicit vectorization of the data.
638 en Learning from Structured Data 
640 en L1-based relaxations for sparsity recovery and graphical model selection in the high-dimensional regime The problem of estimating a sparse signal embedded in noise arises in various contexts, including signal denoising and approximation, as well as graphical model selection. The natural optimization-theoretic formulation of such problems involves "norm" constraints (i.e., penalties on the number of non-zero coefficients), which leads to NP-hard problems in general. A natural approach is to consider the use of the -norm as a computationally tractable surrogate, as has been pursued in both signal processing and statistics.
648 en Teaching Machine Learning from Examples 
650 en Greedy Feature Grouping for Optimal Discriminant Subspaces 
651 en Constructing visual models with a latent space approach 
656 en Learning patterns in omic data: applications of learning theory 
657 en An introduction to grammars and parsing From MJ: "The following is a fairly advanced summary of the material I'll be covering in my talk. Don't be dismayed if you find it hard to understand now; I hope that after my talk it will be much clearer!
658 en Sex in EB and EC It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
659 en Inferring Latent Functions with Gaussian Processes in Differential Equations 
660 en The Shannon Entropy as a Metric in Consumer Behavior 
661 en Nonparametric Additive Models for Panels of Time Series 
663 en Multiresolution Methods for Inverse Problems 
664 en Principal Component and the Long Run 
665 en Convergence Rate in the Prokhorov Metric for Illposed Problems 
666 en Regularization: Quadratic Versus Sparsity-enforcing and Deterministic Versus Stochastic Methods 
667 en A path integral approach to stochastic optimal control Many problems in machine learning use a probabilistic description. Examples are pattern recognition methods and graphical models. As a consequence of this uniform description, one can apply generic approximation methods such as mean field theory and sampling methods. Another important class of machine learning problems are the reinforcement learning problems, aka optimal control problems. Here, also a probabilistic description is used, but up to now efficient mean field approximations have not been obtained. In this presentation, I consider linear-quadratic control of an arbitrary dynamical system and show, that for this class of stochastic control problems the non-linear Hamilton-Jacobi-Bellman equation can be transformed into a linear equation. The transformation is similar to the transformation used to relate the Schrödinger equation to the Hamilton-Jacobi formalism. The computation can be performed efficiently by means of a forward diffusion process that can be computed by stochastic integration or that can be described by a path integral. For this path integral it is expected that a variational mean field approximation could be derived.
668 en Measures of behavior from periodic orbits 
669 en Sequential Superparamagnetic Clustering as Network Self-organisation Process Clustering methods are useful tools for the unsupervised classification and analysis of the elements of a set or scene, e.g., a visual or auditory scene. Such methods can be seen as an integral part of cognition-like operations performed by artificial systems. The problematic is that usually no a priori information is available about the structure, the size or the number of classes. Therefore, unbiased methods that are able to provide a 'natural' classification are of interest. As it has been shown (Blatt, Wiseman, Domany), superparamagnetic clustering (SC) is a promising algorithm that comes close to an ideal unbiased method. SC gives the option of choosing different classes on different resolution levels. It, however, does not directly provide an intrinsic criterion for the choice of the 'most natural' levels, i.e. for finding the most natural classes.
670 en Leave-one-out prediction error as a diagnostic tool We consider here predictability of Systolic Blood Pressure (SAP) time series under paced respiration (Akselrod et al 1985), and show that a suitable index separates healthy subjects from Chronic Heart Failure (CHF) patients. Systolic blood pressure (SAP) is the maximal pressure within the cardiovascular system as the heart pumps blood into the arteries. Paced respiration (breathing is synchronized with some external signal) is a well-established tool for relaxation and for the treatment of chronic pain and insomnia, dental and facial pain, etc. (Freedman and Woodward 1992). Entrainment between heart and respiration rate (cardiorespiratory synchronization) has been detected in subjects undergoing paced respiration (Pomortsev et al 1998). Paced breathing can prevent vasovagal syncope during head-up tilt testing; in healthy subjects under paced respiration the synchronization between the main processes governing cardiovascular system is stronger than the synchronization in the case of spontaneous respiration (Prokhorov et al 2003). However, a number of important questions remain open about paced breathing, including the dependence on the frequency of respiration and whether it affects the autonomic balance.
671 en Simulation Modeling: Issues - Part 1 Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
672 en Simulation Modeling - Part 2 Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
673 en Interdisciplinarity Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
674 en Science, Models and Theories Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
676 en Short interviews MLSS05 Chicago by John Langford 
677 en Simulation Modeling: Examples Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
678 en Designing a Simulation Model Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
679 en Learning on Structured Data Discriminative learning framework is one of the very successful fields of machine learning. The methods of this paradigm, such as Boosting, and Support Vector Machines have significantly advanced the state-of-the-art for classification by improving the accuracy and by increasing the applicability of machine learning methods. One of the key benefits of these methods is their ability to learn efficiently in high dimensional feature spaces, either by the use of implicit data representations via kernels or by explicit feature induction. However, traditionally these methods do not exploit dependencies between class labels where more than one label is predicted. Many real-world classification problems involve sequential, temporal or structural dependencies between multiple labels. We will investigate recent research on generalizing discriminative methods to learning in structured domains. These techniques combine the efficiency of dynamic programming methods with the advantages of the state-of-the-art learning methods.
682 en Learning on Structured Data Discriminative learning framework is one of the very successful fields of machine learning. The methods of this paradigm, such as Boosting, and Support Vector Machines have significantly advanced the state-of-the-art for classification by improving the accuracy and by increasing the applicability of machine learning methods. One of the key benefits of these methods is their ability to learn efficiently in high dimensional feature spaces, either by the use of implicit data representations via kernels or by explicit feature induction. However, traditionally these methods do not exploit dependencies between class labels where more than one label is predicted. Many real-world classification problems involve sequential, temporal or structural dependencies between multiple labels. We will investigate recent research on generalizing discriminative methods to learning in structured domains. These techniques combine the efficiency of dynamic programming methods with the advantages of the state-of-the-art learning methods.
685 en Bayesian Learning Bayes Rule provides a simple and powerful framework for machine learning. This tutorial will be organised as follows:n n 1. I will give motivation for the Bayesian framework from the point of view of rational coherent inference, and highlight the important role of the marginal likelihood in Bayesian Occam's Razor.n n 2. I will discuss the question of how one should choose a sensible prior. When Bayesian methods fail it is often because no thought has gone into choosing a reasonable prior.n n 3. Bayesian inference usually involves solving high dimensional integrals and sums. I will give an overview of numerical approximation techniques (e.g. Laplace, BIC, variational bounds, MCMC, EP...).n n 4. I will talk about more recent work in non-parametric Bayesian inference such as Gaussian processes (i.e. Bayesian kernel "machines"), Dirichlet process mixtures, etc.
686 en On the Borders of Statistics and Computer Science Machine learning in computer science and prediction and classification in statistics are essentially equivalent fields. I will try to illustrate the relation between theory and practice in this huge area by a few examples and results. In particular I will try to address an apparent puzzle: Worst case analyses, using empirical process theory, seem to suggest that even for moderate data dimension and reasonable sample sizes good prediction (supervised learning) should be very difficult. On the other hand, practice seems to indicate that even when the number of dimensions is very much higher than the number of observations, we can often do very well. We also discuss a new method of dimension estimation and some features of cross validation.
688 en Context changes detection by one-class svms For a system that aims at taking into account the user, we need to consider that there are many different behaviors as well as many different users. Hence we need adaptative, unsupervised (or semi-supervised) learning methods. Our idea is to take advantage of wearable computers and wearable sensors (indeed their use is realistic at least for certain categories of people, such as pilots) to retrieve the current context of the user. Wearable sensors can be physiological (EMG, ECG, blood volume pressure...) or physical (accelerometers, microphone...). Contexts are depending on the application using the system and can be behaviors, affective states, combinations of these. Since this problem of context retrieval is very complex, we choose to detect changes at first place instead of labeling directly. Indeed this way we can apply unsupervised and fast methods which saves time for labeling (the labeling task is then applied only when changes are detected). Our interest lies in low level treatments and we present a non parametric change detection algorithm. This algorithm is meant to provide sequences of unlabeled contexts to be analyzed to higher level applications. Detection is made from signals given by non invasive sensors the user is wearing. Note that the methods presented here could as well be adapted to external sensors.
689 en Automatically building domain model in hypermedia applications This paper deals with the automatic building of personalized hypermedia. We build upon ideas developed for educational hypermedia. A standard way to build adaptive educational hypermedia relies on the definition of a domain model and the use of overlay user models. Since much work has been done on learning user models and adapting hypermedia based on such user models, the core problem lies in the automatic definition of a domain model for a static hypermedia. We describe an approach to automatically learn from the hypermedia content such a domain model. This model is a concept hierarchy where concepts are identified by sets of keywords learned from the collection. We propose the use of visualization techniques such as treemaps in order to monitor and analyze efficiently user and domain models.
690 en Feature-Learning from Pairs of Examples in Collections of Supervised Learning Tasks We present an algorithm which uses example pairs of equal or unequal class labels to select a projection on a kernel-induced Hilbert space. A representation of .nite dimensional projections as bounded lin- ear functionals on a space of Hilbert-Schmidt operators is exploited to give bounds on the Rademacher complexity of the class of hypotheses searched, leading to PAC-type performance guarantees for the resulting feature maps. The proposed algorithm returns the projection onto the span of the principal eigenvectors of an empirical operator constructed in terms of the example pairs. Experiments demonstrate an e?ective trans- fer of knowledge between di?erent but related learning tasks.
691 en Identifying Feature Relevance using a Random Forest Many feature selection algorithms are limited in that they attempt to identify relevant feature subsets by examining the features individually. This paper introduces a technique for determining feature relevance using the average information gain achieved during the construction of decision tree ensembles. The technique introduces a node complexity measure and a statistical method for updating the feature sampling distribution based upon confidence intervals to control the rate of convergence. Experiments demonstrate the potential of this method for feature selection and subspace identification.
692 en Online feature selection for contextual time series data We propose a simple and eficient method for online feature selection from time series data. Our method is based on calculating characteristics of the di erent features and calculating similarity values for feature pairs using Gaussian kernels. Our motivation has been to design a method that can be used to select the most relevant context features for activity recognition. Namely, traditional feature selection methods have been designed for offline use and thus are not applicable in our setting. The eficiency of our method is evaluated using toy data and real context data, gathered using a 3D accelerometer.
694 en Introduction and overview of FMRI concepts and terminology 
695 en Models for Trading Exploration and Exploitation using Upper Confidence Bounds 
696 en Building and Employing Probabilistic User Models 
697 en Activity modelling using email and web page classification This work explores the modelling of a user’s current activity using a single document and a very small collection of classified documents. We describe the WeMAC approach for combining evidence from heterogeneous sources to give a prdict the user’s activity. We report evaluation of the WeMAC model using two different document types: emails and web pages; assess its performance on both tiny document sets and larger sets; and assess its performance against a “one bag” approach. We report promising results, with average F1 value of 0.5-0.7.
698 en Overview of decoding of mental states and processes 
704 en Statistical Learning Theory This course will give a detailed introduction to learning theory with a focus on the classification problem. It will be shown how to obtain (pobabilistic) bounds on the generalization error for certain types of algorithms. The main themes will be: * probabilistic inequalities and concentration inequalities * union bounds, chaining * measuring the size of a function class, Vapnik Chervonenkis dimension, shattering dimension and Rademacher averages * classification with real-valued functions  Some knowledge of probability theory would be helpful but not required since the main tools will be introduced.
705 en Monte Carlo Simulation methods The course provides an introduction to independent component analysis and source separation. We start from simple statistical principles; examine connections to information theory and to sparse coding; we give an overview of available algorithmics; we also show how several key ideas of ICA are illuminated by information geometry.
706 en Debates 
707 en Learning to Compare Examples 
708 en Hierarchical Gaussian Naive Bayes Classifier for Multiple-Subject fMRI Data The Gaussian Na¨ıve Bayes (GNB) [2] classifier has been successfully applied to fMRI data. However, it is not specifically designed to account for data from multiple subjects and is usually applied to data from a single subject (referred to as GNB-indiv). An extension to the GNB classifier has been proposed ([4], referred to as GNB-pooled), in which the data from all the subjects are combined together na¨ıvely by assuming that they all come from the same subjects. However, this extension ignores subject-specific variations that might exist. Here I describe another extension of the GNB classifier—the hierarchical GNB classifier [3]—that can account for subject-specific variations, and in addition, has the flexibility to increase or reduce the weight of the contribution of the data from the other subjects based on the number of examples available from the test subject.
709 en fMRI-based decoding of the modified default-mode network in mild cognitive impairment The diagnostic tool to detect early stages of Alzheimer’s Disease, a progressive neurodegenerative disease, is lacking until today. FDG-PET (Fluorodeoxyglucose-Positron Emission Tomography) shows hypometabolic areas in the brains of pre-demented, i.e. patients suffering from mild cognitive impairment (MCI). The reduced activity may be attributed to disrupted connectivity of the resting-, or default-mode network of the brain [1]. In this contribution, we study the detection of such a network using the framework of blind signal processing, a technique to identify hidden sources within a multivariate mixture using source characteristics such as statistical independence or sparseness. The results are compared to FDG-PET data.
710 en Learning to Compare using Operator-Valued Large-Margin Classifiers The proposed method uses homonymous and heteronymous examplepairs to train a linear preprocessor on a kernel-induced Hilbert space. The algorithm seeks to optimize the expected performance of elementary classifiers to be generated from single future training examples. The method is justified by PAC-style generalization guarantees and the resulting algorithm has been tested on problems of geometrically invariant pattern recognition and face verification.
711 en Conformal Multi-Instance Kernels In the multiple instance learning setting, each observation is a bag of feature vectorsn of which one or more vectors indicates membership in a class. The primaryn task is to identify if any vectors in the bag indicate class membership while ignoringn vectors that do not. We describe here a kernel-based technique that definesn a parametric family of kernels via conformal transformations and jointly learnsn a discriminant function over bags together with the optimal parameter settings ofn the kernel. Learning a conformal transformation effectively amounts to weightingn regions in the feature space according to their contribution to classification accuracy;n regions that are discriminative will be weighted higher than regions that aren not. This allows the classifier to focus on regions contributing to classificationn accuracy while ignoring regions that correspond to vectors found both in positiven and in negative bags. We show how parameters of this transformation cann be learned for support vector machines by posing the problem as a multiple kerneln learning problem. The resulting multiple instance classifier gives competitiven accuracy for several multi-instance benchmark datasets from different domains.
712 en Learning Similarity Metrics with Invariance Properties 
713 en Tutorial on Statistical Machine Learning with Applications to Multimodal Processing 
714 en Neighbourhood Components Analysis and Metric Learning Say you want to do K-Nearest Neighbour classification. Besides selecting K, you also have to chose a distance function, in order to define ”nearest”. I’ll talk about a method for learning – from the data itself – a distance measure to be used in KNN classification. The learning algorithm, Neighbourhood Components Analysis (NCA) directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. Of course, the resulting classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. I will also discuss an variant of the method which is a generalization of Fisher’s discriminant and defines a convex optimization problem by trying to collapse all examples in the same class to a single point and trying to push examples in other classes infinitely far away. By approximating the metric with a low rank matrix, these learning algorithms, can also be used to obtain a low-dimensional linear embedding of the original input features allowing that can be used for data visualization and very fast classification in high dimensions.
715 en Fast Discriminative Component Analysis for Comparing Examples Two recent methods, Neighborhood Components Analysis (NCA) and Informative Discriminant Analysis (IDA), search for a class-discriminative subspace or discriminative components of data, equivalent to learning of distance metrics invariant to changes perpendicular to the subspace. Constraining metrics to a subspace is useful for regularizing the metrics, and for dimensionality reduction. We introduce a variant of NCA and IDA that reduces their computational complexity from quadratic to linear in the number of data samples, by replacing their purely non-parametric class density estimates with semiparametric mixtures of Gaussians. In terms of accuracy, the method is shown to perform as well as NCA on benchmark data sets, outperforming several popular linear dimensionality reduction methods.
716 en Generalization Bounds for Clustering 
717 en Improved Risk-Tail Bounds for On-line Algorithms 
718 en Application of expectation consistent approximate inference I will discuss two types of applications of an approximate inference technique (EC = expectation consistent) recently developed together with Ole Winther. The EC method is an extension of the TAP (Thouless, Anderson & Palmer) approach which originated in the field of disordered materials and which has been further developed to become applicable to a variety of scenarios in probabilistic modelling & machine laerning. My first application (joint work with Doerthe Malzahn) deals with an approximation to resampling methods (such as the bootstrap) which allows to estimate eg generalization errors in supervised learning. While the exact resampling approach requires the drawing of many samples from the training data and a costly repeated retraining of the model, the approximation attempts an analytic average which combines the replica trick and an inference method which can be performed much faster. In the second application (ongoing work) I discuss the scenario of many solutions to the EC framework and the possibility of averaging them using Parisi's hierarchical scheme.
719 en Expectation Consistent Approximate Inference We propose a novel framework for approximations to intractable probabilistic models. The method is based on a free energy formulation of inference and allows for a simultaneous computation of marginal expectations and the log partition function for continuous and discrete random variables. Using an exact perturbative representation of the free energy around a tractable model, the approximation uses two tractable probability distributions which are consistent on a set of moments and encode different features of the original intractable distribution. In such a way we are able to include nontrivial correlations which are neglected in a (factorized) variational Bayes approach. We test the framework on toy benchmark problems for binary variables on fully connected graphs and 2D grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Significant improvements are obtained when a spanning tree is used instead.
723 en Brain Computer Interfaces Brain Computer Interfacing (BCI) aims at making use of brain signals for e.g. the control of objects, spelling, gaming and so on. This tutorial will first provide a brief overview of the current BCI research activities and provide details in recent developments on both invasive and non-invasive BCI systems. In a second part -- taking a physiologist point of view -- the necessary neurological/neurophysical background is provided and medical applications are discussed. The third part -- now from a machine learning and signal processing perspective -- shows the wealth, the complexity and the difficulties of the data available, a truely enormous challenge. In real-time a multi-variate very noise contaminated data stream is to be processed and classified. Main emphasis of this part of the tutorial is placed on feature extraction/selection and preprocessing which includes among other techniques CSP and also ICA methods. Finally, I report in more detail about the Berlin Brain Computer (BBCI) Interface that is based on EEG signals and take the audience all the way from the measured signal, the preprocessing and filtering, the classification to the respective application. BCI communication is discussed in a clincial setting and for gaming.
724 en Kikuchi free energies with weak consistency constraints: change point learning in switching linear dynamical systems Exact inference in probabilistic models is often infeasible due to (i) a complicated conditional independence structure, and/or (ii) troublesome local integrals. Most challenging inference problems found in physics, such as the computation of the partition function in an Ising model or Boltzmann machine are examples of problems that suffer from a complex structure. All variables are binary, but the cycles in the model prevent an efficient recursive formulation of an inference algorithm.
725 en Estimating MAP-configurations in graphical models by exploiting structure The max-product algorithm can be used to obtain approximate MAP-assignments of the probability distribution defined by a graphical model. On tree-structured graphical models the MAP-assignment is exact and the max-product algorithm is equivalent to the Viterbi-algorithm. On general models one may run into the following problems: 1. The algorithm does not converge; 2. the single node marginals are not unique. The first problem can be solved by using a provably convergent double loop algorithm. In the second case it is not straightforward how to obtain a global assignment from the locally defined marginals, due to loops in the graph. An obvious solution to the second problem is to use the approximate marginals for pairs (or any tractable number) of nodes and use the correlations to estimate a global MAP-assignment. A simple strategy is to define a satisfiability problem which entails that the global MAP-assignment should be a MAP-assignment of each local marginal. This should in principle solve the problem of non-unique marginals. However, this satisfiability problem is not guaranteed to have a solution. The existence of a solution depends critically on the nature of the interactions between the nodes in the graphical model.
726 en Replica symmetry breaking in the `small world' spin glass We apply the cavity method to an Ising spin glass model on a `small world' lattice, a random bond graph super-imposed upon a 1-dimensional ferromagnetic ring. Using the scheme developed by Mézard & Parisi for the Bethe lattice, we evaluate observables for a model with fixed connectivity and +/- J long range bonds. Furthermore, we determine the stability of the RS solution by making an ansatz on the form of the functional 1RSB order parameters.
727 en Generalized Belief Propagation Receiver for Near-Optimal Detection of Two-Dimensional Channels with Memory We propose a generalized belief propagation (GBP) receiver for two-dimensional (2-D) channels with memory, which is applicative to 2-D inter-symbol interference (ISI) equalization and multi-user detection (MUD). Our experimental study demonstrates that under non-trivial interference conditions, the performance of this fully tractable GBP receiver is almost identical to the performance of the optimal maximum a-posteriori (MAP) receiver.
728 en Approximations with Reweighted Generalized Belief Propagation In (Wainwright et al., 2002) a new general class of upper bounds on the log partition function of arbitrary undirected graphical models has been developed. This bound is constructed by taking convex combinations of tractable distributions. The experimental results published so far concentrates on combinations of tree-structured distributions leading to a convexified Bethe free energy, which is minimized by the tree-reweighted belief propagation algorithm. One of the favorable properties of this class of approximations is that increasing the complexity of the approximation is guaranteed to increase the precision. The lack of this guarantee is notorious in standard generalized belief propagation. We increase the complexity of the approximating distributions by taking combinations of junction trees, leading to a convexified Kikuchi free energy, which is minimized by reweighted generalized belief propagation. Experimental results for Ising grids as well as for fully connected Ising models are presented illustrating advantages and disadvantages of the reweighting method in approximate inference.
729 en Modified Belief Propagation: an Algorithm for Optimization Problems Belief propagation is a well known algorithm to solve various optimization problems, such as error correcting codes graph colouring and satisfiability problems. It generally works well in areas where the replica symmetric approximation holds, but breaks down when replica symmetry breaking occurs. Alternatives such as Survey Propagation have been proposed with great success, but are generally limited to the zero temperature limit. We propose a simple modification to Belief Propagation that can also successfully deal with finite temperature scenarios, and illustrate its efficiency on several examples.
730 en Cluster Variation Method: from statistical mechanics to message passing algorithms The cluster variation method (CVM) is a hierarchy of approximate variational techniques for discrete (Ising--like) models in equilibrium statistical mechanics, improving on the mean--field approximation and the Bethe--Peierls approximation, which can be regarded as the lowest level of the CVM. The foundations of the CVM are briefly reviewed, considering different derivations of the method and related techniques, like for instance TAP equations and the cavity method. Issues of realizability and exactness are also addressed.
731 en Unified survey-belief propagation approach for satisfiability In this talk I shall discuss a modified message-passing BP (Belief Propagation) procedure, which can be generally used to minimize variational Bethe free energies in statistical physics. By a straightforward mapping, this algorithm can be easily applied to combinatorial optimization problems, such as 3-satisfiability, the prototype of NP-hard problems. I show that the method can be also extended to the framework of SP (Survey Propagation), a recently proposed algorithm which, still making use of techniques borrowed from statistical physics, overcomes problems encountered by local search algorithms on hard instances close to the ``SAT-UNSAT'' transition. This allows to obtain a unified and quite stable message passing scheme, which can be used both in the ``full replica-symmetry-breaking'' region, where ordinary Belief Propagation usually does not converge, and in the ``hard'' region near the sat-unsat transition, where it allows to solve subformulae generated by ``survey inspired decimation''.
732 en Path Integral Method for Estimation of Time Series 
742 en A Unified Approach to Deduction and Induction 
744 en Trainable visual models for object classification The general theme of the tutorial will be 'trainable visual models for object classification'. I will cover: the difficulty of the problem a few approaches Perona and Welling Pictorial structures of Felzenszwalb and Huttenlocher Borenstein and Ullman Agarwal and Roth Leibe and Schiele covering the method, invariance, data preparation and then go into detail on the constellation model end with research challenges
745 en Visual Categorization with Bags of Keypoints We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Naïve Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying several semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.
747 en Challenges in Learning the Appearance of Faces for Automated Image Analysis - Part 1 The variability of images of the human face challenges research in machine vision since its beginning. Sources of variability not only include individual appearance but also cover external parameters such as perspective and illumination that influence the image formation process heavily. Research on the analysis of face images currently splits into the directions of face detection and face recognition. Approaches and problem setting in this two areas are still quite different despite the common goal to compensate for the large variability of faces in images. In both areas machine learning strategies are used to learn from example faces a general model of the appearance of human faces. In the first part of our presentation we would like to review the current state of the art in face detection and in face recognition. In a second part we will compare the different strategies used and try to describe the requirements of a general image model that could serve as basis in detection as well as in recognition research. For face detection we will focus on methods based on the estimation of the probability distribution of large number of features computed on face examples. After selecting the subset of features which appear to be most relevant to the task, faces are detected by combining the outcome of suitably defined statistical tests. This method, which is based on positive examples only, seems to give very promising results compared to state of the art techniques based on both positive and negative examples. In face recognition we will concentrate on methods that use an analysis by synthesis framework such as morphable models, active appearance or shape models. Currently these approaches seem the most promising methods able to account for variations in perspective and illumination.
748 en Large scale multiclass classification based on linear optimization One of the most hard tasks in image classification to find a method being applicable on large scale multi-class problems where the sample size and number of the features are huge. Linear discriminant analysis as a classica l method for multi-class classification, which was introduced by Fisher (1936) [4], plays an important role in the machine learning society recently. The kernelized version of this method are discussed in several papers, however they generally deal with the two class version of this approach. Bartlett recognised, in 1938 [2], there is strong relationship between the Fisher Discriminant and the Canonical Correlation Analysis and this statement is valid for the multi-class case as well. Based on this work Barker et al. (2003) [1] and Rosipal et al. (2003) [10] discuss the details about this relationship and show the appropriate kernel approach to this problem. Using Canonical Correlation for multi-class classification in large scale problem suffers from the numerical difficulty to solve the generalised eigenvalue problem to provide the optimum. We present an analogue classificationprocedure based on linear optimisation which is able to extend the scale range of the solvable problems and to give sparse solution. Our method exploits the relationship between the L1 norm SVM and the boosting approach which were presented by Bennett et al. (2000) [3], Mangasarian (1999) [5] and Meir et al. (2003) [6]. Additionally, the formulation based on the soft margin SVM can solve the problem when the number of the features are less than the number of the observations in a given sample.
749 en Learning Sprites A simple and efficient way to model much image and video data is to decompose it into a set of 2-dimensional objects in layers. Each object is characterized by its shape and appearance (as with a "sprite" in computer graphics). Following earlier work on layer decompositions in computer vision (e.g. Wang and Adelson, 1994), Frey and Jojic (1999) stated the sprite-learning problem in terms of transformation-invariant clustering using mixture models and EM. This was later extended (Jojic and Frey, 2001) to learning multiple sprites/objects from a video sequence. The approach of building in knowledge about allowable transformations into the clustering algorithm is an important way that a machine learning algorithm (clustering) needs to be tailored to the computer vision domain. Frey and Jojic's approach to learning multiple sprites uses variational inference simultaneously on all sprites; we also discuss recent work by Williams and Titsias (2004) who describe a greedy sequential algorithm for this task.
750 en Challenges in Learning the Appearance of Faces for Automated Image Analysis - Part 2 The variability of images of the human face challenges research in machine vision since its beginning. Sources of variability not only include individual appearance but also cover external parameters such as perspective and illumination that influence the image formation process heavily. Research on the analysis of face images currently splits into the directions of face detection and face recognition. Approaches and problem setting in this two areas are still quite different despite the common goal to compensate for the large variability of faces in images. In both areas machine learning strategies are used to learn from example faces a general model of the appearance of human faces. In the first part of our presentation we would like to review the current state of the art in face detection and in face recognition. In a second part we will compare the different strategies used and try to describe the requirements of a general image model that could serve as basis in detection as well as in recognition research. For face detection we will focus on methods based on the estimation of the probability distribution of large number of features computed on face examples. After selecting the subset of features which appear to be most relevant to the task, faces are detected by combining the outcome of suitably defined statistical tests. This method, which is based on positive examples only, seems to give very promising results compared to state of the art techniques based on both positive and negative examples. In face recognition we will concentrate on methods that use an analysis by synthesis framework such as morphable models, active appearance or shape models. Currently these approaches seem the most promising methods able to account for variations in perspective and illumination.
751 en Adaptive Feature Selection in Image Segmentation Most practical image segmentation algorithms optimize some mathematical similarity criterion derived from several low-level image features. One possible way of combining different types of features, e.g. color- and texture features on different scales and/or different orientations, is to simply stack all the individual measurements into one high-dimensional feature vector. Due to the nature of such stacked vectors, however, only very few components (e.g. those which are defined on a suitable scale) will carry information that is relevant for the actual segmentation task. We present a novel approach to combining segmentation and feature selection that is capable of overcoming this relevance determination problem. It implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing thediscriminative power of the used partitioning algorithm. On the technical side, we present an efficient optimization algorithm with guaranteed local convergence property. All free model parameters of this method are selected by a resampling-based stability analysis. Experiments for both toy examples and real-world images demonstrate that the built-in feature selection mechanism leads to stable and meaningful partitions of the images.
752 en Learning issues in image segmentation Image segmentation is often defined as a partitioning of pixels or image blocks into homogeneous groups. These groups are characterized by a prototypical vector in feature space, e.g., the space of Gabor filter responses, by a prototypical histograms of features or by pairwise dissimilarities between image blocks. For all three data formats cost functions have been proposed to measure distortion and, thereby, to encode the quality of a partition. Learning in image segmentation can be defined as the inference of prototypical descriptors of segments like codebook vectors or average feature probability within a segment. Contrary to classification or regression, the empirical risk of image segmentation is often composed of sums of dependent random variables like in Normalized Cut, Pairwise Clustering or k-means clustering with smoothness constraints. One of the core challenges for machine learning is to discover what kind of information can be learned from these data sources assuming MRF cost functions as image models. The validation procedure for image segmentations strongly depends on this issue. I will demonstrate the learning and validation issue in the context of image analysis based on color and texture features.
753 en Building local part models for category-level recognition This talk addresses the problem of building models for category-level recognition. The starting point is a set of local invariant features which have been shown to support the robust recognition of specific objects and scenes. In the category-level object recognition context, it is no longer sufficient to use individual features, and it becomes necessary to model intra-class variations, to select discriminant features, and to model spatial relations. Furthermore, it is important to use shape information, as in many cases objects of a given category are different in grey-level appearance, but similar in shape. This leads to a part-based approach to category-level recognition that I will illustrate with several examples, including feature selection, local affine-invariant part models, and contour-based shape description. This is joint work with Gyuri Dorko, Frederic Jurie, Svetlana Lazebnik and Jean Ponce.
754 en What is the Optimal Number of Features? A learning theoretic perspective In this paper we discuss the problem of feature selection for supervised learning from the standpoint of statistical machine learning. We inquire what subset of features will lead to the best classification accuracy. It is clear that if the statistical model is known, or if there are an unlimited number of training samples, any additional feature can only improve the accuracy. However, we explicitly show that when the training set is finite, using all the features may be suboptimal, even if all the features are independent and carry information on the label. We analyze one setting analytically and show how feature selection can increase accuracy. We also find the optimal number of features as a function of the training set size for a few specific examples. This perspective on feature selection is different from the common approach that focuses on the probability that a specific algorithm will pick a completely irrelevant or redundant feature.
755 en Mutual Spectral Clustering: Microarray Experiments Versus Text Corpus This work studies a machine learning technique designed for exploring relations between microarray experiment data and the corpus of gene-related literature available via PubMed. The use of this task is found in that it provides better clusters of genes by fusing both information sources together, while it can also be used to guide the expert through the large corpus of gene-related literature based on insights into microarray experiments and vice versa.
756 en The Development of the AMI System for the Transcription of Speech in Meetings The automatic processing of speech collected in conference style meetings has attracted considerable interest with several large scale projects devoted to this area. This paper describes the development of a baseline automatic speech transcription system for meetings in the context of the AMI (Augmented Multiparty Interaction) project. We present several techniques important to processing of this data and show the performance in terms of word error rates (WERs). An important aspect of transcription of this data is the necessary flexibility in terms of audio pre-processing. Real world systems have to deal with flexible input, for example by using microphone arrays or randomly placed microphones in a room. Automatic segmentation and microphone array processing techniques are described and the effect on WERs is discussed. The system and its components presented in this paper yield compettive performance and form a baseline for future research in this domain.
757 en Multimodal Integration for Meeting Group Action Segmentation and Recognition We address the problem of segmentation and recognition of sequences of multimodal human interactions in meetings. These interactions can be seen asa rough structure of a meeting, and can be used either as input for a meeting browser or as a first step towards a higher semantic analysis of the meeting. A common lexicon of multimodal group meeting actions, a shared meeting data set, and a common evaluation procedure enable us to compare the different approaches. We compare three different multimodal feature sets and four modelling infrastructures: a higher semantic feature approach, multi-layer HMMs, a multistream DBN, as well as a multi-stream mixed-state DBN for disturbed data.
758 en The Rich Transcription 2005 Spring Meeting Recognition Evaluation 
759 en NIST RT'05S evaluation 
760 en Multi-modal Authoring Tool for Populating a Database of Emotional Reactive Animations We aim to create a model of emotional reactive virtual hu- mans. A large set of pre-recorded animations will be used to obtain such model. We have defined a knowledge-based system to store animations of reflex movements taking into account personality and emotional state. Populating such a database is a complex task. In this paper we describe a multimodal authoring tool that provides a solution to this problem. Our multimodal tool makes use of motion capture equipment, a handheld device and a large projection screen.
761 en University of Karlsruhe Acoustic Speaker Tracking System 
762 en The AMI Meeting Corpus: A Pre-Announcement The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.
763 en Macquarie RT05s Speaker Diarisation System 
764 en VACE Multimodal Meeting Corpus In this paper, we report on the infrastructure we have de- veloped to support our research on multimodal cues for understanding meetings.With our focus on multimodality, we investigate the interaction among speech, gesture, posture, and gaze in meetings. For this purpose, a high quality multimodal corpus is being produced.
765 en Automatic Speech Recognition and Speech Activity Detection in the CHIL Smart Room An important step to bring speech technologies into wide deployment as a functional component in man-machine interfaces is to free the users from close-talk or desktop microphones, and enable far-field operation in various natural communication environments. In this work, we consider far-field automatic speech recognition and speech activity detection in conference rooms. The experiments are conducted on the smart room platform provided by the CHIL project.&#160; **The first half** of the paper addresses the development of speech recognition systems for the seminar transcription task. In particular, we look into the effect of combining parallel recognizers in both single-channel and multi-channel settings.&nbsp; **In the second half** of the paper, we describe a novel algorithm for speech activity detection based on fusing phonetic likelihood scores and energy features. It is shown that the proposed technique is able to handle non-stationary noise events and achieves good performance on the CHIL seminar corpus.
766 en Understanding Spatiality 
767 en Constructing a Framework for the Analysis of Complex Geographical Systems 
768 en Maapping European Research Networks 
769 en The Geography of Science 
771 en Air Traffic Control: Issues 
772 en Exploring Spatially Embedded Artificial Neural Networks 
773 en Understanding Spatiality in the Brain or: does Spatiality matter? 
775 en Adaptive Modelling via Pattern Analysis and the Kernel Methods approach There is a dramatic growth in the availability of complex data from a wide range of different applications. The challenge of the data analyzer is to extract knowledge from the raw data by identifying the useful patterns and structures that underlie it. This module introduces adaptive and probabilistic approaches to modeling such complex data. We first consider finding structure in high-dimensional data. The kernel methods approach to identifying non-linear patterns in introduced while addressing the issues of statistical reliability of inferences made from limited data. Subspace identification is considered and correlations across different data modalities are shown to provide a useful approach to eliciting semantic representations. The final section of the course will introduce learning probabilistic models, (e.g. in biological sequence data), fusing prior knowledge and data, complex and approximate inference.
777 en Grammatical Inference: a Tutorial The leactures will introduce the key ideas of grammatical inference and concentrate specially on the algorithmic aspects. Some algorithms that will be described are: The "State merging" family : Gold, Rpni, Edsm... The "Window" languages : Local and k-testable Learning with queries.
779 en Pattern Analysis with Graphs and Trees Spectral representations of graphs, Pattern spaces from graph spectra, Spectral approaches to matching, Heat kernel methods Probabilistic and spectral methods for graph matching and clustering. Applications in computer vision.
781 en Trees, Arrays, Networks and Optimization for Finding Patterns in Biological Sequences a) The use of suffix trees and integer programming for finding optimal virus signatures. b) A current treatment of suffix-arrays and their uses. In the last several years simple linear-time algorithms for building suffix arrays have been developed making explicit suffix-trees mostly obsolete. c) Algorithms for finding signatures (patterns) of historical recombination and gene-conversion in SNP (binary) sequences. The techniques here relate to graph-theory.
782 en Patterns in sets of points: the myriad virtues of eigenproblems 2-"Patterns in sets of points: the myriad virtues of eigenproblems" :-) "In the second hour, one specific powerful type of optimization problem will be highlighted: the eigenvalue problem. A brief discussion of the computational aspects, and an overview of its applications in finding patterns in point sets will be provided. The talk will cover principal component analysis, canonical correlation analysis, Fisher's discriminant, partial least squares, and spectral clustering. We will emphasize connections between these algorithms where appropriate."
783 en A statistical mechanics analysis of ncoded CDMA with regular LDPC codes Ne obstaja
784 en Learning with structured inputs I will present a novel approach to semi-supervised learning that employs a method which we refer to as structural learning (aka multi-task learning). The idea is to learn predictive structures from many auxiliary problems that are created from the unlabeled data (and are related to the target problem), and then transfer the learned structure to the supervised target problem.
785 en Clustering - An overview Clustering, or finding groups in data, is as old as machine learning itself, if not older. However, as more people use clustering in a variety of settings, the last few years we have brought unprecedented developments in this field. This tutorial will survey the most important clustering methods in use today from a unifying perspective. I will then present some of the current paradigms shifts in data clustering.
788 en Finding frequent patterns from data Discovery of frequent patterns = finding positive conjunctions that are true for a given fraction of the observations - this basic idea can be instantiated in many ways: - finding frequent sets from 0/1 data (association mining) - finding frequent episodes in sequences - finding frequent subgraphs in graphs etc. - efficient algorithms exist -- the levelwise approach - theoretical analysis of the algorithms is not trivial - leads to connections to hypergraph transversals etc. - the second part: how can the patterns be used? - sometimes interesting in themselves - can be used to approximate the joint distribution - maximum entropy approaches - combining information from several patterns - ordering patterns
789 en Discussion Panel II 
791 en Probabilistic and Bayesian Modelling II There is a dramatic growth in the availability of complex data from a wide range of different applications. The challenge of the data analyzer is to extract knowledge from the raw data by identifying the useful patterns and structures that underlie it. This module introduces adaptive and probabilistic approaches to modeling such complex data. We first consider finding structure in high-dimensional data. The kernel methods approach to identifying non-linear patterns in introduced while addressing the issues of statistical reliability of inferences made from limited data. Subspace identification is considered and correlations across different data modalities are shown to provide a useful approach to eliciting semantic representations. The final section of the course will introduce learning probabilistic models, (e.g. in biological sequence data), fusing prior knowledge and data, complex and approximate inference.
793 en Information Retrieval and Text Mining This four hour course will provide an overview of applications of machine learning and statistics to problems in information retrieval and text mining. More specifically, it will cover tasks like document categorization, concept-based information retrieval, question-answering, topic detection and document clustering, information extraction, and recommender systems. The emphasis is on showing how machine learning techniques can help to automatically organize content and to provide efficient access to information in textual form.
795 en Learning shared representations for object recognition 
796 en Sparsity analsysis of term weighting schemes and application to text classification We revisit the common practice of feature selection for dimensionality and noise reduction. This typically involves scoring and ranking features based on some weighting scheme and selecting top ranked features for further processing. Experiments show that the performance of text classification methods is sensitive to characteristics of the used feature sets. For example, the size of the feature sets that yield the same performance level for a given classification method can be very different, depending on the feature scoring method used. We expand this exploration by considering representations of individual document vectors that result from a particular feature set. In particular, we observe the average number of features per document vector, i.e., the vector sparsity, or density and introduce sparsity curves to illustrate how the vector density increases with the feature set for different weighting schemes. We show that selecting feature by specifying the vector density parameter, instead of a feature set size, yields comparable results to the commonly adopted practice. However, it has the added benefit of understanding the effect of feature selection on document vector representation and system parameters, such as memory consumption of the classification operations. Furthermore, the corresponding classification performance curves link the sparsity and performance measures and provide further insight on how the feature specificity or distribution of the feature across documents in the corpus, is accounted for by the classification method.
797 en Efficient max-margin Markov learning via conditional gradient and probabilistic inference We present a general and efficient optimisation methodology for for max-margin sructured classification tasks. The efficiency of the method relies on the interplay of several techiques: marginalization of the dual of the structured SVM, or max-margin Markov problem; partial decomposition via a gradient formulation; and finally tight coupling of a max-likelihood inference algorithm into the optimization algorithm, as opposed to using inference as a working set maintenance mechanism only.
798 en Matching Point Sets with respect to the Earth Mover’s Distance Shape matching is a fundamental problem in computer vision: given two shapes A and B, one wants to determine how closely A resembles B, according to some distance measure between the shapes. In order to measure the similarity of A and B independently of trans- formations such as translations and/or rotations, one wants to find a transformed version of, say, A that attains the minimum possible distance to B.
802 en The Pascal Curriculum Development Programme 
804 en Reinforcement Learning 
808 en Machine Learning Flavor of Random Matrices 
812 en Graphical Models and Variational Methods In this course I will discuss how exponential families, a standard tool in statistics, can be used with great success in machine learning to unify many existing algorithms and to invent novel ones quite effortlessly. In particular, I will show how they can be used in feature space to recover Gaussian Process classification for multiclass discrimination, sequence annotation (via Conditional Random Fields), and how they can lead to Gaussian Process Regression with heteroscedastic noise assumptions.
818 en Topology and geometry in molecular graphs All geometrical information is missing in topological, i.e. graph theoretical description of connectivity in molecules. However, a part of molecular geometry could be recovered if some of existing graph drawing algorithms is invoked. One of these algorithms models a graph as a system of balls and springs while the second models a graph as a physical system of attractive and repulsive forces.
820 en Debate 
822 en When Logical Inference Helps Determining Textual Entailment (and When it Doesn´t) We compare and combine two methods to approach the second textual entailment challenge (RTE-2): a shallow method based mainly on word-overlap and a method based on logical inference, using first-order theorem proving and model building techniques. We use a machine learning technique to combine features of both methods. We submitted two runs, one using only the shallow features, yielding an accuracy of 61.6%, and one using features of both methods, performing with an accuracy score of 60.6%. These figures suggest that logical inference didn´t help much. Closer inspection of the results revealed that only for some of the subtasks logical inference played a significant role in performance. We try to explain the reason for these results.
823 en COGEX at the Second Recognizing Textual Entailment Challenge This paper proposes a knowledge representation model and a logic proving setting with axioms on demand which proved to be very successful for the recognizing textual entailment task. LCC´s submission to the Second RTE Challenge exploits the logical entailment between deep semantics and syntax of T and H as well as a shallow lexical alignment of the two texts.
824 en Approaching Textual Entailment with LFG and FrameNet Frames We present a baseline system for modeling textual entailment that combines deep syntactic analysis with structured lexical meaning descriptions in the FrameNet paradigm. Textual entailment is approximated by degrees of structural and semantic overlap of text and hypothesis, which we measure in a match graph. The encoded measures of similarity are processed in a machine learning setting.
825 en Coping with Semantic Uncertainty with VENSES As in the previous RTE Challenge, we present a linguistically-based approach for semantic inference which is built around a neat division of labour between two main components: a grammatically-driven subsystem which is responsible for the level of predicate-arguments well-formedness and works on the output of a deep parser that produces augmented head-dependency structures. A second subsystem tries allowed logical and lexical inferences on the basis of different types of structural transformation intended to produce a semantically valid meaning corrispondence. Grammatical relations and semantic roles are used to generate a weighted score. In the current challenge, a number of additional modules have been added to cope with fine-grained inferential triggers which were not present in the previous datset.
827 en On Max-Margin Markov Networks in Hierarchical Document Classification 
828 en Mining XML documents - Bridging the gap between Machine Learning and Information Retrieval 
832 en Machine Learning, Probability and Graphical Models 
835 en Gaussian Processes 
841 en Better than Being There - Intel Research Collaboratory 
842 en Visual Lexicons: The Quest for Data - Driven Decision Making The eternal AI quest - can machines think as well as man? - seems quaint today compared to the question of how can machines help man to think. True, Deep Blue can beat the world's best chess player, not by thinking, but by exhaustively examining all permutations and combinations in blinding time against a predetermined outcome set of rules. The questions for mankind, though, seem of the form where rules are imprecise at best, and essentially unknowable perhaps. If learnable and knowable even, many other constraints exist that mitigate against "data-driven decision-making". This presentation assesses some of these constraints, and offers some perspective on the value of using visual dynamics and analytics to help overcome such issues.
843 en Tricks of the trade for training SVMs 
844 en Learning interpretable SVMs for biological sequence classification 
845 en Data analysis and support vector machines in recognition of sleep stages 
846 en Kernel-based learning of hierarchial multilabel classification models 
847 en ?-MCD approach to novelty detection 
848 en MarkusSparse Grid Methods The search for interesting variable stars, the discovery of relations between geomorphological properties, satellite observations and mineral concentrations, and the analysis of biological networks all require the solution of a large number of complex learning problems with large amounts of data. A major computational challenge faced in these investigations is posed by the curse of dimensionality. A well known aspect of this curse is the exponential dependence of the size of regular grids on the dimension of the domain. This makes traditional finite element approaches infeasible for high-dimensional domains. It is less known that this curse also affects computations of radial basis function approximations -- in a slightly more subtle way. Sparse grid functions can deal with the major problems of the curse of dimensionality. As they are the superposition of traditional finite element spaces, many well-known algorithms can be generalized to the sparse grid context. Sparse grids have been successfully used to solve partial differential equations in the past and, more recently, have been shown to be competitive for learning problems as well. The talk will provide a general introduction to the major properties of sparse grids and will discuss connections with kernel based methods and parallel learning algorithms. It will conclude with a short review over some recent work on algorithms based on the combination technique.
852 en Reinforcement Learning Reinforcement learning is about learning good control policies given only weak performance feedback: occasional scalar rewards that might be delayed from the events that led to good performance. Reinforcement learning inherently deals with feedback systems rather than (data, class) data samples, providing a more flexible control-like framework than many standard machine algorithms. These lectures will summarise reinforcement learning along 3 axes: # Learning with or without knowledge of the system dynamics. # Using state values as an intermediate solution, or learning a policy directly. # Learning with or without fully observable system states.
854 en Graph Matching Algorithms Graph matching plays a key role in many areas of computing from computer vision to networks where there is a need to determine correspondences between the components (vertices and edges) of two attributed structures. In recent years three new approaches to graph matching have emerged as replacements to more traditional heuristic methods. These new methods are: * Least squares - where the optimal correspondence in determined in terms of deriving the best fitting permutation matrix between sets. * Spectral methods - where optimal correspondences are derived via subspace projections in the graph eigenspaces. * Graphical models - where algorithms such as the junction tree algorithm are used to infer the optimal labeling of the nodes of one graph in terms of the other and that satisfy similarity constraints between vertices and edges. In this lecture we review and compare these methods and demonstrate examples where this applies to point set and line matching.
857 en Exploration Vs. Explotation 
858 en Introduction to the On-line Trading of Exploration and Exploitation Workshop 
859 en A Divergence Prior for Adaptive Learning. 
860 en Learning when test and Trainig Imputs have different distributions 
861 en Active Learning, Model Selection and Covariate Shift 
862 en Visualizing Pairwise Similarity via Semidefinite Programming 
864 en Decision Maps 
867 en Learning Nonparametric Priors from Multiple Tasks 
869 en Dimensionality Reduction by Feature Selection in Machine Learning Dimensionality reduction is a commonly used step in machine learning, especially when dealing with a high dimensional space of features. The original feature space is mapped onto a new, reduced dimensioanllyity space and the examples to be used by machine learning algorithms are represented in that new space. The mapping is usually performed either by selecting a subset of the original features or/and by constructing some new features. This persentation deals with the first approach, feature subset selection. We provide a brief overview of the feature subset selection techniques that are commonly used in machine learning and give a more detailed description of feature subset selection used in machine learning on text data. Performance of some methods used is document categorization is illustrated by providing experimental comparison on real-world data collected from the Web.
870 en Random Exploration and Stabilisation of Cellular Architecture All life forms rely on information processing to maintain their highly organised state. Macromolecules and supramolecular structures are key to the special properties that set living systems apart from dead matter. The course will adopt an engineering perspective to introduce the molecular biology (proteins, RNA, DNA) and the physics (thermodynamics, kinetics, dynamics) required for understanding the operation of the molecularmachinery at work in living cells. On this basis the role andthe processing of information at the molecular level will be discussed - covering topics such as noise, molecular motors, conformational switching and intracellular networks - leading to decision making in cells (chemotaxis, development). Throughout the course the potential transfer of concepts from nature to artificial systems will be explored (robustness, self-repair, nano-engineering, molecular computing).
871 en Learning variable covariances via gradients 
872 en Feasible Language Learning This talk will consider how some recent models of feasible learning might apply to human language learning, with attention to how these results complement traditional linguistic perspectives.
873 en Learning to Signal 
874 en The Dynamics of AdaBoost One of the most successful and popular learning algorithms is AdaBoost, which is a classification algorithm designed to construct a "strong" classifier from a "weak" learning algorithm. Just after the development of AdaBoost nine years ago, scientists derived margin- based generalization bounds to explain AdaBoost's unexpectedly good performance. Their result predicts that AdaBoost yields the best possible performance if it always achieves a "maximum margin" solution. Yet, does AdaBoost achieve a maximum margin solution? Empirical and theoretical studies conducted within this period conjecture the answer to be "yes". In order to answer this question, we look toward AdaBoost's dynamics. We simplify AdaBoost to reveal a nonlinear iterated map. We then analyze the convergence of AdaBoost for cases where cyclic behavior is found; this cyclic behavior provides the key to answering the question of whether AdaBoost always maximizes the margin. As it turns out, the answer to this question turns out to be the opposite of what was thought to be true! In this talk, I will introduce AdaBoost, describe our analysis of AdaBoost when viewed as a dynamical system, briefly mention a new boosting algorithm which always maximizes the margin with a fast convergence rate, and if time permits, I will reveal a surprising new result about AdaBoost and the problem of bipartite ranking.
875 en Fingerprints of Rhthm in Natural Language This talk reviews a list of recent results on the rhythmic classes hypothesis produced by the Tycho Brahe Project. I start with results on the rhythmic classification of speech data based on the speech sonority. Then, I address the question of the identification of fingerprints of rhythm in Brazilian and European written texts. I conclude discussing the role fingerprints of rhythm may play in language acquisition and change.
876 en Diffusion Maps, Spectral Clustering and Reaction Coordinates of Dynamical Systems A central problem in data analysis is the low dimensional representation of high dimensional data, and the concise description of its underlying geometry and density. In the analysis of large scale simulations of complex dynamical systems, where the notion of time evolution comes into play, important problems are the identification of the slow variables and the representation of the reaction coordinates that parameterize them. In this paper we provide a unifying view of these apparently different tasks, by considering a family of diffusion maps, defined as the embedding of complex data onto a low dimensional Euclidian space, via the eigenvectors of suitably normalized random walks defined on the given datasets. We show, both theoretically and by examples how this embedding can be used for dimensionality reduction, manifold learning, geometric analysis of complex data sets and fast simulations of stochastic dynamical systems. Joint work with R.R. Coifman, S. Lafon, M. Maggioni and I.G. Kevrekidis
881 en Robustness and Adaptation in Biochemical Networks of Bacterial Chemotaxis All life forms rely on information processing to maintain their highly organised state. Macromolecules and supramolecular structures are key to the special properties that set living systems apart from dead matter. The course will adopt an engineering perspective to introduce the molecular biology (proteins, RNA, DNA) and the physics (thermodynamics, kinetics, dynamics) required for understanding the operation of the molecularmachinery at work in living cells. On this basis the role andthe processing of information at the molecular level will be discussed - covering topics such as noise, molecular motors, conformational switching and intracellular networks - leading to decision making in cells (chemotaxis, development). Throughout the course the potential transfer of concepts from nature to artificial systems will be explored (robustness, self-repair, nano-engineering, molecular computing).
882 en On the evolution of languages 
883 en Game Dynamics with Learning and Evolution of Universal Grammar I will present a model of language evolution, based on population game dynamics with learning. Specifically, we consider the case of two genetic vari- ants of universal grammar (UG), the heart of the human language faculty, assuming each admits two possible grammars. The dynamics are driven by a communication game. We prove using dynamical systems techniques that if the payoff matrix obeys certain constraints, then the two UGs are stable against invasion by each other, that is, they are evolutionarily stable. These constraints are independent of the learning process. Intuitively, if a muta- tion in UG results in grammars that are incompatible with the established languages, then it will die out because individuals with the mutation will be unable to communicate and therefore unable to realize any potential benefit of the mutation. An example for which the proofs do not apply shows that compatible mutations may or may not be able to invade, depending on the population's history and the learning process. These results suggest that the genetic history of language is constrained by the need for compatibility and that mutations in the language faculty may have died out or taken over depending more on historical accident than on any simple notion of relative fitness. Further results in language game dynamics will require more detailed knowledge of the acquisition process. Following that line of thought, I will also sketch some recent work on a more detailed simulation for studying lan- guage change on historical (rather than geological) timescales. Preliminary results suggest that the human race may have only explored a small part of the space of possible languages.
885 en Bayesian Data Fusion with Gaussian Process Priors : An Application to Protein Fold Recognition Various emerging quantitative measurement technologies are producing genome, transcriptome and proteome-wide data collections which has motivated the de- velopment of data integration methods within an inferential framework. It has been demonstrated that for certain prediction tasks within computational biol- ogy synergistic improvements in performance can be obtained via integration of a number of (possibly heterogeneous) data sources. In [1] six different parameter representations of proteins were employed for fold recognition of proteins using Support Vector Machines (SVM).
886 en Probabilistic and Bayesian Modelling I There is a dramatic growth in the availability of complex data from a wide range of different applications. The challenge of the data analyzer is to extract knowledge from the raw data by identifying the useful patterns and structures that underlie it. This module introduces adaptive and probabilistic approaches to modeling such complex data. We first consider finding structure in high-dimensional data. The kernel methods approach to identifying non-linear patterns in introduced while addressing the issues of statistical reliability of inferences made from limited data. Subspace identification is considered and correlations across different data modalities are shown to provide a useful approach to eliciting semantic representations. The final section of the course will introduce learning probabilistic models, (e.g. in biological sequence data), fusing prior knowledge and data, complex and approximate inference.
888 en When Training and Test Distributions are Different: Characterising Learning Transfer 
889 en Randomized PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension We design an on-line algorithm for Principal Component Analysis. The instances are projected into a probabilistically chosen low dimensional subspace. The total expected quadratic approximation error equals the total quadratic approximation error of the best subspace chosen in hindsight plus some additional term that grows linearly in dimension of the subspace but logarithmically in the dimension of the instances.
890 en Presenting the Evaluating Predictive Uncertainty Challenge 
891 en Welcome 
892 en Lost in Translation from Genes to Organisms Data mining genomes and developmental genetics provides information for building mathematical gene-network models on development. These models can be used to provide insilico predictions about processes such as developmental expression and regulation of genes.
893 en Redundant Bit Vectors for Searching High-Dimensional Regions Many multimedia applications reduce to the problem of searching a database of high-dimensional regions to see whether any overlap a query point. There is a large literature of indexing techniques based on trees, all of which break down given high enough dimension of stored regions. We have created a new data structure, called redundant bit vectors (RBVs), that can effectively index high-dimensional regions.Using RBVs, we can search a database of 240K 64-dimensional hyperspheres, each with a different radius, up to 56 times faster than an optimized learning scan. RBVs are general-purpose, and may be useful for machine learning applications.
894 en Tractable Inference for Probabilistic Models by Free Energy Approximations Probabilistic models explain complex observed data by a set of unobserved, hidden random variables based on the joint distribution of the variables. Statistical inference requires the evaluation of high dimensional sums or integrals. Hence, one has to deal with a vast computational complexity when the number of hidden variables is large and it is important to develop tractable approximations. I will discuss ideas for such approximations which are based on a variational formulation of inference. Quantities of interest, like marginal moments of the distribution are found as minima of an entropic quantity, often called the Gibbs Free Energy. While an exact computation of the Free Energy is computationally intractable, sensible approximations often provide quite accurate results. I will discusss applications of these techniques to the estimation of wind fields from satellite measurements, to a model of an error correcting code in telecommunication and to approximate resampling methods.
895 en Language Models for Information Retrieval 
896 en Learning Structured Outputs via Kernel Dependency Estimation and Stochastic Grammars 
897 en Speaker Diarization For Multi-Microphone Meetings Using Only Between-Channel Differences 
899 en Introduction 
901 en Pascal Challenge TU Darmstadt 
902 en Convolutional Object Finder, A Neural Architecture for Fast and Robust Object Detection 
903 en Textual Entailment Recognition Based on Dependency Analysis and WordNet 
904 en Recognizing Textual Entailment with Edit Distance Algorithms 
905 en What Syntax can Contribute in Entailment Task 
906 en Application of the Bleu Algorithm for Recognizing Textual Entailments 
908 en Textual Entailment as Syntactic Graph Distance: a rule based and a SVM based approach 
909 en Effects of Stress and Genotype on Exploration-Exploitation Dynamics in Reinforcement Learning Stress and genetic background regulate different aspects of behavioral learning through the action of stress hormones and neuromodulators. Similarly, in reinforcement learning (RL) models, exploitation-exploration factor and other meta-parameters control learning dynamics and performance. We found that many different measures of animal learning and performance can be reproduced by simple RL models using dynamic control of the meta-parameters. To study the effects of stress and genotype, we carried out 5-hole-box light conditioning and Morris water maze experiments with 2 different genetic strains of mice, exposing them to different stressors. Then, we used RL models to simulate their behavior. For each experimental session, we estimated a set of model meta-parameters that produced the best fit between the model and the animal performance. Exploration-exploitation factors had similar characteristic dynamics for the two simulated experiments, and there were statistically significant differences between different genetic strains and stress conditions.
912 en Information Access Challenges in the Blogspace 
913 en Dealing with Heterogeneity in Profiles for Personalized Information Retrieval 
914 en Personalized Web Search Engine for Mobile Devices 
915 en BulkFS: a Distributed Fault-Tolerant File System for Massive Data Applications 
916 en Searching Internet of Things 
917 en Experiences with the Nutch search engine Nutch is open-source software that implements a web search engine. It has been used in a variety of applications: vertical search engines, archival web search, search engines that incorporate novel metadata, etc. Nutch is itself implemented using Hadoop, an open-source platform for scalable computing. Hadoop facilitates the development and management of applications that run on large numbers of computers and on very large datasets. Hadoop has been demonstrated on clusters with hundreds of computers and is designed to scale to thousands of computers. This talk will present the architecture, capabilities and current status of these two projects.
918 en Gaussian Process Model for Inferring the Regulatory Activity of Transcription Factor Proteins Inferring the concentration of transcription factors' proteins from the expression levels of target genes is a very active area of research in computational biology. Usually, the dynamics of the gene expression levels are modelled using differential equations where the transcription factor protein concentrations are treated as parameters, subsequently estimated using MCMC. We show how this inference problem can be solved more elegantly by placing a GP prior over the latent functions, obtaining comparable results to the standard MCMC approach in a fraction of the time.
919 en Touch Clarity Exploration versus Exploitation Challenge 
921 en Analysing Gene Expression Data Using Gaussian Processes Complex gene regulatory mechanisms ensure the proper functioning of biological cells. New high-throughput experimental techniques, such as microarrays, provide a snapshot of gene expression levels of thousands of genes at the same time. If repeated on a sample of synchronized cells, time-series profiles of gene activity can be obtained. The aim is to reconstruct the complex gene regulatory network underlying these profiles. Genes often influence each other in a nonlinear fashion and with intricate interaction patterns. Linear models are often unsuited to capture such relationships. Gaussian processes, on the other hand, are ideal for representing nonlinear relationships. A particular attraction is the automatic relevance determination effect, removing unused inputs and resulting in sparse gene networks.
922 en Simulation Modeling: Hints and Tips Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
924 en Simulation Modeling: Group Work Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
925 en Learn to Weight Term in Information Retrieval Using Category Information 
926 en Evaluating Machine Learning for Information Extraction 
927 en Dirichlet Processes, Chinese Restaurant Processes, and all that Bayesian approaches to learning problems have many virtues, including their ability to make use of prior knowledge and their ability to link related sources of information, but they also have many vices, notably the strong parametric assumptions that are often invoked willy-nilly in practical Bayesian modeling. Nonparametric Bayesian methods offer a way to make use of the Bayesian calculus without the parametric handcuffs. In this talk I describe several recent explorations in nonparametric Bayesian modeling and inference, including various versions of "Chinese restaurant process priors" that allow flexible structures to be learned and allow sharing of statistical strength among sets of related structures. I discuss applications to problems in bioinformatics and information retrieval.
928 en Efficient discriminative learning of Bayesian network classifier via Boosted Augmented Naive Bayes The use of Bayesian networks for classification problems has received significant recent attention. Although computationally efficient, the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between its optimization criteria (data likelihood) and the actual goal for classification (label prediction). Recent approaches to optimizing the classification performance during parameter or structure learning show promise, but lack the favorable computational properties of maximum likelihood learning. In this paper we present the Boosted Augmented Naive Bayes (BAN) classifier. We show that a combination of discriminative data-weighting with generative training of intermediate models can yield a computationally efficient method for discriminative parameter learning and structure selection.
929 en Near-Optimal Sensor Placements in Gaussian Processes 
930 en Statistical Relational Learning - Part 3 Statistical relational learning raises many new challenges and opportunities. Because the statistical model depends on the domain's relational structure, parameters in the model are often tied. This has advantages for making parameter estimation feasible, but complicates the model search. Because the "features" involve relationships among multiple objects, there is often a need to intelligently construct aggregates and other relational features.
931 en How to Predict Sequences with Bayes, MDL, and Experts 
932 en BayesANIL - A Bayesian Model for Handling Approximate, Noisy or Incomplete Labeling in Text Classification 
933 en Semi-supervised Graph Clustering: A Kernel Approach 
935 en How to choose the covariance for Gaussian process regression independently of the basis In Gaussian process regression, both the basis functions and their prior distribution are simultaneously specified by the choice of the covariance function. In certain problems one would like to choose the covariance independently of the basis functions (e. g., in polynomial signal processing or Wiener and Volterra analysis). We propose a solution to this problem that approximates the desired covariance function at a finite set of input points for arbitrary choices of basis functions. Our experiments show that this additional degree of freedom can lead to improved regression performance.
937 en Interpreting Covariance Functions & Classification 
939 en Reinforcement Learning MDPs/VI,\\ Q learning (w/ proof),\\ TD(lambda),\\ Function approximation, \\ options, \\ PSRs
940 en Inductive transfer via embeddings into a common feature space We consider the situation in which there is a basic learning task but different sub-tasks define different data generating distributions. Examples include learning to identify spam for various different email users, or parts-of-speech tagging for different text corpora. Our goal is to allow the use of training data coming from one sub-task for prediction under another sub-task distribution.
941 en An Introduction to Network Theory and Spatial Networks 
942 en Improving the Caenorhabditis elegans Genome Annotation using Machine Learning 
943 en Flexible and efficient Gaussian process models I will briefly describe our work on the sparse pseudo-input Gaussian process (SPGP), where we refine the sparse approximation by selecting `pseudo-inputs' using gradient methods. I will then describe several extensions to this framework. Firstly we incorporate supervised dimensionality reduction to deal with high dimensional input spaces. Secondly we develop a version of the SPGP that can handle input-dependent noise. These extensions allow GP methods to be applied to a wider variety of modelling tasks than previously possible.
944 en Eigenfunctions & Approximation Methods 
946 en Predictive methods for Text mining I will give a general overview of using prediction methods in text mining applications, including text categorization, information extraction, summarization, and question answering. I will then discuss some of the more advanced issues encountered in real applications such as structured and complicated output classification, the use of unlabeled data, modeling link structures, collective inference and community effect, and transfer learning under changing environment, etc.
948 en Graphical Models, Variational Methods, and Message-Passing 
949 en Intel Research : User Activity based Adaptive Power Management (APM) Adaptive Power Management (APM) for mobile computers attempts to reduce power consumption by placing components into low power states with low impact on "perceived" performance.The state of the art commercial solutions are timeout policies. Research in APM has focussed on modeling system dynamics and not usage patterns. We describe a system that learns when to turn off components based on usage patterns and context.
950 en Learning RoboCup-Keepaway with Kernels We give another success story of using kernel-based methods to solve a dificult reinforcement learning problem, namely that of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in keepaway are the high-dimensionality of the state space (rendering conventional grid-based function approximation like tilecoding infeasable) and the stochasticity due to noise and multiple learning agents needing to co- operate. We use approximate policy iteration with sparsified regular- ization networks to carry out policy evaluation. Preliminary results indicate that the behavior learned through our approach clearly out- performs the best results obtained with tilecoding by Stone et al.
952 en Exponential Families In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
954 en Support Vector Machines Support vector machines (SVM) and kernel methods are important machine learning techniques. In this short course, we will introduce their basic concepts. We then focus on the training and optimization procedures of SVM. Examples demonstrating the practical use of SVM will also be discussed. Basically we focus on classification. If time is allowed, we will also touch SVM regression.
956 en An Exchanging-based Refinement to Sparse Gaussian Process Regression We propose a backward deletion procedure to Sparse Gaussian Process Regression (SGPR) model, which can be used to refine a number of se- quential forward selection algorithms addressed recently. Some experi- mental results demonstrate the effectiveness of our approach.
960 en Introduction 
962 en Machine Learning in Bioinformatics 
964 en Signal Processing 
969 en On Optimal Estimators in Learning Theory This talk addresses some problems of supervised learning in the setting formulated by Cucker and Smale. Supervised learning, or learning from examples, refers to a process that builds on the base of available data of inputs xi and outputs yi, i=1,...,m, a function that best represents the relation between the inputs x in X and the corresponding outputs y in Y. The goal is to find an estimator fz on the base of given data z := ((x1,y1),...,(xm,ym)) that approximates well the regression function fp defined on Z=X x Y. We assume that (xi,yi), i=1,...,m are independent and distributed according to p. There are several important ingredients in the mathematical formulation of this problem. We follow the way that has become standard in approximation theory and has been used in recent papers. In this approach we first choose a function class W (a hypothesis space H) to work with. After selecting a class W we have the following two ways to go. The first one is based on the idea of studying approximation of the L2(px) projection fW := (fp)W of fp onto W. Here, px is the marginal probability measure. This setting is known as the improper function learning problem or the projection learning problem. In this case we do not assume that the regression function fp comes from a specific (say, smoothness) class of functions. The second way is based on the assumption fp in W. This setting is known as the proper function learning problem. For instance, we may assume that fp has some smoothness. We will give some upper and lower estimates in both settings.
972 en Adventures with Camille A computational simulation of a minimalist language learner
973 en Multiscale analysis on graphs Analysis on graphs has recently been shown to lead to powerful algorithms in learning, in particular for regression, classification andclustering. Eigenfunctions of the Laplacian on a graph are a natural basis for analyzing functions on a graph, as we have seen in presentations of recent work by partecipants to this conference. In this talk we introduce a new flexible set of basis functions, called Diffusion Wavelets, that allow for a multiscale analysis of functions on a graph, very much in the same way classical wavelets perform a multiscale analysis in Euclidean spaces. They allow efficient, representation, compression, denoising of functions on the graph, and are very well-suited for learning, as well as unsupervised algorithms. They are also associated with a multiscale decomposition of the graph, which has applications by itself. We will discuss this construction with several examples, going from signal processing on manifolds and graphs, to some recent preliminary applications to clustering and learning.
974 en Categorical Perception + Linear Learning = Shared Culture In a group of entities who learn by observing one another's behavior, some simple assumptions about the nature of perception, the nature of individual beliefs, and the nature of learning lead naturally to collective convergence on a random set of shared beliefs, without any structure of authority or any explicit collective decision process. This process will be exemplified in the case of a simple model for developing word pronunciations.
975 en Some Aspects of Learning Rates for SVMs We present some learning rates for support vector machine classification. In particular we discuss a recently proposed geometric noise assumption which allows to bound the approximation error for Gaussian RKHSs. Furthermore we show how a noise assumption proposed by Tsybakov can be used to obtain learning rates between 1/sqrt(n) and 1/n. Finally, we describe the influence of the approximation error on the overall learning rate.
977 en Online Learning and Bregmann Divergences 
978 en Lunch debate 25.5.2005 
979 en Lunch debate 27.5.2005 
980 en Lunch debate 23.5.2005 
981 en Lunch debate 24.5.2005 
982 en Robust Speaker Segmentation for Meetings: The ICSI-SRI Spring 2005 Diarization System We address the problem of segmentation and recognition of sequences
983 en Can Chimeric Persons be Used in Multimodal Biometric Authentication Experiments? 
984 en Tracking Multiple Simultaneous Speakers with Probabilistic Data Associaton Filters 
986 en Generic dialogue modeling for multi-application dialogue systems We present a novel approach to developing interfaces for
987 en Projective Kalman Filter: Multiocular Tracking of 3D Locations Towards Scene Understanding This paper presents a novel approach to the problem of estimating and tracking 3D locations of multiple targets in a scene using measurements gathered from multiple calibrated cameras. Estimation and tracking is jointly achieved by a newly conceived computational process, the Projective Kalman —lter (PKF), allowing the problem to be treated in a single, uni—ed framework. The projective nature of observed data and information redundancy among views is exploited by PKF in order to overcome occlusions and spatial ambiguity. To demonstrate the e®ectiveness of the proposed algorithm, the authors present tracking results of people in a SmartRoom scenario and compare these results with existing methods as well.
988 en On serial architectures for multiple classifier systems One of the recently emerged paradigms in machine learning is multiple classifier fusion. A large number of methods for constructing multiple classifier systems (MCS) have been suggested in the literature. The majority of these draw on a parallel architecture, involving a fusion of multiple classifiers via some form of linear or nonlinear combination rule. Intuitively, one can look at parallel fusion as an attempt to improve the performance by combining several independent estimates of a class aposteriori probability and thereby reducing the variance of the combined estimate. For a given probability margin between two competing hypotheses, this reduced variance then results in a lower probability of incurring an additional classification error over and above the Bayes' error. Much less attention has been paid to multiple classifier system schemes that aim to enhance the performance by manipulating the margin between competing hypotheses. An increased margin can normally be achieved by class grouping. This approach often leads to serial multiple classifier system architectures. Depending on whether the grouping structure is fixed or created dynamically, the resulting multiple classifier is either a decision tree or a chain like multistage system. In this paper the theory underpinning this MCS approach will be overviewed and its implications discussed. It will be shown that the theory leads to diverse class grouping/margin manipulation strategies. Their relative advantages will be discussed. The effectiveness of some of these strategies will be illustrated on a practical problem of object recognition.
990 en Multimodal Input for Meeting Browsing and Retrieval Interfaces: Preliminary Findings 
992 en Universal Modeling: Introduction to modern MDL We give a tutorial introduction to the *modern* Minimum Description Length (MDL) Principle, taking into account the many refinements and developments that have taken place in the 1990s. These do not seem to be widely known outside the information theory community. We will especially emphasize the use of MDL in classification. We also consider the connections between MDL, Bayesian inference, maximum entropy inference and structural risk minimization.
996 en Information Retrieval and Language Technology The course will give an overview of how statistical learning can help organize and access information that is represented in textual form. In particular, it will cover tasks like text classification, information retrieval, information extraction, topic detection, and topic tracking. The course will introduce the basic techniques for representing text and analyze their statistical properties. An emphasis of the course will be on giving an overview of interesting learning problems in this area, providing starting points for future research.
999 en Working with Systems Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
1000 en Engineered Complexity Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
1003 en Combinatory Hybrid Elementary Analysis of Text We propose to CHEAT approach to the MorphoChallenge contest: Combinatory Hybrid Elementary Analysis of Text. The idea is: acquire results from a number of other candidate systems; CHEAT will read in the output files of each of the other systems, and then line-by-line select the "majority vote" analysis - the analysis which most systems have gone for. If there is a tie, take the result produced by the system with the highest F-measure; if the other systems´ output files are ordered best-first, then this is achieved by simply taking the first of the tied results. To justify our approach, we need to show that this really is unsupervised learning, as defined on the MorphoChallenge website; arguably the CHEAT approach involves super-sized unsupervised learning, as it combines three different layers of unsupervised learning.
1004 en Morphological Learning as Principled Argument We develop a morphological learner that evaluates evidence supporting specific claims that a string of letters is a distributional meaningful unit. The distributional evidence is evaluated by selectional properties of morphs, while evidence towards meaning is modelled by looking at the relationship between stems and words. To assess a proposed affix, it gets a probability measure of meaning by comparing all the possible stems the affix occur with to the particular subset that also occur as words. Since for a stem to be a word counts as evidence towards its meaning, the ratio formed by taking stems that are words to the whole set of possible stems for an affix gives a predictive probability measure for the affix that measures the chance that it has combined with a meaningful stem. This measure, taken in conjunction with the selectional statistics of stems and affixes, provides a basis for deciding on the best morphological structure for a given word. The results for English show a combined precision and recall of 45.
1005 en Introduction to Complexity Science Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
1006 en Introduction Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
1007 en Normalized Alignment of Dependency Trees for Detecting Textual Entailment In this paper, we investigate the usefulness of normalized alignment of dependency trees for entailment prediction. Overall, our approach yields an accuracy of 60% on the RTE2 test set, which is a significant improvement over the baseline. Results vary substantially across the different subsets, with a peak performance on the summarization data. We conclude that normalized alignment is useful for detecting textual entailment, but a robust approach will probably need to include additional sources of information.
1009 en Chronological Sampling for Email Filtering User models for email filtering should be developed from appropriate training and test sets. A k-fold cross-validation is commonly presented in the literature as a method of mixing old and new messages to produce these data sets. We show that this results in overly optimistic estimates of the email filter’s accuracy in classifying future messages because the test set has a higher probability of containing messages that are similar to those in the training set. We propose the k-fold chronological cross-validation method that preserves the chronology of the email messages in the test set.
1010 en User models from implicit feedback for proactive information retrieval Our research consortium develops user modeling methods for proactive applications. In this project we use machine learning methods for predicting users’ preferences from implicit relevance feedback. Our prototype application is information retrieval, where the feedback signal is measured from eye movements or user’s behavior. Relevance of a read text is extracted from the feedback signal with models learned from a collected data set. Since it is hard to define relevance in general, we have constructed an experimental setting where relevance is known a priori.
1011 en Improving Infoville XXI using Machine Learning Techniques Infoville XXI is a citizen web portal of Valencia (Spain). This paper presents several approaches based on Machine Learning that help in improving the site. Three ways of improvement are taken into account: (i) user clickstream forecasting, (ii) user profiling by clustering and (iii) recommendation of services to users, being the last two techniques part of a methodological framework with general applicability that tries to be useful for a range of web sites as wide as possible. Results obtained with data sets from this web portal show that the most appropriate techniques for user clickstream forecasting become Support Vector Machines and Multilayer Perceptrons, whilst Adaptive Resonance Theory and Self-Organizing Maps appear to be the most suitable techniques for clustering. Final recommendation and adaptation of the recommender system is currently being developed by using Learning Vector Quantization and Reinforcement Learning.
1013 en Log pre-processing and grammatical inference for Web usage mining In this paper, we propose a WEB USAGE MINING pre-processing method to retrieve missing data from the server log files. Moreover, we propose two levels of evaluation: directly on reconstructed data, but also after a machine learning step by evaluating inferred grammatical models. We conducted some experiments and we showed that our algorithm improves the quality of user data. Keywords: log pre-processing, web usage mining, grammatical inference, evaluation
1017 en Algorithms for Association Rules 
1018 en Unsupervised fMRI Analysis Recently machine learning methodology has been used increasing to analyze the relationship between stimulus categories and fMRI responses [2, 14, 15, 11, 13, 8, 9, 1, 12, 7]. Here, we introduce a new unsupervised machine learning approach to fMRI analysis approach, in which the simple categorical description of stimulus type (e.g. type of task) is replaced by a more informative vector of stimulus features. We compared this new approach with a standard Support Vector Machine (SVM) analysis of fMRI data using a categorical description of stimulus type. The following study differs from conventional unsupervised approaches in that we make use of the stimulus characteristics. We use kernel Canonical Correlation Analysis (KCCA) to learn the correlation between the fMRI volume and the corresponding stimulus features presented at a particular time point. CCA can be seen as the problem of finding basis vectors for two sets of variables such that the correlation of the projections of the variables onto these basis vectors are mutually maximised. KCCA first projects the data into a higher dimensional feature space before performing CCA in the new feature space.
1019 en Hidden Process Models:Decoding Overlapping Cognitive States with Unknown Timing We use Hidden Process Models (HPMs) to evaluate different models of a functional Magnetic Resonance Imaging (fMRI) study in which subjects decide whether stimuli match. We demonstrate the ability of HPMs to simultaneously estimate the hemodynamic response functions and the onset times of a set of cognitive processes underlying an fMRI time series, and to compare different models in a principled way.
1020 en What Mental States? Exploring How Dimensionality Reduction Might Contribute to the Refinement of Cognitive Models Questions in cognitive neuroscience are often framed in terms of correspondences between known types: How is brain state X related to cognitive state Y? What are the correlations or mappings between particular structures and functions? Such framings are well suited for confirmatory testing of coarse-grained hypotheses. They are not necessarily informative, however, for the purpose of exploring finer physical and functional structure. To the contrary, physical states are typically aggregated over anatomical regions of interest, while tasks are designed to optimize one or a few functional contrasts of interest rather than to cover a fuller behavioral or cognitive range.
1021 en Enhancing functional magnetic resonance imaging with supervised learning This paper reports novel applications of supervised learning methods intended to directly impact fMRI technology with the aim of improving data acquisition and analysis.
1022 en Classifying single trial fMRI: What can machine learning learn? We describe three experiments combining neuroimaging and machine learning. The first experiment compares the performance of maximum likelihood and neural net classifiers for "brain reading" of fMRI data in the visual cortex. The second experiment applies the optimal classifier to measure the development of the face region in children and adolescents. While the previous experiments used block designs, the third experiment describes an event-related experiment where the classification algorithm learned something real, but not what was planned. The corroboration and validation of the classification results with brain images will be demonstrated.
1023 en Exploring human object-vision with hi-res fMRI and information-based analysis 
1024 en Challenges and limitations in interpreting learnt classifiers 
1025 en Implications of decoding for theories of neural representation 
1026 en Spatiotemporal classification 
1027 en Condition numbers, regularisation and uncertainty principles of linear algebraic equations There exist several condition numbers for the linear least squares (LS) problem minx ||Ax-b||. These range from a simple normwise measure that may overestimate the true numerical condition by several orders of magnitude, to refined sharp bounds. These different condition numbers will be compared and it will be shown that the computational implementation of the refined measures is problematic, but the simplest measure is easy to compute accurately. Examples are used to illustrate the differences between the condition numbers. The implications of these properties for the regularisation of ill-conditioned linear algebraic equations is considered and it is shown that it emphasizes the role of the prior. The LS problem occurs frequently in regression, and this operation plays the same role as the analysis stage in a filter bank. Similarly, the matrix-vector (MV) product b:=Ax is equivalent to the synthesis stage of a filter bank because it corresponds to the reconstruction of the signal from the basis functions. The final section of the talk will consider the condition numbers of the LS problem and MV product, and it will be shown that if the condition number of A is large, then these two operations cannot be simultaneously ill-conditioned, or simultaneously well-conditioned, that is, they obey an uncertainty principle.
1028 en From clustering to algorithms In this talk we firstly provide a rigorous probabilistic proof of the clustering phenomenon taking place in the space of solution of random combinatorial problems. Secondly we will discuss a generalization of the survey propagation equations efficiently exploring the clustered geometry. Finally, we discuss the computational consequences of the possibility of finding single clusters by describing a \"physical\" lossy compression scheme. Performance are optimized when the number of well separated clusters is maximal in the underlying physical model.
1032 en An Objective Evaluation Criterion for Clustering 
1034 en Probabilistic Graphical Models My lectures will cover the basics of graphical models, also known as Bayes(ian) (Belief) Net(work)s. We will cover the basic motivations for using probabilities to represent and reason about uncertain knowledge in machine learning, and introduce graphical models as a qualitative and quantitative specification of large joint probability distributions. We will see how many common classification, regression and clustering models can be cast in this framework. We will cover the basic algorithm (called belief propagation) for inference in graphical model structures. We will also cover the major approaches to learning models from data (parameter estimation). The course will focus on directed models and the basic algorithms, but time and student desire permitting, I will also try to give some preliminary explanations of undirected models, approximate inference and learning, structure discovery and current applications.
1036 en Self-Adapting Architecture (SAA) New tools and concepts for manipulation of objects in architecture are presented. The concepts include manipulation of local coordinate systems and objects contained in them, as well as performing boolean 3D operations on them. Growing cellular automata (GCA) are used to build and manipulate a large amount of data and relations which occur during the process of construction.
1037 en Visualization of graphs using their product structure Graphs are combinatorial structures given by a set of vertices and a set of edges giving the adjacencies between pairs of vertices. Drawing graphs nicely is a challenging task. If a graph has some particular structure it is often very useful to use the knowledge about this for visualizing the object. In many cases, there are polynomial time algorithms for recognition of product graphs and graph bundles. We provide several examples and give a short survey of results and open problems.
1038 en Approximate Graph Products Products of graphs allow a rather compressed coding from the data structure point of view and often transparent graphical representations. Graphs that differ little from products in the sense that addition or deletion of a small number of edges turns them into a product offer similar advantages.
1039 en Women in Science in Slovenia, the new Member State Slovenia is a Central European country and has in a history always been between the East and West of Europe, which determined many of the state political and social activities. These reflect also in the situation of women in science.
1041 en Introduction 
1042 en Bounds and estimates for BP convergence on binary undirected graphical models Belief Propagation (BP) has become a popular method for inference on graphical models. Accurate approximations for intractable quantities (e.g. single-node marginals) can be obtained within rather modest computation times. However, for large interaction strengths (i.e. potentials that are highly dependent on their arguments) or densely connected graphs, BP can fail to converge. This can be remedied by damping the iteration equations, using sequential update schemes or using entirely different algorithms such as double-loop algorithms, that directly minimize the Bethe free energy. However, the value of this approach is questionable, since there is empirical evidence that failure of convergence of BP often indicates low quality of the Bethe approximation.
1043 en Advanced message passing techniques for distributed storage Distributed storage plays an important role in networks where individual nodes are limited in their storage capabilities and where files are infrequently required. The scenario examined is one where file information is encoded and then divided to a number of segments, which are then distributed over a graph representing the network. Nodes requesting a particular file collect the required number of file segments from neighbouring nodes to retrieve the original information.
1045 en Using upper confidence bounds to control exploration and exploitation 
1046 en Gradient-Based Estimates of Return Distributions 
1047 en Data Mining as a Tool for Analysing Environmental Systems 
1049 en The NITE XML Toolkit meets the ICSI Meeting Corpus: import, annotation, and browsing The NITE XML Toolkit (NXT) provides library support for working with multimodal language corpora. We describe work in progress to explore its potential for the AMI project by applying it to the ICSI Meeting Corpus. We discuss converting existing data into the NXT data format; using NXT’s query facility to explore the corpus; hand-annotation and automatic indexing; and the integration of data obtained by applying NXT-external processes such as parsers.
1050 en Tandem Connectionist Feature Extraction for Conversational Speech Recognition Multi-Layer Perceptrons (MLPs) can be used in automatic speech recognition in many ways. A particular application of this tool over the last few years has been the Tandem approach, as described by Hermansky et al in a number of publications. Here we discuss the characteristics of the MLP-based features used for the Tandem approach, and conclude with a report on their application to conversational speech recognition. The paper shows that MLP transformations yield variables that have regular distributions, which can be further modified by using logarithm to make the distribution easier to model by a Gaussian-HMM. Two or more vectors of these features can easily be combined without increasing the feature dimension. We also report recognition results that show that MLP features can significantly improve recognition performance for the NIST 2001 Hub-5 evaluation set with models trained on the Switchboard Corpus, even for complex systems incorporating MMIE training and other enhancements.
1051 en Automatic pedestrian tracking using discrete choice models and image correlation techniques In this paper we deal with the multi-object tracking problem, with specific reference to the visual tracking of pedestrians, assuming that the pedestrian-detection step is already done. We use a Bayesian framework to combine the visual information provided by a simple image correlation algorithm with a behavioral model ( discrete choice model ) for pedestrian dynamic, calibrated on real data. We aim to show how the combination of the image information with a model of pedestrian behavior can provide appreciable results in real and complex scenarios.
1052 en Mountains, Exploration, Education, Rich Media and Design I want to talk about some things that I feel passionate about: mountains, exploration, history, learning and design. In particular, I want to talk about learning and understanding within our society, which is becoming increasingly difficult to function in, much less influence - largely due to technology-induced complexity. I have to believe that the objective of good design is to render the world more manageable, not less so. I do believe that the more technology is developed apart from a social context, and specific problem domains, the less likely it will meet this test. So I have chosen my domain: mountaineering and exploration in the area surrounding the Himalaya. Using examples, I want to put forward the case that we have the potential to design tools that could have as much impact on education and learning tomorrow, as the introduction of the blackboard had when it was introduced in Canada in the mid-1800s. I will also argue that doing so represents a proposition which is as hard to do as it is easy to say – that is, it is a worthy 10 year project. I believe that the current slump in the technology sector is a largely self-induced, and deserved consequence of bad design applied to poorly thought out problems. Through my examples, I want to argue that there is a way out of this, and to point to a particular path. As I said: it’s about exploration. So a good place to start is to establish some appropriate bearings.
1053 en Zakim - A multimodal sofware system for large-scale teleconferencing This paper describes Zakim, the multimodal teleconference system used at the World Wide Web Consortium \cite{W3C}. While the system presented here does not make use of advanced research work, the technology presented have been developed with robustness in mind, as it is used almost round-the-clock by several hundred people. The context and requirements are introduced first, followed by a description of the system's features. Lastly we describe how the structure of W3C teleconferences has been modified by the use of this system, and discuss some issues and possible improvements.
1054 en A Programming Model for Next Generation Multimodal Applications Multimodal interfaces are becoming essential for the next generation of conversational, collaborative, and mobile device solutions. Until now, however, their development has been hampered by the lack of a standards-based means for authoring them -- i.e. for their programming model. In addition to the programming model, the underlying technologies used by these applications such as synchronization and modality fusion techniques are still at an early stage of development.
1055 en An Efficient Online Algorithm for Hierarchical Phoneme Classification We present an algorithmic framework for supervised classification learning where the set of labels is organized in a predefined hierarchical structure. This structure is encoded by a rooted tree which induces a metric over the label set. Our approach combines ideas from large margin kernel methods and Bayesian analysis. Following the large margin principle, we associate a prototype with each label in the tree and formulate the learning task as an optimization problem with varying margin constraints. In the spirit of Bayesian methods, we impose similarity requirements between the prototypes corresponding to adjacent labels in the hierarchy. We describe a new online algorithm for solving the hierarchical classification problem and derive a worst case loss-bound for the algorithm. We demonstrate the merits of our approach with a series of experiments on synthetic data and speech data.
1056 en Immersive Conferencing Directions at FX Palo Alto Laboratory At FX Palo Alto Laboratory, we have been recording and reusing meetings for five years, and have been developing technologies to support meeting recording, collaboration, and videoconferencing. I will briefly summarize our experience and results, and will then present and/or demonstrate a few of our more interesting research directions. Some of our work uses a video image as an interface, allowing devices and information to be accessed "through the screen."
1057 en Recognition of Isolated Complex Mono- and Bi-Manual 3D Hand Gestures using Discrete IOHMM In this paper, we address the problem of the recognition of isolated complex mono- and bi-manual hand gestures. In the proposed system, hand gestures are represented by the 3D trajectories of blobs. Blobs are obtained by tracking colored body parts in real-time using the EM algorithm. In most of the studies on hand gestures, only small vocabularies have been used. In this paper, we study the results obtained on a more complex database of mono- and bi-manual gestures. These results are obtain processing algorithm, namely Input/Output Hidden Markov Model (IOHMM), implemented within the framework of an open source machine learning library.
1058 en Using Static Documents as Structured and Thematic Interfaces to Multimedia Meeting Archives Documents play a central role in multimodal applications such as meeting recording and browsing. They provide a variety of structures, in particular thematic, for segmenting meetings, structures that are often hard to extract from audio and video. In this article, we present four steps for creating a strong link between documents and multimedia meeting archives. First, a document-centric meeting environment is presented, then, a document analysis tool, which builds a multi-layered representation of documents and creates indexes that are further used by document/speech and document/video alignment methods.
1059 en Evolution and Evolvability It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
1061 en Slow subspace learning from stationary processes The talk presents a method of unsupervised learning from stationary, vector-valued processes. The method selects a subspace on the basis of an objective which can be used to bound the expected classification error for a family of tasks posessing a temporal continuity property. We prove bounds on the objective’s estimation error in terms of mixing coefficients and consistency for absolutely regular processes. Experiments with image processing demonstrate the algorithms ability to learn geometrically invariant feature maps.
1062 en Graphical Models for Structural Pattern Recognition In the "structural" paradigm for visual pattern recognition, or what some call "strong" pattern recognition, one is not satisfied with simply assigning a class label to an input object, but instead we aim at finding exactly which parts of the template object correspond to which parts of the scene. This is a much harder problem in principle, because it is inherently combinatorial on the number of parts (features) involved, both in the template object and in the scene. This talk describes a summary of our research efforts in setting this as a mathematical optimization problem and solving it efficiently by exploiting geometric constraints. The key insight involves encoding geometric constraints as conditional independency assumptions in a probabilistic graphical model. Due to some geometric facts, it is possible to show that such models are very well behaved: they allow for exact probabilistic inference in polynomial time. The result is a unified framework for structural visual pattern recognition that is able to handle in a principled way a variety of problems, including point pattern matching in its many instances: invariant to translations, isometries, scalings, affine or projective transformations. Attributed graph matching problems, such as matching road networks, can also be solved within such framework. Limitations and future directions will be discussed.
1066 en Optimization for Kernel Methods Optimization methods play a crucial role in kernel methods such as Support Vector Machines and Kernel Logistic Regression. In a variety of scenarios, different optimization algorithms are better suited than others. The aims of the six lectures in this topic are: (1) to introduce a range of optimization problems that arise in the solution of classification problems by kernel methods; (2) to briefly review the relevant optimization algorithms; and (3) to point out in some detail as to which optimization methods are suited for these varied problems.
1068 en Presentation of proposed outline challenge 
1070 en mSpace 
1072 en Rapid Stochastic Gradient Descent: Accelerating Machine Learning The incorporation of online learning capabilities into real-time computing systems has been hampered by a lack of efficient, scalable optimization algorithms for this purpose: second-order methods are too expensive for large, nonlinear models, conjugate gradient does not tolerate the noise inherent in online learning, and simple gradient descent, evolutionary algorithms, etc., are unacceptably slow to converge. I am addressing this problem by developing new ways to accelerate stochastic gradient descent, using second-order gradient information obtained through the efficient computation of curvature matrix-vector products. In the stochastic meta-descent (SMD) algorithm, this cheap curvature information is built up iteratively into a stochastic approximation of Levenberg-Marquardt second-order gradient steps, which are then used to adapt individual gradient step sizes. SMD handles noisy, correlated, non-stationary signals well, and approaches the rapid convergence of second-order methods at only linear cost per iteration, thus scaling up to extremely large nonlinear systems. To date it has enabled new adaptive techniques in computational fluid dynamics and computer vision. Our most recent development is a version of SMD operating in reproducing kernel Hilbert space.
1074 en On-line linear learning algorithms Prediction with expert advice. Learning with linear experts. The Perceptron algorithm and its extensions. On-line learning with kernels. Mistake bounds. From mistake bounds to risk bounds.
1076 en Patterns, Randomness and Information Information, Complexity, Patterns, Randomness and Compression. And how these ideas can be traced back through Hermann Weyl to Leibniz in 1686, and connect them with Godel &amp; Turing and with the question of how math compares &amp; contrasts with physics and with biology.
1077 en Learning Hierarchical Multi-Category Text Classification Models 
1078 en Introduction 
1080 en Algorithmic and Combinatorial Foundations of Pattern Discovery 
1086 en The Analysis of Patterns 
1087 en Introduction to Boosting This course provides an introduction to theoretical and practical aspects of Boosting and Ensemble Learning. I will begin with a short description of the learning theoretical foundations of weak learners and their linear combination. Then we point out the useful connection between Boosting and the Theory of Optimization, which facilitates the understanding of Boosting and later on enables us to move on to new Boosting algorithms, applicable to a broader spectrum of problems. In the course we will discuss "tricks of the trade", algorithmic issues, experimental results and a few applications.
1088 en The stability of a good clustering If we have found a "good" clustering C of data set X, can we prove that C is not far from the (unknown) best clustering C* of this data set? Perhaps surprisingly, the answer to this question is sometimes yes. We can show bounds on the distance( C, C* ) for two clustering cost functions: the Normalized Cut and the squared distance cost of K-means clustering. These bounds exist in the case when the data X admits a "good" clustering for the given cost.
1091 en Introduction & Opening 
1094 en Analysis of Support Vector Machine Classification 
1097 en Discussion 
1099 en Using Unlabeled Data in Generalization Error Bounds 
1100 en Dirichlet Processes and Nonparametric Bayesian Modelling Bayesian modeling is a principled approach to updating the degree of belief in a hypothesis given prior knowledge and given available evidence. Both prior knowledge and evidence are combined using Bayes' rule to obtain the a posterior hypothesis. In most cases of interest to machine learning, the prior knowledge is formulated as a prior distribution over parameters and the evidence corresponds to the observed data. By applying Bayes' formula we can perform inference about new data. Having observed sufficient data, the a posteriori parameter distribution is increasingly concentrated and the influence of the prior distribution diminishes. Under some assumptions (in particular that the likelihood model is correct and that the true parameters have positive a priori probability), the a posteriori distribution converges to a point distribution located at the true parameters. The challenges in Bayesian modeling are, first, to find suitable application specific statistical models and, second, to (approximately) solve the resulting inference equations.
1101 en Numerical Methods for Solving Least Squares Problems with Constraints In this talk, we discuss the problem of solving linear least squares problems and Total Least Squares problems with linear constraints and/or a quadratic constraint. We are particularly interested in developing stable numerical methods when the data matrix is singular or near singular. Of particular interest are matrices which are large and sparse and for which iterative methods must be employed. The quadratically constrained problems arise in problems where regularization is required. For such problems, a Lagrange multiplier is required and that calculation may be quite intensive. The method we propose will quickly yield an estimate of the parameter and allow for finding the least squares solution.
1102 en Policy-gradient Reinforcement Learning 
1104 en Panel 1: Perspectives on Textual Entailment 
1105 en Two Related Lexico-Syntactic Approaches to Entailment Two approaches to Textual Entailment are presented. They both rely on lexico-syntactic information. The two approaches differ mainly in the way the syntactic relationships are derived. In one approach, the syntactic relationships are drawn from a phrase-based parse tree. In the other, we use information provided by a dependency parser. The first approach performs at 0.59 precision and 0.6047 average precision. The second system´s performance is 0.5837 precision and 0.5785 average precision.
1107 en Learning to Distinguish Valid Textual Entailments This paper proposes a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. While this formulation is adequate for representing local (word-level) phenomena such as synonymy, it is incapable of representing global interactions, such as that between verb negation and the addition/removal of qualifiers, which are often critical for determining entailment.
1109 en Recognizing Textual Entailment with LCC´s GROUNDHOG System We introduce a new system for recognizing textual entailment (known as GROUNDHOG) which utilizes a classification-based approach to combine lexico-semantic information derived from text processing applications with a large collection of paraphrases acquired automatically from the WWW. Trained on 200,000 examples of textual entailment extracted from newswire corpora, our system managed to classify more than 75% of the pairs in the 2006 PASCAL RTE Test Set correctly.
1110 en Discussion Panel I 
1111 en Gaussian Processes for Principal Component Analysis We show how the supervised method of Gaussian Processes may be used for Principal Component Analysis using two intuitions about the nature of the ¯rst principal component ¯lters.
1112 en Gaussian Processes for Prediction in Intensive Care In this paper we present the use of Gaussian Processes for regression in the application of prediction in Intensive Care. We propose a preliminary solution to predicting the evolution of a patient's state during his stay in intensive care by means of deffined patient speciffic characteristics.
1113 en Gaussian Process Implicit Surfaces Many applications in computer vision and computer graphics require the definition of curves and surfaces. Implicit surfaces are a popular choice for this because they are smooth, can be appropriately constrained by known geometry, and require no special treatment for topology changes. In this paper we use Gaussian processes for this and derive a covariance function equivalent to the thin plate spline regularizer which has desirable properties for shape modelling. We demonstrate our approach for both 2D curves and 3D surfaces. The benefit of using a Gaussian process for this is the meaningful probabilistic representation of the function.
1114 en Learning the prior for the PAC-Bayes bound 
1115 en Learning Human Pose and Motion Models for Animation Computer animation is an extraordinarily labor-intensive process; obtaining high-quality motion models could make the process faster and easier. I will describe methods for learning models of human poses and motion from motion capture data. I will begin with a pose model based on the Gaussian Process Latent Variable Model (GPLVM), and the application of this model to Inverse Kinematics posing. I will then describe the Gaussian Process Dynamical Model (GPDM) for modeling motion dynamics. I may also mention a few other extensions to the GPLVM for modeling motion data. I will discuss the properties of these models (both good and bad) and potential directions for future work.
1116 en Gaussian Processes for Monocular 3D People tracking We advocate the use of Gaussian Processes (GPs) to learn prior models of human pose and motion for 3D people tracking. The Gaussian Process Latent variable model (GPLVM) provides a low-dimensional embedding of the human pose, and defines a density function that gives higher probability to poses close to the training data. The Gaussian Process Dynamical Model (GPDM) provides also a complex dynamical model in terms of another GP. With the use of Bayesian model averaging both GPLVM and GPDM can be learned from relatively small amounts of training data, and they generalize gracefully to motions outside the training set. We show that such priors are effective for tracking a range of human walking styles, despite weak and noisy image measurements and a very simple image likelihood. Tracking is formulated in terms of a MAP estimator on short sequences of poses within a sliding temporal window.
1117 en Microphone Array Driven Speech Recognition: Influence of Localization on the Word Error Rate Interest within the automatic speech recognition research community has recently focused on the recognition of speech where the microphone is located in the medium field, rather than being mounted on a headset and positioned next to the speakers mouth to realize the long-term goal of ubiquitous computing. This is a natural application for beamforming techniques using a microphone array. A crucial ingredient for optimal performance of beamforming techniques is the speaker location. Hence, to apply such techniques, a source localization algorithm is required. In prior work, we proposed using an extended Kalman filter to directly update position estimates in a speaker localization system based on time delays of arrival.We also have enhanced our audio localizer with video information. In this work, we investigate the influence of the speaker position on the word error rate of an automatic speech recognition system operating on the output of a beamformer, and compare this error rate with that obtained with a close talking microphone. Moreover, we compare the effectiveness of different localization algorithms. We tested our algorithm on a data set consisting of seminars held by actual speakers. Our experiments revealed that accurate speaker tracking is crucial for minimizing the errors of a farfield speech recognition system.
1118 en Analysing Meeting Records: An Ethnographic Study and Technological Implications Whilst there has been substantial research into technology to support meetings, there has been relatively little study of how meeting participants currently make records and how these records are used to direct collective and individual actions outside the meeting. This paper empirically investigates current meeting recording practices to determine how these might be better supported by technology. Our main findings were that participants create two types of meeting record. Public records are a collectively negotiated contract of decisions and commitments. Personal records, in contrast, are a highly personalised reminding tool, recording both actions and the context surrounding these actions. These observations are then used to critique current meeting support technology and to suggest new directions for research.
1119 en Toward joint segmentation and classification of dialog acts in multiparty meetings We present baseline results for the joint segmentation and classification of dialog acts (DAs) of the ICSI Meeting Corpus. Two simple approaches based on word information are investigated and compared with previous work on the same task. We also describe several metrics to assess the quality of the segmentation alone as well as the joint performance of segmentation and classification of DAs.
1120 en Mistake bounds and risk bounds for on-line learning algorithms In statistical learning theory, risk bounds are typically obtained via the manipulation of suprema of empirical processes measuring the largest deviation of the empirical risk from the true risk in a class of models. In this talk we describe the alternative approach of deriving risk bounds for the ensemble of hypotheses obtained by running an arbitrary learning algorithm in an-on line fashion. This allows us to replace the uniform large deviation argument with a simpler argument based on the analysis of the empirical process engendered by the on-line learner. The large deviations of such empirical processes are easily controlled by a single application of Bernstein's inequality for martingales, and the resulting risk bounds exhibit strong data-dependence.
1121 en Suboptimality of MDL and Bayes in Classification under Misspecification We show that forms of Bayesian and MDL learning that are often applied to classification problems can be *statistically inconsistent*. We present a large family of classifiers and a distribution such that the best classifier within the model has generalization error (expected 0/1-prediction loss) almost 0. Nevertheless, no matter how many data are observed, both the classifier inferred by MDL and the classifier based on the Bayesian posterior will behave much worse than this best classifier in the sense that their expected 0/1-prediction loss is substantially larger. Our result can be re-interpreted as showing that under misspecification, Bayes and MDL do not always converge to the distribution in the model that is closest in KL divergence to the data generating distribution. We compare this result with earlier results on Bayesian inconsistency by Diaconis, Freedman and Barron.
1122 en Robustness properties of support vector machines and related methods The talk brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on convex risk minimization have - besides other good properties - also the advantage of being robust if the kernel and the loss function are chosen appropriately. Our results cover classification and regression problems. Assumptions are given for the existence of the influence function and for bounds on the influence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. We also consider Robust Learning from Bites, a simple method to make some methods from convex risk minimization applicable for huge data sets for which currently available algorithms are much to slow. As an example we use a data set from 15 German insurance companies.
1123 en Universal Principles, Approximation and Model Choices Universal principles are ones which make no reference to the subject matter of the data and include Maximum Likelihood, Bayes, AIC and MDL. In this talk we criticize the use of such principles to solve the problem of model choice. The criticism will be mainly directed against MDL but corresponding arguments can be made against the other principles. A concept of approximation will be introduced and its use in choosing a model illustrated by examples from non-parametric statistics.
1124 en On minimax estimation of infinite dimensional vector of binomial proportions We consider the problem of minimax estimation of infinite dimensional vector of binomial proportions. Under some conditions we derive the asymptotic behavior of the minimax risk over some nonparametric classes. Further, we discuss the issue of adaptation and the problem of optimal allocation of observations.
1125 en Invariance in kernel methods - distance and integration kernels 
1126 en Independent Component Analysis The course provides an introduction to independent component analysis and source separation. We start from simple statistical principles; examine connections to information theory and to sparse coding; we give an overview of available algorithmics; we also show how several key ideas of ICA are illuminated by information geometry.
1131 en Extensions of Gaussian Processes for Ranking: Semi-Supervised and Active Learning 
1132 en Online Learning and Game Theory We consider online learning and its relationship to game theory. In an online decision-making problem, as in Singer's lecture, one typically makes a sequence of decisions and receives feedback immediately after making each decision. As far back as the 1950's, game theorists gave algorithms for these problems with strong regret guarantees. Without making statistical assumptions, these algorithms were guaranteed to perform nearly as well as the best single decision, where the best is chosen with the benefit of hindsight. We discuss applications of these algorithms to complex learning problems where one receives very little feedback. Examples include online routing, online portfolio selection, online advertizing, and online data structures. We also discuss applications to learning Nash equilibria in zero-sum games and learning correlated equilibria in general two-player games.
1134 en Welcome to Chicago, and a (brief!) introduction to machine learning 
1135 en Generalization bounds When a learning algorithm produces a classifier, a natural question to ask is "How well will it do in the future?" To make statements about the future given the past, some assumption must be made. If we make only an assumption that all examples are drawn independently and identically from some (unknown) distribution, we can answer the question. The answer to this question is directly applicable to classifier testing and confidence reporting. It also provides a simple general explanation of "overfitting", and influences algorithm design.
1138 en Introduction to Kernel Methods 
1139 en Semi-supervised Learning, Manifold Methods 
1140 en Introduction to Kernel Methods 
1149 en Can Adaptive Regularization Help? 
1150 en Estimation of gradients and coordinate covariation in classification We introduce an algorithm that simultaneously estimates a classification function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to find salient variables and estimate how they covary. An efficient implementation with respect to both memory and time is given.
1151 en Optimality of Bayesian Transduction - Implications for Input Non-stationarity 
1152 en Learning Classifiers in Distribution and Cost-sensitive Environments 
1153 en Estimating the Joint AUC of Labelled and Unlabelled Data 
1156 en A Domain Adaptation Formal Framework Addressing the Training/Test Distribution Gap 
1157 en Risk Hull Method for Inverse Problems 
1158 en Bayesian Methods for Inverse Problems 
1159 en Inverse Problems in Biology 
1161 en A Statistical View of Some Regularization Methods for Ill-posed Problem 
1162 en Methods and Convergence Results for Non Linear Inverse Problems 
1164 en Inverse Problems with Error in the Operator 
1165 en Some Key Challenges in Web Crawlers and Content-Based Search Engines 
1166 en AdaBoost is Universally Consistent We consider the risk, or probability of error, of the classifier produced by AdaBoost, and in particular the stopping strategy to be used to ensure universal consistency. (A classification method is universally consistent if the risk of the classifiers it produces approaches the Bayes risk---the minimal risk---as the sample size grows.) Several related algorithms---regularized versions of AdaBoost---have been shown to be universally consistent, but AdaBoost's universal consistency has not been established. Jiang has demonstrated that, for each probability distribution satisfying certain smoothness conditions, there is a stopping time for sample size n, so that if AdaBoost is stopped after iterations, its risk approaches the Bayes risk for that distribution. Our main result is that if AdaBoost is stopped after iterations, it is universally consistent, where n is the sample size and .
1168 en Debate 
1170 en Morfessor in the Morpho Challenge In this work, Morfessor, a morpheme segmentation model and algorithm developed by the organizers of the Morpho Challenege, is outlined and references are made to earlier work. Although Morfessor does not take part in the official Challenge competition, we report experimental results for the morpheme segmentation of English, Finnish and Turkish words. The obtained results are very good. Morfessor outperforms the other algorithms in the Finnish and Turkish tasks and comes second in the English task. In the Finnish speech recognition task, Morfessor achieves the lowest latter error rate.
1171 en Unsupervised Morphological Segmentation Based on Segment Predictability and Word Segments Alignment Word segments are relevant cues for the automatic acquisition of semantic relationships from morphologically related words. Indeed, morphemes are the smallest meaning-bearing units. We present an unsupervised method for the segmentation of words into sub-units devised for this objective. The system relies on segment predictability to discover a set of prefixes and suffixes and performs word segments alignment to detect morpheme boundaries.
1172 en A Sample-Complexity Analysis of Learning from Labeled and Unlabeled Data 
1173 en Debate 
1177 en Error Bounds for Correlation Clustering 
1178 en Two-step Approach to Unsupervised Morpheme Segmentation This paper describes two steps of a morpheme boundary segmentation algorithm. The task is solely to find boundaries between morphemes bar and any further analysis such as phoneme deletions, insertions or alternations that may occur between or within morphemes. The algorithm presented here was designed under the premise that it is not supposed to utilize any knowledge about the language it should analyse. Neither is it supposed to rely on any kind of human supervision. The first step is to use a high-precision, low-recall algorithm to find a relatively small number of mostly correct segmentations, see (Bordag, 2005). In the second step, these segmentations are used to train a classification, which is then applied to all words to find morpheme boundaries within them.
1179 en A Simpler, Intuitive Approach to Morpheme Induction We present a simple, psychologically plausible algorithm to perform unsupervised learning of morphemes. The algorithm is most suited to Indo-European languages with a concatenative morphology, and in particular English. We will describe the two approaches that work together to detect morphemes: 1) finding words that appear as substrings of other words, and 2) detecting changes in transitional probabilities. This algorithm yields particularly good results given its simplicity and conciseness: evaluated on a set of 532 human-segmented English words, the 252-line program achieved an F-score of 80.92% (Precision: 82.84% Recall: 79.10%).
1180 en Machine Learning for Games The course gives an introduction to the application of machine learning techniques to games. The course will consist of two parts, part I dealing with computer/video games, part II dealing with traditional board/strategy games. Alongside, I will introduce necessary background material including aspects of neural networks, reinforcement learning, and graphical models. 1. In recent years various aspects of computer games have been developed to near perfection. These include high-performance graphics, realistic surround sound, and detailed physical simulations. However, the control of non-player characters (NPCs), also known as game AI, has fallen behind to the point that the resulting gaming experience often suffers. Machine learning offers a framework for making NPCs adaptive to both the environment and the human player. This technology has therefore the potential to greatly enhance gaming experience. Furthermore, at development time machine learning techniques can be employed to automate the creation of (intelligent) NPC behavior, thereby replacing the current standard of scripting and trial-and-error. The examples presented include imitation learning for avatars and reinforcement learning in fighting games. 2. Classical board games such as Chess, Go, and Backgammon have been a traditional theme in artificial intelligence. While chess has essentially been solved by traditional AI approaches, world-class Backgammon engines could only be developed based on machine learning techniques, originally in the combination of neural networks and reinforcement learning. For the traditional board game Go, neither of the two approaches has been successful so far. In this part of the course I will explain and discuss the machine learning approach to Backgammon. I will then give an introduction to the game of Go and discuss what machine learning may be able to contribute to the field of computer Go with a particular focus on modeling the uncertainty that emerges from the game's overwhelming complexity.
1182 en Information-Theoretic Metric Learning We formulate the metric learning problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the Mahalanobis distance function. Via a surprising equivalence, we show that this problem can be solved as a low-rank kernel learning problem. Specifically, we minimize the Burg divergence of a low-rank kernel to an input kernel, subject to pairwise distance constraints. Our approach has several advantages over existing methods. First, we present a natural information-theoretic formulation for the problem. Second, the algorithm utilizes the methods developed by Kulis et al. [6], which do not involve any eigenvector computation; in particular, the running time of our method is faster than most existing techniques. Third, the formulation offers insights into connections between metric learning and kernel learning.
1183 en Multitask learning: the Bayesian way Multi-task learning lends itself particularly well to a Bayesian approach. Cross-inference between tasks can be implemented by sharing parameters in the likelihood model and the prior for the task-specific model parameters. Choosing different priors, one can implement task clustering and task gating. Throughout my presentation, predicting single-copy newspaper sales will serve as a running example.
1184 en A Bayesian Probability Calculus for Density Matrices One of the main concepts in quantum physics is a density matrix, which is a symmetric positive definite matrix of trace one. Finite probability distributions can be seen as a special case when the density matrix is restricted to be diagonal. We develop a probability calculus based on these more general distributions that includes definitions of joints conditionals and formulas that relate these, including analogs of the Theorem of Total Probability and various Bayes rules for the calculation of posterior density matrices. The resulting calculus parallels the familiar ``conventional probability calculus and always retains the latter as a special case when all matrices are diagonal.
1185 en Multi-task feature learning We present a method for learning a low-dimensional representation which is shared across a set of multiple related tasks. The method builds upon the well-known 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks. We show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it.
1187 en On Efficient Sequential Decision Making in Structured Problems 
1190 en Teaching Data Mining: a Specific Experience 
1191 en Multiarmed Bandits and Partial Monitoring Exploration and Exploitation using Upper Confidence Bounds 
1192 en Debate 
1193 en Exploration - Exploitation for Statistical Software Testing 
1194 en Mutual Cuts in Graphs: Learning in Bioinformatics 
1195 en Discriminative Training for Object Recognition Using Image Patches 
1196 en The UoS LAVA group Approach to Generic Image Categorisation 
1197 en Kernel-Based Classification of Visual Objects using a Sparse Image Representation 
1198 en Linear SVM Classification of Visual Objects using a Dense Image Representation 
1199 en 101 Visual object classes - Introduction 
1200 en Overview of the Challenge and Results 
1201 en Many are Better than One: Improving probabilistic estimates of decision trees using Ensembles 
1202 en Lessons Learned in the Challenge: Making Predictions and Scoring Them 
1203 en Mixtures of Neural Nets 
1204 en Heteroscedastic Kernel Regression Models 
1206 en Clustering from an Optimization viewpoint Exploration and Exploitation using Upper Confidence Bounds 
1208 en The two faces of ML. 
1211 en Web Based Probabilistic Textual Entailment 
1212 en Textual Entailment Recognition Based on Inversion Transduction Grammar 
1213 en MITRE’s Submissions to the EU Pascal RTE Challenge 
1215 en VENSES – a Linguistically-Based System for Semantic Evaluation 
1216 en UCD IIRG Approach to the Textual Entailment Challenge 
1217 en Robust Textual Inference Using Diverse Knowledge Sources 
1218 en Textual Entailment Resolution via Atomic Propositions 
1219 en Combining Shallow and Deep NLP Methods for Recognizing Textual Entailment 
1220 en Welcome 
1223 en Learning Visual Distance Function for Object Identification from one Example Comparing images is essential to several computer vision problems, like image retrieval or object identification. The comparison of two images heavily relies on the definition of a good distance function. Standard functions (e.g. the euclidean distance in the original feature space) are too generic and fail to encode the domain specific information. In this paper, we propose to learn a similarity measure specific to a given category (e.g. cars). This distance is learned from a training set of pairs of images labeled “same” or “different”, indicating if the two images represent the same object (e.g. same car model) or not. After learning, this measure is used to predict how similar two images of never seen objects are.
1224 en Gradient Methods for Machine Learning Gradient methods locally optimize an unknown differentiable function, and thus provide the engines that drive much machine learning. Here we'll take a look under the hood, beginning with brief overview of classical gradient methods for unconstrained optimization: * Steepest descent, * Newton's method * Levenberg-Marquardt * BFGS * Conjugate gradient. To cope with the flood of data we find ourselves in today, stochastic approximation of the gradient from subsamples of data becomes a necessity. Unfortunately the noise this introduces into the gradient is not tolerated well by the classical gradient methods, with the exception of steepest descent, which however is very slow to converge. We'll see how local step size adaptation can be used to accelerate the convergence of stochastic gradient descent, culminating in the recent stochastic meta-descent (SMD) algorithm. SMD requires certain Hessian-vector products which can be computed efficiently via algorithmic (or automatic) differentiation (AD), a set of techniques that help automate the correct implementation of gradient methods in general. We'll discuss the basic concepts of AD, and learn simple ways to implement the forward mode of AD, and with it the fast Hessian-vector product.
1225 en Where is used ML? 
1229 en Exploration versus Exploitation 
1230 en RNA Structure Prediction Including Pseudoknots Based on Stochastic Multiple Context-Free Grammar Several grammars have been proposed for modeling RNA pseudoknotted structure. In this paper, we focus on multiple contextfree grammars (MCFGs), which are natural extension of context-free grammars and can represent pseudoknots, and extend a specific subclass of MCFGs to a probabilistic model called SMCFG.
1231 en Plains, Brains and Automobiles Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting "emergent" system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face.\\ In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
1232 en Building blocks for semantic search engines: Ranking and compact indexing in entity-relation graphs We see an evolutionary path to supporting semantic search over text facilitated by 1. extractors and annotators for ever-growing collections of entity and relation types and 2. search systems that exploit a smooth continuum between structured entities and relations on one hand and uninterpreted text on the other. The extractors and annotators will be imperfect and incomplete.
1233 en Part 1: A Novel Bayesian Approach for Uncovering Potential Spectroscopic Counterparts for Clinical Variables in 1H NMR Metabonomic Applications Metabonomic approaches based on spectroscopic data are in their infancy in biomedicine. A key challenge in clinical metabonomics is uncovering and understanding the relations between the multidimensional spectroscopic data and the clinical measures currently used for disease risk assessment and diagnostics. A novel Bayesian approach for revealing clinically relevant signals is presented here for a real 1H NMR metabonomics data set. The results are not only mathematically superior but also biochemically fully coherent.
1235 en Improved Functional Prediction of Proteins by Learning Kernel Combinations in Multilabel Setting Kernel methods have been successfully applied to a variety of biological data analysis problems. One problem of using kernels, however, is the lacking interpretability of the decision functions. It has been proposed to address this problem by using multiple kernels together with some combination rules, where each of the kernels measures different aspects of the data. Methods for learning sparse kernel combinations have the potential to extract relevant measurements for a given task.
1236 en The Challenge of Predicting Gene Function The biological sciences are undergoing an explosion in the amount of available data. New data analysis methods are needed to deal with the data. A central problem in bioinformatics is the assignment of function to sequenced open reading frames (ORFs). The most common approach is based on inferred homology using a statistically based se- quence similarity (SIM) method e.g. PSI-BLAST.
1238 en Hierarchical Multilabel Classification Trees for Gene Function Prediction Prediction of gene function is a so-called hierarchical multilabel classification (HMC) task: a single instance can be labelled with multiple classes rather than just one (i.e., a gene can have multiple functions), and these classes are organized in a hierarchy. Many machine learning methods focus on learning predictive models with a single target variable. One can then learn to predict all classes separately and combine the predictions afterwards.
1239 en Objective Bayesian Nets for Breast Cancer Prognosis According to objective Bayesianism, an agent’s degrees of belief should be determined by a probability function, out of all those that satisfy constraints imposed by background knowledge, that maximises entropy. A Bayesian net offers a way of efficiently representing a probability function and efficiently drawing inferences from that function.
1240 en Context dependent visualization of protein function Assignment of protein function is a nontrivial task due to the fact that the same proteins may be involved in different biological processes, depending on the state of the biological system and protein localization. Therefore, protein function is context dependent and textual annotations commonly utilized to describe protein function lack the flexibility to address such contextuality.
1241 en Support vector machines loss with l1 penalty We consider an i.i.d. sample from (X,Y), where X is a feature and Y a binary label, say with values +1 or -1. We use a high-dimensional linear approximation of the regression of Y on X and support vector machine loss with l1 penalty on the regression coefficients. This procedure does not depend on the (unknown) noise level or on the (unknown) sparseness of approximations of Bayes rule, but nevertheless its prediction error is smaller for smaller noise levels and/or sparser approximations. Thus, it adapts to unknown properties of the underlying distribution. In an example, we show that up to terms logarithmic in the sample size, the procedure yields minimax rates for the excess risk.
1243 en Automatic Cluster Complexity and Quantity Selection: Towards Robust Speaker Diarization 
1245 en Statistical Aspects of Pattern Analysis Abstract: The lectures will introduce the role of statistics in pattern analysis with a discussion of the difference between pattern significance and pattern stability. We will go on to discuss composite hypothesis testing and the Bonferroni correction. Concentration inequalities will be introduced and used to assess the statistical reliability of empirical estimates. We move to consider uniform convergence in order to analyse pattern stability. Rademacher complexity will be discussed as a theoretical tool for the bounding of uniform convergence.
1247 en Kernel Methods 
1248 en The Complexity of Learning Verification Informally, one branch of learning theory focuses on making statements of the form 'this learned classifier is at least X good'. A common intuition underlying many bounds of this form is that some form of 'prior' or 'bias' must exist on the set of all classifiers in order to make such statements. This intuition can be made precise in a few forms. I'll discuss the ways by which 'bias' and 'prior' allow verifiable learning as well as the limitations of 'prior' in addressing this problem.
1250 en Suffix tree and Hidden Markov techniques for pattern analysis Suffix tree construction. Mention the new linear time array constructions - - using suffix trees for finding motifs with gaps (some new observations: 0.5 - 1 hours). - finding cis-regulatory motifs by comparative genomics (1 hour) - Hidden Markov techniques for haplotyping
1253 en Online Loss Bounds 
1254 en Statistical Relational Learning - Part 1 Problems that arise from linkage and autocorrelation among objects must be taken into account. Because instances are linked together, classification typically involves complex inference to arrive at "collective classification" in which the labels predicted for the test instances are determined jointly rather than individually. Unlike iid problems, where the result of learning is a single classifier, relational learning often involves instances that are heterogeneous, where the result of learning is a set of multiple components (classifiers, probability distributions, etc.) that predict labels of objects and logical relationships between objects.
1255 en Statistical Relational Learning - Part 2 Statistical machine learning is in the midst of a "relational revolution". After many decades of focusing on independent and identically-distributed (iid) examples, many researchers are now studying problems in which the examples are linked together into complex networks. These networks can be a simple as sequences and 2-D meshes (such as those arising in part-of-speech tagging and remote sensing) or as complex as citation graphs, the world wide web, and relational data bases.
1257 en Supervised Clustering with Support Vector Machines Supervised clustering is the problem of training a clustering algorithm to produce desirable clusterings: given sets of items and complete clusterings over these sets, we learn how to cluster future sets of items. Example applications include noun-phrase coreference clustering, and clustering news articles by whether they refer to the same topic. In this paper we present an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure. The algorithm may optimize a variety of different clustering functions to a variety of clustering performance measures. We empirically evaluate the algorithm for noun-phrase and news article clustering.
1258 en Origins of Sex It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
1259 en Probing Landscapes It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
1262 en Privacy and Background Knowledge The digitization of our daily lives has led to an explosion in the collection of data by governments, corporations, and individuals. Protection of confidentiality of this data is of utmost importance. However, knowledge of statistical properties of private data can have significant societal benefit, for example, in decisions about the allocation of public funds based on Census data, or in the analysis of medical data from different hospitals to understand the interaction of drugs. I will start by introducing two application scenarios, privacy-preserving data analysis and privacy-preserving data publishing. I will show how in simple models background knowledge can lead to severe breaches of privacy in both applications, and I will describe how proper modeling of background knowledge can avoid privacy breaches. I will outline first algorithmic steps towards privacy-preserving data analysis and data publishing with background knowledge, and I will conclude with open problems.
1263 en Easy Learning Theory for Highly Scalable Algorithms 
1264 en Game theoretic models in molecular biology There are many challenges in computational modeling of biological processes. Few processes such as signaling pathways operate in- dependently of others but rather involve substantial coordination and shared resources. The level of abstraction appropriate for understand- ing different processes, e.g, viewing a pathway as a filter or a molecular cascade, varies by context and the type of predictions sought.
1265 en Reinforcement Learning Theory The tutorial is on several new pieces of Reinforcement learning theory developed in the last 7 years. This includes:\\ 1. Sample based analysis of RL including E3 and sparse sampling.\\ 2. Generalization based analysis of RL including conservative policy iteration and RL-to-Classification reductions.\\ For each of these forms of theory, we cover the basic results and cover the weaknesses and strengths of the approach in context.
1266 en Generative Models for Decoding Real-Valued Natural Experience in FMRI Functional Magnetic Resonance Imaging (FMRI) provides an unprecedented window into the complex functioning of the human brain, typically detailing the activity of thousands of voxels for hundreds of time points. The interpretation of FMRI is complicated, however, because of the unknown connection between the hemodynamic response and neural activity, and the unknown spatiotemporal characteristics of the cognitive patterns themselves. Recent work has exploited techniques from machine learning to find patterns of voxel activity related to brain processes (see e.g., [1]). Many of these techniques involve decoding, inferring the value or category class of a stimulus !S given a pattern of voxel activations !V . Decoding can generally be split into two approaches, discriminative and generative [2]. With a discriminative model one learns the conditional distribution P(!S |!V ) directly by minimizing a loss such as minimum classification error. Alternatively, the generative approach obtains this conditional probability through Bayes rule; one posits and fits models for P(!S ) and P(!V |S) instead. Both approaches can reliably establish the existence of sufficient decoding information.
1269 en Bayesian Kernel Methods 
1275 en Fitness Landscapes It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
1277 en Random projection, margins, kernels, and feature-selection Random projection is a simple technique that can often provide insight into questions such as "why is it good to have a large margin?" or "what are kernels really doing and how are they similar to feature selection?" In this talk I will describe some simple learning algorithms using random projection. I will then discuss how, given a kernel as a black-box function, we can use various forms of random projection to extract an explicit small feature space that captures much of the power of the given kernel function.
1279 en Large-Margin Thresholded Ensembles for Ordinal Regression We propose a thresholded ensemble model for ordinal regression problems. The model consists of a weighted ensemble of confidence functions and an ordered vector of thresholds. Using such a model, we could theoretically and algorithmically reduce ordinal regression problems to binary classification problems in the area of ensemble learning. Based on the reduction, we derive novel large-margin bounds of common error functions, such as the classification error and the absolute error. In addition, we also design two novel boosting approaches for constructing thresholded ensembles. Both our approaches have comparable performance to the state-of-the-art algorithms, but enjoy the benefit of faster training. Experimental results on benchmark datasets demonstrate the usefulness of our boosting approaches.
1282 en Multy Armed Bandit Problem 
1284 en Agnostic Active Learning The great promise of active learning is that via interaction the number of samples required can be reduced to logarithmic in the number required for standard batch supervised learning methods. To achieve this promise, active learning must be able to cope with noisy data. We show how it is possible to cope with even malicious noise in an active learning setting, removing noise an obstacle to regular application of active learning.
1285 en Neighbourhood Components Analysis Say you want to do K-Nearest Neighbour classification. Besides selecting K, you also have to chose a distance function, in order to define "nearest". I'll talk about a novel method for *learning* -- from the data itself -- a distance measure to be used in KNN classification. The learning algorithm, Neighbourhood Components Analysis (NCA) directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and very fast classification in high dimensions. Of course, the resulting classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. If time permits, I'll also talk about newer work on learning the same kind of distance metric for use inside a Gaussian Kernel SVM classifier.
1286 en Triple jump acceleration for the EM algorithm and its extrapolation-based variants The Aitken's acceleration is one of the most commonly used method to speed up the fixed-point iteration computation, including the EM algorithm. However, it requires to compute or approximate the Jacobian of the EM mapping matrix, which can be intractable for complex models. We will present our current research topic, the triple-jump acceleration, to accelerate the EM and some of its extrapolation-based variants by approximating their Jacobians. One advantage of the triple jump framework is that we can directly use the EM and its extrapolation-based variants as black boxes and achieve acceleration easily. We can update parameter vectors globally with the same approximated Jacobian, or locally based on each decomposable component with different Jacobians. Experimental results show that the triple jump methods consistently accelerate EM, parameterized EM (pEM) and adaptive EM (aEM) for a variety of probabilistic models.
1288 en Introduction to Learning Theory The goal of this course is to introduce the key concepts of learning theory. It will not be restricted to Statistical Learning Theory but will mainly focus on statistical aspects. Instead of giving detailed proofs and precise statements, this course will aim at providing some useful conceptual tools and ideas useful for practitioners as well as for theoretically-driven people.
1291 en Compositional Evolution It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
1294 en Learning and Regularization Using non Positive Kernels 
1296 en Tutorial on Machine Learning Reductions There are several different classification problems commonly encountered in real world applications such as 'importance weighted classification', 'cost sensitive classification', 'reinforcement learning', 'regression' and others. Many of these problems can be related to each other by simple machines (reductions) that transform problems of one type into problems of another type.  Finding a reduction from your problem to a more common problem allows the reuse of simple learning algorithms to solve relatively complex problems. It also induces an organization on learning problems — problems that can be easily reduced to each other are 'nearby' and problems which can not be so reduced are not close.
1298 en Information Geometry This tutorial will focus on entropy, exponential families, and information projection. We'll start by seeing the sense in which entropy is the only reasonable definition of randomness. We will then use entropy to motivate exponential families of distributions — which include the ubiquitous Gaussian, Poisson, and Binomial distributions, but also very general graphical models. The task of fitting such a distribution to data is a convex optimization problem with a geometric interpretation as an "information projection": the projection of a prior distribution onto a linear subspace (defined by the data) so as to minimize a particular information-theoretic distance measure. This projection operation, which is more familiar in other guises, is a core optimization task in machine learning and statistics. We'll study the geometry of this problem and discuss two popular iterative algorithms for it.
1299 en Exponential Families in Feature Space In this course I will discuss how exponential families, a standard tool in statistics, can be used with great success in machine learning to unify many existing algorithms and to invent novel ones quite effortlessly. In particular, I will show how they can be used in feature space to recover Gaussian Process classification for multiclass discrimination, sequence annotation (via Conditional Random Fields), and how they can lead to Gaussian Process Regression with heteroscedastic noise assumptions.
1300 en Robust heteroscedastic linear discriminant analysis and LCRC posterior features in meeting data recognition 
1302 en Speech-to-Speech Translation Services for the Olympic Games 2008 
1303 en Coevolution It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
1304 en Content Analysis: Marriage of Signal Processing and Machine Learning 
1305 en A Multimodal Analysis of Floor Control in Meetings 
1306 en Overlap in Meetings: ASR Effects and Analysis by Dialog Factors, Speakers, and Collection Site 
1308 en Detecting Action Items in Multi-Party Meetings: Annotation and Initial Experiments 
1309 en Gesture Features for Coreference Resolution 
1310 en Juicer: A Weighted Finite-State Transducer speech decoder 
1311 en Multistream Recognition of Dialogue Acts in Meetings 
1312 en The applications in ML. 
1313 en Wifi Localization with Gaussian Processes Estimating the location of a mobile device from wireless signal strength is an interesting research problem, especially given the complexity of signal propagation through space in the presence of obstacles such as buildings, walls, or people. Gaussian processes have already been used to solve such signal strength localization problems. We extend this work to indoor WiFi localization and present novel kernel functions which increase the accuracy of the Gaussian process model, especially when faced with sparse training data. We additionally present preliminary results of simultaneous mapping and localization using Gaussian process latent variable modeling.
1316 en Concentration Inequalities with Machine Learning Applications 
1320 en Stochastic Learning 
1322 en Debate: Panel 1 
1324 en Sum up of what has been done in PASCAL 
1327 en Some Mathematical Tools for Machine Learning These are lectures on some fundamental mathematics underlying many approaches and algorithms in machine learning. They are not about particular learning algorithms; they are about the basic concepts and tools upon which such algorithms are built. Often students feel intimidated by such material: there is a vast amount of "classical mathematics", and it can be hard to find the wood for the trees. The main topics of these lectures are Lagrange multipliers, functional analysis, some notes on matrix analysis, and convex optimization. I've concentrated on things that are often not dwelt on in typical CS coursework. Lots of examples are given; if it's green, it's a puzzle for the student to think about. These lectures are far from complete: perhaps the most significant omissions are probability theory, statistics for learning, information theory, and graph theory. I hope eventually to turn all this into a series of short tutorials. Please let me know of any errors, etc. ; ://from Chris Burges homepage : [[http://research.microsoft.com/~cburges]] **Lecture contains:**\\ Lagrange multipliers: * Lagrange the Mathematician * Lagrange multipliers: an indirect approach can be easier * Multiple Equality Constraints * Multiple Inequality Constraints * Two points on a d-sphere * The Largest Parallelogram * Resource allocation * A convex combination of numbers is maximized by choosing the largest * The Isoperimetric problem * For fixed mean and variance, which univariate distribution has maximum entropy? * An exact solution for an SVM living on a simplex Notes on some Basic Statistics * Probabilities can be Counter-Intuitive (Simpson's paradox; the Monty Hall puzzle) * IID-ness: Measurement Error decreases as 1/sqrt{n} * Correlation versus Independence * The Ubiquitous Gaussian: ** Product of Gaussians is Gaussian ** Convolution of two Gaussians is a Gaussian ** Projection of a Gaussian is a Gaussian ** Sum of Gaussian random variables is a Gaussian random variables ** Uncorrelated Gaussian variables are also independent ** Maximum Likelihood Estimates for mean and covariance (prove required matrix identities) ** Aside: For 1-dim Laplacian, max. likelihood gives the median * Using cumulative distributions to derive densities Principal Component Analysis and Generalizations * Ordering by Variance * Does Grouping Change Things? * PCA Decorrelates the Samples * PCA gives Reconstruction with Minimal Mean Squared Error * PCA preserves Mutual Information on Gaussian data * PCA directions lie in the span of the data * PCA: second order moments only * The Generalized Rayleigh Quotient ** Non-orthogonal principal directions ** OPCA ** Fisher Linear Discriminant ** Multiple Discriminant Analysis Elements of Functional Analysis * High Dimensional Spaces * Is Winning Transitive? * Most of the Volume is Near the Surface: Cubes * Spheres in n-dimensions * Banach Spaces, Hilbert Spaces, Compactness * Norms * Useful Inequalities (Minkowski and Holder) * Vector Norms * Matrix Norms * The Hamming Norm * L1, L2, L_infty norms - is L0 a norm? * Example: Using a Norm as a Constraint in Kernel Algorithms
1329 en Learning with Kernels The Course on Learning with Kernels covers * Elements of Statistical Learning Theory * Kernels and feature spaces * Support vector algorithms and other kernel methods * Applications
1330 en Sex and Units of Selection It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
1333 en Support Vector Machines and Kernels 
1334 en A Gaussian Approximation for Stochastic Nonlinear Dynamical Processes with Annihilation 
1335 en Using Maximal Embedded Syntatic Subtrees for Textual Entailment Recognition In this paper we address the textual entailment task by using tree mining and matching technique. Our results show that accuracy can be improved when using a combination of lexical entailment with syntactic matching. The best result we received by combinig two components is the following: 70% of recall and 57,5% of precision on the test set.
1336 en UNED at PASCAL RTE-2 Challenge This paper reports the description of the developed system and the results obtained in the participation of the UNED in the Second Recognizing Textual Entailment (RTE) Challenge. New techniques and tools have been added: enriched queries to WordNet, detection of numeric expresions and their entailment, and Support Vector Machine classification (SVM) are the more relevant. The accuracy performed is slightly higher than the one from the previous edition system.
1337 en Exploration exploitation in Go: UCT for Monte-Carlo Go 
1338 en Challenge results: PASCAL Exploration Vs Exploitation challenge results for phase II 
1340 en Semantic Annotation in the Alvis Project 
1341 en Variational Bayes for Continuous-time Nonlinear State-space Models 
1342 en How Random is a Coin Toss? Bayesian Inference and the Symbolic Dynamics of Deterministic Chaos 
1343 en The Gaussian Variational Approximation of Stochastic Differential Equations 
1344 en Learning Textual Entailment from Examples In this paper we present a novel approach for learning entailment relations from positive and negative examples. We define a similarity between two text-hypothesis pairs based on a syntatic and lexical information. We experimented our model within the RTE 2006 challenge obtaining the accuracy of 63.88and 62.50% for the two submissions.
1346 en Independent Component Analysis In independent component analysis (ICA), the purpose is to linearly decompose a multidimensional data vector into components that are as statistically independent as possible. For nongaussian random vectors, this decomposition is not equivalent to decorrelation as is done by principal component analysis, but something considerably more sophisticated. ICA allows one to separate nongaussian source signals from their linear mixtures 'blindly', i.e. using no other information than the congaussianity of the source signals. ICA can also be used to extract features from image and sound signals according to the principle of redundancy reduction that has its origins in the neurosciences. In my talks I will review the basic theory and theoretical background of ICA together with some recent theoretical developments.
1348 en Analysis of Dynamics It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
1349 en Representations of graphs A graph is a mathematical structure that is sometimes hard to separate from its visualization. An important branch of graph theory studies graph drawing problems. Recently a mathematical approach to graph visualization has been developed under the name of "graph representations". In this tutorial we present an outline of the theory of graph representations.
1350 en Visualizing Cauchy’s Interlacing Property for Line Distance Matrices In the paper it is proven that line distance matrices of size n have one positive and n ! 1 negative eigenvalues. Visual representation of Cauchy's interlacing property for line distance matrices is considered.
1351 en An Integrated framework for the management of video collection Video document retrieval is now an active part of the domain of multimedia retrieval.However, unlike for other media, the management of a collection of video documents adds the problem of efficiently handling an overwhelming volume of temporal data. Challenges include balancing efficient content modeling and storage against fast access at various levels. In this paper, we detail the framework we have built to accommodate our developments in content-based multimedia retrieval.We show that not only our framework facilitates the developments of processing and indexing algorithms but it also opens the way to several other possibilities such as rapid interface prototyping or retrieval algorithms benchmarking. In this respect, we discuss our developments in relation to wider contexts such as MPEG-7 and The TREC Video Track.
1352 en Introduction 
1354 en How to predict with Bayes, MDL, and Experts Most passive Machine Learning tasks can be (re)stated as sequence prediction problems. This includes pattern recognition, classification, time-series forecasting, and others. Moreover, the understanding of passive intelligence also serves as a basis for active learning and decision making. In the recent past, rich theories for sequence prediction have been developed, and this is still an ongoing process. On the other hand, we are arriving at the stage where some important results are already termed classical. While much of the current Learning Theory is formulated under the assumption of independent and identically distributed (i.i.d.) observations, this lecture series focusses on situations without this prerequisite (e.g. weather or stock-market time-series).
1358 en Generalized Principal Component Analysis (GPCA) Data segmentation is usually though of as a chicken-and-egg problem. In order to estimate a mixture of models one needs to first segment the data, and in order to segment the data one needs to know the model parameters. Therefore, data segmentation is usually solved in two stages 1. Data clustering and 2. Model fitting.  Other iterative methods use, e.g. the Expectation Maximization (EM) algorithm. This talk will show that for a wide class of segmentation problems with multi-linear structure (including clustering subspaces of unknown and varying dimensions), the chicken-and-egg dilemma can be tackled as follows: 1. Fit a set of polynomials to all data points, without clustering the data 2. Obtain the model parameters for each group from the derivatives of these polynomials.  Applications of GPCA to image/video/motion segmentation, face clustering, and identification of hybrid dynamical models systems will also be presented.
1360 en Text Categorization This course will cover the principal topics important to creating a working text categorization system. It will focus on the components of such a system and processes required to create it based on the practical experiences of the Scamseek project. The role of machine learning will be the center of the discussion but the surrounding tasks of language modeling, computational linguistics and software engineering will all be discussed to varying degrees. Discussion of some aspects of the Scamseek project are restricted under secrecy agreements with ASIC.
1361 en Multi-stream modeling with applications in speech and multimodal processing After a brief discussion of the problem arising from the processing and modeling of multiple stream (multi-channel, multi-sensor) signals, we will discuss a few statistical structures (such as multi-stream HMM and asynchronous HMM) that can accommodate multiple (asynchnonous) observation streams (possibly exhibiting different frame rates). Indeed, it will be shown on different speech recognition and multimodal fusion tasks that it might sometimes be a good idea to be able to ``desynchronize'' the streams in order to maximize their joint likelihood. Different applications in speech recognition, such as multi-band and multi-stream speech processing, will be discussed. Finally, multimodal applications significantly benefiting from this multi-stream paradigm will also be discussed, including audio-visual speech recognition and modeling of human interaction in meetings (by modeling the joint behaviours of participants through multiple audio and visual features).
1362 en Toward Dependency Path based Entailment We present our submission to the RTE2 challenge which takes steps in the direction of dynamically entailing hypotheses via their dependency paths. We evaluate semantic similarity between sentences utilizing corpus co-occurence estimates of various dependency path features and show a 2.7% improvement on the RTE1 dataset.
1363 en Testing Parametric Models in Statistical Inverse Problems 
1364 en Acquisition of Knowledge About Spatial Location in Rats: the Associative Mechanism 
1365 en Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods The Octopus arm is a highly versatile and complex limb. How the Octopus controls such a hyper-redundant arm (not to mention eight of them!) is as yet unknown. Robotic arms based on the same mechanical principles may render present day robotic arms obsolete. In this talk, I will describe how we tackle this problem using an online reinforcement learning algorithm, based on a Bayesian approach to policy evaluation known as Gaussian process temporal difference (GPTD) learning.
1369 en Artificial Companions What will an artificial Companion be like? Who will need them and how much good or harm will they do? Will they change our lives and social habits in the radical way technologies have in the past: just think of trains, phones and television? Will they force changes in the law so that things that are not people will be liable for damages; up till now, it is the case that if a machine goes wrong, it is always the maker or the programmer, or their company, which is at fault.
1370 en A Mixed-lingual Phonological Component in Polyglot TTS Synthesis Polyglot text-to-speech synthesis, i.e. the synthesis of sentences containing one or more inclusions from other languages, primarily depends on an accurate morpho-syntactic analyzer for such mixed-lingual texts. From the output of this analyzer, a mixed-lingual phonological component can derive a correct pronunciation by application of language-specific phonological transformation rules that are restricted by syntactic, graphemic and phonological context constraints
1371 en Meeting Modelling The paper presents a framework for corpus based multimodal research. Part of this framework is applied in the context of meeting modelling. A generic model for different aspects of meetings is discussed. This model leads to a layered description of meetings where each layer adds a level of interpretation for distinct aspects based on information provided by lower layers. This model should provide a starting point for selecting annotation schemes for layers of the meeting and for defining a hierarchy between individual layers.
1372 en S-SEER: A Multimodal Office Activity Recognition System with Selective Perception I will present the use of layered probabilistic representations for modeling the activities of people in a system named S-SEER. I will describe how we use the representation to do sensing, learning and inference at multiple levels of temporal granularity and abstraction. The approach centers on the use of a cascade of Hidden Markov Models (HMMs) named Layered Hidden Markov Models (LHMMs) to diagnose states of a user's activity based on real-time streams of evidence from video, audio and computer (keyboard and mouse) interactions.
1373 en Mixture of SVMs for Face Class Modeling We present a method for face detection which uses a new {SVM} structure trained in an expert manner in the eigenface space. This robust method has been introduced as a post processing step in a real-time face detection system. The principle is to train several parallel {SVMs} on subsets of some initial training set and then train a second layer {SVM} on the margins of the first layer of {SVMa}. This approach presents a number of advantages over the classical {SVM}: firstly the training time is considerably reduced and secondly the classification performance is improved, we will present some comparisions with the single {SVM} approach for the case of human face class modeling.
1374 en Towards Computer Understanding of Human Interactions People meet in order to interact - disseminating information, making decisions, and creating new ideas. Automatic analysis of meetings is therefore important from two points of view: extracting the information they contain, and understanding human interaction processes. Based on this view, this article presents an approach in which relevant information content of a meeting is identified from a variety of audio and visual sensor inputs and statistical models of interacting people.
1375 en On the Adequacy of Baseform Pronunciations and Pronunciation Variants This paper presents an approach to automatically extract and evaluate the ``stability'' of pronunciation variants (i.e., adequacy of the model to accommodate this variability), based on multiple pronunciations of each lexicon words and the knowledge of a reference baseform pronunciation. Most approaches toward modelling pronunciation variability in speech recognition are based on the inference (through an ergodic HMM model) of a pronunciation graph (including all pronunciation variants), usually followed by a smoothing (e.g., Bayesian) of the resulting graph.
1377 en Tree Edit Distance for Recognizing Textual Entailment: Estimating the Cost of Insertion The focus of our participation in PASCAL RTE2 was estimating the cost of the information of the hypothesis which is missing in the text and can not be matched with entailment rules. We have tested different system settings for calculating the importance of the words of the hypothesis and investigated the possibility of combining them with machine learning algorithm.
1378 en Tandem Connectionist Feature Extraction for Conversational Speech Recognition Multi-Layer Perceptrons (MLPs) can be used in automatic speech recognition in many ways. A particular application of this tool over the last few years has been the Tandem approach, as described by Hermansky et al in a number of publications. Here we discuss the characteristics of the MLP-based features used for the Tandem approach, and conclude with a report on their application to conversational speech recognition. The paper shows that MLP transformations yield variables that have regular distributions, which can be further modified by using logarithm to make the distribution easier to model by a Gaussian-HMM. Two or more vectors of these features can easily be combined without increasing the feature dimension. We also report recognition results that show that MLP features can significantly improve recognition performance for the NIST 2001 Hub-5 evaluation set with models trained on the Switchboard Corpus, even for complex systems incorporating MMIE training and other enhancements.
1379 en On the Adequacy of Baseform Pronunciations and Pronunciation Variants This paper presents an approach to automatically extract and evaluate the ``stability'' of pronunciation variants (i.e., adequacy of the model to accommodate this variability), based on multiple pronunciations of each lexicon words and the knowledge of a reference baseform pronunciation. Most approaches toward modelling pronunciation variability in speech recognition are based on the inference (through an ergodic HMM model) of a pronunciation graph (including all pronunciation variants), usually followed by a smoothing (e.g., Bayesian) of the resulting graph.
1380 en Gaussian Processes for Active Sensor Management In this paper we study the active sensor management problem using continuous optimal experimental design (OED) framework. This task comprises the determination of allocation for a limited number of sensors over the spatial domain and the number of repetitive measurements in these locations in order to improve the overall system performance. We present a principled approach to active sensor management with repetitive measurements for Gaussian Processes (GPs) using a generalised D-optimality criteria and soft margin constrains. The resulting optimum of the convex optimization of the optimal experimental design for GP is generally sparse, in the sense that measurements should be taken at only a limited set of possible sensor locations. We demonstrate the use of our method on arti¯cial dataset.
1382 en Statistical Translation, Heat Kernels, and Expected Distances High dimensional structured data such as text and images is often poorly understood and misrepresented in statistical modeling. The standard histogram representation suffers from high variance and performs poorly in general. We explore novel connections between statistical translation, heat kernels on manifolds and graphs, and expected distances. These connections provide a new framework for unsupervised metric learning for text documents. Experiments indicate that the resulting distances are generally superior to their more standard counterparts.
1385 en Minimum Likelihood Image Feature and Scale Detection Based on the Brownian Image Model We present a novel approach to image feature and scale detection based on the fractional Brownian image model in which images are realisations of a Gaussian random process on the plane. Image features are points of interest usually sparsely distributed in images. We propose to detect such points and their intrinsic scale by detecting points in scale-space that locally minimises the likelihood under the model.
1386 en Verification of Facts across Document Boundaries 
1387 en Automated Text Summarization using MEAD: Experience with the IMF Staff Reports 
1388 en Integrate Text Clustering Features in Text Categorization System 
1389 en Active, Semi-Supervised Learning for Textual Information Access 
1390 en Cross-lingual Linking of News Clusters in Various Languages Avoiding the Usage of Bilingual Linguistic Resources 
1391 en Telugu - English Dictionary Based Cross Language Query Focused Multi-Document Summarization 
1392 en Research Directions in SMART 
1393 en Modularity It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
1394 en A Self-Organizing Map for Relation Extraction from Wikipedia using Structured Data Representations 
1395 en Topic Models in ALVIS 
1398 en Ranking by Stealing Human Cycles Ranking objects is a challenging task for machines. The main difficulty is that some characteristics of interest lack objective criteria. As the Internet becomes more widely used, it is possible to integrate the human capability of evaluating unmeasurable properties with the computational power of machines. A good example is the Internet voting for photos, foods and many others. In this talk, we propose a paired comparison framework, in which users are asked to show preferences in a pair of objects. Experiments on a photo ranking task show that the paired method outperforms the commonly used scoring method.
1402 en The future of MLMI 
1403 en Empirical Inference 
1404 en The present of MLMI 
1405 en Augmented Multiparty Interaction 
1408 en Bayesian Inference: Principles and Practice The aim of this course is two-fold: to convey the basic principles of Bayesian machine learning and to describe a practical implementation framework. Firstly, we will give an introduction to Bayesian approaches, focussing on the advantages of probabilistic modelling, the concept of priors, and the key principle of marginalisation. Secondly, we will exploit these ideas to realise practical algorithms for sparse linear regression and classification, as exemplified by models such as the "relevance vector machine".
1412 en An Introduction to Pattern Classification 
1415 en Constrained Hidden Markov Models for Population-based Haplotyping Analysis of genetic variation in human populations is critical to the understanding of the genetic basis for complex diseases. Although genomes of several species have been sequenced, it is still too expensive to sequence genomes of several individuals to analyze genetic variation. Furthermore, most of the genome is invariant among individuals.
1417 en Predicting co-evolving pairs in Pfam using information theory where entropy is determined by phylogenetic mutation events The accurate prediction of co-evolving pairs in protein sequences plays an important role in tertiary protein structure prediction and protein engineering. Using information theory to detect co-evolving pairs is impacted by the phylogenetic effect on entropy measurements. Mutual Information (MI) is used to detect co-evolving pairs in a protein family by re-sampling based on mutation events in the phylogenetic tree (RPE).
1418 en Estimation of human endogeneous retrovirus activities from expressed sequence databases Human endogenous retroviruses (HERVs) are remnants of ancient retrovirus infections and now reside within the human DNA. Recently HERV expression has been detected in both normal tissues and diseased patients. However, the activities (expression levels) of individual HERV sequences are mostly unknown.
1419 en Sparse Log Gaussian Processes via MCMC for Spatial Epidemiology Log Gaussian processes (LGP) are an attractive manner to construct intensity surfaces for the purposes of spatial epidemiology. The intensity surfaces are naturally smoothed by placing a GP prior over the relative log Poisson rate. In this work a fully independent training conditional (FITC) sparse approximation is used to speed up GP computations. The sampling of the latent values is sped up with transformations taking into account the approximate conditional posterior precision.
1421 en Probabilistic Inference for Graph Classification Graph data is getting increasingly popular in, e.g., bioinfor- matics and text processing. A main dificulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible sub- graphs, the dimensionality gets too large for usual statistical methods.
1423 en Large Scale Ranking Problem: some theoretical and algorithmic issues The talk is divided into two parts. The first part focuses on web-search ranking, for which I discuss training relevance models based on DCG (discounted cumulated gain) optimization. Under this metric, the system output quality is naturally determined by the performance near the top of its rank-list. I will mainly focus on various theoretical issues for this learning problem. The second part discusses related algorithmic issues in the context of optimizing the scoring function of a statistical machine translation system according to the BLEU metric (standard measure of translation quality). Our approach treats machine translation as a black-box, and can optimize millions of system parameters automatically. This has not been attempted before in this context. I will present our method and some initial results.
1424 en Model based identification of transcription factor activity from microarray data With the increase in volume of gene expression data available from high throughput microarray experiments, much research interest has been directed at building mathematical models of the process of gene regulation. Such models have primarily been used for the so called reverse engineering of regulatory networks; inferring possible regulatory interactions directly from microarray data, for example [1–4]. By using microarray data, all of these techniques make the implicit assumption that there is a direct relationship between the level of mRNA of genes coding for transcription factors (TFs) and the mRNA levels of their gene-targets.
1426 en Exploration vs. Exploitation Challenge Framework 
1431 en Advanced Statistical Learning Theory This set of lectures will complement the statistical learning theory course and focus on recent advances in the domain of classification. 1- PAC Bayesian bounds: a simple derivation, comparison with Rademacher averages. \\ 2 - Local Rademacher complexity with classification loss, Talagrand's inequality. Tsybakov noise conditions. \\ 3 - Properties of loss functions for classification (influence on approximation and estimation, relationship with noise conditions). \\ 4 - Applications to SVM - Estimation and approximation properties, role of eigenvalues of the Gram matrix.
1433 en Linguistic Resources for Meeting Recognition 
1434 en Least Squares Filtering of Speech Signals for Robust ASR 
1435 en Developing a consistent view on emotion-oriented computing The network of excellence HUMAINE is currently making a co-ordinated, interdisciplinary effort to develop a consistent view on emotion-oriented computing. This overview paper proposes a “map” of the research area, distinguishing core technologies from applicationoriented and psychologically oriented work. First results from the ongoing research in the thematic workpackages are reported.
1436 en Speech-to-Text Evaluation System 
1437 en Automatic Dominance Detection In Meetings Using Support Vector Machines We show that, using a Support Vector Machine classifier, it is possible to determine with a 75% success rate who dominated a particular meeting on the basis of a few basic features. We discuss the corpus we have used, the way we had people judge dominance and the details of the classifier and features that were used.
1438 en Multimodal visual resource discovery 
1439 en TUT 2005 SLOC System 
1440 en Detection and Resolution of References to Meeting Documents This article proposes a method for document/speech align- ment based on explicit references made in speech to documents and parts of documents, in the context of multimodal meetings. The motivation and the main components of the method are —rst described brie°y. Then, the article focuses on the two main stages of dialogue processing: the detec- tion of the expression referring to documents in transcribed speech, and the recognition of the documents and document elements that they refer to. The detailed evaluation of the implemented modules, —rst separately and then in a pipeline, shows that results are well above the baseline, and that the various features of the proposed algorithms are all relevant. The integration of this document/speech alignment technique with other ones is —nally discussed.
1443 en Boosting 
1447 en Variational Bayesian methods for audio indexing 
1448 en Concepts of grid computing 
1450 en Computer Vision 
1451 en Learning Rankings for Information Retrieval 
1452 en Learning from Network Traffic: Computing Kernels over Connection Content 
1453 en Spectral Clustering and Transductive Inference for Graph Data 
1454 en Ranking as Learning Structured Outputs 
1455 en Exploiting Hyperlinks to Learn a Retrieval Model 
1456 en Object Correspondence as a Machine Learning Problem 
1458 en Kernels in Bioinformatics 
1459 en The Pyramid Match Kernel: Efficient Learning with Sets of Features 
1461 en What is the future of ML? 
1462 en Piecewise 1-Dim Self Organizing Map P1D-SOM 
1465 en Nonparametric Transformation to White Noise 
1466 en Iterative Regularization Scheme and Early Stopping in Learning from Examples 
1467 en Empirical Bayesian test for the smoothness In the context of adaptive nonparametric curve estimation problem, a common assumption is that the function (signal) to estimate belongs to a nested family of functional classes, parameterized by a quantity which often has a meaning of smoothness amount. It has already been realized that the problem of estimating the smoothness is meaningless. What can then be inferred about the smoothness? We try to answer this question and discuss the implications of our results for the hypothesis testing problem for the smoothness parameter. The imbedded model structure (nested family of classes) accounts for the fact that a consistent test can be constructed only for the one-sided hypothesis. The test statistic is based on the marginalized maximum likelihood estimator of the smoothness for the appropriate choice of the prior distribution on the unknown signal.
1468 en Learning techniques in Planning In this lecture, I aim to provide an overview of the learning techniques that have found use in automated planning. Unlike most the clustering and classification tasks that have dominated the recent machine learning literature, learning in planning requires handling relational and first order representations, and foregrounds the need for knowledge-intensive learning techniques. I will start with a brief review of the planning models, and discuss the opportunities for learning in planning. I will then provide a survey of the explanation-based, case-based and inductive learning techniques that have been successfully used to tackle them.
1469 en Asymptotic Normality of a Nonparametric Instrumental Variables Estimator 
1470 en An Introduction to Instrumental Variables 
1471 en Nonparametric Estimation of the Regression Function in an Errors-in-variables 
1472 en Introduction to convex programming, interior point methods, and semi-definite programming 
1473 en Statistical Analysis of Non Injective Inverse Problems 
1474 en Estimation of the Solution of a Differential Equation: an Inverse Problem 
1475 en Learning Theory and Inverse Problems 
1477 en PASCAL Visualisation Challenge - Part 2 
1479 en Introduction to Kernel Methods The course will cover the basics of Support Vector Machines and related kerne methods: 1. Kernels and Feature Spaces \\ 2. Large Margin Classification \\ 3. Basic Ideas of Learning Theory \\ 4. Support Vector Machines \\ 5. Examples of Other Kernel Algorithms
1480 en A simple feature extraction for high dimensional image representations We investigate a method to find local clusters in low dimensional subspaces of high dimensional data, e.g. in high dimensional image descriptions. Using cluster centers instead of the full set of data will speed up the performance of learning algorithms for object recognition, and will possibly also improve performance because overfitting might be avoided.
1481 en Markov Chain Monte Carlo Methods 0. A fundamental theorem of simulation\\ 1. Markov chain basics\\ 2. Slice sampling\\ 3. Gibbs sampling\\ 4. Metropolis-Hastings algorithms\\ 5. Variable dimension models and reversible jump MCMC\\ 6. Perfect sampling\\ 7. Adaptive MCMC and population Monte Carlo
1484 en Boosting 
1487 en Modeling Dialectic 
1488 en A study on visual focus of attention modeling using head pose 
1489 en Classification of high dimensional data: High Dimensional Discriminant Analysis We propose a new method of discriminant analysis, called High Dimensional Discriminant Analysis (HHDA). Our approach is based on the assumption that high dimensional data live in di erent subspaces with low dimensionality. Thus, HDDA reduces the dimension for each class independently and regularizes class conditional covariance matrices in order to adapt the Gaussian framework to high dimensional data. This regularization is achieved by assuming that classes are spherical in their eigenspace. HDDA is applied to recognize objects in real images and its performances are compared to classical classi cation methods.
1490 en Integrating two features or kernels within one SVM classifier 
1491 en Letter-to-Phoneme Conversion Challenge - Part 2 
1492 en A statistical learning approach to subspace identification of dynamical systems Among the different approaches to identification of linear dynamical systems, subspace identification has become increasingly popular in the last decade. The reasons are the algorithmic simplicity thanks to the absence of non-convex optimization problems, the numerical stabil- ity and the statistical properties. Interestingly, concerning the statistical side, research in subspace identification has been concentrated on proving properties related to asymptotic unbiasedness. In this extended abstract we motivate how the use of an appropriate regularization can be helpful in the small sample case. Furthermore, this regularization allows one to use the kernel trick to identify systems where the input term in the state and output equations is a nonlinear function of the input variables.
1494 en Learning structured data via flow represented actions of support vector machines 
1500 en Statistical Learning Theory and Empirical Processes 
1506 en PASCAL Visualisation Challenge - Part 1 
1507 en Pattern Classification and Large Margin Classifiers These lectures will provide an introduction to the theory of pattern classification methods. They will focus on relationships between the minimax performance of a learning system and its complexity. There will be four lectures. The first will review the formulation of the pattern classification problem, and several popular pattern classification methods, and present general risk bounds in terms of Rademacher averages, a measure of the complexity of a class of functions. The second lecture will consider pattern classification in a minimax setting, and show that, in this setting, the Vapnik-Chervonenkis dimension is the key measure of complexity. The third lecture will focus on a theme of computational complexity. It will present the elegant relationship between the complexity of a class, as measured by its VC-dimension, and the computational complexity of functions from the class. This lecture will also review general results on the computational complexity of the pattern classification problem, and its tight relationship with that of an associated empirical risk optimization problems. The fourth lecture will consider large margin classification methods, such as AdaBoost, support vector machines, and neural networks, viewing them as convex relaxations of intractable empirical minimization problems. It will review several statistical properties of these large margin methods, in particular, a characterization of the convex optimization problems that lead to accurate classifiers, and relationships between these methods and probability models.
1509 en Kernels on histograms through the transportation polytope For two integral histograms and of equal sum, the Monge-Kantorovich distance MK(r,c) between r and c parameterized by a d ? d cost matrix T is the minimum of all costs <F,T> taken over matrices F of the transportation polytope U(r,c). Recent results suggest that this distance is not negative definite, and hence, through Schoenberg's well-known result, may not be a positive definite kernel for all t > 0. Rather than using directly MK to define a similarity between r and c, we present in this talk kernels on r and c based on the whole transportation polytope U(r,c). We prove that when r and c have binary counts, which is equivalent to stating that r and c depict clouds of points of equal size, the permanent of their Gram matrix induced by the cost matrix T is a positive definite kernel under favorable conditions on T. We also show that the volume of the polytope U(r,c), that is the number of integral transportation plans, is a positive definite quantity in r and c through the Robinson-Schensted-Knuth correspondence between transportation matrices and Young Tableaux.
1510 en Predicting Electricity Distribution Feeder Failures Using Boosting and Online Learning 
1511 en Selection of Basis Functions in Regression as Search Guided by the Evidence 
1512 en A Simple and Efficient Algorithm for Variable Ranking and Redundancy Detection 
1514 en Clustering of Brain Tumours Through Constrained Manifold Learning Using Class Information 
1515 en Splice form prediction using Machine Learning Accurate ab initio gene finding is still a major challenge in computational biology. We employ state-of-the-art machine learning techniques based on Hidden Semi-Markov-SVMs to assay and improve the accuracy of genome annotations. We applied our system, called mSplicer, on the Caenorhabditis elegans genome and were able to drastically improve its annotation.
1516 en Multi-Classification by Using Tri-class SVM 
1517 en Feature Selection and Causality Inference 
1518 en Patterns in sets of points: an overview 1 - "Patterns in sets of points: an overview" "We illustrate the importance of optimization principles in the search for interesting patterns, more in particular for patterns in sets of points embedded in a metric space. This talk will be a journey along the types of patterns in point sets that can efficiently be searched for, and general principles will be outlined. We provide examples from dimensionality reduction, classification, clustering, and others. The emphasis will be on patterns that can be expressed in terms of linear functions of the data."
1519 en The Acquisition of Propositional Logic Syntax 
1520 en Preliminary Experiments with On-Line Adaptive GARCH Models 
1521 en Bioinformatics Challenge: Learning in Very High Dimensions with Very Few Samples Dedicated machine learning procedures have already become an integral part of modern genomics and proteomics. However, these very high dimensional and low learning sample tasks often stretch these procedures well beyond natural boundaries of their applicability. A few such challenges will be a subject of this series of lectures. We will start with a brief overview of classification of genomics (microarray) data. In particular we shall discuss, in some detail, examples of applications to cancer genomics and proteomics. Then we concentrate on a phenomenon of anti-learning, a case of supervised classification where standard supervised learning techniques systematically produce classifiers perfect on learning sample but with independent test error rates higher than that of the default (random) classification rule. The examples of natural and synthetic anti-learning data will be given and analysed from the stand point of implications to practical supervised and unsupervised classification. A series of practical tutorials will be organized in parallel. Participants will be exposed to classification of microarray data including first-hand experience with anti-learning.
1526 en Anti-Learning The Biological domain poses new challenges for statistical learning. In the talk we shall analyze and theoretically explain some counter-intuitive experimental and theoretical findings that systematic reversal of classifier decisions can occur when switching from training to independent test data (the phenomenon of anti-learning). We demonstrate this on both natural and synthetic data and show that it is distinct from overfitting. The natural datasets discussed will include: prediction of response to chemo-radio-therapy for esophageal cancer from gene expression (measured by cDNA-microarrays); prediction of genes affecting the aryl hydrocarbon receptor pathway in yeast. The main synthetic classification problem will be the approximation of samples drawn from high dimensional distributions, for which a theoretical explanation will be outlined.
1531 en Measures of Statistical Dependence A number of important problems in signal processing depend on measures of statistical dependence. For instance, this dependence is minimised in the context of instantaneous ICA, in which linearly mixed signals are separated using their (assumed) pairwise independence from each other. A number of methods have been proposed to measure this dependence, however they generally assume a particular parametric model for the densities generating the observations. Recent work suggests that kernel methods may be used to find estimates that adapt according to the signals they compare. These methods are currently being refined, both to yeild greater accuracy, and to permit the use of the signal properties over time in improving signal separability. In addition, these methods can be applied in cases where the statistical dependence between observations must be maximised, which is true for certain classes of clustering algorithms.
1533 en Support Vector and Kernel Methods The lectures will introduce the kernel methods approach to pattern analysis through the particular example of support vector machines for classification. The presentation touches on: generalization, optimization, dual representation, kernel design and algorithmic implementations. We then broaden the discussion to consider general kernel methods by introducing different kernels, different learning tasks, and subspace methods such as kernel PCA. The aim is to give a view of the subject that will enable a newcomer to the field to gain his bearings so that they can move to apply or develop the techniques for their particular application.
1534 en Visualization 
1535 en Distributed Data Mining Data mining is the automated analysis of large volumes of data looking for relationships and knowledge that are implicit in data. Data mining and knowledge discovery in large amounts of data can benefit from the use of parallel and distributed computational environments to improve both performance and quality of data selection. The goal of this tutorial is to provide researchers and practitioners with an introduction to mining large data sets by exploiting techniques from high performance parallel and distributed computing.n n This tutorial is organized in two parts. In the first part an introduction to high performance parallel and distributed computing is provided. Different forms of parallelism that can be exploited in data mining techniques and algorithms are analyzed. The second part presents a review of distributed data mining approaches. For each data mining technique, different ways for parallel implementation are presented and discussed. Furthermore, parallel and distributed data mining systems and algorithms are discussed. Finally, current research issues and perspectives in high-performance data mining are outlined. n
1536 en Relational Data Mining and ILP 
1537 en Parallel session 4 - Hands-on section Data mining with R 
1538 en Student sessions - The Ecolead Project 
1540 en Bayesian Methods In the last decade probabilistic graphical models - in particularn Bayes networks and Markov networks - became very popular as toolsn for structuring uncertain knowledge about a domain of interest andn for building knowledge-based systems that allow sound and efficientn inferences about this domain. The core idea of graphical models isn that usually certain independence relations hold between the attributesn that are used to describe a domain of interest. In most uncertaintyn calculi -- and in particular in probability theory -- the structure ofn these independence relations is very similar to properties concerningn the connectivity of nodes in a graph. As a consequence, it is triedn to capture the independence relations by a graph, in which each noden represents an attribute and each edge a direct dependence betweenn attributes. In addition, provided that the graph captures only validn independences, it prescribes how a probability distribution on then (usually high-dimensional) space that is spanned by the attributesn can be decomposed into a set of smaller (marginal or conditional)n distributions. This decomposition can be exploited to derive evidencen propagation methods and thus enables sound and efficient reasoningn under uncertainty. The lecture gives a brief introduction into then core ideas underlying graphical models, starting from their relationaln counterparts and highlighting the relation between independence andn decomposition. Furthermore, the basics of model construction andn evidence propagation are discussed, with an emphasis on join/junctionn tree propagation. A substantial part of the lecture is then devotedn to learning graphical models from data, in which quantitative learningn (parameter estimation) as well as the more complex qualitative orn structural learning (model selection) are studied. The lecture closesn with a brief discussion of example applications.n
1541 en Stochastic Search Methods Knowledge discovery can be regarded as exploration of high-dimensional and multi-modal search spaces. However, finding the global optimum of an objective function with many degrees of freedom and numerous local optima is computationally demanding. Systems capable of learning and adaptivity therefore fundamentally rely on search techniques. These should sample the search space in such a way that there is a high probability of finding near-optimal solutions. Many search techniques have been developed and most of them are specialized to solve specific types of problems. An important class of search techniques are stochastic algorithms where certain steps are based on random choice. Most stochastic search algorithms were shown effective and efficient in finding near-optimal solutions to complex problems, but with no guarantee of finding true global optima. The presentation covers stochastic search techniques inspired by phenomena found in nature: simulated annealing and evolutionary algorithms. n n Simulated annealing is a popular stochastic algorithm designed in analogy with the physical process of cooling a molten substance where condensing of matter into a crystalline solid takes place. In this context, searching for an optimal solution is like finding a configuration of the cooled system with minimum free energy. Because of its ability of escaping from local optima, simulated annealing is a powerful algorithm for numerical and combinatorial optimization.n n Evolutionary algorithms are simplified models of the search processes of natural evolution. They simulate collective learning within an population of individuals that represent potential solutions to the considered problem. The population evolves towards better regions of the search space by means of stochastic transformations of individuals and feedback on their performance from the environment. n n Three variants of evolutionary algorithms are discussed: evolution strategies, genetic algorithms and genetic programming. Evolution strategies were proposed as a technique of evolutionary experimentation in engineering design and are nowadays used as numerical optimization algorithms. Genetic algorithms are general-purpose search and optimization algorithms based on string representation of candidate solutions and variation operators mimicking genetic processes in nature. Genetic programming is an extension of genetic algorithms designed to evolve not simple solutions to given problems, but computer programs to solve the problems. It is an important step towards automatic programming of computers and has already reached the stage of producing human-competitive solutions in a wide range of application domains.
1543 en Analysis of Time Series The study of time series is an essential aspect of Intelligent Data Analysis. The field is very broad, and it has been treated with very different methodological approaches, ranging from differential equations to stochastic models and to AI-based systems. The lesson will present time series analysis as a part of the general problem of modelling dynamical systems. The framework of systems’ theory will provide a general view of such problem, and it will permit to coherently overview the majority of the time series analysis approaches. In more detail, the principles of systems theory will be first discussed; the concept of “dynamical system” will be investigated and some results of systems theory will be presented. The notions of state, equilibrium, linearity, observability and reachability will be discussed. Some modelling tools will be then introduced, ranging from black-box to structural models. Stochastic linear and non linear models will be briefly described, including AR, MA, and ARMAX models. Moreover, a method to obtain structural information from input/output data will be introduced. The lesson will finally show how the knowledge on systems dynamics can be effectively exploited in the time series clustering problem. Distance-based, model-based and template-based will be revisited in order to account for information on the systems dynamics.n
1544 en Rule Induction 
1546 en Fuzzy Logic The tutorial will introduce the basics of fuzzy logic for data analysis. Fuzzy Logic can be used to model and deal with imprecise information, such as inexact measurements or available expert knowledge in the form of verbal descriptions. We will first introduce the concepts of fuzzy sets, degrees of membership and fuzzy set operators. After discussions on fuzzy numbers and arithmetic operations using them, the focus will shift to fuzzy rules and how such systems of rules can be derived from available data.n
1547 en Student sessions 
1549 en Statistical Methods These two sessions give an introduction to classical statistics.n n The first talk is a brief coverage of some basic statistical theory that I feel might be useful during the week. The topics included are: probability, likelihood inference, Bayesian inference and the concept of the bias variance tradeoff.n n The second talk covers two main areas of statistics, namely linear modelling and exploratory multivariate analysis. Linear modelling has an elaborate set of procedures for determining which of the potential inputs should be included in a model for predicting an output. The inputs can be of any data type (categorical or numerical), as can the output to some extent. This sort of modelling has been used successfully for many years on what would be considered quite small sets of data. Some linear models are now being used on much larger problems, for example, in the construction of scorecards for deciding whether to approve somebody’s application for a credit card. This is the primary type of statistical model that would be used in scientific research. Exploratory multivariate analysis is a collection of techniques that are all based on the same sort of mathematical background (vectors/matrix algebra). They are also all intended to investigate the structure of observations that are vectors. Some of these techniques are being used on a massive scale in business; for example, hierarchical cluster analysis is used to build (offline) classifications of all the postal codes (each code represents about 16 houses) in a system such as Mosaic which is then used for targeting of marketing. The specific techniques covered are: principal components analysis; correspondence analysis; scaling; cluster analysis.
1550 en Opening of the ACAI 05 Summer School 
1551 en Data Mining and Decision Support Integration The aim of this presentation is twofold: (1) to introduce the field of Decision Support (DS), and (2) to provide an overview of possible approaches and benefits of combining DS with Data Mining (DM) in solving real-life decision and data-analysis problems. Related to DS, we define the concepts of decision problem and decision-making, introduce the taxonomy of disciplines related to DS, overview the approach of decision analysis, introduce the method of multi-attribute modeling, and illustrate it through real-life examples of housing loan allocation and risk assessment in medicine. In the main part, we investigate the ways to combine and integrate DS and DM, which generally involve the following categories: (1) DS for DM, (2) DM for DS, (3) DM, then DS, (4) DS, then DM, and (5) DM and DS. Each category is illustrated by a practical example. Two categories are investigated in greater detail. The category “(1) DS for DM” is represented by a method for selecting a best DM-induced classifier based on ROC space exploration. For the category “(5) DM and DS”, we explore an approach of developing qualitative multi-attribute models by combining the systems DEX and HINT. DEX is a DS tool for expert-based (“hand-crafted”) development of models, whereas HINT is a DM tool that develops models from data by a method based on function decomposition.
1552 en Evaluation Methodology There is a variety of learning methods capable of inducing predictive models from data. In order to be able to decide which method to use on a particular data set of interest, we need systematic way to evaluate and compare the performance of different methods. This talk describes and illustrates the key criteria and methods for performance assessment and comparison. It first introduces predictive error as the most widely used criteria for evaluating predictive models. Since we want to evaluate predictive error of the model on independent test data, unseen in the learning process, the first part of the talk focuses on methods for evaluating predictive error on test data and resolving the well known bias-variance trade-off in machine learning. We also overview techniques for pair-wise comparison of learning methods' performance. While first part of the talk deals with classification task (i.e., predicting discrete variables) only, the second part of the talk provides wider perspective on evaluating methods for predicting class probability distribution, numeric variables, and dealing with situations where the error depends on type of the misclassification. Finally, we learn how to assess other aspects of predictive performance, such as complexity of the induced models and their comprehensibility.
1554 en Web mining 
1555 en Text mining 
1557 en Link analysis with pajek Pajek is a program (for Windows) for large network analysis and visualization. It is freely available for noncommercial use at [[http://vlado.fmf.uni-lj.si/pub/networks/pajek/|http://vlado.fmf.uni-lj.si/pub/networks/pajek/]] Besides ordinary networks Pajek supports also multi-relational and temporal networks. In large network analysis we are often interested in important parts of given network. There are several ways how to determine them. The islands approach is based on an importance measure of vertices or lines. Let (V,L,p) be a network with vertex property p : V ? R and let t be a real number. If we delete all vertices (and corresponding links) with the property value less than t, we get subnetwork called vertex-cut at level t. The number and sizes of its components depend on t. Often we consider only components of size at least k and not exceeding K. The components of size smaller than k are discarded as noninteresting, while the components of size larger than K are cut again at some higher level. Vertex-island is a connected subnetwork which vertices have greater property value than the vertices in its neighborhood. It is easy to see that the components of vertex-cuts are all vertex-islands. We developed an efficient algorithm that identifies all maximal vertex-islands of sizes in the interval k..K in a given network. For networks with weighted lines we can similarly define line-islands. The line-islands algorithm is based on line-cuts. Both algorithms are very general - they can be applied for any vertex/line importance measure. Their complexity is for sparse networks subquadratic - they can be applied to very large networks. We will illustrate them applying different importance measures on selected (large) networks. We will also present the use of pattern searching in analysis of genealogies and some approaches to analysis of (multi-relational) temporal networks.
1558 en Handling noisy data In the practice of machine learning, learning data typically contain errors. Imperfections in data can be due to various, often unavoidable causes: measurement errors, human mistakes, errors of expert judgement in classifying training examples etc. We refer to all of these as noise. Noise can also come from the treatment of missing values, when an example with unknown attribute value is replaced by a set of weighted examples corresponding to the probability distribution of the missing value. n The typical consequences of noise in learning data are: (a) low prediction accuracy of induced hypotheses on new data, and (b) large hypotheses that are hard to interpret and to understand by the user. For example, decision trees with hundreds or thousands of nodes are not suitable for interpretation by the domain expert. We say that such complex hypotheses overfit the data. Overfitting occurs when the hypothesis not only reflects the genuine regularities in the domain, but it also traces noise in data. n To alleviate the harmful effects of noise, we have to prevent overfitting. To do this, one common idea is to simplify induced hypotheses. In the learning of rules or decision trees, this leads to tree pruning or rule truncation. The main question in hypothesis simplification is: How can we know that our hypothesis is of “the right size”, not too simple and not too complex? For example in tree pruning, when should we stop the pruning? The decision can be based on the estimated accuracy of a hypothesis before pruning and after pruning, and then the estimated accuracy is maximised. However, estimating the accuracy can be difficult, and involves the problem of estimating probabilities from small samples. Several methods for this will be discussed in this lecture, and the effects of simplification will be illustrated. A somewhat related approach of deciding about the “right size” of a hypothesis is based on the minimum description length principle (MDL). Another way of reducing the effects of noise is to use background or prior knowledge about the domain of learning. For example, in the learning from numerical data, a useful idea is to make the learning algorithm respect the known qualitative properties of the target concept.
1559 en Attribute estimation One of crucial tasks in machine learning is the evaluation of the quality of attributes. For that purpose a number of measures have been developed that estimate the usefulness of the attribute for predicting the target variable. We will describe separately measures for classification (which are appropriate also for relational problems) and for regression. Most of the measures estimate the quality of one attribute independently of the context of other attributes. However, algorithm ReliefF and its regressional version RReliefF take into account also the context of other attributes and are therefore appropriate for problems with strong dependencies between attributes. The following measures will be described:n - Measures for guiding the search in classification and relational problems are: information gain, Gain ratio, distance measure, minimum description length (MDL), J-measure, Gini-index and ReliefF.n - The quality of attributes in regression can be evaluated using the following measures: expected change of variance, regressional ReliefF, and minimum description length principle (MDL).n
1566 en Multimedia representation standards 
1570 en Introduction to Multimedia Digital Libraries 
1573 en Feature extraction & content description I 
1574 en Cross-modal analysis 
1577 en Machine learning for access and retrieval I 
1579 en Feature extraction & content description II 
1585 en Evaluation and benchmarking 
1589 en Machine learning for access and retrieval II 
1605 en Supply Network Shannon 
1606 en Existing Living Lab Initiatives, Best Practice overview - Our Example and Experience 
1607 en CORELABS support to European RTD projects, initiatives and stakeholders 
1614 en The Emerging discipline of Collaborative Networks 
1615 en Basic Concepts of Game Theory 
1616 en Portfolio of Tools - Application 
1617 en VBE Scenario 2 presentation 
1618 en VBE Scenario 1 presentation 
1619 en VBE Scenario 3 presentation - Demonstration Activities ITESM - IECOS 
1621 en VOM Scenario 1 presentation 
1625 en VBE intro 
1626 en Txt Skillplan 
1627 en Joensuu Science Park Ltd. 
1629 en Graph Theory contributions forCNOs modeling and analysis 
1631 en Distributed Loosely - Controlled and Evolving Ontology Engineering 
1632 en CNO successful case study – Treviso Technologie Cluster 
1633 en ECOLEAD; An Overview 
1634 en PVC 
1635 en VO 
1636 en VBE 
1637 en Concept of Demonstration 
1638 en Training activities 
1639 en ICT-I 
1640 en Virtual Breeding Environment to Support Collaboration in the Aeronautical Supplie Chain 
1641 en Interactive Session "Barriers and Solutions" 
1643 en Formal Theories and modeling in CNOs 
1644 en A Formal Theory Of Bm Virtual Enterprises Structures 
1645 en A Distributed Knowledge Base For Manufacturing Scheduling 
1646 en Efficiently Managing Virtual Organizations Through Distributed Innovation Management Processes 
1648 en Model-based Integration of Business and technology 
1649 en Sme-Service Networks For Cooperative Operation Of Robot Installations 
1650 en Chaordic Systems Thinking: An Introduction of Chaos and Complexity in Organisations and Management 
1651 en Experimential Learning Session – Parallel working groups 
1652 en The aim of this workshop - Multipliers 
1653 en Information Infrastructures And Sustainability 
1655 en EVCN Engeneer Virtual Community Network - PVC for Local Developement 
1656 en An overview of trust and trust building in networks 
1657 en Innovative working environment: Network Oasis project 
1658 en "CORELABS" Living Labs definition and concept 
1659 en "VEN" The Story so Far 
1660 en Living Labs Emerging Practices 1 
1661 en Trust building models and self-organizing systems / complexity theory 
1662 en ICT support for Virtual Organization Management 
1663 en Supply Network Shannon 
1665 en Semiotics in CNOs 
1671 en ECOLEAD - PVC in HR Management for AIESEC AlumniAIESEC Demonstration Scenario/ WP4 
1673 en Joensuu Science Park Ltd. 
1674 en Welcome to Training - Brussels 06 
1681 en PVC Scenario 1 presentation 
1682 en PVC Scenario 2 presentation 
1684 en Virtual Organizations Breeding Environment - Concepts, expected results, intermediate results 
1685 en Potential cross-border cluster opportunities 
1686 en Virtual Organizations Management - Concepts, expected results, intermediate results 
1693 en Knowledge Based Virtual Communities for Collaborative work in the EU Vehicle Repair Works 
1695 en Sumatra TT - Demonstration 
1696 en One Class formula - Human Robot interaction 
1699 en Presentation of the future on CNO research 
1700 en Introducing time horizons to enterprise networking architecture 
1701 en The “Concurrent Innovation” paradigm for Integrated Product/Service Organisation (PSO) Development 
1702 en AMI communities, Living Labs, collaborative working environments and ICTs for enterprises - birth, development and future 
1704 en Principles of Benchmarking and Application in the VO Context 
1706 en PVC: Professional Virtual Communities 
1707 en Summary of Main Challenges for a Stram VBE - The Virtual Factory´s Practical Experience 
1708 en Towards a meta-methodology for collaborative networked organisations 
1709 en The typed domain - a recipe for creating Virtual Enterprises 
1712 en SLO-ITA cross-border clustering opportunities 
1713 en Security infrastructures for CNs 
1714 en Integrated Project Validation approaches in the sense of Living Labs - Collaboration@rural 
1716 en Emerging technologies for CNO supporting infrastructures 
1717 en VOM in relation to VBE & PVC 
1718 en Social protocols in Professional Virtual Communitie 
1719 en PVC basic concepts & typologies 
1720 en Value System & metrics 
1721 en Service Oriented Architectures 
1723 en Demonstration activities Methodology 
1729 en Java library for support of text mining and retrieval 
1730 en PVC for local development 
1731 en Debate - ECOLEAD Brussels meeting 
1732 en Unknown title 
1733 en Agent Technology for Virtual Enterprises 
1737 en Performance Management in VOs 
1738 en Extended Products in VOs 
1739 en Professional Virtual Communities / Concepts, expected results, intermediate results 
1740 en Sumatra TT - Towards a universal data preprocessor 
1743 en CNO trends and emerging collaborative forms 
1744 en CNO Base Concepts 
1745 en e-Contracting 
1746 en VO Breeding Environments & competency management 
1747 en Cross-border collaboration - strenghtening the europen competitiveness 
1749 en Co-Operation Management - CeBeNetwork GmbH 
1750 en ORONA Innovation Network (OIN) Part 1 - Introduction to the network 
1751 en CNO successful case study - Virtuelle Fabrik 
1752 en Conceptual Framework for Collaborative Design and Operations 
1753 en Automotive Cluster of Slovenia - Presentation, collaborative opportunities 
1754 en Torino Wireless Cluster 
1755 en ISOIN-VBE Demonstration in the Andalusian Aeronautical cluster 
1759 en Data Integration using Semantic Technology: a Use Case 
1760 en Integrated Access to Biological Data: a Use Case 
1761 en Matching Hierarchical Classifications with Attributes 
1762 en Toward Large-Scale Shallow Semantics for Higher-Quality NLP Building on the successes of the past decade’s work on statistical methods, there are signs that continued quality improvement for QA, summarization, information extraction, and possibly even machine translation require more-elaborate and possibly even (shallow) semantic representations of text meaning. But how can one define a large-scale shallow semantic representation system and contents adequate for NLP applications, and how can one create the corpus of shallow semantic representation structures that would be required to train machine learning algorithms? This talk addresses the components required (including a symbol definition ontology and a corpus of (shallow) meaning representations) and the resources and methods one needs to build them (including existing ontologies, human annotation procedures, and a verification methodology). To illustrate these aspects, several existing and recent projects and applicable resources are described, and a research programme for the near future is outlined. Should NLP be willing to face this challenge, we may in the not-too-distant future find ourselves working with a whole new order of knowledge, namely (shallow) and doing so in increasing collaboration (after a 40-years separation) with specialists from the Knowledge Representation and reasoning community.
1763 en An Infrastructure for Acquiring High Quality Semantic Metadata 
1764 en Extracting Instances of Relations From Web Documents Using Redundancy 
1765 en Empiric Merging of Ontologies - A Proposal of Universal Uncertainty Representation Framework 
1766 en Community-Driven Ontology Matching 
1767 en Knowledge Management in the Petroleum Industry 
1768 en Contextual Intelligence for Mobile Services through Semantic Web Technology 
1769 en Semantic Laboratory Notebook: Managing biomedical research notes and experimental data 
1770 en Semantic web technology Roadmap 
1771 en Semantic Business Automation 
1772 en News Agency needs: XML News 
1774 en Use of Ontology for production of access systems on Legislation Jurisprudence and Comments 
1776 en Interview with Denny Vrandecic 
1777 en Interview with York Sure 
1778 en Interview with Aldo Gangemi 
1781 en Interview with John Domingue 
1782 en Annotation for the Semantic Web 
1784 en Interview with Eduard Hovy 
1786 en Semantic Web Service Systems - Part 2 The proposed tutorial presents the Web Service Execution Environment WSMX and the Internet Reasoning Server IRS as an integrated environment for development and execution of Semantic Web Services on basis of the Web Service Modeling Ontology WSMO.
1787 en Automatic Extraction from Hierarchical Relations From Text This tutorial will cover the hot topic of multimedia and the Semantic Web with focus on multimedia search engines, automatic semantic annotation of multimedia, and use of semantic web tools in the production of new media formats. The tutorial will also cover some open-source tools, thus enabling the participants to put easily their newly learned skills into practice. The material to be presented will include the latest research results from several European projects on multimedia and semantically-enabled knowledge technologies (Prestospace, NM2, MediaCampaign, and SEKT).
1788 en Dinamic Assembly of personalised Learning Content on the Semantic web 
1789 en Interactive Ontology-based User Knovledge 
1790 en Is Semantic Web technology ready for Healthcare? 
1791 en Issues and strategies for digitization and preservation of the digital content 
1792 en Interview with Jerome Euzenat 
1793 en Usability and the Semantic Web In addition to its technical implications, the semantic web vision gives rise to some challenges concerning usability and interface design. What difficulties can arise when persons with little or no relevant training try to * formulate knowledge (e.g., with ontology editors or annotation tools) in such a way that it can be exploited by semantic web technologies; * leverage semantic information while querying or browsing? What strategies have been applied in an effort to overcome these difficulties, and what are the main open issues that remain? This talk will address these questions, referring to examples and results from a variety of research efforts, including the project SemIPort, which concerns semantic methods and tools for information portals, and Halo 2, in which tools have been developed and evaluated that enable scientists to formalize and query college-level scientific knowledge.
1794 en Interview with Frank van Harmelen 
1796 en Content Aggregation on Knowledge Bases Using Graph Clustering 
1797 en Interview with Rudi Studer 
1798 en Interview with Aljosa Pasic 
1799 en Interview with John Davies 
1800 en Semantic Web(s) and Language Technology 
1801 en Software AG 
1802 en Training Management Sistems for Aircraft Engeneering 
1803 en Semantic Network Analysis of Ontologies - Part 1 
1804 en Where Does It Break? Or: Why the Semantic Web is Not Just 'Research as Usual' Work on the Semantic Web is all to often phrased as a technological challenge: how to improve the precision of search engines, how to personalise web-sites, how to integrate weakly-structured data-sources, etc. This suggests that we will be able to realise the Semantic Web by merely applying (and at most refining) the results that are already available from many branches of Computer Science. I will argue in this talk that instead of (just) a technological challenge, the Semantic Web forces us to rethink the foundations of many subfields of Computer Science. This is certainly true for my own field (Knowledge Representation), where the challenge of the Semantic Web continues to break many often silently held and shared assumptions underlying decades of research. With some caution, I claim that this is also true for other fields, such as Machine Learning, Natural Language Processing, Databases, and others. For each of these fields, I will try to identify silently held assumptions which are no longer true on the Semantic Web, prompting a radical rethink of many past results from these fields.
1805 en Semantic Network Analysis of Ontologies - Part 2 
1806 en Benchmark Suites for Improving the RDF(S) Importers and Exporters of Ontology Development Tools 
1807 en Modeling Ontology Evaluation 
1809 en Introduction 
1810 en Interview with Sinuhe Arroyo 
1811 en Interview with Natasa Frayling-Milic 
1812 en Interview with Anthony Jameson 
1813 en Interview with Pascal Hitzler 
1814 en Semantic Web Service Systems - Part 1 The proposed tutorial presents the Web Service Execution Environment WSMX and the Internet Reasoning Server IRS as an integrated environment for development and execution of Semantic Web Services on basis of the Web Service Modeling Ontology WSMO.
1831 en Flexible XML Retrieval using Summaries 
1832 en Compact indexing of versioned data 
1833 en Music of the (p)Spheres This lecture is talking about **Nearest Neighbours** //Once upon a time...// **//Musica universalis//**//** **or **music of the spheres** is a medieval\\ philosophical concept that regards the proportions in the movements of\\ the** celestial bodies** - the **Sun, Moon** and planets - as a form of **musica**,\\ the medieval **Latin** name for **music**. This music was not thought of as\\ an audible **sound**, but simply as a **mathematical concept**. The **Greek**\\ philosopher **Pythagoras** was frequently credited with originating the\\ concept, which stemmed from his semi-**mystical**, semi-**mathematical**\\ philosophy and its associated system of **numerology **of\\ **Pythagoreanism**. // //Some **Surat Shabda Yoga, Satgurus** considered the\\ music of the spheres to be a term synonymous with the Shabda or the\\ Audible Life Stream in that tradition, because they considered\\ **Pythagoras **to be a Satguru as well.//
1834 en Mixture Models and Collaborative Filtering Algorithms 
1835 en Efficient Lazy Algorithms for Minimal-Interval Semantics 
1836 en Efficient Top-k Queries for XML Information Retrieval 
1837 en Applications of Influence Diagrams to Information Retrieval 
1838 en "Tuning": Error Optimisation in Ad-Hoc Retrieval 
1839 en Ongoing research on sentence retrieval and novelty detection 
1840 en XML Compression and Search 
1841 en (Semantic) Structure in Structured Document Retrieval 
1842 en Web mining for natural language engineering tasks 
1843 en From query based Information Retrieval to context driven Information Supply 
1844 en Applications of Query Mining 
1845 en Boosting Performance of Web Search Engines Using Query Logs 
1846 en Introduction 
1847 en Current Approaches to Personalized Web Search 
1848 en Image Search "Live" 
1849 en Using Rank Propagation and Probabilistic Counting for Link-based Spam Detection 
1850 en Theoretical analysis of Link Analysis Ranking 
1851 en Graph Fibrations, graph isomorphism and PageRank 
1852 en Estimating Corpus Size via Queries 
1853 en Searching the Web with Low Space Approximations 
1854 en Mobile Search on Ubiquitous Collaborative Annotations of Space 
1855 en Efficient and Decentralized PageRank Approximation in a P2P Web Search Network 
1856 en Semantic Overlay Networks for P2P Web Search 
1857 en Exploiting Temporal Features for Structured Queries 
1859 en GATE and IBM´s UIMA - interoperability layer 
1860 en Finite state transduction for Information Extraction and other tasks: ANNIE, JAPE - Part 3 
1861 en Towards GATE version 4 
1862 en Information Retrieval in GATE 
1863 en The GATE GUI 
1864 en Finite state transduction for Information Extraction and other tasks: ANNIE, JAPE - Part 1 
1865 en Working with ontologies 
1866 en Corpora, evaluation tools 
1867 en The ANNIC Concordancer 
1868 en Introduction: GATE after 10 years 
1869 en Development eco-system: building components, usage in Eclipse, unit tests 
1870 en Annotation Factories and GLEAM 
1871 en GATE API´s, CREOLE lifecycle, JAVA for JAPE 
1875 en Finite state transduction for Information Extraction and other tasks: ANNIE, JAPE - Part 2 
1876 en Towards JAPE 4 
1878 en Interview with Ali Imitaz 
1879 en Interview with Fijavz Renato 
1880 en Interview with Richard Stevens 
1881 en Interview with Rainer Kuhn 
1883 en Web Content Mining with Human Language Technologies: Acquiring Ontological Relationships from Wikipedia Using RMRS 
1884 en Web Content Mining with Human Language Technologies: Mining and Assessing Discussions on the Web through Speech Act Analysis 
1885 en The Semantic Web and Networked Governance: Promise and Challenges The virtual state is a metaphor meant to draw attention to the structures and processes of the state that are becoming increasingly aligned with the structures and processes of the semantic web. Semantic Web researchers understand the potential for information sharing, enhanced search, improved collaboration, innovation, and other direct implications of contemporary informatics. Yet many of the broader democratic and governmental implications of increasingly networked governance remain elusive, even in the world of public policy and politics.
1886 en In-Use 1: OntoWiki - A Tool for Social, Semantic Collaboration 
1888 en Where the Social Web Meets the Semantic Web The Semantic Web is an ecosystem of interaction among computer systems. The social web is an ecosystem of conversation among people. Both are enabled by conventions for layered services and data exchange. Both are driven by human-generated content and made scalable by machine-readable data. Yet there is a popular misconception that the two worlds are alternative, opposing ideologies about how the web ought to be. Folksonomy vs. ontology. Practical vs. formalistic. Humans vs. machines. This is nonsense, and it is time to embrace a unified view. I subscribe to the vision of the Semantic Web as a substrate for collective intelligence. The best shot we have of collective intelligence in our lifetimes is large, distributed human-computer systems. The best way to get there is to harness the "people power" of the Web with the techniques of the Semantic Web. In this presentation I will show several ways that this can be, and is, happening.
1889 en In-Use 2: Information Integration via an End-to-End Distributed Semantic Web System 
1890 en In-Use 1: NEWS: bringing Semantic Web Technologies into News Agencies 
1891 en Research 12: IRS-III: A Broker for Semantic Web Services based Applications 
1892 en Research 12: A Software Engineering Approach to Design and Development of Semantic Web Service Applications 
1895 en Research 8: A Browser for Heterogeneous Semantic Web Repositories 
1896 en Research 12: RS2D: Fast Adaptive Search for Semantic Web - Services in Unstructured P2P Networks 
1897 en Learning from the Masters: Understanding Ontologies found on the Web - Part 7 The purpose of this tutorial is to help attendees gain sufficient experience of working with OWL and tools to allow them to fruitfully explore new ontologies that they may encounter. In other words, they should be able to do the equivalent of “view source” on an ontology. Also, they will get better fluency in the use and abuse of OWL by examining features, limitations, and workarounds in real contexts, as well as gaining an understanding of the impact of future extensions of OWL, in particular of rules and the proposed revision of the language called OWL 1.1.
1898 en Opening Welcome 
1899 en Interview with Tom Gruber **Tom Gruber is an innovator in technologies that extend human intelligence.** Building on early work in computer-mediated learning and artificial intelligence, he focuses on creating environments for collective intelligence. We discussed with him at he International Semantic Web Conference 2006 in Athens, Georgia in the USA.
1900 en Interview with Leo Sauermann 
1901 en Interview with Stefan Decker 
1902 en Interview with Massimo Paolucci 
1903 en Interview with Benjamin Grosof 
1905 en Interview with Bijan Parsia Bijan Parsia is a lecturer at the University of Manchester (UK) in the School of Computer Science. We discussed with him at the ISWC 2006 in Athens, GA in the USA. *What is your current topic of research? *Future research? *Semantic web dream come true? *Comments on the tutorial and messages to the community?
1906 en Context Sensitivity in Knowledge Rich Systems The main goal of this tutorial is to provide an extensive survey of the past and current work in the area of context related topics. This includes analysis of the past work: (1) defining the notion of “context”, (2) present logic-based formalisms for dealing with contexts, (3) present probabilistic/fuzzy approaches to model context, (4) demonstrate “modelling the context” and “reasoning with contexts” in real-life applications. In addition, the presented work we will provide a synthesis of the past work in the light of a unified categorization of context-related approaches along several dimensions which appear as relevant from theoretical and practical point of view.
1907 en Web Content Mining with Human Language Technologies: Concept-Instance Relation Extraction from Simple Noun Sequences Using a Full-Text Search Engine 
1908 en Web Content Mining with Human Language Technologies: Constructing Dictionaries for Named Entity Recognition on Specific Domains from the Web 
1909 en Interview with Kook Han Sung 
1910 en Interview with Atanas Kiryakov 
1911 en Interview with Marja Riitta Koivunen 
1912 en Interview with Tony Stuart 
1913 en Interview with Rudi Studer 
1918 en Workshop: Enhancing Data and Processes Integration and Interoperability in Emergency Situations: a SWS based Emergency Management System 
1919 en Workshop: Personalized Question Answering: A Use Case for Business Analysis 
1920 en Workshop: Spatial Integration of Semantic Web Services: the e-Merges Approach 
1921 en Workshop: Conceptual Search: Incorporating Geospatial Data Into Semantic Queries 
1922 en Workshop: Discussions on elements of a Geo Working Group charter 
1923 en Workshop: GRDF 
1924 en Semantic Web Rules with Ontologies, and their E-Services Applications Rules are a main emerging area of the Semantic Web. There has been significant progress in just the last three years in several aspects of Semantic Web rules. This includes exciting developments in the underlying knowledge representation formalisms as well as advances in integration of rules with ontologies; translations between heterogeneous commercial rule engines; development of open-source tools for inferencing and interoperability; standards proposals and efforts (including RuleML, SWRL, Semantic Web Service Framework, and recently W3C Rule Interchange Format); proposals for rule-based semantic Web services; and pilot applications in the emerging area of e-services. This tutorial will provide an introduction to these developments and will explore techniques, applications, and challenges. We will also touch upon the issues of business value, adoption, investment, and strategy considerations.
1926 en Semantic Authoring and Annotation: The Dynamics and Semantics of Collaborative Tagging 
1927 en Semantic Authoring and Annotation: Cross-media Document Annotation and Enrichment 
1928 en The Semantic Web: Suppliers and Customers The notion of the Semantic Web can be coined as a Web of data when bringing database content to the Web or as a Web of enriched human-readable content when encoding the semantics of web-resources in a machine-interpretable form. \\ It has been clear from the beginning that realizing the Semantic Web vision will require interdisciplinary research. At this the fifth ISWC, it is time to re-examine the extent to which interdisciplinary work has played and can play a role in Semantic Web research, and even how Semantic Web research can contribute to other disciplines. Core Semantic Web research has drawn from various disciplines, such as knowledge representation and formal ontologies, reusing and further developing their techniques in a new context.
1929 en Learning from the Masters: Understanding Ontologies found on the Web The purpose of this tutorial is to help attendees gain sufficient experience of working with OWL and tools to allow them to fruitfully explore new ontologies that they may encounter. In other words, they should be able to do the equivalent of “view source” on an ontology. Also, they will get better fluency in the use and abuse of OWL by examining features, limitations, and workarounds in real contexts, as well as gaining an understanding of the impact of future extensions of OWL, in particular of rules and the proposed revision of the language called OWL 1.1. [[rimg:iswc06_parsia_uofw]] ;This lecture given by Bernardo Cuenca Grau is combined with Bijan Parsia and will encopass Part 1, Part 3, Part 4 of the complete lecture. :Part 2, 5, 6 and 7 of this lecture can be found at [[iswc06_parsia_uofw|//Bijan Parsia 's lecture//]]
1931 en Context Sensitivity in Knowledge Rich Systems The main goal of this tutorial is to provide an extensive survey of the past and current work in the area of context related topics. This includes analysis of the past work: (1) defining the notion of “context”, (2) present logic-based formalisms for dealing with contexts, (3) present probabilistic/fuzzy approaches to model context, (4) demonstrate “modelling the context” and “reasoning with contexts” in real-life applications. In addition, the presented work we will provide a synthesis of the past work in the light of a unified categorization of context-related approaches along several dimensions which appear as relevant from theoretical and practical point of view.
1933 en Learning from the Masters: Understanding Ontologies found on the Web The purpose of this tutorial is to help attendees gain sufficient experience of working with OWL and tools to allow them to fruitfully explore new ontologies that they may encounter. In other words, they should be able to do the equivalent of “view source” on an ontology. Also, they will get better fluency in the use and abuse of OWL by examining features, limitations, and workarounds in real contexts, as well as gaining an understanding of the impact of future extensions of OWL, in particular of rules and the proposed revision of the language called OWL 1.1. [[rimg:iswc06_grau_uofwi]] ;This lecture given by Bijan Parsia is combined with Bernardo Cuenca Grau and will encopass Part 2, Part 5, Part 6, Part 7 of the complete lecture. :Part 1, 3 and 4 of this lecture can be found at [[iswc06_grau_uofwi|//Bernardo Cuenca Grau's lecture//]]
1934 en Workshop: Improving the recruitment process through ontology-based querying 
1935 en Workshop: OntoCAT: An Ontology Consumer Analysis Tool and Its Use on Product Services Categorization Standards 
1936 en Research 8: Extending faceted navigation for RDF data 
1937 en In-Use 2: Explaining Conclusions from Diverse Knowledge Sources 
1938 en Research 3: Formal Model for Ontology Mapping Creation 
1939 en In-Use 1: Semantic web technology for expert knowledge sharing and discovery 
1940 en Research 3: Three Semantics for Distributed Systems and their Relations with Alignment Composition 
1941 en Research 3: Towards Knowledge Acquisition from Information Extraction 
1942 en In-Use 2: Semantic Desktop 2.0: The Gnowsis Experience 
1943 en Research 8: Fresnel: A Browser-Independent Presentation Vocabulary for RDF 
1946 en Research 16: Tree-structured Conditional Random Fields for Semantic Annotation 
1947 en Research 16: Evaluating Conjunctive Triple Pattern Queries over Large Structured Overlay Networks 
1948 en Research 13: Using Ontologies for Extracting Product Features from Web Pages 
1949 en Research 16: On How to Perform a Gold Standard Based Evaluation of Ontology Learning 
1950 en Research 6: Ontology Query Answering on Databases 
1951 en Research 13: Crawling and Indexing Semantic Web Data 
1952 en Research 5: Ontology-driven Information Extraction with OntoSyphon 
1953 en Research 5: A Framework for Schema-Driven Relationship Discovery from Unstructured text 
1954 en Web Content Mining with Human Language Technologies: Welcoming and Introduction 
1955 en Web Content Mining with Human Language Technologies: Instance Classification using Co-Occurrences on the Web 
1956 en Semantic Authoring and Annotation: Semantic Authoring by Tagging with Annotea Social Bookmarks and Topics 
1957 en Semantic Authoring and Annotation: A Browser-based Tool for Collaborative Distributed Annotation for the Semantic Web 
1958 en Semantic Authoring and Annotation: A Semi-Automatic Semantic Annotation and Authoring Tool for a Library Help Desk Service 
1959 en Semantic Authoring and Annotation: Using WEESA to Semantically Annotate Cocoon Web Applications 
1960 en Research 16: A Relaxed Approach to RDF Querying 
1961 en Research 4: ONTOCOM: A Cost Estimation Model for Ontology Engineering 
1962 en Research 15: Provenance Explorer – Tailored Provenance Views Using Semantic Inferencing 
1963 en Research 15: Semantic Metadata Generation for Large Scientific Workflows 
1964 en Research 17: Integrating and Querying Parallel Leaf Shape Descriptions 
1965 en Research 15: Automatic Annotation of Web Services based on Workflow Definitions 
1966 en Research 17: Mining Information for Instance Unification 
1967 en Research 17: A Method for Learning Part-Whole Relations 
1968 en Research 1: Extracting Relations in Social Networks from Web using Similarity between Collective Contexts 
1969 en Research 1: Innovation Detection based on User-Interest Ontology of Blog Community 
1970 en Research 5: Ontology-Driven Automatic Entity Disambiguation in Unstructured Text 
1971 en Research 1: Modeling Social Attitudes on the Web 
1972 en Web Content Mining with Human Language Technologies: Coreference resolution on RDF Graphs generated from Information Extraction: first results 
1973 en Interview with Martin Dzbor 
1974 en Interview with Tim Berners Lee - inventor of the WWW **Sir Timothy John "Tim" Berners-Lee**, \\ - the inventor of the World Wide Web, \\ - director of the World Wide Web Consortium (which oversees its continued development), \\ - and a senior researcher and holder of the 3Com Founders Chair at MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) ; :Source: [[http://en.wikipedia.org/wiki/Tim_Berners-Lee|Wikipedia]]
1975 en Interview with Peter Fox 
1976 en Interview with Jane Fountain Fountain is the author of Building the Virtual State: Information Technology and Institutional Change (Brookings Institution Press, 2001) which was awarded an Outstanding Academic Title in 2002 by Choice. The book has become a classic text in the field and has been translated into and published in Chinese, Japanese and Portuguese. Fountain is currently researching the successor volume to Building the Virtual State, which will examine technology-based cross-agency innovations in the U.S. federal government and their implications for governance and democratic processes, and Women in the Information Age (to be published by Cambridge University Press), which focuses on gender, information technology, and institutional behavior. Professor Fountain also directs the Science, Technology, and Society Initiative (STS) and the Women in the Information Age Project (WITIA). The STS Initiative serves as a catalyst for collaborative, multi-disciplinary research partnerships among social, natural and physical scientists. WITIA examines the participation of women in computing and information-technology related fields and, with its partner institutions, seeks to increase the number of women experts and designers in information and communication technology fields. She has served on several governing bodies and advisory groups in the public, private and nonprofit sectors in the U.S. and abroad. Her executive teaching and invited lectures have taken her to several developing countries and governments in transition including those of Saudi Arabia, the United Arab Emirates, Nicaragua, Chile, Estonia, Hungary, and Slovenia as well as to countries including Japan, Canada, New Zealand, Australia and the countries of the European Union.
1977 en Semantic Authoring and Annotation: Welcoming and Introduction 
1978 en Semantic Authoring and Annotation: Specifying the Collaborative Tagging System 
1979 en In-Use 4: Ontogator --- A Semantic View-Based Search Engine Service for Web Applications 
1980 en In-Use 4: Active Semantic Electronic Medical Record 
1981 en In-Use 4: A Mixed Initiative Semantic Web Framework for Process Composition 
1983 en In-Use 3: Enabling an Onlince Community for Sharing Oral Medicine Cases Using Semantice Web Technologies 
1984 en In-Use 3: Towards Semantic Interoperability in a Clinical Trials Management System 
1985 en In-Use 3: Semantically-Enabled Large-Scale Science Data Repositories 
1986 en In-Use 4: Construction and Use of Role-ontology for Task-based Service Navigation System 
1987 en Context Sensitivity in Knowledge Rich Systems The main goal of this tutorial is to provide an extensive survey of the past and current work in the area of context related topics. This includes analysis of the past work: (1) defining the notion of “context”, (2) present logic-based formalisms for dealing with contexts, (3) present probabilistic/fuzzy approaches to model context, (4) demonstrate “modelling the context” and “reasoning with contexts” in real-life applications. In addition, the presented work we will provide a synthesis of the past work in the light of a unified categorization of context-related approaches along several dimensions which appear as relevant from theoretical and practical point of view.
1988 en Industry 3: Semantic Web @ W3C: Activities, Recommendations and State of Adoption The presentation presents the status of the Semantic Web from W3C's perspective, referring to the finished and active works in terms of W3C groups, view of available tools, and ideas floating around for possible future work. References and examples of real Semantic Web applications will also show the widening adoption of the technology by the industrial community.
1989 en Industry 3: How Co-Occurrence can Complement Semantics? Analysis of texts is an obvious way for semantic annotation and extraction of structured knowledge. A basic task is the recognition of references to entities (people, locations, organizations, etc). A next step is relation extraction, e.g. identifying that an organization is located in a particular city. Automatic extraction of such relations is a tough linguistic problem - the solutions are either very partial, expensive to implement, or slow. On the other hand, relationships are crucial for the usability of the extracted knowledge for navigation and search purposes. We demonstrate how efficient co-occurrence analysis, performed on top of semantic annotation, can be used for several purposes: relation extraction, faceted search, and popularity timelines. The faceted search interface allows an easy way for augmenting full-text search by means of entity references, derived through co-occurrence profiling and semantic relationships. Although this sort of analytics can be used in virtually any domain, their development within the KIM platform was driven by the requirements for news analysis and research. We demonstrate the usage of these interfaces on top of 1 million news articles - a corpus of the major international news for the last five years. This sort of co-occurrence analysis has the potential of aiding identity resolution, which is recognized to be a crucial problem for several tasks: cross-document co-reference resolution, record linkage, object linking, and data integration.
1990 en Industry 3: Knowledge Representation in Practice: Project Halo and the Semantic Web Vulcan's Project Halo is an ambitious, multiyear research program to develop a detailed scientific knowledge base that can answer AP-level questions and provide explanations in a user-appropriate manner. It is one of the largest AI research programs in the US today. Halo's current focus is building AI tools that allow graduate students in chemistry, biology, and physics to author scientific knowledge adequate to answer sophisticated natural-language questions without relying on trained knowledge engineers. Halo researchers have been working to link Semantic Web technology with the other knowledge representations in the system. This talk will lay out Halo's technologies and results to date, and describe the technical and UI issues we have faced in getting users to author scientific conceptual knowledge.
1991 en Industry 2: Integrating Enterprise Data with Semantic Technologies The Semantic Web has reached a level of maturity that allows RDF and OWL to be adopted by large commercial software companies. Many of the products that are based on these standards promise the ability to provide more effective solutions to the increasing IT complexity that many industries are facing. This presentation will describe Oracle's interest in being an early adopter of Semantic Web technologies, as well as providing a technical overview of the RDF Data Model in the latest release of the Oracle Database. Experiences gained from implementing Semantic Web technologies will also be provided.
1992 en Industry 2: From the Bench to the Bedside: The role of Semantics in enabling the vision of Translational Medicine Biomedical research and healthcare clinical transactions are generating huge volumes of information. Biomedical research literature doubles every 19 years and AIDS literature in particular doubles every 22 months. Biomedical research is now an information-based science marked by factory-scale sequencing generating huge amounts of data. A clinician, on the other hand, needs approximately 2 million facts to practice. There is a critical need to speed "translation" of genomic research insights into clinical research and practice and vice versa. In this talk, we will discuss the challenges faced by a healthcare enterprise in realizing the vision of Translational Medicine. In this talk, we will discuss some of these challenges, such as: * The need to create structured and semantic representations of genotypic and phenotypic data such as orders, observations, molecular diagnostic test results, etc. * The need for as-needed data and information integration achieved in an incremental and cost-efficient manner. * The need for actionable decision support for suggesting molecular diagnostic tests in response to phenotypic information and therapies in response to genotypic test results. * The need for knowledge update, propagation and consistency to keep abreast of the rapid pace of knowledge discovery being witnessed in the life sciences, a crucial pre-requisite to reduce the cost of knowledge acquisition and maintenance. There is a need and applicability of semantic web-based specifications and technologies to address the above challenges. We will present semantics-based approaches to address the challenges enumerated above. The role and applicability of various semantic web standards (such as RDF and OWL) in the proposed solutions will also be discussed .
1993 en Industry 2: Deploying Enterprise Level, Ontology-Driven Faceted Search Currently, one of the most practical application spaces for Semantic Web technologies is creating an ontological layer over existing legacy databases. Such layering allows flexible applications to be built without the cost of restructuring large amounts of data while maintaining the performance advantages of a relational database. Whereas applications designed to directly query a database encode business logic in specific queries, ontological layer offers a flexible framework whereby dynamically generated queries are resilient to schema changes. This same approach can be used to query multiple decentralized databases from a seemingly centralized point of view, allowing access to multiple database schemas via a single interface. In an ontology, Semantic Web technologies such as RDFS, OWL and SWRL can be used to specify composition rules and abstractions, making it possible to answer complex questions without developing complex queries. TopQuadrant has applied this approach to develop and deploy a flexible faceted search system over a network of large, decentralized legacy databases. The system uses ontologies in two distinct ways: as an abstraction layer over an underlying relational data model; and as a search interface model driving the system itself. This model-based approach allows dynamic system configuration simply through changes to the model. The model controls what data can be searched, what facets can be used for building queries, and even how data should be displayed. The system combines the structured power of ontologies with more conventional keyword-based search over a related unstructured document corpus. The resulting hybrid system provides capabilities beyond what is possible with either approach alone. This talk will describe the process used for developing the ontological layer; discuss challenges and technical solutions in integrating the databases and bringing together structured and unstructured search. We will also show the benefits of using ontology to specify the search interface and interaction.
1994 en Industry 1: Semantic Solutions: Generating Business Value from Semantic Web Technologies Thanks to the efforts of many researchers over the past five years, Semantic Web technologies have reached the point where they are now enabling new types of business solutions. In this presentation, we will show how IBM Research is using Semantic Web technologies and Semantic Super Computing to generate new insight from the Web, intranets and large document repositories. It includes an introduction and overview of the use of Semantic Super Computing to automatically identify, index and augment semantic information, covers relevant foundational technologies and concludes with a case study of one emerging application of the technology that we refer to as the Wikification of Corporate Corpora
1995 en Industry 1: Managing Richly Connected Information We examine research issues that arise when most information items in an enterprise can be linked to each other via short paths, implicit or explicit. In such high-recall settings, the treatment of metadata management, indexing and ranking needs new attention. Additional issues arise as to the best way to handle updates to the connections, whether on or off the transaction path. Even traditional techniques, such as classification and clustering of documents, which stand to benefit from the extra information provided by the so-called network of meaning, need to be reexamined for how best to exploit the extra information. The talk ends with an examination of some promising avenues for using high recall as a driver for the next wave of business process automation
1996 en Research 4: A Survey of the Web Ontology Landscape 
1997 en Research 4: Ranking Ontologies with AKTiveRank 
1998 en Interview with Marko Grobelnik 
1999 en Interview with Mark Greaves **Dr. Mark Greaves is currently Director, Knowledge Systems at Vulcan, Inc.** Vulcan is the private investment vehicle for Paul Allen (co-founder of Microsoft, www.vulcan.com). At Vulcan, he is sponsoring advanced research in large knowledge bases and advanced web technologies, including Project Halo (www.projecthalo.com). We discussed with him at the ISWC 2006 in Athens, GA in the USA. *What is Vulcan? *What is the Halo Project? *What is the main reason why this was not possible ten years ago? *How these technologies go beyond what they do now? *Will this tecnology be openly available in the future? *Has Europe any chances to catch up the US in this field?
2001 en Panel Discussion 1 
2002 en Interview with Oleksiy Skrypnyk 
2003 en Welcome and Introduction - Introduction of the Second Day Session Agenda Ihor Hurnjak, Lviv Chamber of Commerce and Industry (Ukraine) • Welcome words and Introduction ? Dmytro Aftanas, President of Lviv Chamber of Commerce and Industry (Ukraine) • Welcome words from Host Organization ? Volodymyr Vorobey, Specialist in Knowledge Economy, cognovís GmbH & Co. • Welcome words from the co-organizer cognovís GmbH & Co. and E4 Partners • Introduction to the First Day Session ? Session Moderator • Introduction of the First Day Session Agenda • Introduction of the Common rules for speeches and after-speech discussions
2004 en A “five minute bridge” between the first day and the second day sessions 
2005 en IT-projects for small and medium business: problems, solutions and challenges of Ukraine 
2006 en SME development environment, needs and trends: Role of ICT in SMEs in Ukraine and our efforts in promoting ICT as a tool for business development 
2007 en Optimization of the distribution systems with the use of nowadays mobile technologies 
2009 en Implementation of e-commerce projects in Ukraine, Best Practices 
2010 en Experience of JSC “ENZYM” in attraction of information technologies, Case Study 
2011 en Panel Discussion 2 
2012 en Welcome and Introduction **Ihor Hurnjak**, Lviv Chamber of Commerce and Industry (Ukraine): • Welcome words and Introduction\\ **Dmytro Aftanas**, President of Lviv Chamber of Commerce and Industry (Ukraine): • Welcome words from Host Organization\\ **Volodymyr Vorobey, Specialist in Knowledge Economy, cognovís GmbH & Co.: • Welcome words from the co-organizer cognovis GmbH & Co. and E4 Partners, • Introduction to the First Day Session\\ **Session Moderator: •Introduction of the First Day Session Agenda •Introduction of the Common rules for speeches and after-speech discussions
2013 en Comarch ERP Case Study: CDN XL – integrated system of enterprise management 
2014 en SMEs in European research collaboration. A research institute perspective 
2015 en Interview with Anatoliy Furda 
2016 en IT Case Study from SMEs: Technological decisions on the example of OJSC “Lvivkholod” 
2017 en ERP-systems for SMEs in Ukraine, Best Practices 
2018 en CRF presentation and its work in European Research Programs. <br> E4 Project: Extended Enterprise management in Enlarged Europe 
2020 en Interview with Kamal Nigam Videolectures.Net team spoke to Kamal Nigam in Pittsburgh at CMU where we asked him the following questions: *Can yoou describe your field of work? *Where can your work be applied? *What is your topic of research within the ML community? *What about the future of your research within the ML community? *Your work is Interdisciplinary? *How can you commnet on the future of ML in general? *What are your personal goals?
2021 en Interview with Fei-Fei Li **After a short stay at the ECE Dept. in UIUC, Fei-Fei Li is now back to Princeton as an assistant professor in the Computer Science Dept., where she is the PI of the Vision Lab.** The Videolectures.Net team spoke to her in Pittsburgh at CMU where we asked her the following questions:  *What is your field of work? *What is your topic of research within the Machine Learning community? *Where do you see your group in 5 years time? *What are your goals? *Why are you moving to Princeton University? *If you would have a Machine Learning dream come true...
2022 en Generative Latent Space Models for Text and Image 
2023 en Image Analysis 
2024 en Interview with Tom Mitchell **Tom Mitchell is the first Chair of Department of the first Machine Learning Department in the World**, based at Carnegie Mellon. The Videolectures.Net team spoke to him in Pittsburgh at CMU where we discussed about how he started the department, what was the response of the broader community and its past, present and future. //"The university said you can only have a department if you have a discipline that is going to be here in one hundred years otherwise you can not have a department."//
2025 en Joint Mining of Biological Text and Images: Case Studies 
2026 en Generative Models for Visual Objects and Object Recognition via Bayesian Inference 
2027 en Undirected Graphical Models for Text & Image 
2028 en Interview with Robert Murphy 
2029 en Semisupervised Learning Approaches 
2030 en Introduction to the Machine Learning over Text & Images - Autumn School by Eric Xing 
2031 en Text Information Extraction 
2032 en Interview with Eric Xing Statistical machine learning theory and applications in computational biology are the **fields of interest of Eric Xing**. We asked him at this interview at Carnegie Mellon University what actually is statistical machine learning theory, where can solutions of this research be applied and if **being in the first ML Department** in the world does make his research any easier.
2033 en Text Classification 
2034 en Interview with Christos Faloutsos We met Christos Faloustos in Pittsburgh at CMU where we discussed with him about his current work. Here he describes all his work in the last ten years and at the same time he points out his groups progress, projects, goals, achievements plus future plans.
2095 en Improving SVM Text Classification Performance through Threshold Adjustment 
2096 en Application of Inductive Logic Programming to Structure-Based Drug Design 
2097 en Text Categorisation Using Document Profiling 
2098 en Using Belief Networks and Fisher Kernels for Structured Document Classification 
2099 en Majority Classification by means of Association Rules 
2100 en Explaining Text Clustering Results using Semantic Structures 
2101 en Efficient Statistical Pruning of Association Rules 
2102 en Minimal k-Free Representations of Frequent Sets 
2103 en Knowledge Discovery 
2104 en Best paper awards announcement 
2108 en End to End Service Assurance 
2110 en Parametric Autentication. How I know who you are? 
2112 en NGOSS Key Domains Fulfillment, Assurance, Billing 
2114 en Next Generation OSS Workshop 
2115 en Optimizing Local Probability Models for Statistical Parsing - Best Student Paper Runner-up 
2116 en Logistic Model Trees - Best Student Paper 
2119 en Taking causality seriously: Propensity score methodology applied to estimate the effects of marketing interventions - Best PKDD paper 
2120 en Next Generation Data Mining Tools: Power laws and self-similarity for graphs, streams and traditional data 
2123 en Classification Approach Towards Ranking and Sorting 
2124 en Color Image Segmentation: Kernel Do the Feature Space 
2125 en A Decomposition Of Classes Via Clustering To Explain And Improve Naive Bayes 
2126 en Two-eyed algorithms and problems - Best ECML paper 
2127 en NGOSS Workshop 
2137 en Introduction to workpackages - WP4, WP5, WP6 
2138 en Customer Viewpoint 
2139 en Metadata Extraction: Human Language technology and the Semantic Web - Part 5 
2140 en Metadata Extraction: Human Language technology and the Semantic Web - Part 6 
2141 en SDK Cluster 
2142 en SDK API - Towards European Semantic Interfaces 
2143 en Architecture Proposal 
2144 en Project Structure 
2146 en Metadata Extraction: Human Language technology and the Semantic Web part 7 
2147 en Metadata Extraction: Human Language technology and the Semantic Web - Part 3 
2148 en Human Language Technology for the Semantic Web 
2149 en Visualization 
2150 en Metadata Extraction: Human Language technology and the Semantic Web - Part 4 
2151 en Metadata Extraction: Human Language technology and the Semantic Web - Part 1 
2152 en OWL 
2153 en Semantic Web: Next Generation Web Services and Knowledge Management 
2155 en Semantic Web and Ontology Management Technology 
2157 en Ontology Learning - Knowledge Discovery and the Semantic Web 
2161 en Introduction to workpackages - WP13 
2163 en Knowledge Access 
2164 en Cyc: Knowledge Begets Knowledge 
2165 en Building semantic applications 
2167 en Knowledge Discovery - Part 1 The basic idea of Knowledge discovery is to let a computer search for knowledge whereas the humans give just broad directions about where and how to search. Surprisingly, it is often the case that already relatively simple techniques are able to uncover useful hidden truth beneath the surface of the known facts and relationships. Knowledge discovery could be defined as a research area with several subfields with the most representative Machine Learning and Data Mining (Mitchell, 1997; Fayyad et al., 1996; Witten and Frank, 1999; Hand et al., 2001) and Data bases. Different real-life problems have been successfully addressed using Knowledge discovery methods including Data mining and Decision support (Mladenic et al., 2003; Mladenic and Lavrac, 2003). Semantic Web (Barnes-Lee and Fischetti, 1999) on the other hand, can be seen as mainly dealing with integration of many, already existing ideas and technologies with the specific focus of upgrading the existing nature of web-based information systems to a more “semantic” oriented nature. In this context Semantic Web could be viewed as a frontier of Knowledge Management with some emphasis on web-based applications. There are several dimensions along which Knowledge Discovery (KD) can bring important contributions to Semantic Web. Since KD techniques are mainly about discovering structure in the data, this can serve as one of the key mechanisms for structuring knowledge into an ontological structure being further used in Knowledge management process. An interesting aspect is that data and corresponding semantic structures change in time. As the consequence, we need to be able to adapt ontologies that are modeling the data accordingly. Sub-field of KD called “stream mining” deals with these kinds of problems. It is also important to point out that scalability is one of the central issues in KD, especially in the sub-areas such as Data mining where one needs to be able to deal with real-life datasets of the terra-byte sizes. Semantic Web is ultimately concerned with real-life data on the web which have exponential growth.
2168 en Knowledge Discovery - Part 2 The basic idea of Knowledge discovery is to let a computer search for knowledge whereas the humans give just broad directions about where and how to search. Surprisingly, it is often the case that already relatively simple techniques are able to uncover useful hidden truth beneath the surface of the known facts and relationships. Knowledge discovery could be defined as a research area with several subfields with the most representative Machine Learning and Data Mining (Mitchell, 1997; Fayyad et al., 1996; Witten and Frank, 1999; Hand et al., 2001) and Data bases. Different real-life problems have been successfully addressed using Knowledge discovery methods including Data mining and Decision support (Mladenic et al., 2003; Mladenic and Lavrac, 2003). Semantic Web (Barnes-Lee and Fischetti, 1999) on the other hand, can be seen as mainly dealing with integration of many, already existing ideas and technologies with the specific focus of upgrading the existing nature of web-based information systems to a more “semantic” oriented nature. In this context Semantic Web could be viewed as a frontier of Knowledge Management with some emphasis on web-based applications. There are several dimensions along which Knowledge Discovery (KD) can bring important contributions to Semantic Web. Since KD techniques are mainly about discovering structure in the data, this can serve as one of the key mechanisms for structuring knowledge into an ontological structure being further used in Knowledge management process. An interesting aspect is that data and corresponding semantic structures change in time. As the consequence, we need to be able to adapt ontologies that are modeling the data accordingly. Sub-field of KD called “stream mining” deals with these kinds of problems. It is also important to point out that scalability is one of the central issues in KD, especially in the sub-areas such as Data mining where one needs to be able to deal with real-life datasets of the terra-byte sizes. Semantic Web is ultimately concerned with real-life data on the web which have exponential growth.
2169 en Metadata Generation and Applications 
2170 en Semantic Web and Ontology Management 
2171 en Introduction to SEKT project 
2172 en Introducing Knowledge Access in SEKT 
2173 en Learning Semantic Sub-graphs for Document Summarization 
2176 en Cyc: Knowledge Begets Knowledge 
2177 en A short Tutorial on Semantic Web 
2178 en Text Mining for Ontology Learning 
2180 en Semantic Web and Ontology Management Technology 
2181 en Metadata Extraction: Human Language technology and the Semantic Web 
2182 en Information extraction 
2183 en Human Language Technology for the Semantic Web 
2185 en Introduction to workpackages - WP15 
2186 en Language Technologies 
2187 en Language Technologies 
2191 en Building semantic applications 
2192 en Knowledge Access 
2193 en BulTreeBank 
2195 en Introducing Knowledge Access in SEKT 
2196 en Semantic Web a Concise Introduction 
2197 en Language Resiurces Infrastructure for Bulgarian 
2198 en Introduction to SEKT project 
2199 en Gate 
2200 en Onthology Based Information Extraction and Gate 
2201 en Text Mining for Ontology Learning 
2202 en Onthology Based Information Extraction 
2203 en Unknown title 
2204 en Unknown title 
2205 en Introduction of SEKT partner - University of Sheffield, UK 
2206 en Introduction of SEKT partner - Sirma AI Ltd, Bulgaria 
2207 en Introduction of SEKT partner - Vrije Universiteit Amsterdam, Netherlands 
2208 en Introduction of SEKT partner - Intelligent Software Components S. A., Spain 
2209 en Introduction of SEKT partner - Autonomous University of Barcelona, Spain 
2210 en Introduction to workpackages - WP1 
2211 en Introduction to workpackages - WP2 
2212 en Introduction to workpackages - WP3 
2213 en Introduction to workpackages - WP4 
2214 en Unknown title 
2215 en Information extraction 
2216 en Why talk about Eclipse? 
2217 en Language Technologies 
2218 en Language Technologies This tutorial covers the use of Human Language Technologies for the Semantic Web and Web Services. It includes sections on HLT and Text Mining for the Semantic Web, various forms of Information Extraction, Ontology Population and Semantic Metadata Creation, and Evaluation.n n The tutorial begins with an introduction to Human Language Technology, looking at both its background and development, and then situating it within the context of text mining and other tasks involving knowledge discovery from large collections of unstructured text, which are necessary for the development of the semantic web. The second section concerns information extraction, a major component of text mining. Information extraction involves extracting facts and structured information from unstructured data. We contrast this with Information retrieval, which concerns extracting documents from large text collections, and with data mining, which concerns discoveing patterns in structured data. We introduce GATE, and architecture for language engineering, and its resources for information extraction, and then expand the idea of traditional information extraction to focus on semantic web-enabled technology such as ontology population and semantic metadata creation, both of which involve the use of information extraction based on ontologies. We look at some current state-of-the-art semantic annotation systems such as KIM, Magpie, MnM and OntoMat. In the third section, we discuss evaluation methods for such technology, based on the idea that traditional methods are insufficient when applied to semantic web technology, due to the presence of hierarchical (ontological) information rather than flat structures. We also take a brief look at usability issues of annotation systems. Finally, the tutorial gives demonstrations of two examples of HLT in use for the semantic web. First we present RichNews, which aims to automate the annotation of news programs, segmenting, describing and classifying news broadcasts from transcripts. Second, we present work on ontology-based and mixed initiative information extraction carried out in the context of SEKT.n
2219 en Human Language technology for the Semantic Web In this talk I will present an overview of Human Language Technologyn (HLT) and its use in Semantic Web development. HLT is concerned with automatic linguistic processing towards the semantic analysis and extraction of information from textual data. In the context of the Semantic Web the use of HLT is in knowledge markup of web documents for ontology population and text mining for ontology evolution (extension and modification of ontology models). The talk will include examples of both as currently developed in the context of the SmartWeb project on "Mobile Broadband Access to the Semantic Web" - http://www.smartweb-projekt.de/
2222 en Ontology Management 
2223 en Introduction of SEKT partner - BT, UK 
2224 en Quaero 
2225 en Yahoo! Research Overview 
2227 en Introduction of SEKT partner - University of Innsbruck, Austria 
2228 en Introduction of SEKT partner - JSI, Slovenia 
2229 en Introduction of SEKT partner - University of Karlsruhe, Institute AIFB, Germany 
2230 en Introduction of SEKT partner - Empolis GmbH, Germany 
2231 en Introduction of SEKT partner - Ontoprise GmbH Intelligente Lösungen für das Wissensmanagement, Germany 
2232 en Introduction of SEKT partner - Kea-pro GmbH, Switzerland 
2266 en Ecopolitical debate and the politics of Nature During the last thirty years philosophers in the West have critiqued the underlying assumptions of Modern philosophy in relation to the natural world. This development has been part of an ongoing expansion of philosophical work involving cross cultural studies of world views or ultimate philosophies. **Since philosophical studies in the West have often ignored the natural world, and since most studies in ethics have focused on human values, those approaches which emphasize ecocentric values have been referred to as ecophilosophy.** Just as the aim of traditional philosophy is sophia or wisdom, so the aim of ecophilosophy is ecosophy or ecological wisdom. **The Practice of ecophilosophy is an ongoing, comprehensive, deep inquiry into values, the nature of the world and the self.** The mission of ecophilosophy is **to explore a diversity of perspectives on human-Nature contexts and interrelationships**. It fosters deeper and more harmonious relationships between place, self, community and the natural world. This aim is furthered by comparing the diversity of ecosophies from which people support the platform principles of the global, long range, deep ecology movement.
2309 en Questions and Answers 
2311 en Force, law and the prospects of survival 
2349 en Opentaps training Free yourself from expensive commercial ERP software, difficult to maintain legacy applications, and messy integration projects. opentaps is a complete open source solution for your enterprise. Its sophisticated features and modern architecture will help bring together your entire organization, automate business processes, and improve efficiency.
2355 en Debate about LCE 
2356 en Debate 
2357 en Interview with Mario Orasche Gcp gamma capital partners is an independent Venture Capital investment firm, focused on financing dynamic technology companies in Austria, Germany, Switzerland and CEE countries.
2361 en Spatial Data Mining Querie language in a GIS System The strength of GIS is in providing a rich data infrastructure for combining disparate data in meaningful ways by using a spatial arrangement (e.g., proximity). As a toolbox, a GIS allows planners to perform spatial analysis using geo-processing functions such as map overlay, connectivity measurements or thematic map coloring. Although, this makes effective the geographic visualization of individual variables, complex multi-variate dependencies are easily overlooked. The required step to take GIS beyond a tool for automating cartography is to incorporate the ability of analyzing and condensing a large number of geo-referenced variables into a single forecast or score. This is where data mining promises great potential benefits and the reason why there is such a hand-in-glove fit between GIS and data mining. INGENS (INductive GEographic iNformation System) is a prototype GIS which integrates data mining tools to assist users in their task of topographic map interpretation. The spatial data mining process is aimed at a user who controls the parameters of the process by means of a query written in a mining query language. In this talk, I present SDMQL (Spatial Data Mining Query Language), a spatial data mining query language used in INGENS. Currently, SDMQL supports two data mining tasks: inducing classification rules and discovering association rules. For both tasks the language permits the specification of the task-relevant data, the kind of knowledge to be mined, the background knowledge and the hierarchies, the interestingness measures and the visualization for discovered patterns. Some constraints on the query language are identified by the particular mining task. I describe the syntax of the query language and finally I briefly illustrate the application to a real repository of maps.
2371 en Modeling real-world networks using Kronecker multiplication Given a large, real graph, how can we generate a syntheticn graph that matches its properties, i.e., it has similar degreen distribution, similar (small) diameter, similar spectrum, etc?n n First, we propose a graph generator that is mathematically tractablen and generates realistic graphs. The main idea is to use a non-standardn matrix operation, the Kronecker product, to generate graphs that wen refer to as ''Kronecker graphs''. We show that Kronecker graphsn naturally obey all the above properties; in fact, we can rigorouslyn prove that they do so.n n Once we have the model, we fit it to real graph to generate a syntheticn graph that matches its properties, i.e., it has similar degree distribution,n similar (small) diameter, similar spectrum, etc?n n We present a fast and scalable algorithm for fitting the Kronecker graphn generation model to real networks. A naive approach to fitting wouldn take super-exponential time. In contrast, our algorithm takes linear time,n by exploiting the structure of Kronecker matrix multiplication and byn using sampling.n n Experiments on large real and synthetic graphs show that our approachn recovers the true parameters and indeed mimics very well the patternsn found in the target graphs. Once fitted, the model parameters and then resulting synthetic graphs can be used for anonymization, extrapolations,n and graph summarization.n n //The presentation starts in Slovenian language and switches to English a few minutes into the lecture.\\n Another lecture on the same topic can be found at [[icml07_leskovec_smrg]].//
2373 en Data Mining Vs. Semantic Web This tutorial covers the field of datamining in general, talks about its possible applications (special case studies can be added on request), and elaborates on the issue of hardware accelerators for datamining. The introduction gives a formal and an informal definition (through an example), plus it points to possible missunderstandings typical of the topic. The part on methods and algorithms covers a number of different approaches, each one presented thru animation, using the examples that are both colourfull and unusual, but excellent for pointing into the essence. The part on tools lists about a dozen different tools, and selects one for a detailed case study. The part on applications includes examples from a variety of different fields (engineering, science, medicine, psychiatry, etc...) The part on hardware accelerators is available on special request. This tutorial was presented so far many times for industry and academia in the USA and Europe, and received the best tutorial award at several conferences.
2377 en Biologically Inspired Flint Glass Flint glass is optical glass that has relatively high refractive index and low Abbe number. Flint glasses are arbitrarily defined as having an Abbe number of 50 to 55 or less. The currently known flint glasses have refractive indices ranging between 1.45 and 2.00. A concave lens of flint glass is commonly combined with a convex lens of crown glass to produce an achromatic doublet lens because of their compensating optical properties. With respect to glass, the term "flint" derives from the flint nodules found in the chalk deposits of southeast England that were used as a source of high purity silica by George Ravenscroft, circa 1662, to produce a potash lead glass that was the predecessor to English lead crystal. Traditionally, flint glasses contain around 4%—60% lead oxide; however, the manufacture and disposal of these glasses are sources of pollution. In many modern flint glasses, the lead can be replaced with other additives such as titanium dioxide and zirconium dioxide without significantly altering the optical properties of the glass. Flint glass can be fashioned into rhinestones which are used as diamond simulants.
2384 en Funding the Semantic Web : A cross-continental assessment and outlook In the recent years semantic technologies have demonstrated their usefulness and applicability in a variety of domains, the Semantic Web being the most prominent one. The Semantic Web has started to move from academic research to deployed business-critical and scientific applications, with support from recommendations (standards) developed under W3C governance and a growing list of commercial technologies and products is being developed. These developments seem to be early but firm steps in establishing semantics as a core column of computer science and application development. The outreach of this development can only be assessed to limited degree at the moment, but most likely will affect key aspects of society and the way we communicate. This high potential was recognized early by funding agencies all over the world. However, after the first strong funding in US by DARPA, subsequent research funding seems to be limited. Europe seems to have seem more substantial and sustained funding, at least during last few years. Now may be a good time to assess what has been achieved so far and how funding agencies see future research directions, funding opportunities and funding environments, i.e., what are the planned strategies and instruments of funding agencies to maximize the impact of future research in semantics. We consider it specifically interesting to the research community to hear the opinions and plans of the major funding bodies around the world and to learn about their view on future issues/requirements/applications/challenges related to semantics and Semantic Web-- and by extension their opinion on the needs of industry, government and education for research in the Semantic Web and related areas.
2385 en The Role of Semantic Web in Web 2.0: Partner or Follower? Currently, the web phenomenon that is driving the best developers and captivating the best entrepreneurs is Web 2.0. Web 2.0 encompasses some of today's most exciting web-based applications: mashups, blogs/wikis/feeds, interface remixes, and social networking/tagging systems. Although most Web 2.0 applications rely on an implicit, lightweight, shared semantics in order to deliver user value, by several metrics (number of startups funded, number of "hype" articles in the trade press, number of conferences), Web 2.0 technologies are significantly outdistancing semweb technologies in both implementation and mindshare. Hackers are staying up late building mashups with AJAX and REST and microformats, and only rarely including RDF and OWL. This panel will consider whether semantic web technology has a role in Web 2.0 applications, in at least the context of the following areas: 1. Web 2.0 and Semantics: What unique value can semantic web technologies supply to Web 2.0 application areas? How do semantic web technologies match up with the semantic demands of Web 2.0 applications? 2. Semantics and Web "Ecosystems": Web 2.0 applications often strive to build participatory ecosystems of content that is supplied and curated by their users. Can these users effectively create, maintain, map between, and use RDF/OWL content in a way that reinforces the ecosystem? 3. Semantic Web in Practice: Does semantic web technology enable the cost-effective creation of Web 2.0 applications that are simple, scalable, and compelling for a targeted user community? Can semantic web technology genuinely strengthen Web 2.0 applications, or will it just be a footnote to the Web 2.0 wave?
2386 en Closing Cerimony - Awards, Next Year's Presentation 
2392 en Context Sensitivity in Knowledge Rich Systems: Part 2, Defining context 
2393 en Research 10: On the Semantics of Linking and Importing in Modular Ontologies 
2394 en Research 11: Block Matching for Ontologies 
2395 en Research 11: PowerMap: Mapping the Real Semantic Web on the Fly 
2396 en Semantic Desktop and Social Semantic Collaboration: Promiscuous Semantic Federation: Semantic Desktops meet Web 2.0 
2397 en Semantic Desktop and Social Semantic Collaboration: A Case Study in Engineering a Knowledge Base for an Intelligent Personal Assistant 
2398 en Semantic Desktop and Social Semantic Collaboration: Open Constitution Based Knowledge Communities in the Semantic Web 
2399 en Semantic Desktop and Social Semantic Collaboration: Introduction 
2400 en Semantic Desktop and Social Semantic Collaboration: The Beagle++ Toolbox: Towards an Extendable Desktop Search Architecture 
2401 en Semantic Desktop and Social Semantic Collaboration: Method of Retrieving a Web Browsing Experience Using Semantic Periods 
2402 en Semantic Desktop and Social Semantic Collaboration: Semantic Clipboard - Semantically Enriched Data Exchange Between Desktop Applications 
2403 en Semantic Desktop and Social Semantic Collaboration: SemDAV: A File Exchange Protocol for the Semantic Desktop 
2404 en Semantic Desktop and Social Semantic Collaboration: Supporting Mobile Service Interaction through Semantic Service Description Annotation and Automatic Interface Generation 
2405 en Semantic Desktop and Social Semantic Collaboration: Spontaneous Collaboration via Browsing of Semantic Data on Mobile Devices 
2406 en Semantic Desktop and Social Semantic Collaboration: Application Design and Interoperability for Managing Personal Information in the Semantic Desktop 
2407 en Semantic Desktop and Social Semantic Collaboration: PIMO Population and Semantic Annotation for the Gnowsis Semantic Desktop 
2408 en Semantic Desktop and Social Semantic Collaboration: # An Overview of Information Management and Knowledge Work Studies: Lessons for the Semantic Desktop 
2411 en Spatiotemporal Modelling of Intracellular Signalling in Bacterial Chemotaxis Whilst theoretical models have been used to understand aspects of bacterial chemotaxis systems for the past thirty or so years, little work has focused on the importance that spatial localisation of proteins within the cytoplasm of the cell has on the overall functionality of the intracellular network. In this talk we will examine spatio-temporal models of signal transduction developed to describe the phosphotransfer pathway within E. coli. This model framework will then be extended to examine the importance of protein localisation within R. sphaeroides, a species which contains considerably more phosphotransfer proteins than E. coli and the spatial localisation of which plays a particularly important role in activating certain elements of the phosphotransfer network. The difficulties encountered in obtaining robust parameter estimates for reaction rates within the R. sphaeroides will also be detailed. Joint work with S. L. Porter, P. K. Maini and J. P. Armitage.
2412 en Conservation Laws and Identifiability of Models for Cellular Metabolism New experimental techniques in the biosciences provide us with high-quality data allowing quantitative mathematical modeling. When fitting model parameters to experimental data, it is important to know whether all parameters can be uniquely estimated from available data. In this paper we discuss a class of models for metabolism, where the introduction of conserved moieties may cause an otherwise identifiable model to be unidentifiable. A general method for reparametrization to identifiable rate expressions is presented, and the general results are exemplified by three well-cited models for yeast metabolism. Joint work with Milena Anguelova, Gunnar Cedersuna, Carl Johan Franzen, Mikael Johansson
2413 en Bayesian Inference for Systems Biological Models via a Diffusion Approximation As post-genomic biology becomes more predictive, the ability to infer rate parameters (known as reverse-engineering) of biochemical networks will become increasingly important. One approach is to replace the underlying model by a diffusion approximation and the model is identified using discrete-time (and often incomplete) data that is subject to error. Unfortunately, likelihood based inference can be problematic as closed form transition densities of nonlinear diffusions are rarely available. A widely used solution involves the introduction of latent data points between every pair of observations to allow an Euler-Maruyama approximation of the true transition densities to become accurate. Markov chain Monte Carlo (MCMC) methods can then be used to sample the posterior distribution of latent data and model parameters; however, naive schemes suffer from a mixing problem that worsens with the degree of augmentation. A reparameterisation is therefore implemented to overcome this difficulty and the methodology is applied to a simple prokaryotic auto-regulatory gene network. Joint work with Darren J. Wilkinson
2414 en Experimental Design for Efficient Identification of Gene Regulatory Networks using Sparse Bayesian Models Identifying large gene regulatory networks is an important task, while the acquisition of data through perturbation experiments (e.g., gene switches, RNAi) is expensive. It is thus desirable to use an identification method that effectively incorporates available prior knowledge --- such as sparse connectivity --- and that allows to design experiments such that maximal information is gained from each one.n n Our main contributions are twofold: a method for consistent inference of network structure is provided, incorporating prior knowledge about sparse connectivity. The algorithm is time efficient and robust to violations of model assumptions. Moreover, we show how to use it for optimal experimental design, reducing the number of required experiments substantially. We employ sparse linear models, and show how to perform full Bayesian inference for these. We not only estimate a single maximum likelihood network, but compute a posterior distribution over networks, using a novel variant of the expectation propagation method. The representation of uncertainty enables us to do effective experimental design in a standard statistical setting: experiments are selected such that on average the experiments are maximally informative. Few methods have addressed the design issue so far. Compared to the most well-known one, our method is more transparent, and is shown to perform qualitatively superior. In the former, hard and unrealistic constraints have to be placed on the network structure for mere computational tractability, while such are not required in our method. We demonstrate reconstruction and optimal experimental design capabilities on tasks generated from realistic non-linear network simulators.n n Joint work with Florian Steinke and Koji Tsuda.
2415 en Reconstructing Transcriptional Networks using Bayesian State Space Model A major challenge in systems biology is the ability to model complex regulatory interactions. In previous work, we have used Linear-Gaussian state-space models (SSMs), also known as Linear Dynamical Systems (LDS) or Kalman filter models to 'reverse-engineer' regulatory networks from high-throughput data sources, such as microarray gene expression profiling. SSM models are a subclass of dynamic Bayesian networks used for modeling time series data and have been used extensively in many areas of control and signal processing. The parameters of an SSM can be learned using maximum likelihood (ML) methods. However, in general the ML approach is prone to overfitting, especially when fitting models with many variables with relatively small amounts of data. We have instead turned to a fully Bayesian analysis, which avoids overfitting and provides error bars on all model parameters ? in this paradigm the objective function is simply the probability of the data, that which results from integrating out the parameters of the model with respect to their prior distribution. Optimizing a model with respect to such an objective function avoids overfitting in the conventional sense. In practice, a Bayesian learning scheme infers distributions over all the parameters and makes modeling predictions by taking into account all possible parameter settings. In doing so we penalize models with too many parameters, embodying an automatic Occam's Razor effect. We describe results from simulation studies based on synthetic mRNA time series data. Receiver Operating Characteristic (ROC) analysis demonstrates an overall accuracy in transcriptional network reconstruction from the mRNA time series measurements alone of approximately 68% Area Under the Curve (AUC) for 12 time points and better still for data sampled at a higher rate. Incorporation of prior information about known regulatory connections improves this accuracy in a fashion which appears be be linear with the number of known connections included. The implications of these simulation studies for experimental design will be discussed. Joint work with Matthew J. Beal and Juan Li.
2416 en Reaction and Diffusion on Fractal Sets Systems biologists are interested in modelling chemical reactions in the intracellular environment, and to date much of what is done is based on the use of mass action kinetics to construct models of elementary reactions. Mass action kinetic models are based on a number assumptions which are not obviously valid in the intracellular environment. The cytoplasm is far from an ideal, isotropic wellmixed solution and often the concentrations of important chemical species are very small. Molecular crowding can have significant thermodynamic effects, but also must play an important dynamical role. An interesting approach that has been adopted to this has its roots in fractal geometry - a given molecule, depending upon its size and shape and the sizes and shapes of the molecules which surround it will find itself able to move in an environment of restricted dimension (see for example[1, 2]). Simple ideas have been suggested which give spatially homogeneous rate-like equations which attempt to account for this. It has been suggested, for example, that rate laws which depend on non-integer powers of the concentration of species might be used, and alternatively that the rate constants for elementary reactions which involve the encounter of different species (as opposed to spontaneous decomposition of individual molecules) should be time-dependent[1]. In this case the rates decay in time - the suggested form is the Zipf-Mandlebrot law which tends to a power law decay at long times, it is suggested that this power law characterises the dimension of the restricted environment of each chemical species[2]. Both of these approaches suffer from shortcomings. The use of non-integer powers of concentrations can only be justified in very limited circumstances, and has been shown to be inferior to the time-dependent rate parameter when describing certain lattice gas computer simulations of chemical reactions. However, the latter is clearly not invariant to time translation - the origin of time has a particular significance, and it is not clear as a general principle what the correct choice of time origin should be. Moreover, experimental techniques are being refined to the extent that spatio-temporal resolution of the species within a single cell is becoming possible. We might, therefore, aspire to constructing theories which describe the dynamics for spatially non-uniform distributions of active species. We have recently been working on a class of simple models of this type. These are spatio-temporal dynamical systems which model reaction and diffusion on a certain class of fractal sets. It has been known for some time now that it is possible to define random walks, and hence diffusion, on a certain class of fractals (indeed, it was this observation that motivated the work described above[1]). A simple example if this class is the Sierpinsky Gasket which has constrictions to the diffusion process in the sense that it can be disconnected by the removal of a finite set of points. The talk will focus mainly on this example, but we shall also suggest ways which could lead to more general models. Supported by the Manchester Institute for Mathematical Science (MIMS).
2417 en Modelling Transcriptional Regulation with Gaussian Processes Modelling the dynamics of transcriptional processes in the cell requires the knowledge of a number of key biological quantities. While some of them are relatively easy to measure, such as mRNA decay rates and mRNA abundance levels, it is still very hard to measure the active concentration levels of the transcription factor proteins that drive the process and the sensitivity of target genes to these concentrations. In this paper we show how these quantities for a given transcription factor can be inferred from gene expression levels of a set of known target genes. We treat the protein concentration as a latent function with a Gaussian process prior, and include the sensitivities, mRNA decay rates and baseline expression levels as hyperparameters. We apply this procedure to a human leukemia dataset, focusing on the tumour repressor p53 and obtaining results in good accordance with recent biological studies. Joint work with Guido Sanguinetti and Magnus Rattray.
2418 en Parameter Estimation of ODE's with Regression Splines: Application to Biological Networks The construction and the estimation of quantitative models of gene regulatory networks and metabolic networks is one of the task of Systems Biology. Such models are useful because they provide tools for simulating and predicting biological systems. Various approaches have been proposed, such as graphical models , Bayesian dynamical models or Ordinary Differential Equations (ODE's) . For the latter, one can also expect to derive parameters that often have a meaningful biological sense. We focus on the estimation of a parameter theta indexing a (vector) ODE, from an observed time series (concentration profiles) which may be nonlinear (e.g. due to the use of Michaelis-Menten dynamics or mass action law). Even when the likelihood is simple (in the case of Gaussian error noise), the computation of the Maximum Likelihood Estimator remains hard because of the burden of the optimization step. Indeed, the implicit definition of the model necessitates the integration of the ODE for each evaluation of the likelihood. Moreover, the likelihood may have numerous local maxima we need to avoid, hence the exploration of the parameter space may be computer-intensive. We propose then an alternative (frequentist) estimator of theta based on a preliminary spline estimator of the solution of the ODE. We use a simple characterization of theta that enables to derive a learning algorithm avoiding the integration of the ODE, and that can split the estimation of a vector differential equation in several estimations of scalar differential equations. We illustrate this algorithm with different models used in Systems Biology and we sketch how it can be adapted to various settings encountered by the practitioner.n n Joint work with Chris Klaassen and Florence d'Alché-Buc.
2419 en Identifiability of Delay Parameters for Nonlinear Time-delay Systems with Applications in Systems Biology The concept of parameter identifiability will be introduced briefly, followed by a short description of how this property can be tested for ODE-systems in general by rank calculations. Then, the extension of this analysis to delay systems, recently developed by Xia et al. [1] and Zhang et al. [2] will be reviewed. In these works, the authors use the framework of modules over non-commutative rings to formulate an analogous rank test for parameter identifiability of delay systems with known time-delays. Our ongoing work will then be motivated by a model of cellular signal transduction by Timmer et al. [3], where the sojourn time of STAT-5 in the nucleus is modelled by an unknown time delay which is estimated numerically by Timmer et al.. We will show how the identifiability of the time delay parameter is determined by the form of the external input-output representation of the system. Working in the mathematical framework of [1] and [2], we formulate explicit criteria based on rank calculations for the space spanned by the gradients of the output derivatives. Finally, several examples of biological systems from the literature will be discussed. Joint work with Bernt Wennberg.
2420 en Dynamic Modelling of Microarray Data We recently released rHVDM (Hidden Variable Dynamic Modelling), an R/Bioconductor package that predicts targets of a known transcription factor using time course microarray data. The key feature behind the algorithm is a simple ODE model of mRNA concentration. In the first stage of rHVDM, transcription factor activity (the hidden variable) is deduced from the expression time profile of a small number of known targets. This information is then used to screen other genes for dependency on that transcription factor. The accuracy of the technique has been demonstrated with Affymetrix microarray time course data and verified experimentally using siRNA knockdown of a targeted transcription factor (p53). While implementing the rHVDM algorithm and refining it for release we encountered a number of problems. These included parameter identifiability, parameter count reduction, algorithmic speed, parameter domain restriction, confidence interval estimation, and measurement noise. I will discuss each of these issues individually, along with the techniques we used to address them.
2421 en Maximum Likelihood Estimation for a Gene Regulatory Network Defined by Differential Equations Gene regulation may be described by a set of deterministic differential equations describing the time rate evolution of the gene product concentrations, and containing parameters accounting for the regulatory relationships occurring in the gene network. We will present maximum likelihood based estimators of the parameters arising in this formalism and we will prove that they have desirable properties. Our results may be applied to a gene regulation model yielding the early Drosophila segments formation relying on a statistical modelling of gene expression data obtained by confocal laser scanning microscopy. The proposed statistical model accounts for the uncertainty in the measurement of gene expression and the uncertainty in the time at which the measurements are performed.
2422 en Model Reduction for Parameter Estimation Estimating parameters in biochemical network models is a central but often difficult problem. A general approach that may be worth developing further is first to seek simplified or "reduced" models with fewer dynamical degrees of freedom, estimate parameters for the reduced models, and then use that information to constrain the corresponding parameters in the full model. This approach can leverage appropriate human expertise and could in principle be applied recursively. The choice of variables to eliminate during model reduction could also be made by clustering or other machine learning methods. Some relevant model reductions already exist for quasi-equilibrium models of transcriptional regulation networks, which could provide a starting point for this strategy.
2423 en System Identification of Enzymatic Control Processes Using Population Monte Carlo Methods We demonstrate the superiority of Population Monte Carlo techniques over standard Metropolis Markov Chain Monte Carlo (MCMC) methods for inferring optimal parameters for a particular mechanistic model of a biological process given noisy experimental data. As our understanding of biological processes increases, the proposed models to describe them become more complex. With such potentially large numbers of equations and parameters, it is no longer feasible to hand-pick parameter values and be sure that the most appropriate values have been chosen. Monte Carlo methods are becoming more widely used for estimating parameter values, however we show that the standard Metropolis MCMC approach fails to converge on optimal values for even relatively simple models and that a more sophisticated method, in the form of non-Markovian Population Monte Carlo, may be successfully employed to produce consistent and accurate results. We illustrate the basic problem using the minimal model for the circadian genetic network in Arabidopsis thaliana, which consists of 3 linked differential equations containing a total of 6 parameters, with an additional noise parameter incorporated to estimate the variance of noise in the data. Joint work with Mark Girolami.
2424 en Estimating Parameters and Hidden Variables in a Non-linear State-space Model of Regulatory Networks Understanding and identifying biological complex systems at work in the cell requires to develop models able to capture the stochastic nature of biological processes as well as their dynamics. Focusing on gene regulatory networks, we propose a new quantitative model in the form of a dynamical Bayesian network that allows to represent both genes and proteins in the same framework. We start from the nonlinear differential equations of Michaelis-Menten which are the gold-standard to represent biochemical interactions and develop a discrete-time and probabilistic model from these equations. Compared to previous works such as Nachman et al [1], our model takes into account the dependency between the regulatory proteins and the genes that code for them as well as protein-protein interactions and protein degradations. In the resulting nonlinear dynamical system, the proteins concentrations are hidden while gene expressions are observed. In order to learn the model's parameters, we first construct a discrete-time probabilistic model corresponding to our continuous-time state-space model and then derive a Kalman smoother algorithm based on the unscented transformation [2] to recursively estimate the parameters and unobserved protein activities. The generality of the learning method opens the door to various adaptations of the model if required by the biology. Numerical results on parameter and state estimation for the repressilator [3] and other several small networks are presented and show the relevance of the model.
2425 en Benchmarking parameter estimation and reverse engineering strategies Parameter estimation has become a central problem in systems biology, both in the form of calibration of bottom-up models or as a component of reverse engineering algorithms. With a proliferation of algorithms proposed for these purposes it has become important to compare them in objective ways. I will argue that in silico biochemical network models are extremely useful for this purpose. Several networks will be presented that are challenging tests for parameter estimation and network inference. An issue that arises from the use of in silico networks, though, is whether they can provide realistic data. The application of this benchmarking methodology will be illustrated with a comparison of four reverse engineering methods. \\ //Joint work with Diogo Camacho, Paola Vera Licona, and Reinhard Laubenbacher//
2426 en Debate about future meetings 
2427 en Pipelined Vector Processing and Scientific Computation For almost two decades (1976-1993) technical-scientific high-performance computing has been dominated by vector processing, pioneered by Seymour Cray and his Cray Research Incorporated. This technology is explained in detail, and the significant cost-performance advantages are outlined. While parallel processing is more en-vogue today, and in fact is dominating scientific computation since 1993, a comeback of vector processing may be on the horizon due to the IBM/Sony/Toshiba Cell processor being a parallel vector processor, and being the highest-performance single chip available today.
2428 en Social-media blog tagging: Metadata or “just more content” ? The authoring of tags -- unlike the authoring of traditional metadata -- is highly popular among users. This harbours unprecedented opportunities for organizing content. However, tags are still poorly understood. What do they ''mean'', in what senses are they similar to or different from metadata? Different tags support different communities, but how exactly do they reflect the plurality of opinions,what is the relation to individual differences in authoring and reading? In this paper, we offer a definition and empirical evidence for the claim that ''tags are not metadata, but just more content''. The analysis rests on a multi-annotator classification of a blog corpus using the WordNet domain labels system (WND), the development of a system of text-classification methods using WordNet and WND, and a quantitative and qualitative comparative analysis of these classifications. We argue that the notion of a ''gold standard'' may be meaningless in social media, and we outline possible consequences for labelling and search-engine development.
2429 en Novel functional magnetic materials based on magneto-structural transitions V zadnjih letih so se v svetu intenzivirale raziskave na podro?ju materialov z magnetokalori?nimi lastnostmi. Ena od pomembnih aplikacij takih materialov je izdelava ekolo?ko neopore?nih magnetnih hladilnih sistemov. V predavanju bodo predstavljeni materiali, ki ka?ejo gigantski magnetokalori?en efekt in magnetni oblikovni spomin. V razpravi bo predstavljena teorija do katere mere narava magneto-strukturne sklopitve vpliva na latentno toploto in spremembo magnetne entropije. Predstavljene bodo metode kako dose?i maksimalen magnetokalori?en efekt, metode izdelave teh materialov in njihove karakterizacije.
2430 en IBM Speech Activity System 
2431 en The 2006 ISL Rich Transcription Speech-to-Text System 
2432 en Agnostic Active learning 
2433 en PERFORMANCE BOUNDS FOR KERNEL PCA 
2434 en Anti-Learning Signature in Biological Classification 
2435 en Generalization to Unseen Cases: (No) Free Lunches and Good-Turing estimation 
2437 en The use of machine translation tools for cross-lingual text-mining 
2438 en A Support Vector Method for Multivariate Performance Measures We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.
2444 en Identity difference and tolleration Prof. Anna Elisabetta Galeotti z Univerze v Torinu. Predavateljica, ki je profesorica politi?ne filozofije, in je med drugim ?tudirala v Cambridgeu, predavala na Evropski univerzi v Firencah, na Institute for Advanced Studies v Princetonu in na harvardski univerzi, svoje znanstvenoraziskovalno delo posebej posve?a omenjeni tematiki, o njej je izdala tudi knjigo 'Una proposta pluralista', (Liguori, 1994)
2449 en Chill Data for the RT-06 Transcriptions 2006 
2450 en Systems for Speech Activity Detection 
2451 en Videotext Recognition System 
2452 en Speech Activity Detection and Speaker Diarization for Lectures 
2453 en Two Contrastive Systems: 1. Based on HMM/BIC and 2. Based Solely on HMM 
2454 en IBM RT-06 Speech to Text Evaluation System 
2455 en ICSI Speech Recognition System 
2456 en The Limsi RT-06 Lecture Transcription System 
2457 en UPC Speech Activity Detector in RT-06 Evaluation 
2458 en The AMI Meeting STT System - Release 2 
2459 en RT-06 Evaluation Data Transcribed by LDC 
2460 en Introduction to the rt06 workshop 
2461 en RT-06 Speaker Diarization Results and Speech Activity Detection Results 
2462 en Text Recognition Evaluation 
2463 en AMI RT-06 SAD and SPKR Submission 
2464 en Toward Adaptive Information Fusion in Multimodal Systems Techniques for information fusion are at the heart of multimodal system design. In this talk, I'll summarize recent work on predictive modeling of users' multimodal integration patterns, including that (1) there are large individual differences in users' dominant speech and pen multimodal integration patterns, (2) these patterns can be identified almost immediately and remain highly consistent for individual users over time, (3) they are highly resistant to change, even when users are given strong selective reinforcement or explicit instructions to switch patterns, and (4) these distinct patterns appear to derive from enduring differences among users in cognitive style. I'll also discuss findings on systematic entrenchment of users' dominant multimodal integration pattern when under load, including as task difficulty increases and during error handling. I'll conclude by highlighting work we are now pursuing that combines predictive user modeling with machine learning techniques to accelerate, generalize, and improve the reliability of information fusion during multimodal system processing. Implications of this research will be discussed for the design of adaptive multimodal systems with substantially improved performance characteristics.
2465 en Completion of biological networks : the output kernel trees approach Elucidating biological networks appears nowadays as one of the most important challenge in systems biology. Due to the availability of various sources of data, machine learning has to play a major role regarding this issue, given its large spectrum of tools ranging from generative models to concept learning methods. In this work the focus is narrowed on the completion of biological interactions networks for which some of the interactions between variables (usually genes or proteins) are already known.
2466 en Introduction and welcome 
2467 en Part 2: A Novel Bayesian Approach for Uncovering Potential Spectroscopic Counterparts for Clinical Variables in 1H NMR Metabonomic Applications Metabonomic approaches based on spectroscopic data are in their infancy in biomedicine. A key challenge in clinical metabonomics is uncovering and understanding the relations between the multidimensional spectroscopic data and the clinical measures currently used for disease risk assessment and diagnostics. A novel Bayesian approach for revealing clinically relevant signals is presented here for a real 1H NMR metabonomics data set. The results are not only mathematically superior but also biochemically fully coherent.
2468 en Exploration vs. Exploitation Challenge 
2469 en Discounted UCB 
2470 en A Simpler, Intuitive Approach to Morpheme Induction 
2471 en Simple UCB for parallel tracking 
2472 en Exploration Vs. Exploitation Chalenge part 2 
2473 en Answer Validation Excercise - AVE 
2474 en Entailment Cases in Intelligent Tutoring Systems 
2475 en Textual inference data set 
2476 en Classical Aproach Interpretation 
2477 en Summarisation and the RTE Challenge 
2479 en Learning a Distance Metric for Structured Network Prediction Man-made or naturally-formed networks typically exhibit a high degree of structural regularity. In this paper, we introduce the problem of structured network prediction: given a set of n entities and a desired distribution for connectivity, return a likely set of edges connecting the entities together in a network having the specified degree distribution. Prediction is useful for initializing a network, augmenting an existing network, and for filtering existing networks, when the structure of the network is known. In order to capture the inter-dependencies amongst pairwise predictions to learn parameters of our model, we build upon recent structured output models. Novel in our approach is the use of partially labeled training examples, and a network structure sensitive loss function. We present encouraging results of the model predicting equivalence graphs and links in a social network.
2482 en ROC analisys for subgroup evaluation 
2483 en Welcome 
2490 en Assesment of food product by time domain NMR and MRI V predavanju bodo predstavljene napredne preiskave ?ivil z neinvazivnimi metodami jedrske magnetne resonance in slikanja z magnetno resonanco. Predavanje bo s konkretnimi zgledi osredoto?eno na preu?evanje vpliva postopkov, ki jih razvija prehrambena tehnologija, na rok trajanja, strukturo ?ivil in sposobnost ?ivil, da ?im hitreje ve?ejo vodo.
2492 en Presentation of the book »Free Culture« by Lawrence Lessig About Free Culture Lawrence Lessig could be called a cultural environmentalist. One of America’s most original and influential public intellectuals, his focus is the social dimension of creativity: how creative work builds on the past and how society encourages or inhibits that building with laws and technologies. In his two previous books, CODE and THE FUTURE OF IDEAS, Lessig concentrated on the destruction of much of the original promise of the Internet. Now, in FREE CULTURE, he widens his focus to consider the diminishment of the larger public domain of ideas. In this powerful wake-up call he shows how short-sighted interests blind to the long-term damage they’re inflicting are poisoning the ecosystem that fosters innovation. All creative works—books, movies, records, software, and so on—are a compromise between what can be imagined and what is possible—technologically and legally. For more than two hundred years, laws in America have sought a balance between rewarding creativity and allowing the borrowing from which new creativity springs. The original term of copyright set by the Constitution in 1787 was seventeen years. Now it is closer to two hundred. Thomas Jefferson considered protecting the public against overly long monopolies on creative works an essential government role. What did he know that we’ve forgotten? Lawrence Lessig shows us that while new technologies always lead to new laws, never before have the big cultural monopolists used the fear created by new technologies, specifically the Internet, to shrink the public domain of ideas, even as the same corporations use the same technologies to control more and more what we can and can’t do with culture. As more and more culture becomes digitized, more and more becomes controllable, even as laws are being toughened at the behest of the big media groups. What’s at stake is our freedom—freedom to create, freedom to build, and ultimately, freedom to imagine. ;LINKS AND DOWNLOADS: : [[http://www.free-culture.cc/freecontent/|Free Culture / Free Content]] : [[http://www.lessig.org|Lessig.org]] : [[http://www.free-culture.cc/freeculture.pdf|Free Culture - pdf book==]] //FREE CULTURE is available for free under a Creative Commons license.\\ You may redistribute, copy, or otherwise reuse/remix this book provided that you do so for non-commercial purposes and credit Professor Lessig.//
2493 en Ethnomusicology of the Nineties: Perspectives of the History of Research Bruno Nettl, rojen v Pragi leta 1930, je "profesor emeritus glasbe in antropologije" na University of Illinois, Urbana-Champaign, ZDA. Doktorat je pridobil na Indiana University. ?eprav je njegov domicil ?e desetletja University of Illinois, je kot gostujo?i profesor deloval na ?tevilnih univerzah v ZDA in drugje po svetu. Prejel je vrhunske nagrade za dose?ke s podro?ij etnomuzikologije, muzikologije, antropologije in folkloristike Med njegove najpogosteje citirane knjige sodijo: North American Indian Musical Styles (1954); An Introduction to Folk Music in the United States (1960); Theory and Method in Ethnomusicology (1964); Folk and Traditional Music of the Western Continents (1965); Eight Urban Musical Cultures [ed.] (1978); The Study of Ethnomusicology: 29 Issues and Concepts (1983); The Western Impact on World Music (1985); The Radif of Persian Music: Studies of Structure and Cultural Context (1987); Blackfoot Musical Thought: Comparative Perspectives (1989); Comparative Musicology and Anthropology of Music: Essays on the History of Ethnomusicology [co-ed.] (1991); Excursions in World Music [co-ed.] (1992); New Perspectives on Improvisation [ed.] (1992); Heartland Excursions: Ethnomusicological Reflections on Schools of Music (1995); Encounters in Ethnomusicology, a Memoir (2002) and The Study of Ethnomusicology: 31 Issues and Concepts (2005). Marsikatera med njimi je bila prevedena v druge jezike.
2494 en Presentation of Ljudmila - Ljubljana digital media lab 
2495 en General idea of content at the festival 
2496 en Egoboobits More at [[http://www.egoboobits.net/|EGOBOO.bits]]
2497 en How to licence your work under CC 
2498 en An empirical case 
2499 en Authors rights 
2500 en The Brazilian Experience in the Creative Commons 
2501 en Project - Constructing an Internet archive 
2502 en Sharing the Creative Commons 
2503 en Project - 5 minutes for our mother tongue 
2504 en How I came across Creative Commons 
2506 en Open Source Enterprise Resource Planning and Order Management System for Eastern European Tool and Die Making Workshops 
2507 en Conclusion and Debate 
2508 en Tool East Solution 
2509 en Exploitation 
2510 en Technological Network TiC LENS 
2512 en OPTOMEC Inc. 
2514 en Dissemination and Exploitation 
2516 en Modern Medical Implants - Use of Advanced Manufacturing Technologies (LENS) 
2518 en Topology, structure and defects in carbon nanosystems In this talk we will explore how in the last twenty years carbon science has made the fundamental step from the flat world of graphite into the three dimensional world of fullerenes and nanotubes, the building blocks of the carbon nanotechnology revolution. We will look at the history of the discovery of buckminsterfullerene and carbon nanotubes, and explore analogies in diverse fields of biology, architecture and sport. The understanding of defects in nanocarbons is essential in order to control their diverse properties. For example irradiating bundles of carbon nanotubes produces defects which increase their bending strength by a factor of 16. At the same time such defects can store energy and were the cause of the UK “Windscale” nuclear fire in the 1950s. Recent advances in computational modelling and electron microscopy mean that we now have a much better understanding of the structure, formation and evolution of intrinsic defects, opening up the intriguing possibility of selective spatial creation of defects – atomic level defect engineering.
2519 en Decision Support, Multi-Attribute Decission Modelling and DEXI 
2520 en Introduction 
2521 en Overview of Environmental applications of Machine Learning 
2531 en Bio entrepeneur - Boot Camp for beginners Goal 1 : Understand the expectations of their future financial partners Venture Capitalists come to explain their ways of working, as well as what makes a "good" business plan. In the past, we have worked with EVCA and EASD representatives, in addition to independent VCs from European countries. Goal 2 : Share experiences with entrepreneurs of the country Biotech managers come to describe their "start up" experience, explaining both the easy and more complex aspects of business planning. Goal 3 : Provide TOOLS Learn how to use a platform of tools, including the BioBootCamp™ software (a program specially developed in the Excel and Microsoft Office environment), so that participants may generate their own business plans after the workshop. Participants may use it following the training to create unlimited simulations to assess the feasibility of a project at no cost and to follow the economics and financials of their new company during the 3 first years. They will learn how to write a convincing Executive Summary to open the doors of investors' offices. Plus of The "Bioentrepreneur Boot Camp for beginners" Participants also get a set of "Notes", which includes information about Venture Capital and web sites relating to Bio business. These web sites allow bio-entrepreneurs involved in Bioentrepreneur Boot Camp to continue networking and to identify other areas of need. At the end of the training workshop, each attendee takes away a personal copy of the BioBootCamp™ software and information package to work on their own project.
2555 en Poetry Reading 
2556 en Poetry Reading 
2557 en Poetry Reading 
2558 en Poetry Reading 
2560 en A set-output point of view on FDR control in multiple testing 
2563 en Bounding the k-family-wise error-rate using resampling methods 
2565 en Random probes for variable selection 
2568 en Intrinsic bounds on the BH multiple comparison procedure 
2576 en Multiple hypotheses testing in functional neuroimaging applications 
2577 en Poetry Reading 
2578 en Poetry Reading 
2579 en Semantic Annotated La Tex 
2580 en Poetry Reading 
2581 en Making Semantic Web Real 
2583 en Poetry Reading 
2584 en Context Sensitivity in Knowledge Rich Systems - Contents of parts 2 
2585 en Tags and Dependencies: An integrated View of Document Annotation 
2588 en SASA - A Semi Automatic Semantic Annotator for Personal Knowledge Management 
2589 en Poetry Reading 
2591 en Towards Trust for Semantic Web Annotations 
2624 en Debate about presentations 
2626 en Poetry Reading 
2628 en Determining significance in neuroimaging studies using covariate-modulated false discovery rate 
2632 en Presenting Vilenica Crystal Award 
2633 en Discussion on Poetry 
2635 en Data Mining and Knowledge Discovery 
2636 en Systems and Techniques for Decision Support 
2637 en E-Science and Computational Scientific Discovery 
2639 en Opening announcement for IPS 
2642 en Nanotechnology 
2645 en Electricity Features: Issues and Choices 
2646 en CHP Electricity Regulation by the EU for Facing the Liberalized Electricity Market 
2647 en Technology Management 
2648 en Cyc Representing, Acquiring and Using Knowledge This lecture will be given by Michael Witbrock, chief director of Cycorp (http://www.cyc.com) which is involved in the construction of the largest knowledge database and system for assumptions. On the lecture he will present us how to encode knowledge and how this knowledge is being used by computers with the intent to perform new unknown facts. He will also show some demos, which are pointing to the capabilities of Cyc which with its quality performs better results than any other known modern method which can be found on the web. We have to mention also that Cycorp has opened its new branch office in Slovenia. \\ On the seminar we will discuss the Cyc system which is said to be one of the most controversial experiments in computer and artificial intelligence history. The idea of Cyc has its beginnings in the 80s when the goal of a group of scientists from the University of Stanford was to build base of knowledge which would incorporate most of the knowledge we operate with in everyday life. On top of the knowledge base a mechanism of conclusion making would be used and would enable the use of encoded knowledge for the formulation of unknown knowledge. After 20 years of development the Cyc system contains a great quantity of common sense knowledge encoded in formal logic. The system was used for a series of difficult applications where a deeper view into the stored information was needed. \\ On the lecture we will be able to see how Cyc works in practice. Cyc and Jozef Stefan Institut, Slovenia have opened a new branch of Cyc to continue the development of the system. Michael Witborck is also the director of the Slovene affiliation.
2649 en Resambling-based confidence regions and multiple tests for a correlated random vector 
2650 en Conditions for validity of re-sampling based 
2651 en Gene-based bin-analysis of genome-wide association studied With the improvement of genotyping technologies and the exponentially growing number of available markers, case-control genome-wide association studies promise to be a key tool for investigation of complex diseases. However new analytical methods have to be developed to face the problems induced by this data scale-up, such as statistical multiple testing, data quality control, biological interpretation and computational tractability. We present a novel method to analyze genome-wide association studies results. The algorithm is based on a Bayesian model that integrates genotyping errors and genomic structure dependencies. Probability values are assigned to genomic regions termed bins, which are defined from a gene-biased partitioning of the genome, and the false-discovery rate is estimated. We have applied this algorithm to data coming from three genome-wide association studies of Multiple Sclerosis. The method practically overcomes the scale-up problems and permits to identify new putative regions statistically associated with the disease.
2652 en Sea Levels and Climate Change Popular interest and scientific concerns have focussed on the potential for sea level rise and increased risks of coastal flooding in a future warmer world. This talk will review the recent global evidence for changes, including the 2007 IPCC Report and look in detail at changes observed at the principal UK Tide Gauge site, Newlyn, from 1915 to 2005. Changes may include not only mean sea level rises, but also increased meterorological effects on surges, and changes in tidal regimes.
2653 en Research 7: Causal link matrix and AI planning: A model for Web service composition Automated composition of Web services or the process of forming new value added Web services is one of the most promising challenges in the semantic Web service research area. Semantics is one of the key elements for the automated composition of Web services because such a process requires rich machine-understandable descriptions of services that can be shared. Semantics enables Web service to describe their capabilities and processes, nevertheless there is still some work to be done. Indeed Web services described at functional level need a formal context to perform the automated composition of Web services. The suggested model (i.e., Causal link matrix) is a necessary starting point to apply problem-solving techniques such as regression-based search for Web service composition. The model supports a semantic context in order to find a correct, complete, consistent and optimal plan as a solution. In this paper an innovative and formal model for an AI planning-oriented composition is presented.
2654 en Research 7: Web Service Composition via Generic Procedures & Customizing User Preferences 
2655 en Research 7: A Constraint-based Approach to Horizontal Web Service Composition 
2658 en Scanning the brain and probing the mind Functional neuroimaging offers insight into the working of human mind and brain. It is used to study how the brain function changes in different neurological and psychiatric diseases. But can it be also used to explore the mysteries of love, to determine what people like, as a lie detector, or as an interface between the brain and the computer? Are we finally able to read the human mind?
2661 en Abiding Issues in the Study of North American Indian Music 
2662 en A Concert of Carnatic Music 
2664 en Interactions between antibodies and receptors of imune responses: from basic science to medicine use 
2665 en Nello Cristianini asking Gregory Chaitin about "Pattern" This is an ultra short interview where the posed question is the one that even Gottfried Wilhelm Leibniz posed to himself "What is the pattern" and what research did Gregory Chaitin on this subject.
2691 en Introduction to the Workshop on Multiple Simultaneous Hypothesis Testing 
2692 en Regularization of Kernel Methods by Decreasing the Bandwidth of the Gaussian Kernel 
2703 en Introduction to the ICML07 Conference 
2704 en Kernel Tricks, Means and Ends I will present my thoughts on what made kernel machines popular and what may or may not keep them going. I will also discuss applications in different domains, including computer graphics.
2706 en Practical Statistical Relational Learning The tutorial will be composed of three parts:** ** # ** Foundational areas.** The first part will consist of a brief introduction to each of the four foundational areas of SRL: logical inference, inductive logic programming, probabilistic inference, and statistical learning. Obviously, in the short time available no attempt will be made to comprehensively survey these areas; rather, the focus will be on providing the key concepts and techniques required for the subsequent parts. For example, the logical inference part will focus on the basics of satisfiability testing, and the probabilistic/statistical parts on Markov networks. The duration of this part will be approximately two hours (half hour per subtopic). # **Putting the pieces together.** The second part will introduce the key ideas in SRL and survey major approaches, using Markov logic as the unifying framework. It will present state-of-the-art algorithms for statistical relational learning and inference, and give an overview of the Alchemy open-source software. This part will essentially consist of putting together the pieces introduced in the first part. Its duration will be approximately an hour. # **Applications.** The third and final part will describe how to efficiently develop state-of-the-art non-i.i.d. applications in various areas, including: hypertext classification, link-based information retrieval, information extraction and integration, natural language processing, social network modeling, computational biology, and ubiquitous computing. This part will also include practical tips on using SRL, Markov logic and Alchemy - the kind of information that is seldom found in research papers, but is key to developing successful applications. The duration of this part will be approximately an hour.
2707 en Bayesian models of human inductive learning In everyday learning and reasoning, people routinely draw successful generalizations from very limited evidence. Even young children can infer the meanings of words, hidden properties of objects, or the existence of causal relations from just one or a few relevant observations -- far outstripping the capabilities of conventional learning machines. How do they do it? And how can we bring machines closer to these human-like learning abilities? I will argue that people's everyday inductive leaps can be understood as approximations to Bayesian computations operating over structured representations of the world, what cognitive scientists have called "intuitive theories" or "schemas". For each of several everyday learning tasks, I will consider how appropriate knowledge representations are structured and used, and how these representations could themselves be learned via Bayesian methods. The key challenge is to balance the need for strongly constrained inductive biases -- critical for generalization from very few examples -- with the flexibility to learn about the structure of new domains, to learn new inductive biases suitable for environments which we could not have been pre-programmed to perform in. The models I discuss will connect to several directions in contemporary machine learning, such as semi-supervised learning, structure learning in graphical models, hierarchical Bayesian modeling, and nonparametric Bayes.
2708 en Graphical Models for HIV Vaccine Design I will discuss two applications of graphical models to HIV vaccine design. The first helps determine how strongly our immune system fights HIV. The second helps identify which parts of HIV can be successfully attacked by our immune system. I will also discuss how these applications have exposed a weakness in the process of learning graphical models from data---namely, the inability to quantify how many arcs in a learned graphical model are spurious. I will offer a solution based on the False Discovery Rate.
2709 en Welcome Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
2710 en Introduction to Reinforcement Learning and Bayesian learning Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
2711 en Model-based Bayesian RL Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
2712 en Gaussian Process Temporal Difference Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
2714 en Demo - Control of an octopus arm using GPTD Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
2715 en ILP Invited Panel - Structured Machine Learning: The Next 10 Years 
2716 en Best Paper - Information-Theoretic Metric Learning In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem: that of minimizing the LogDet divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets.
2717 en Learning Distance Function by Coding Similarity We consider the problem of learning a similarity function from a set of positive equivalence constraints, i.e. "similar" point pairs. We define the similarity in information theoretic terms, as the gain in coding length when shifting from independent encoding of the pair to joint encoding. Under simple Gaussian assumptions, this formulation leads to a non-Mahalanobis similarity function which is effcient and simple to learn. This function can be viewed as a likelihood ratio test, and we show that the optimal similaritypreserving pro jection of the data is a variant of Fisher Linear Discriminant. We also show that under some naturally occurring sampling conditions of equivalence constraints, this function converges to a known Mahalanobis distance (RCA). The suggested similarity function exhibits superior performance over alternative Mahalanobis distances learnt from the same data. Its superiority is demonstrated in the context of image retrieval and graph based clustering, using a large number of data sets.
2718 en A Transductive Framework of Distance Metric Learning by Spectral Dimensionality Reduction Distance metric learning and nonlinear dimensionality reduction are two interesting and active topics in recent years. However, the connection between them is not thoroughly studied yet. In this paper, a transductive framework of distance metric learning is proposed and its close connection with many nonlinear spectral dimensionality reduction methods is elaborated. Furthermore, we prove a representer theorem for our framework, linking it with function estimation in an RKHS, and making it possible for generalization to unseen test samples. In our framework, it suffices to solve a sparse eigenvalue problem, thus datasets with 105 samples can be handled. Finally, experiment results on synthetic data, several UCI databases and the MNIST handwritten digit database are shown.
2719 en Dirichlet Aggregation: Unsupervised Learning towards an Optimal Metric for Proportional Data Proportional data (normalized histograms) have been frequently occurring in various areas, and they could be mathematically abstracted as points residing in a geometric simplex. A proper distance metric on this simplex is of importance in many applications including classification and information retrieval. In this paper, we develop a novel framework to learn an optimal metric on the simplex. Ma jor features of our approach include: 1) its flexibility to handle correlations among bins/dimensions; 2) widespread applicability without being limited to ad hoc backgrounds; and 3) a "real" global solution in contrast to existing traditional local approaches. The technical essence of our approach is to fit a parametric distribution to the observed empirical data in the simplex. The distribution is parameterized by affinities between simplex vertices, which is learned via maximizing likelihood of observed data. Then, these affinities induce a metric on the simplex, defined as the earth mover's distance equipped with ground distances derived from simplex vertex affinities.
2720 en Bias/variance analysis of relational domains 
2721 en Learning Probabilistic Stochastic Models from Probabilistic Examples 
2722 en Learning from Interpretations: A Rooted Kernel for Ordered Hypergraphs The paper presents a kernel for learning from ordered hypergraphs, a formalization that captures relational data as used in Inductive Logic Programming (ILP). The kernel generalizes previous approaches to graph kernels in calculating similarity based on walks in the hypergraph. Experiments on challenging chemical datasets demonstrate that the kernel outperforms existing ILP methods, and is competitive with state-of-the-art graph kernels. The experiments also demonstrate that the encoding of graph data can affect performance dramatically, a fact that can be useful beyond kernel methods.
2723 en Scalable Modeling of Real Graphs using Kronecker Multiplication Given a large, real graph, how can we generate a synthetic graph that matches its properties, i.e., it has similar degree distribution, similar (small) diameter, similar spectrum, etc? We propose to use "Kronecker graphs", which naturally obey all of the above properties, and we present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to real networks. A naive approach to fitting would take super-exponential time. In contrast, KronFit takes linear time, by exploiting the structure of Kronecker product and by using sampling. Experiments on large real and synthetic graphs show that KronFit indeed mimics very well the patterns found in the target graphs. Once fitted, the model parameters and the resulting synthetic graphs can be used for anonymization, extrapolations, and graph summarization.
2724 en Recovering Temporally Rewiring Networks: A model-based approach A plausible representation of relational information among entities in dynamic systems such as a living cell or a social community is a stochastic network which is topologically rewiring and semantically evolving over time. While there is a rich literature on modeling static or temporally invariant networks, much less has been done toward modeling the dynamic processes underlying rewiring networks, and on recovering such networks when they are not observable. We present a class of hidden temporal exponential random graph models (htERGMs) to study the yet unexplored topic of modeling and recovering temporally rewiring networks from time series of node attributes such as activities of social actors or expression levels of genes. We show that one can reliably infer the latent timespecific topologies of the evolving networks from the observation. We report empirical results on both synthetic data and a Drosophila lifecycle gene expression data set, in comparison with a static counterpart of htERGM.
2725 en Entire Regularization Paths for Graph Data Graph data such as chemical compounds and XML documents are getting more common in many application domains. A main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible subgraph patterns, the dimensionality gets too large for usual statistical methods. We propose an efficient method to select a small number of salient patterns by regularization path tracking. The generation of useless patterns is minimized by progressive extension of the search space. In experiments, it is shown that our technique is considerably more efficient than a simpler approach based on frequent substructure mining.
2726 en Learning to Compress Images and Video We present an intuitive scheme for lossy color-image compression: Use the color information from a few representative pixels to learn a model which predicts color on the rest of the pixels. Now, storing the representative pixels and the image in grayscale suffice to recover the original image. A similar scheme is also applicable for compressing videos, where a single model can be used to predict color on many consecutive frames, leading to better compression. Existing algorithms for colorization - the process of adding color to a grayscale image or video sequence - are tedious, and require intensive human-intervention. We bypass these limitations by using a graph-based inductive semi-supervised learning module for colorization, and a simple active learning strategy to choose the representative pixels. Experiments on a wide variety of images and video sequences demonstrate the efficacy of our algorithm.
2727 en Adaptive Mesh Compression in 3D Computer Graphics using Multiscale Manifold Learning This paper investigates compression of 3D ob jects in computer graphics using manifold learning. Spectral compression uses the eigenvectors of the graph Laplacian of an object's topology to adaptively compress 3D objects. 3D compression is a challenging application domain: ob ject models can have > 105 vertices, and reliably computing the basis functions on large graphs is numerically challenging. In this paper, we introduce a novel multiscale manifold learning approach to 3D mesh compression using diffusion wavelets, a general extension of wavelets to graphs with arbitrary topology. Unlike the "global" nature of Laplacian bases, diffusion wavelet bases are compact, and multiscale in nature. We decompose large graphs using a fast graph partitioning method, and combine local multiscale wavelet bases computed on each subgraph. We present results showing that multiscale diffusion wavelets bases are superior to the Laplacian bases for adaptive compression of large 3D ob jects.
2728 en Graph Clustering With Network Structure Indices Graph clustering has become ubiquitous in the study of relational data sets. We examine two simple algorithms: a new graphical adaptation of the k -medoids algorithm and the Girvan-Newman method based on edge betweenness centrality. We show that they can be effective at discovering the latent groups or communities that are defined by the link structure of a graph. However, both approaches rely on prohibitively expensive computations, given the size of modern relational data sets. Network structure indices (NSIs) are a proven technique for indexing network structure and efficiently finding short paths. We show how incorporating NSIs into these graph clustering algorithms can overcome these complexity limitations. We also present promising quantitative and qualitative evaluations of the modified algorithms on synthetic and real data sets.
2729 en Map Building without Localization by Dimensionality Reduction Techniques This paper proposes a new map building framework for mobile robot named Localization-Free Mapping by Dimensionality Reduction (LFMDR). In this framework, the robot map building is interpreted as a problem of reconstructing the 2-D coordinates of ob jects so that they maximally preserve the local proximity of the ob jects in the space of robot's observation history. Not only traditional linear PCA but also recent manifold learning techniques can be used for solving this problem. In contrast to the SLAM framework, LFMDR framework does not require localization procedures nor explicit measurement and motion models. In the latter part of this paper, we will demonstrate "visibility-only" and "bearing-only" localization-free mappings which are derived by applying LFMDR framework to the visibility and bearing measurements respectively.
2730 en Scalable Training of L1-regularized Log-linear Models The l-bfgs limited-memory quasi-Newton method is the algorithm of choice for optimizing the parameters of large-scale log-linear models with L2 regularization, but it cannot be used for an L1-regularized loss due to its non-differentiability whenever some parameter is zero. Eficient algorithms have been proposed for this task, but they are impractical when the number of parameters is very large. We present an algorithm OrthantWise Limited-memory Quasi-Newton (owlqn), based on l-bfgs, that can eficiently optimize the L1-regularized log-likelihood of log-linear models with millions of parameters. In our experiments on a parse reranking task, our algorithm was several orders of magnitude faster than an alternative algorithm, and substantially faster than lbfgs on the analogous L2-regularized problem. We also present a proof that owl-qn is guaranteed to converge to a globally optimal parameter vector.
2731 en Support Cluster Machine For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.
2732 en Trust Region Newton Methods for Large-Scale Logistic Regression Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also compare it with linear SVM implementations.
2733 en Large-scale RLSC Learning Without Agony The advances in kernel-based learning necessitate the study on solving a large-scale non-sparse positive definite linear system. To provide a deterministic approach, recent researches focus on designing fast matrixvector multiplication techniques coupled with a conjugate gradient method. Instead of using the conjugate gradient method, our paper proposes to use a domain decomposition approach in solving such a linear system. Its convergence property and speed can be understood within von Neumann's alternating pro jection framework. We will report significant and consistent improvements in convergence speed over the conjugate gradient method when the approach is applied to recent machine learning problems.
2734 en Unsupervised Prediction of Citation Influences Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact that papers have on each other, and helps to identify key contributions. To this end, we devise a probabilistic topic model that explains the generation of documents; the model incorporates the aspects of topical innovation and topical inheritance via citations. We evaluate the model's ability to predict the strength of influence of citations against manually rated citations.
2735 en Three New Graphical Models for Statistical Language Modelling The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best ngram models.
2736 en Mixtures of Hierarchical Topics with Pachinko Allo cation The four-level pachinko al location model (PAM) (Li & McCallum, 2006) represents correlations among topics using a DAG structure. It does not, however, represent a nested hierarchy of topics, with some topical word distributions representing the vocabulary that is shared among several more specific topics. This paper presents hierarchical PAM -- an enhancement that explicitly represents a topic hierarchy. This model can be seen as combining the advantages of hLD's topical hierarchy representation with PAM's ability to mix multiple leaves of the topic hierarchy. Experimental results show improvements in likelihood of held-out documents, as well as mutual information between automatically-discovered topics and humangenerated categories such as journals.
2737 en Unsupervised Estimation for Noisy-Channel Models Shannonâ€™s Noisy-Channel model, which describes how a corrupted message might be reconstructed, has been the corner stone for much work in statistical language and speech processing. The model factors into two components: a language model to characterize the original message and a channel model to describe the channelâ€™s corruptive process. The standard approach for estimating the parameters of the channel model is unsupervised Maximum-Likelihood of the observation data, usually approximated using the Expectation-Maximization (EM) algorithm. In this paper we show that it is better to maximize the joint likelihood of the data at both ends of the noisy-channel. We derive a corresponding bi-directional EM algorithm and show that it gives better performance than standard EM on two tasks: (1) translation using a probabilistic lexicon and (2) adaptation of a part-of-speech tagger between related languages.
2738 en Hierarchical Maximum Entropy Density Estimation We study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups, such as a hierarchy. For this problem, we propose a maximum entropy formulation, which systematically incorporates the groups and allows us to share the strength of prediction across similar datasets. We derive general performance guarantees, and show how some previous approaches, such as hierarchical shrinkage and hierarchical priors, can be derived as special cases. We demonstrate the proposed technique on synthetic data and in a realworld application to modeling the geographic distributions of species hierarchically grouped in a taxonomy. Specifically, we model the geographic distributions of species in the Australian wet tropics and Northeast New South Wales. In these regions, small numbers of samples per species significantly hinder effective prediction. Substantial benefits are obtained by combining information across taxonomic groups.
2739 en Learning to Combine Distances for Complex Representations The k-Nearest Neighbors algorithm can be easily adapted to classify complex objects (e.g. sets, graphs) as long as a proper dissimilarity function is given over an input space. Both the representation of the learning instances and the dissimilarity employed on that representation should be determined on the basis of domain knowledge. However, even in the presence of domain knowledge, it can be far from obvious which complex representation should be used or which dissimilarity should be applied on the chosen representation. In this paper we present a framework that allows to combine different complex representations of a given learning problem and/or different dissimilarities defined on these representations. We build on ideas developed previously on metric learning for vectorial data. We demonstrate the utility of our method in domains in which the learning instances are represented as sets of vectors by learning how to combine different set distance measures.
2740 en Non-Isometric Manifold Learning: Analysis and an Algorithm In this work we take a novel view of nonlinear manifold learning. Usually, manifold learning is formulated in terms of finding an embedding or "unrolling" of a manifold into a lower dimensional space. Instead, we treat it as the problem of learning a representation of a nonlinear, possibly non-isometric manifold that allows for the manipulation of novel points. Central to this view of manifold learning is the concept of generalization beyond the training data. Drawing on concepts from supervised learning, we establish a framework for studying the problems of model assessment, model complexity, and model selection for manifold learning. We present an extension of a recent algorithm, Locally Smooth Manifold Learning (L S M L), and show it has good generalization properties. L S M L learns a representation of a manifold or family of related manifolds and can be used for computing geodesic distances, finding the projection of a point onto a manifold, recovering a manifold from points corrupted by noise, generating novel points on a manifold, and more.
2741 en Manifold-adaptive dimension estimation Intuitively, learning should be easier when the data points lie on a low-dimensional submanifold of the input space. Recently there has been a growing interest in algorithms that aim to exploit such geometrical properties of the data. Oftentimes these algorithms require estimating the dimension of the manifold first. In this paper we propose an algorithm for dimension estimation and study its finite-sample behaviour. The algorithm estimates the dimension locally around the data points using nearest neighbor techniques and then combines these local estimates. We show that the rate of convergence of the resulting estimate is independent of the dimension of the input space and hence the algorithm is "manifold-adaptive". Thus, when the manifold supporting the data is low dimensional, the algorithm can be exponentially more efficient than its counterparts that are not exploiting this property. Our computer experiments confirm the obtained theoretical results.
2743 en Semi-supervised Structured Prediction Models 
2744 en Salience Assignment for Multiple-Instance Regression 
2745 en Toward Learning Mixture-of-Parts Pictorial Structures 
2746 en Learning CRFs with Hierarchical Features: An Application to Go 
2747 en Discriminative Graphical Models for Protein Quaternary Structure Motif Detection 
2748 en Structural Prediction in Statistical Alignment and Translation 
2749 en Parameter Learning for Loopy Markov Random Fields with Structural Support Vector Machines 
2750 en Learning a Layered Graph with a Maximal Number of Distinct Paths Between Source and Sink 
2751 en Sculpting Implants in situ: Light-Adjustable Intraocular Lens Ko se na?e o?esne le?e z leti starajo, se pogosto razvije o?esna mrena. Vsako leto se opravi preko 14 milijonov operacij, kjer se o?esna mrena odstrani in se vsadijo umetne le?e. S trenutno uveljavljenimi le?ami ?e vedno tretjina pacientov z o?esno mreno po posegu potrebuje o?ala za optimalen vid. Da bi dosegli ?eljeni cilj brez dodatnih korekcij, bi morale biti le?e sposobne prilagoditve po kon?ani terapiji. Novi materiali, ki so jih razvili na Caltech in?titutu, omogo?ajo post-operativne neinvazivne korekcije le?. V predavanju bo profesor Julia Kornfield predstavila skupne raziskave s prof. Bob Grubbsom iz Odseka za kemijo in o?esnim kirurgom prof. Dan Schwartzem iz UCSF. Skupaj so razvili material, ki omogo?a in vivo prilagoditve le?. Svetlobno-prilagodljive le?e so se izkazale za uspe?ne v klini?nih testih saj so rezultati ponovljivi in napovedljivi.
2753 en Patient-Cooperative Rehabilitation Robotics in Zurich 
2754 en FreeForm modeling of spinal implants 
2755 en Assessment of hand kinematics and its control in dexterous manipulation 
2756 en Grip force response in graphical and haptic virtual environment 
2757 en A Hierarchical SOM to Identify and Recognize Objects in Sequences of Stereo Images 
2758 en Can haptic interface be used for evaluating upper limb prosthesis in children and adults 
2760 en Introduction and Welcome to the workshop 
2761 en Introduction to Machine Learning This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.
2762 en Basics of algorithmics, computation models, formal languages Between the many theoretical computer science issues that one should ben aware of when working in Machine learning, we visit, in this series ofn lectures, two.n n The first corresponds to strings, and through the study of strings, then questions about more complex structures like trees and graphs. We describen the main algorithmic and combinatorial questions about substrings andn subsequences, and concentrate our attention to the topological questions:n ordering strings and computing distances and kernels.n n The second is complexity. Not only should we be aware (and have an reasonable control of the techniques involved) of the usual barriers, butn we should know something about classes for randomized algorithms. We alson show some examples concerning Las Vegas and Monte Carlo techniques.n
2763 en Basics of probability and statistics 
2764 en Introduction to CLOP Machine Learning Toolbox 
2765 en Bipartite Graph Matching for Computing the Edit Distance of Graphs In the field of structural pattern recognition graphs constitute a very common and powerful way of representing patterns. In contrast to string representations, graphs allow us to describe relational information in the patterns under consideration. One of the main drawbacks of graph representations is that the computation of standard graph similarity measures is exponential in the number of involved nodes. Hence, such computations are feasible for rather small graphs only. One of the most flexible error-tolerant graph similarity measures is based on graph edit distance. In this paper we propose an approach for the efficient compuation of edit distance based on bipartite graph matching by means of Munkres’ algorithm, sometimes referred to as the Hungarian algorithm. Our proposed algorithm runs in polynomial time, but provides only suboptimal edit distance results. The reason for its suboptimality is that implied edge operations are not considered during the process of finding the optimal node assignment. In experiments on semi-artificial and real data we demonstrate the speedup of our proposed method over a traditional tree search based algorithm for graph edit distance computation. Also we show that classification accuracy remains nearly unaffected.
2766 en Matching of Tree Structures for Registration of Medical Images Many medical applications require a registration of different images of the same organ. In many cases, such a registration is accomplished by manually placing landmarks in the images. In this paper we propose a method which is able to find reasonable landmarks automatically. To achieve this, nodes of the vessel systems, which have been extracted from the images by a segmentation algorithm, will be assigned by the so-called association graph method and the coordinates of these matched nodes can be used as landmarks for a non-rigid registration algorithm.
2769 en Two-on-two robot soccer A short demo of two-on-two robotic soccer, featuring the Cornell team's legacy and current players/robots.
2771 en Leonardo: Goal assistance with divergent beliefs This demonstrates the robot Leonardo's ability to reason and act competently in situations when the other (in this case, human) agents have different belief states.
2772 en Cosmo: The lifelike pedagogical agent 
2773 en Autonomous UAV capabilities This short demo summarizes the capabilities of some autonomous unmanned air vehicles/helos, flying outdoors, that do not use GPS navigation techniques.
2774 en Autonomous UAV search and rescue This longer demo films the application of some autonomous unmanned air vehicles/helos, flying outdoors, for a search and rescue operation.
2775 en iAQ: A Program that Discovers Rules This video presents an entertaining program that discovers rules from data and outputs them in the form of English text and speech
2776 en BICA: An idea that can change the world This describes and demonstrates in simulation the capabilities of an agent controlled by a biologically-inspired cognitive architecture.
2777 en NERO 2.0: Neuro Evolving Robotic Operatives This demonstrates the NERO real-time strategy game and the capabilities of its agents. The technology involves neuroevolution.
2779 en Interactive derivation viewer This describes the IDV, a tool for graphically rendering derivations that are written in the Thousands of Problems for Theorem Provers (TPTP) language.
2780 en Artificial intelligence: An instance of Aibo ingenuity This describes research related to using RL for, among other tasks, learning behaviors for an Aibo robot.
2781 en K-nearest neighbor classification In this short animated video the k-nearest neighbor classifier is introduced with simple 3D visuals. A real-world application, word pronunciation, is used to exemplify how the classifier learns and classifies. The video features a synthesized voice over.
2782 en A service robot named Markovito This shows the Peoplebot Markovito as it delivers messages and objects between offices. It can perform speech communication, face recognization, global localization, uses a probablistic grid map, and is controlled by a Factored MDP.
2783 en Color-based object recognition This demonstrates a robodog that recognize objects in a "fetch" task. The software runs on a world-wide computing grid, distributing the computational load over several beowolf clusters.
2784 en Power agents at the Mars Desert Research Station A comprehensive demonstration of the agents being used at the MDRS, scripted with inspiration from the HAL 9000. Permission granted for additional video length, although the main video ends at 5min.
2786 en Motion planning of multiple agents in virtual environments Describes and demonstrates in simulation the use of coordination graphs to avoid collisions of multiple agents in tasks requiring motion of multiple agents.
2788 en Humanoids for autonomous operations The video describes a Humanoid robotics project at JPL, claiming a first practical application of humanoid robotics.
2790 en Multimodal Interactive Robot Agent (MIRA robot head) A short video demonstrating the speech communication, reasoning abilities, and humour of MIRA.
2792 en MARQS: Media album retrieval by query sketch An advertisement-like short demo of a tool for retrieving photos from an album by sketching.
2793 en Robot Swarm localization using trilateration A description and demonstration of a robust approach for ground robot formation movement behaviors.
2794 en Morphogenesis: Shaping swarms of intelligent robots * Describes, simulates, and demonstrates in hardware the utility of (rule-based) morphogenesis for shaping robot swarms. * For more videos, pictures, and information on Morphogenesis and Morphology Control, see the following site: [[http://iridia.ulb.ac.be/supp/IridiaSupp2007-003/index.html|Photos, videos and information]] * You can also download a high-quality version of this video in various formats from: [[http://iridia.ulb.ac.be/%7Ealyhne/aaai-07/index.html|HQ version of the video]] * You can check the following web sites if you want to know more about the robots, swarm robotics, and swarm intelligence: [[http://www.swarmanoid.com|The Swarmanoid Project web-page]], [[http://www.swarm-bots.org|The Swarm-bots Project web-page]] ;Authors web pages: :[[http://iridia.ulb.ac.be/%7Ealyhne|Anders Lyhne Christensen's homepage]]\\ :[[http://iridia.ulb.ac.be/%7Erogrady|Rehan O'Grady's homepage]]\\ :[[http://iridia.ulb.ac.be/%7Emdorigo/|Marco Dorigo's homepage]]
2795 en Autonomous robot cleaning crew De-centralized collaborative planning and simulation demo for coordinating agent/robot tasks.
2796 en Dance evolution The only submission from undergraduates, this unique video challenges AI to learn how to dance by demonstrating how neuroevolution can be used to (interactively) evolve dancing techniques.
2797 en Learning without overlearning This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.
2798 en Introduction to feature selection This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.
2799 en Graph-based Methods for Retinal Mosaicing and Vascular Characterization In this paper, we propose a highly robust point-matching method (Graph Transformation Matching - GTM) relying on finding the consensus graph emerging from putative matches. Such method is a two- phased one in the sense that after finding the consensus graph it tries to complete it as much as possible. We successfully apply GTM to image registration in the context of finding mosaics from retinal images. Feature points are obtained after properly segmenting such images. In addition, we also introduce a novel topological descriptor for quantifying disease by characterizing the arterial/venular trees. Such descriptor relies on diffusion kernels on graphs. Our experiments have showed only statistical signifficance for the case of arterial trees, which is consistent with previous findings.
2800 en Stereo Vision for Obstacle Detection: a Graph-Based Approach We propose a new approach to stereo matching for obstacle detection in the autonomous navigation framework. An accurate but slow reconstruction of the 3D scene is not needed; rather, it is more important to have a fast localization of the obstacles to avoid them. All the methods in the literature, based on a punctual stereo matching, are ineffective in realistic contexts because they are either computationally too expensive, or unable to deal with the presence of uniform patterns, or of perturbations between the left and right images. Our idea is to face the stereo matching problem as a matching between homologous regions. The stereo images are represented as graphs and a graph matching is computed to find homologous regions. Our method is strongly robust in a realistic environment, requires little parameter tuning, and is adequately fast, as experimentally demonstrated in a comparison with the best algorithms in the literature.
2801 en A Continuous-Based Approach for Partial Clique Enumeration In many applications of computer vision and pattern recog- nition which use graph-based knowledge representation, it is of great interest to be able to extract the K largest cliques in a graph, but most methods are geared either towards extracting the single clique of max- imum size, or enumerating all cliques, without following any particular order. In this paper we present a novel approach for partial clique enu- meration, that is, the extraction of the K largest cliques of a graph. Our approach is based on a continuous formulation of the clique problem de- veloped by Motzkin and Straus, and is able to avoid extracting the same clique multiple times. This is done by casting the problem into a game- theoretic framework and iteratively rendering unstable the solutions that have already been extracted.
2802 en A Bound for Non-Subgraph Isomorphism In this paper we propose a new lower bound to a subgraph isomorphism problem. This bound can provide a proof that no subgraph isomorphism between two graphs can be found. The computation is based on the SDP relaxation of a – to the best of our knowledge – new combinatorial optimisation formulation for subgraph isomorphism. We consider problem instances where only the structures of the two graph instances are given and therefore we deal with simple graphs in the first place. The idea is based on the fact that a subgraph isomorphism for such problem instances always leads to 0 as lowest possible optimal objective value for our combinatorial optimisation problem formulation. Therefore, a lower bound that is larger than 0 represents a proof that a subgraph isomorphism don’t exist in the problem instance. But note that conversely, a negative lower bound does not imply that a subgraph isomorphism must be present and only indicates that a subgraph isomorphism is still possible.
2803 en A Correspondence Measure for Graph Matching using the Discrete Quantum Walk In this paper we consider how coined quantum walks can be applied to graph matching problems. The matching problem is ab- stracted using an auxiliary graph that connects pairs of vertices from the graphs to be matched by way of auxiliary vertices. A coined quantum walk is simulated on this auxiliary graph and the quantum interference on the auxiliary vertices indicates possible matches. When dealing with graphs for which there is no exact match, the interference amplitudes to- gether with edge consistencies are used to define a consistency measure. We have tested the algorithm on graphs derived from the NCI molecule database and found it to significantly reduce the space of possible match- ings thereby allowing the graphs to be matched directly. An analysis of the quantum walk in the presence of structural errors between graphs is used as the basis of the consistency measure. We test the performance of this measure on graphs derived from images in the COIL-100 database.
2804 en Other ML/DM software (R, Weka, Yale) 
2805 en Feature construction This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.  Play with the Gisette dataset of the feature selection challenge. See how with simple feature extraction methods, performances can be improved over the pure “agnostic” approach.
2806 en Casuality and feature selection 
2807 en Probability, Information Theory and Bayesian Inference 
2808 en Embedded Methods This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.
2809 en The EM algorithm and Mixtures of Gaussians 
2811 en Kernels and Gaussian Processes 
2812 en PANEL: Experiences in research, teaching, and applications of ML 
2813 en Lectures on Clustering These lectures give an introduction to data clustering: we discuss an few algorithms, but also look at theoretical questions related ton clustering. n\\**The first two lectures** are devoted to spectral clustering:n graph Laplacians and their properties, spectral clustering algorithms,n mathematical derivations of the algorithms, and some implementationn issues. Moreover, we discuss the related modularity approach forn detecting communities in networks. n\\**The third lecture** is devoted ton the very general question "what clustering is". We try to look atn clustering from different angles, discuss different definitions ofn clustering, and look into theoretical foundations of clustering inn general.n\\**In the last lecture** we work on the question how the numbern of clusters should be defined. The focus is on two popular approaches:n the gap statistics and the stability approach.
2814 en Image Classification Using Marginalized Kernels for Graphs We propose in this article an image classification technique based on kernel methods and graphs. Our work explores the possibility of applying marginalized kernels to image processing. In machine learning, performant algorithms have been developed for data organized as real valued arrays; these algorithms are used for various purposes like classification or regression. However, they are inappropriate for direct use on complex data sets. Our work consists of two distinct parts. In the first one we model the images by graphs to be able to represent their structural properties and inherent attributes. In the second one, we use kernel functions to project the graphs in a mathematical space that allows the use of performant classification algorithms. Experiments are performed on medical images acquired with various modalities and concerning di_erent parts of the body.
2815 en Hierarchy Construction Schemes within the Scale Set Framework Segmentation algorithms based on an energy minimisation framework often depend on a scale parameter which balances a fit to data and a regularising term. Irregular pyramids are defined as a stack of graphs successively reduced. Within this framework, the scale is often defined implicitly as the height in the pyramid. However, each level of an irregular pyramid can not usually be readily associated to the global optimum of an energy or a global criterion on the base level graph. This last drawback is addressed by the scale set framework designed by Guigues. The methods designed by this author allow to build a hierarchy and to design cuts within this hierarchy which globally minimise an energy. This paper studies the influence of the construction scheme of the initial hierarchy on the resulting optimal cuts. We propose one sequential and one parallel method with two variations within both. Our sequential methods provide partitions near the global optima while parallel methods require less execution times than the sequential method of Guigues even on sequential machines.
2816 en Deducing Local Influence Neighbourhoods With Application to Edge-Preserving Image Denoising Traditional image models enforce global smoothness, and more recently Markovian Field priors. Unfortunately global models are inadequate to represent the spatially varying nature of most images, which are much better modeled as piecewise smooth. This paper advocates the concept of local influence neighbourhoods (LINs). The influence neighbourhood of a pixel is defined as the set of neighbouring pixels which have a causal influence on it. LINs can therefore be used as a part of the prior model for Bayesian denoising, deblurring and restoration. Using LINs in prior models can be superior to pixel-based statistical models since they provide higher order information about the local image statistics. LINs are also useful as a tool for higher level tasks like image segmentation. We propose a fast graph cut based algorithm for obtaining optimal influence neighbourhoods, and show how to use them for local filtering operations. Then we present a new expectation-maximization algorithm to perform locally optimal Bayesian denoising. Our results compare favourably with existing denoising methods.
2817 en An Introduction to Ensemble and Boosting 
2818 en Graph Spectral Image Smoothing A new method for smoothing both gray-scale and color images is presented that relies on the heat diffusion equation on a graph. We represent the image pixel lattice using a weighted undirected graph. The edge weights of the graph are determined by the Gaussian weighted distances between local neighbouring windows. We then compute the associated Laplacian matrix (the degree matrix minus the adjacency matrix). Anisotropic diffusion across this weighted graph-structure with time is captured by the heat equation, and the solution, i.e. the heat kernel, is found by exponentiating the Laplacian eigen-system with time. Image smoothing is accomplished by convolving the heat kernel with the image, and its numerical implementation is realized by using the Krylov subspace technique. The method has the effect of smoothing within regions, but does not blur region boundaries. We also demonstrate the relationship between our method, standard diffusion-based PDEs, Fourier domain signal processing and spectral clustering. Experiments and comparisons on standard images illustrate the effectiveness of the method.
2819 en Probabilistic Relaxation Labeling by Fokker-Planck Diffusion on a Graph In this paper we develop a new formulation of probabilistic relaxation labeling for the task of data classification using the theory of diffusion processes on graphs. The state space of our process as the nodes of a support graph which represent potential object-label assignments. The edge-weights of the support graph encode data-proximity and label consistency information. The state-vector of the diffusion process represents the object-label probabilities. The state vector evolves with time according to the Fokker-Planck equation.We show how the solution state vector can be estimated using the spectrum of the Laplacian matrix for the weighted support graph. Experiments on various data clustering tasks show effectiveness of our new algorithm.
2820 en A general purpose segmentation algorithm using analytically evaluated random walks An ideal segmentation algorithm could be applied equally to the problem of isolating organs in a medical volume or to editing a digital photograph without modifying the algorithm, changing parameters, or sacrificing segmentation quality. However, a general-purpose, multiway segmentation of objects in an image/volume remains a challenging problem. In this talk, I will describe a recently developed approach to this problem that inputs a few training points from a user (e.g., from mouse clicks) and produces a segmentation by computing the probabilities that a random walker leaving unlabeled pixels/voxels will first strike the training set. By exact mathematical equivalence with a problem from potential theory, these probabilities may be computed analytically and deterministically. The algorithm is developed on an arbitrary, weighted, graph/mesh in order to maximize the broadness of application. I will illustrate the use of this approach with examples from several segmentation problems (without modifying the algorithm or the single free parameter), compare this algorithm to other approaches and discuss the theoretical properties that describe its behavior.
2821 en Qualitative Spatial Relationships for Image Interpretation by using Semantic Graph In this paper, a new way to express complex spatial relations is proposed in order to integrate them in a Constraint Satisfaction Problem with bilevel constraints. These constraints allow to build semantic graphs, which can describe more precisely the spatial relations between subparts of a composite object that we look for in an image. For example, it allows to express complex spatial relations such as “is surrounded by”. This approach can be applied to image interpretation and some examples on real images are presented.
2822 en Separation of the Retinal Vascular Graph in Arteries and Veins The vascular structure of the retina consists of two kinds of vessels: arteries and veins. Together these vessels form the vascular graph. In this paper we present an approach to separating arteries and veins based on a pre-segmentation and a few hand-labelled vessel segments. We use a rule-based method to propagate the vessel labels through the vascular graph. We embed this task as double-layered constrained search problem steered by a heuristical AC-3 algorithm to overcome the NPhard computational complexity. Results are presented on vascular graphs generated from hand-made as well as on automatical segmentation.
2823 en Theory and Applications of Kernel Space ;Basics of kernel definitions and theory are first given.\\Then 3 algorithms are described with an explicit reference to the representer theorem:n: Support vector Mahcines,n: Support Vector Regression andn: Kernel Principal Components Analysis.nnThe last course is devoted to examples of kernel design (Mahalanobis kernles and Fisher kernels)
2824 en Machine Learning in Vision 
2825 en Learning the topology of a data set 
2826 en Graph Based Shapes Representation and Recognition In this paper, we propose to represent shapes by graphs. Based on graphic primitives extracted from the binary images, attributed relational graphs were generated. Thus, the nodes of the graph represent shape primitives like vectors and quadrilaterals while arcs describing the mutual primitives relations. To be invariant to transformations such as rotation and scaling, relative geometric features extracted from primitives are associated to nodes and edges as attributes. Concerning graph matching, due to the fact of NP-completeness of graph-subgraph isomorphism, a considerable attention is given to different strategies of inexact graph matching. We also present a new scoring function to compute a similarity score between two graphs, using the numerical values associated to the nodes and edges of the graphs. The adaptation of a greedy graph matching algorithm with the new scoring function demonstrates significant performance improvements over traditional exhaustive searches of graph matching.
2827 en System for extracting data (facts) from large amount of unstructured documents 
2828 en Comparing Sets of 3D Digital Shapes through Topological Structures New technologies for shape acquisition and rendering of digital shapes have simplified the process of creating virtual scenes; nonetheless, shape annotation, recognition and manipulation of both the complete virtual scenes and even of subparts of them are still open problems. Once the main components of a virtual scene are represented by structural descriptions, this paper deals with the problem of comparing two (or more) sets of 3D objects, where each model is represented by an attributed graph. We will define a new distance to estimate the possible similarities among the sets of graphs and we will validate our work using a shape graph.
2829 en A Quadratic Programming Approach to the Graph Edit Distance Problem In this paper we propose a quadratic programming approach to computing the edit distance of graphs. Whereas the standard edit distance is defined with respect to a minimum-cost edit path between graphs, we introduce the notion of fuzzy edit paths between graphs and provide a quadratic programming formulation for the minimization of fuzzy edit costs. Experiments on real-world graph data demonstrate that our proposed method is able to outperform the standard edit distance method in terms of recognition accuracy on two out of three data sets.
2830 en Graph-Based Perceptual Segmentation of Stereo Vision 3D Images at Multiple Abstraction Levels This paper presents a new technique based on perceptual information for the robust segmentation of noisy 3D scenes acquired by stereo vision. A low-pass geometric ¯lter is ¯rst applied to the given cloud of 3D points to remove noise. The tensor voting algorithm is then applied in order to extract perceptual geometric information. Finally, a graph-based segmenter is utilized for extracting the di®erent geometric structures present in the scene through a region-growing procedure that is applied hierarchically. The proposed algorithm is evaluated on real 3D scenes acquired with a trinocular camera.
2831 en Morphological Operators for Flooding, Leveling and Filtering Images Using Graphs We define morphological operators on weighted graphs in order to speed up image transformations such as floodings, levelings and waterfall hierarchies. The image is represented by its region adjacency graph in which the nodes represent the catchment basins of the image and the edges link neighboring regions. The weights of the nodes represent the level of flooding in each catchment basin ; the weights of the edges represent the altitudes of the pass points between adjacent regions.
2832 en Graph Based Multilevel Temporal Segmentation of Scripted Content Videos This paper concentrates on a graph-based multilevel temporal segmentation method for scripted content videos. In each level of the segmentation, a similarity matrix of frame strings, which are series of consecutive video frames, is constructed by using temporal and spatial contents of frame strings. A strength factor is estimated for each frame string by using a priori information of a scripted content. According to the similarity matrix reevaluated from a strength function derived by the strength factors, a weighted undirected graph structure is implemented. The graph is partitioned to clusters, which represent segments of a video. The resulting structure defines a hierarchically segmented video tree. Comparative performance results of different types of scripted content videos are demonstrated.
2833 en Assessing the Performance of a Graph-based Clustering Algorithm Graph-based clustering algorithms are particularly suited for dealing with data that do not come from a Gaussian or a spherical distribution. They can be used for detecting clusters of any size and shape without the need of specifying the actual number of clusters; moreover, they can be profitably used in cluster detection problems. In this paper, we propose a detailed performance evaluation of four different graph-based clustering approaches. Three of the algorithms selected for comparison have been chosen from the literature. While these algorithms do not require the setting of the number of clusters, they need, however, some parameters to be provided by the user. So, as the fourth algorithm under comparison, we propose in this paper an approach that overcomes this limitation, proving to be an effective solution in real applications where a completely unsupervised method is desirable.
2834 en A Fast Construction of the Distance Graph Used for the Classification It has been demonstrated that the diffcult problem of classifying heterogeneous projection images, similar to those found in 3D electron microscopy (3D-EM) of macromolecules, can be successfully solved by finding an approximate Max k-Cut of an appropriately constructed weighted graph. Despite of the large size (thousands of nodes) of the graph and the theoretical computational complexity of finding even an approximate Max k-Cut, an algorithm has been proposed that finds a good (from the classification perspective) approximate solution within several minutes (running on a standard PC). However, the task of constructing the complete weighted graph (that represents an instance of the projection image classification problems) is computationally expensive. Due to the large number of edges, the computation of edge weights can take tens of hours for graphs containing several thousand nodes. We propose a method, which utilizes an early termination technique, to significantly reduce the computational cost of constructing such graphs. We compare, on synthetic data sets that resemble projection sets encountered in 3D-EM, the performance of our method with that of a brute-force approach and a method based on nearest neighbor search.
2835 en On the Relation Between the Median and the Maximum Common Subgraph of a Set of Graphs Given a set of elements, the median can be a useful concept to get a representative that captures the global information of the set. In the domain of structural pattern recognition, the median of a set of graphs has also been defined and some properties have been derived. In addition, the maximum common subgraph of a set of graphs is a well known concept that has various applications in pattern recognition. The computation of both the median and the maximum common subgraph are highly complex tasks. Therefore, for practical reasons, some strategies are used to reduce the search space and obtain approximate solutions for the median graph. The bounds on the sum of distances of the median graph to all the graphs in the set turns out to be useful in the definition of such strategies. In this paper, we reduce the upper bound of the sum of distances of the median graph and we relate it to the maximum common subgraph.
2836 en Generalized vs Set Median String for Histogram Based Distances: Algorithms and Classification Results in the Image Domain We compare different statistical characterizations of a set of strings, for three different histogram-based distances. Given a distance, a set of strings may be characterized by its generalized median, i.e., the string —over the set of all possible strings— that minimizes the sum of distances to every string of the set, or by its set median, i.e., the string of the set that minimizes the sum of distances to every other string of the set. For the first two histogram-based distances, we show that the generalized median string can be computed efficiently; for the third one, which biased histograms with individual substitution costs, we conjecture that this is a NP-hard problem, and we introduce two different heuristic algorithms for approximating it. We experimentally compare the relevance of the three histogram-based distances, and the different statistical characterizations of sets of strings, for classifying images that are represented by strings.
2837 en Constellations and the Graph Embedding using Quantum Commute Times In this paper, we explore analytically and experimentally the commute time of the continuous-time quantum walk. For the classical random walk, the commute time has been shown to be robust to errors in edge weight structure and to lead to spectral clustering algorithms with improved performance. Our analysis shows that the commute time of the continuous-time quantum walk can be determined via integrals of the Laplacian spectrum, calculated using Gauss-Laguerre quadrature. We analyse the quantum commute times with reference to their classical counterpart. Experimentally, we show that the quantum commute times can be used to emphasise cluster-structure.
2838 en Constellations and the Unsupervised Learning of Graphs In this paper, we propose a novel method for the unsupervised clustering of graphs in the context of the constellation approach to object recognition. Such method is an EM central clustering algorithm which builds prototypical graphs on the basis of fast matching with graph transformations. Our experiments, both with random graphs and in realistic situations (visual localization), show that our prototypes improve the set median graphs and also the prototypes derived from our previous incremental method. We also discuss how the method scales with a growing number of images.
2839 en Graph Embedding in Vector Spaces by Means of Prototype Selection The field of statistical pattern recognition is characterized by the use of feature vectors for pattern representation, while strings or, more generally, graphs are prevailing in structural pattern recognition. In this paper we aim at bridging the gap between the domain of feature based and graph based object representation. We propose a general approach for transforming graphs into n-dimensional real vector spaces by means of prototype selection and graph edit distance computation. This method establishes the access to the wide range of procedures based on feature vectors without loosing the representational power of graphs. Through various experimental results we show that the proposed method, using graph embedding and classification in a vector space, outperforms the tradional approach based on k-nearest neighbor classification in the graph domain.
2840 en Grouping Using Factor Graphs: an Approach for Finding Text with a Camera Phone We introduce a new framework for feature grouping based on factor graphs, which are graphical models that encode interactions among arbitrary numbers of random variables. The ability of factor graphs to express interactions higher than pairwise order (the highest order encountered in most graphical models used in computer vision) is useful for modeling a variety of pattern recognition problems. In particular, we show how this property makes factor graphs a natural framework for performing grouping and segmentation, which we apply to the problem of finding text in natural scenes. We demonstrate an implementation of our factor graph-based algorithm for finding text on a Nokia camera phone, which is intended for eventual use in a camera phone system that finds and reads text (such as street signs) in natural environments for blind users.
2841 en Graph kernels and applications in chemoinformatics Several problems in chemistry can be formulated as classification or regression problems over molecules which, when represented by their planar structure, can be seen as labeled graphs. Several approaches have been proposed recently to define positive definite kernels over labeled graphs, paving the way to the use of powerful kernel methods in chemoinformatics. In this talk I will review some of these approaches and present relevant applications in computational chemistry.
2842 en An Efficient Ontology-Based Expert Peering System This paper proposes a novel expert peering system for information exchange. Our objective is to develop a real-time search engine for an online community where users can query experts, who are simply other participating users knowledgeable in that area, for help on various topics.We consider a graph-based scheme consisting of an ontology tree where each node represents a (sub)topic. Consequently, the fields of expertise or profiles of the participating experts correspond to subtrees of this ontology. Since user queries can also be mapped to similar tree structures, assigning queries to relevant experts becomes a problem of graph matching. A serialization of the ontology tree allows us to use simple dot products on the ontology vector space effectively to address this problem. As a demonstrative example, we conduct extensive experiments with different parameterizations. We observe that our approach is efficient and yields promising results.
2843 en Computing Homology Group Generators of Images Using Irregular Graph Pyramids We introduce a method for computing homology groups and their generators of a 2D image, using a hierarchical structure i.e. irregular graph pyramid. Starting from an image, a hierarchy of the image is built, by two operations that preserve homology of each region. Instead of computing homology generators in the base where the number of entities (cells) is large, we first reduce the number of cells by a graph pyramid. Then homology generators are computed efficiently on the top level of the pyramid, since the number of cells is small, and a top down process is then used to deduce homology generators in any level of the pyramid, including the base level i.e. the initial image. We show that the new method produces valid homology generators and present some experimental results.
2844 en Approximating TSP Solution by MST based Graph Pyramid The traveling salesperson problem (TSP) is difficult to solve for input instances with large number of cities. Instead of finding the solution of an input with a large number of cities, the problem is approximated into a simpler form containing smaller number of cities, which is then solved optimally. Graph pyramid solution strategies, in a bottom-up manner using Boruvka’s minimum spanning tree, convert a 2D Euclidean TSP problem with a large number of cities into successively smaller problems (graphs) with similar layout and solution, until the number of cities is small enough to seek the optimal solution. Expanding this tour solution in a top-down manner to the lower levels of the pyramid approximates the solution. The new model has an adaptive spatial structure and it simulates visual acuity and visual attention. The model solves the TSP problem sequentially, by moving attention from city to city with the same quality as humans. Graph pyramid data structures and processing strategies are a plausible model for finding near-optimal solutions for computationally hard pattern recognition problems.
2845 en The Construction of Bounded Irregular Pyramids with a Union-Find Decimation Process The Bounded Irregular Pyramid (BIP) is a mixture of regular and irregular pyramids whose goal is to combine their advantages. Thus, its data structure combines a regular decimation process with a union-find strategy to build the successive levels of the structure. The irregular part of the BIP allows to solve the main problems of regular structures: their inability to preserve connectivity or to represent elongated objects. On the other hand, the BIP is computationally efficient because its height is constrained by its regular part. In this paper the features of the Bounded Irregular Pyramid are discussed, presenting a comparison with the main pyramids present in the literature when applied to a colour segmentation task.
2846 en Extending the Notion of AT-Models for Integer Homology Computation When the ground ring is a field, the notion of algebraic topological model (AT-model) is a useful tool for computing (co)homology, representative (co)cycles of (co)homology generators and the cup product on cohomology of nD digital images as well as for controlling topological information when the image suffers local changes. In this paper, we formalize the notion of lambda-AT-model (lambda being an integer) which extends the one of AT-model and allows the computation of homological information in the integer domain without computing the Smith Normal Form of the boundary matrices. We present an algorithm for computing such a model, obtaining Betti numbers, the prime numbers p involved in the invariant factors (corresponding to the torsion subgroup of the homology), the amount of invariant factors that are a power of p and a set of representative cycles of the generators of homology mod p, for such p.
2847 en ML in Bioinformatics After a brief introduction to the use of machine learning in computational biology, we focus on the problem of biological networks inference. We define the problem as a problem of kernel learning using prediction in kernelized output spaces. Methods based on Output kernel Tree are presented to solve the problem. Results on two benchmarks are shown.
2849 en Opening 
2853 en Mining Frequent Closed Unordered Trees Through Natural Representations 
2854 en Learning with spectral representations and use of MDL principles 
2855 en Characterizing Implications of Injective Partial Orders 
2856 en Genetic Approximate Matching of Attributed Relational Graphs 
2857 en Molecular Graph Kernels for Drug Discovery 
2858 en Graphs Regularization for Data Sets and Images: Filtering and Semi-Supervised Classification 
2859 en Graph Signature: A Simple Approach for Clustering Similar Graph 
2860 en Szemerédi's Regularity Lemma and PairwiseClustering 
2861 en Web People Search Disambiguation using Random Walks 
2862 en Opening of the PASCAL Workshop 
2863 en Topic Learning From Few Examples 
2864 en From Knowledge-based Systems to Skill-based Systems: Sailing as a Machine Learning Challenge 
2865 en A Simple Algorithm for Topic Identification in 0-1 Data 
2866 en Image Registration for Medical Diagnosis and Intervention 
2867 en Information Technology Solutions for Diabetes Management and Prevention: Current Challenges and Future Research directions 
2868 en Ambulatory blood pressure monitoring is highly sensitive for detection of early cardiovascular rsk factors in young adults 
2869 en Development of Implantable SAW Probe for Epilepsy Prediction 
2870 en Evaluation of non-invasive blood pressure simulators 
2871 en Wearable Wireless Biopotential Electrode for ECG Monitoring 
2872 en The Education and Training of the Medical Physicist in Europe. The European Federation of Organisations for Medical Physics - EFOMP Policy Statements and Efforts 
2874 en Presentation of Cochlear Implant to Deaf People 
2875 en Electrically Elicited Stapedius Muscle Reflex in Cochlear Implant System fitting 
2876 en Use of rapid prototyping technology in comprehensive rehabilitation of a patient with congenital facial deformity or partial finger or hand amputation 
2877 en Experimental evaluation of training device for upper extremities sensory-motor ability augmentation 
2878 en New Experimental Results in Assessing and Rehabilitating the Upper Limb Function by Means of the Grip Force Tracking Method 
2879 en Using computer vision in a rehabilitation method of a human hand 
2880 en Assessment of a system developed for virtual teaching 
2882 en From Academy to Industry: Translational Research in Biophysics 
2883 en Bases and rationale of the electrochemotherapy 
2886 en Standard versus 3D optimized MRI-based planning for uterine cervix cancer brachyradiotherapy – The Ljubljana experience 
2887 en Problems faced after the transition from a film to a DDR Radiology Department 
2888 en Verification of planned relative dose distribution for irradiation treatment technique using half-beams in the area of field abutment 
2889 en Scattered radiation spectrum analysis for the breast cancer diagnostics 
2890 en EMITEL - an e-Encyclopedia for Medical Imaging Technology 
2891 en Laminar Axially Directed Blood Flow Promotes Blood Clot Dissolution: Mathematical Modeling Verified by MR Microscopy 
2892 en Ten problems for the next 10 years 
2893 en The next 10 years of ILP 
2894 en SRL - The next decade 
2895 en Reunited the splinter groups into one big relevanr force 
2896 en Declarative Vs. Procedural 
2897 en Statistical Predicate Invention We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the ob jects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.
2898 en Adaptive Dimension Reduction Using Discriminant Analysis and K-means Clustering Regularized Kernel Discriminant Analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. The performance of RKDA depends on the selection of kernels. In this paper, we consider the problem of learning an optimal kernel over a convex set of kernels. We show that the kernel learning problem can be formulated as a semidefinite program (SDP) in the binary-class case. We further extend the SDP formulation to the multi-class case. It is based on a key result established in this paper, that is, the multi-class kernel learning problem can be decomposed into a set of binary-class kernel learning problems. In addition, we propose an approximation scheme to reduce the computational complexity of the multi-class SDP formulation. The performance of RKDA also depends on the value of the regularization parameter. We show that this value can be learned automatically in the framework. Experimental results on benchmark data sets demonstrate the efficacy of the proposed SDP formulations.
2899 en Optimal Dimensionality of Metric Space for Classification For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.
2900 en Learning for Efficient Retrieval of Structured Data with Noisy Queries Increasingly large collections of structured data necessitate the development of efficient, noise-tolerant retrieval tools. In this work, we consider this issue and describe an approach to learn a similarity function that is not only accurate, but that also increases the effectiveness of retrieval data structures. We present an algorithm that uses functional gradient boosting to maximize both retrieval accuracy and the retrieval efficiency of vantage point trees. We demonstrate the effectiveness of our approach on two datasets, including a moderately sized real-world dataset of folk music.
2901 en Introduction to the panel 
2902 en Debate 
2903 en Robust Non-linear Dimensionality Reduction using Successive 1-Dimensional Laplacian Eigenmapse Non-linear dimensionality reduction of noisy data is a challenging problem encountered in a variety of data analysis applications. Recent results in the literature show that spectral decomposition, as used for example by the Laplacian Eigenmaps algorithm, provides a powerful tool for non-linear dimensionality reduction and manifold learning. In this paper, we discuss a significant shortcoming of these approaches, which we refer to as the repeated eigendirections problem. We propose a novel approach that combines successive 1dimensional spectral embeddings with a data advection scheme that allows us to address this problem. The proposed method does not depend on a non-linear optimization scheme; hence, it is not prone to local minima. Experiments with artificial and real data illustrate the advantages of the proposed method over existing approaches. We also demonstrate that the approach is capable of correctly learning manifolds corrupted by significant amounts of noise.
2905 en Stability for selecting the number of clusters: literature review, questions, and ideas 
2907 en A formal analysis of stability - lessons and open questions 
2908 en Cluster Stability for Finite Samples 
2909 en On a statistical model of cluster stability 
2910 en Cluster Stability Analysis Based on the Assessment of Individual Clusters 
2911 en Stability and convergence 
2912 en Comparing clustering with confidence 
2913 en Graph mincut, transductive inference and spectral clustering: some new elements 
2914 en Cluster stability and robust optimization 
2915 en Impromptu session 
2925 en Druga Godba Festival- performance of the tango quintet Astorpia In recent times, Slovene musicians have been busy proving that they can successfully test themselves against virtually any style of music, traditional or classical. For several years now, tango has been a part of everyday life here, and its popularity keeps on growing. This new, tight and well-honed ensemble- most of whose members have classical music training- can undoubtedly contribute greatly to increasing the popularity of tango in Slovenia still further, but they also offer much more than just reinterpretations of the old Argentinian standards. Since the name Astorpia itself alludes to one of the most important *new tango* artists, Astor Piazzolla it should come as no surprise that we find five of his compositions on their first album MAR DEL PLATA, among them Libertango and Invierno Porteno, which have already become classics and make up part of a cycle dedicated to the four seasons. In addition to these pieces, which Astorpia manage to imbue with renewed vigour and freshness, their repertoire includes compositions by a number of other artists. They are not averse to playing wittily with the established rhythms of tango, not even to placing them in a Balkan context, in a similar way to composer Milos Simic. In one of their songs, the group take a different direction entirely with a fiery Czardas. Viva el Tango!
2930 en The Centibots 100 Robot Project The Centibots system was a multi-robotic system developed in part by SRI. Its team of 100 small robots were built from off-the-shelf components. This video describes the distributed robot control software and subsequent demonstration.
2931 en How to say "No" to a robot This describes an integrated robotic system for spatial understanding and situated interaction in indoor environments. Robot communication is performed using only natural language, but sometimes it needs more than a "natural" language to understand.
2933 en Statistical Modeling of Relational Data KDD has traditionally been concerned with mining data from a single relation. However, most applications involve multiple interacting relations, either explicitly (in relational databases) or implicitly (in semi-structured and multimodal data). Examples include link analysis, social networks, bioinformatics, information extraction, security, ubiquitous computing, etc. Mining such data has become a topic of keen interest in the KDD community in recent years. The key difficulty is that data in relational domains is no longer i.i.d. (independent and identically distributed), greatly complicating statistical modeling. However, research has now advanced to the point where robust, easy-to-use, general-purpose techniques and languages for mining non-i.i.d. data are available. The goal of this tutorial is to add a sufficient subset of these concepts and techniques to the toolkits of both researchers and practitioners. n n
2934 en Text Mining and Link Analysis for Web and Semantic Web The tutorial on Text Mining and Link Analysis for Web Data will focus on two main analytical approaches when analyzing web data: text mining and link analysis for the purpose of analyzing web documents and their linkage. First, the tutorial will cover some basic steps and problems when dealing with the textual and network (graph) data showing what is possible to achieve without very sophisticated technology. The idea of this first part is to present the nature of un-structured and semi-structured data. Next, in the second part, more sophisticated methods for solving more difficult and challenging problems will be shown. In the last part, some of the current open research issues will be presented and some practical pointers on the available tolls for solving previously mentioned problems will be provided. n n
2936 en Learning Bayesian Networks Bayesian networks are graphical structures for representing the probabilistic relationships among a large number of variables and doing probabilistic inference with those variables. The 1990's saw the emergence of excellent algorithms for learning Bayesian networks from passive data.n n I will discuss the constraint-based learning method using an intuitive approach that concentrates on causal learning. Then I will discuss the Bayesian approach with some simple examples. I will show how, using the Bayesian approach, we can even learning something about causal influences from passive data on two variables. Finally, I will show some applications to finance and marketing.
2942 en Challenges in Social Network Data: Processes, Privacy and Paradoxes The proliferation of rich social media, on-line communities, and collectively produced knowledge resources has accelerated the convergence of technological and social networks, producing environments that reflect both the architecture of the underlying information systems and the social structure on their members. In studying the consequences of these developments, we are faced with the opportunity to analyze social network data at unprecedented levels of scale and temporal resolution; this has led to a growing body of research at the intersection of the computing and social sciences. **Do you have a question for this lecturer&#160;at the KDD 2007?&nbsp; We encourage you to start a debate, comment on each lecturers video or send us an email and we will ask them for you!\\ ** //**Disclamer:**// //Videolectures.Net emphasises that the quality of this video was notably improved, because of low light quality conditions provided in the lecture auditorium. //
2943 en From Mining the Web to Inventing the New Sciences Underlying the Internet As the Internet continues to change the way we live, find information, communicate, and do business, it has also been taking on a dramatically increasing role in marketing and advertising. Unlike any prior mass medium, the Internet is a unique medium when it comes to interactivity and offers ability to target and program messaging at the individual level. Coupled with its uniqueness in the richness of the data that is available for measurability, in the variety of ways to utilize the data, and in the great dependence of effective marketing on applications that are heavily data-driven, makes data mining and statistical data analysis, modeling, and reporting an essential mission-critical part of running the on-line business.
2944 en Calculating Latent Demand in the Long Tail An analytical framework for using powerlaw theory to estimate market size for niche products and consumer groups.He is the author of New York Times bestselling book The Long Tail: Why the Future of Business is Selling Less of More, which as published in 2006, and runs a blog on the subject at longtail.com. In 2007 he was named one of the “Time 100,” the newsmagazine’s list of the 100 men and women whose power, talent or moral example is transforming the world. [[http://en.wikipedia.org/wiki/Chris_Anderson_%28writer%29|Chris Anderson - Wikipedia article]]
2945 en Information Genealogy: Uncovering the Flow of Ideas in Non-Hyperlinked Document Databases We now have incrementally-grown databases of text documents ranging back for over a decade in areas ranging from personal email, to news-articles and conference proceedings. While accessing individual documents is easy, methods for overviewing and understanding these collections as a whole are lacking in number and in scope. In this paper, we address one such global analysis task, namely the problem of automatically uncovering how ideas spread through the collection over time. We refer to this problem as Information Genealogy. In contrast to bibliometric methods that are limited to collections with explicit citation structure, we investigate content-based methods requiring only the text and timestamps of the documents. In particular, we propose a language-modeling approach and a likelihood ratio test to detect influence between documents in a statistically well-founded way. Furthermore, we show how this method can be used to infer citation graphs and to identify the most influential documents in the collection. Experiments on the NIPS conference proceedings and the Physics ArXiv show that our method is more effective than methods based on document similarity.
2946 en Upping the Baseline for High-Precision Text Classifiers Many important application areas of text classifiers demand high precision and it is common to compare prospective solutions to the performance of Naive Bayes. This baseline is usually easy to improve upon, but in this work we demonstrate that appropriate document representation can make outperforming this classifier much more challenging. Most importantly, we provide a link between Naive Bayes and the logarithmic opinion pooling of the mixture-of-experts framework, which dictates a particular type of document length normalization. Motivated by document-specific feature selection we propose monotonic constraints on document term weighting, which is shown as an effective method of fine-tuning document representation. The discussion is supported by experiments using three large email corpora corresponding to the problem of spam detection, where high precision is of particular importance.
2947 en From Trees to Forests and Rule Sets - A Unified Overview of Ensemble Methods Ensemble methods are one of the most influential developments in Machine Learning over the past decade. They perform extremely well in a variety of problem domains, have desirable statistical properties, and scale well computationally. By combining competing models into a committee, they can strengthen “weak” learning procedures. nn;This tutorial explains two recent developments with ensemble methods:n:**Importance Sampling** reveals “classic” ensemble methods (bagging, random forests, and boosting) to be special cases of a single algorithm. This unified view clarifies the properties of these methods and suggests ways to improve their accuracy and speed.n:**Rule Ensembles** are linear rule models derived from decision tree ensembles. While maintaining (and often improving) the accuracy of the tree ensemble, the rule-based model is much more interpretable. nnThis tutorial is aimed at both novice and advanced data mining researchers and practitioners especially in Engineering, Statistics, and Computer Science. Users with little exposure to ensemble methods will gain a clear overview of each method. Advanced practitioners already employing ensembles will gain insight into this breakthrough way to create next-generation models.nn;**John Elder's lecture**: n: In a Nutshell, Examples & Timelinen: Predictive Learning n: Decision Treesnn;**Giovanni Seni's lecture**: n: Model Selection (Bias-Variance Tradeoff , Regularization via shrinkage) n: Ensemble Learning & Importance Sampling (ISLE)n: Generic Ensemble Generationn: Bagging, Random Forest, AdaBoost, MARTn: Rule Ensemblesn: Interpretation
2948 en Fast Direction-Aware Proximity for Graph Mining In this paper we study asymmetric proximity measures on directed graphs, which quantify the relationships between two nodes or two groups of nodes. The measures are useful in several graph mining tasks, including clustering, link prediction and connection subgraph discovery. Our proximity measure is based on the concept of escape probability. This way, we strive to summarize the multiple facets of nodes-proximity, while avoiding some of the pitfalls to which alternative proximity measures are susceptible. A unique feature of the measures is accounting for the underlying directional information. We put a special emphasis on computational efficiency, and develop fast solutions that are applicable in several settings. Our experimental study shows the usefulness of our proposed direction-aware proximity method for several applications, and that our algorithms achieve a significant speedup (up to 50,000x) over straightforward implementations.
2949 en Correlation Search in Graph Databases Correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects. However, the research of correlation mining from graph databases is still lacking despite the fact that graph data, especially in various scientific domains, proliferate in recent years. In this paper, we propose a new problem of correlation mining from graph databases, called Correlated Graph Search (CGS). CGS adopts Pearson’s correlation coefficient as a correlation measure to take into consideration the occurrence distributions of graphs. However, the problem poses significant challenges, since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential. We derive two necessary conditions which set bounds on the occurrence probability of a candidate in the database. With this result, we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidates. To further improve the efficiency, we develop three heuristic rules and apply them on the candidate set to further reduce the search space. Our extensive experiments demonstrate the effectiveness of our method on candidate reduction. The results also justify the efficiency of our algorithm in mining correlations from large real and synthetic datasets.
2950 en A Framework For Community Identification in Dynamic Social Networks We propose frameworks and algorithms for identifying communities in social networks that change over time. Communities are intuitively characterized as “unusually densely knit” subsets of a social network. This notion becomes more problematic if the social interactions change over time. Aggregating social networks over time can radically misrepresent the existing and changing community structure. Instead, we propose an optimization-based approach for modeling dynamic community structure. We prove that finding the most explanatory community structure is NP-hard and APX-hard, and propose algorithms based on dynamic programming, exhaustive search, maximum matching, and greedy heuristics. We demonstrate empirically that the heuristics trace developments of community structure accurately for several synthetic and real-world examples.
2951 en Fast Best-Effort Pattern Matching in Large Attributed Graphs We focus on large graphs where nodes have attributes, such as a social network where the nodes are labelled with each person’s job title. In such a setting, we want to find subgraphs that match a user query pattern. For example, a ‘star’ query would be, “find a CEO who has strong interactions with a Manager, a Lawyer, and an Accountant, or another structure as close to that as possible”. Similarly, a ‘loop’ query could help spot a money laundering ring. Traditional SQL-based methods, as well as more recent graph indexing methods, will return no answer when an exact match does not exist. Our method can find exact-, as well as near-matches, and it will present them to the user in our proposed ‘goodness’ order. For example, our method tolerates indirect paths between, say, the ‘CEO’ and the ‘Accountant’ of the above sample query, when direct paths do not exist. Its second feature is scalability. In general, if the query has nq nodes and the data graph has n nodes, the problem needs polynomial time complexity O(nnq ), which is prohibitive. Our G-Ray (“Graph X-Ray”) method finds high-quality subgraphs in time linear on the size of the data graph. Experimental results on the DLBP author-publication graph (with 356K nodes and 1.9M edges) illustrate both the effectiveness and scalability of our approach. The results agree with our intuition, and the speed is excellent. It takes 4 seconds on average for a 4- node query on the DBLP graph.
2952 en Support Feature Machine for Classification of Abnormal Brain Activity In this study, a novel multidimensional time series classification technique, namely support feature machine (SFM), is proposed. SFM is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data. This paper also describes an application of SFM for detecting abnormal brain activity. Epilepsy is a case in point in this study. In epilepsy studies, electroencephalograms (EEGs), acquired in multidimensional time series format, have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain. From multi-dimensional EEG time series data, SFM was used to identify seizure pre-cursors and detect seizure susceptibility (pre-seizure) periods. The empirical results showed that SFM achieved over 80% correct classification of per-seizure EEG on average in 10 patients using 5-fold cross validation. The proposed optimization model of SFM is very compact and scalable, and can be implemented as an online algorithm. The outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through EEG classification.
2953 en The Knowledge Revolution in Healthcare 
2954 en Automatic Labeling of Multinomial Topic Models Multinomial distributions over words are frequently used to model topics in text collections. A common, major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic. So far, such labels have been generated manually in a subjective way. In this paper, we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way. We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information between a label and a topic model. Experiments with user study have been done on two text data sets with different genres. The results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models. Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA, LDA, and their variations.
2955 en Mining Statistically Important Equivalence Classes The support condence framework is the most common measure used in itemset mining algorithms, for its antimonotonicity that efectively simplifies the search lattice. This computational convenience brings both quality and statistical laws to the results as observed by many previous studies. In this paper, we introduce a novel algorithm that produces itemsets with ranked statistical merits under sophisticated test statistics such as chi-square, risk ratio, odds ratio, etc. Our algorithm is based on the concept of equivalence classes. An equivalence class is a set of frequent itemsets that always occur together in the same set of transactions. Therefore, itemsets within an equivalence class all share the same level of statistical signifiance regardless of the variety of test statistics. As an equivalence class can be uniquely determined and concisely represented by a closed pattern and a set of generators, we just mine closed patterns and generators, taking a simultaneous depth first search scheme. This parallel approach has not been exploited by any prior work. We evaluate our algorithm on two aspects. In general, we compare to LCM and FPclose which are the best algorithms tailored for mining only closed patterns. In particular, we compare to epMiner which is the most recent algorithm for mining a type of relative risk patterns, known as minimal emerging patterns. Experimental results show that our algorithm is faster than all of them, sometimes even multiple orders of magnitude faster. These statistically ranked patterns and the eficiency have a high potential for real life applications, especially in biomedical and nancial fields where classical test statistics are of dominant interest.
2956 en Local Decomposition for Rare Class Analysis Given its importance, the problem of predicting rare classes in large-scale multi-labeled data sets has attracted great attentions in the literature. However, the rare-class problem remains a critical challenge, because there is no natural way developed for handling imbalanced class distributions. This paper thus fills this crucial void by developing a method for Classification using lOcal clusterinG (COG). Specifically, for a data set with an imbalanced class distribution, we perform clustering within each large class and produce sub-classes with relatively balanced sizes. Then, we apply traditional supervised learning algorithms, such as Support Vector Machines (SVMs), for classification. Indeed, our experimental results on various real-world data sets show that our method produces significantly higher prediction accuracies on rare classes than state-of-the-art methods. Furthermore, we show that COG can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions.
2957 en Trajectory Pattern Mining The increasing pervasiveness of location-acquisition technologies (GPS, GSM networks, etc.) is leading to the collection of large spatio-temporal datasets and to the opportunity of discovering usable knowledge about movement behaviour, which fosters novel applications and services. In this paper, we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectories of moving objects. We introduce trajectory patterns as concise descriptions of frequent behaviours, in terms of both space (i.e., the regions of space visited during movements) and time (i.e., the duration of movements). In this setting, we provide a general formal statement of the novel mining problem and then study several different instantiations of different complexity. The various approaches are then empirically evaluated over real data and synthetic benchmarks, comparing their strengths and weaknesses.
2959 en Introduction to the KDD07 Conference 
2960 en Interview with Gregory Piatetsky-Shapiro **Gregory Piatetsky-Shapiro, Ph.D.** is the President of [[http://www.kdnuggets.com/|KDnuggets]], which provides [[http://www.kdnuggets.com/consulting.html|research and consulting]] services in the areas of data mining, knowledge discovery, bioinformatics, and business analytics. Previously, he led data mining and consulting groups at GTE Laboratories, Knowledge Stream Partners, and Xchange. He has extensive experience developing CRM, customer attrition, cross-sell, segmentation and other models for some of the leading banks, insurance companies, and telcos. He also worked on clinical trial, microarray, and proteomic data analysis for several leading biotech and pharmaceutical companies.
2963 en A Data Miner’s Story – Getting to Know the Grand Challenges 
2964 en Extracting Relevant Named Entities for Automated Expense Reimbursement Expense reimbursement is a time-consuming and labor-intensive process across organizations. In this talk, we present an automated expense reimbursement system developed at IBM Almaden Research Center. Our complete solution involves (1) an electronic document management infrastructure that provides multi-channel image capture, transport and storage of paper documents, such as receipts; (2) an unconstrained data mining approach to extracting relevant named entities from un-structured document images; (3) automation of manual auditing procedures using extracted metadata. The main focus of this presentation is our approach to automatically extracting important metadata, once we aggregate documents through such a scalable infrastructure. Extracting relevant named entities robustly from document images with unconstrained layouts and diverse formatting is a fundamental technical challenge to image-based data mining, question answering, and other information retrieval tasks. In many applications that require such capability, applying traditional language modeling techniques to the stream of OCR text does not give satisfactory result due to the absence of linguistic contexts, such as language constructs and punctuation. We present a novel approach for extracting relevant named entities from document images by learning the statistical dependencies between page layout and language features collectively from the sequence of geometrically decomposed regions on a document using a discriminative conditional random fields (CRFs) framework. We integrate this named entity extraction engine into our expense reimbursement solution and evaluate the system performance on large collections of real world receipt images provided by IBM World Wide Reimbursement Center.\\
2965 en Cleaning Disguised Missing Data: A Heuristic Approach In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely, such as causing significant biases and misleading results in hypothesis tests, correlation analysis and regressions. The very limited previous studies on cleaning disguised missing data use outlier mining and distribution anomaly detection. They highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers. To tackle the problem of cleaning disguised missing data, in this paper, we first model the distribution of disguised missing data, and propose the embedded unbiased sample heuristic. Then, we develop an effective and efficient method to identify the frequently used disguise values which capture the major body of the disguised missing data. Our method does not require any domain background knowledge to find the suspicious disguise values. We report an empirical evaluation using real data sets, which shows that our method is effective – the frequently used disguise values found by our method match the values identified by the domain experts nicely. Our method is also efficient and scalable for processing large data sets.
2966 en Distributed Classification in Peer-to-Peer Networks This work studies the problem of distributed classification in peer-to-peer (P2P) networks. While there has been a significant amount of work in distributed classification, most of existing algorithms are not designed for P2P networks. Indeed, as server-less and router-less systems, P2P networks impose several challenges for distributed classification: (1) it is not practical to have global synchronization in large- scale P2P networks; (2) there are frequent topology changes caused by frequent failure and recovery of peers; and (3) there are frequent on-the-fly data updates on each peer. In this paper, we propose an ensemble paradigm for distributed classification in P2P networks. Under this paradigm, each peer builds its local classifiers on the local data and the results from all local classifiers are then combined by plurality voting. To build local classifiers, we adopt the learning algorithm of pasting bites to generate multiple local classifiers on each peer based on the local data. To combine local results, we propose a general form of Distributed Plurality Voting (DPV ) protocol in dynamic P2P networks. This protocol keeps the single-site validity for dynamic networks, and supports the computing modes of both one-shot query and continuous monitoring. We theoretically prove that the condition C0 for sending messages used in DPV0 is locally communication-optimal to achieve the above properties. Finally, experimental results on real-world P2P networks show that: (1) the proposed ensemble paradigm is effective even if there are thousands of local classifiers; (2) in most cases, the DPV0 algorithm is local in the sense that voting is processed using information gathered from a very small vicinity, whose size is independent of the network size; (3) DPV0 is significantly more communication-efficient than existing algorithms for distributed plurality voting.
2967 en Detecting Motifs Under Uniform Scaling Time series motifs are approximately repeated patterns found within the data. Such motifs have utility for many data mining algorithms, including rule-discovery, novelty-detection, summarization and clustering. Since the formalization of the problem and the introduction of efficient linear time algorithms, motif discovery has been successfully applied to many domains, including medicine, motion capture, robotics and meteorology. In this work we show that most previous applications of time series motifs have been severely limited by the definition’s brittleness to even slight changes of uniform scaling, the speed at which the patterns develop. We introduce a new algorithm that allows discovery of time series motifs with invariance to uniform scaling, and show that it produces objectively superior results in several important domains. Apart from being more general than all other motif discovery algorithms, a further contribution of our work is that it is simpler than previous approaches, in particular we have drastically reduced the number of parameters that need to be specified.
2969 en iLink: Search and Routing in Social Networks - Part 1 The growth of Web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks. This paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration. We propose a general interaction model for the underlying social networks and then a specific model (iLink) for social search and message routing. A key contribution here is the development of a general learning framework for making such online peer production systems work at scale. The iLink model has been used to develop a system for FAQ generation in a social network (FAQtory), and experience with its application in the context of a full-scale learning-driven workflow application (CALO) is reported. We also discuss methods of adapting iLink technology for use in military knowledge sharing portals and other message routing systems. Finally, the paper shows the connection of iLink to SQM, a theoretical model for social search that is a generalization of Markov Decision Processes and the popular Pagerank model.
2970 en Practical Guide to Controlled Experiments on the Web: Listen to Your Customers not to the HiPPO The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments, also called randomized experiments (single-factor or factorial designs), A/B tests (and their generalizations), split tests, Control/Treatment tests, and parallel flights. Controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior. We provide a practical guide to conducting online experiments, where end-users can help guide the development of features. Our experience indicates that significant learning and return-on-investment (ROI) are seen when development teams listen to their customers, not to the Highest Paid Person’s Opinion (HiPPO). We provide several examples of controlled experiments with surprising results. We review the important ingredients of running controlled experiments, and discuss their limitations (both technical and organizational). We focus on several areas that are critical to experimentation, including statistical power, sample size, and techniques for variance reduction. We describe common architectures for experimentation systems and analyze their advantages and disadvantages. We evaluate randomization and hashing techniques, which we show are not as simple in practice as is often assumed. Controlled experiments typically generate large amounts of data, which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest, leading to new hypotheses and creating a virtuous cycle of improvements. Organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses. Based on our extensive practical experience with multiple systems and organizations, we share key lessons that will help practitioners in running trustworthy controlled experiments.
2971 en Relational Data Pre-Processing Techniques for Improved Securities Fraud Detection Commercial datasets are often large, relational, and dynamic. They contain many records of people, places, things, events and their interactions over time. Such datasets are rarely structured appropriately for knowledge discovery, and they often contain variables whose meanings change across different subsets of the data. We describe how these challenges were addressed in a collaborative analysis project undertaken by the University of Massachusetts Amherst and the National Association of Securities Dealers (NASD). We describe several methods for data preprocessing that we applied to transform a large, dynamic, and relational dataset describing nearly the entirety of the U.S. securities industry, and we show how these methods made the dataset suitable for learning statistical relational models. To better utilize social structure, we first applied known consolidation and link formation techniques to associate individuals with branch office locations. In addition, we developed an innovative technique to infer professional associations by exploiting dynamic employment histories. Finally, we applied normalization techniques to create a suitable class label that adjusts for spatial, temporal, and other heterogeneity within the data. We show how these pre-processing techniques combine to provide the necessary foundation for learning high-performing statistical models of fraudulent activity.
2972 en An Event-based Framework for Characterizing the Evolutionary Behavior of Interaction Graphs Interaction graphs are ubiquitous in many fields such as bioinformatics, sociology and physical sciences. There have been many studies in the literature targeted at studying and mining these graphs. However, almost all of them have studied these graphs from a static point of view. The study of the evolution of these graphs over time can provide tremendous insight on the behavior of entities, communities and the flow of information among them. In this work, we present an event-based characterization of critical behavioral patterns for temporally varying interaction graphs. We use non-overlapping snapshots of interaction graphs and develop a framework for capturing and identifying interesting events from them. We use these events to characterize complex behavioral patterns of individuals and communities over time. We demonstrate the application of behavioral patterns for the purposes of modeling evolution, link prediction and influence maximization. Finally, we present a diffusion model for evolving networks, based on our framework.
2977 en Domain-Constrained Semi-Supervised Mining of Tracking Models in Sensor Networks Accurate localization of mobile objects is a major research problem in sensor networks and an important data mining application. Specifically, the localization problem is to determine the location of a client device accurately given the radio signal strength values received at the client device from multiple beacon sensors or access points. Conventional data mining and machine learning methods can be applied to solve this problem. However, all of them require large amounts of labeled training data, which can be quite expensive. In this paper, we propose a probabilistic semi-supervised learning approach to reduce the calibration effort and increase the tracking accuracy. Our method is based on semi-supervised conditional random fields which can enhance the learned model from a small set of training data with abundant unlabeled data effectively. To make our method more efficient, we exploit a Generalized EM algorithm coupled with domain constraints. We validate our method through extensive experiments in a real sensor network using Crossbow MICA2 sensors. The results demonstrate the advantages of methods compared to other state-of-the-art objecttracking algorithms.
2978 en Framework for Classification and Segmentation of Massive Audio Data Streams In recent years, the proliferation of VOIP data has created a number of applications in which it is desirable to perform quick online classification and recognition of massive voice streams. Typically such applications are encountered in real time intelligence and surveillance. In many cases, the data streams can be in compressed format, and the rate of data processing can often run at the rate of Gigabits per second. All known techniques for speaker voice analysis require the use of an offline training phase in which the system is trained with known segments of speech. The state-of-the-art method for text-independent speaker recognition is known as Gaussian Mixture Modeling (GMM), and it requires an iterative Expectation Maximization Procedure for training, which cannot be implemented in real time. In this paper, we discuss the details of such an online voice recognition system. For this purpose, we use our micro-clustering algorithms to design concise signatures of the target speakers. One of the surprising and insightful observations from our experiences with such a system is that while it was originally designed only for efficiency, we later discovered that it was also more accurate than the widely used Gaussian Mixture Model (GMM). This was because of the conciseness of the micro-cluster model, which made it less prone to over training. This is evidence of the fact that it is often possible to get the best of both worlds and do better than complex models both from an efficiency and accuracy perspective.
2979 en LungCAD: A Clinically Approved, Machine Learning System for Lung Cancer Detection We present LungCAD, a computer aided diagnosis (CAD) system that employs a classification algorithm for detecting solid pulmonary nodules from CT thorax studies. We briefly describe some of the machine learning techniques developed to overcome the real world challenges in this medical domain. The most significant hurdle in transitioning from a machine learning research prototype that performs well on an in-house dataset into a clinically deployable system, is the requirement that the CAD system be tested in a clinical trial. We describe the clinical trial in which LungCAD was tested: a large scale multi-reader, multi-case (MRMC) retrospective observational study to evaluate the effect of CAD in clinical practice for detecting solid pulmonary nodules from CT thorax studies. The clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with LungCAD, both for detecting nodules and identifying potentially actionable nodules; this, along with other findings from the trial, has resulted in FDA approval for LungCAD in late 2006.
2980 en Truth Discovery with Multiple Conflicting Information Providers on the Web The world-wide web has become the most important information source for most of us. Unfortunately, there is no guarantee for the correctness of information on the web. Moreover, different web sites often provide conflicting information on a subject, such as different specifications for the same product. In this paper we propose a new problem called Veracity, i.e., conformity to truth, which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites. We design a general framework for the Veracity problem, and invent an algorithm called TruthFinder, which utilizes the relationships between web sites and their information, i.e., a web site is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy web sites. Our experiments show that TruthFinder successfully finds true facts among conflicting information, and identifies trustworthy web sites better than the popular search engines.
2981 en Detecting Changes in Large Data Sets of Payments Cards Data: A Case Study An important problem in data mining is detecting changes in large data sets. Although there are a variety of change detection algorithms that have been developed, in practice it can be a problem to scale these algorithms to large data sets due to the heterogeneity of the data. In this paper, we describe a case study involving payment card data in which we built and monitored a separate change detection model for each cell in a multi-dimensional data cube. We describe a system that has been in operation for the past two years that builds and monitors over 15,000 separate baseline models and the process that is used for generating and investigating alerts using these baselines.
2982 en Event Summarization for System Management In system management applications, an overwhelming amount of data are generated and collected in the form of temporal events. While mining temporal event data to discover interesting and frequent patterns has obtained rapidly increasing research efforts, users of the applications are overwhelmed by the mining results. The extracted patterns are generally of large volume and hard to interpret, they may be of no emphasis, intricate and meaningless to non-experts, even to domain experts. While traditional research efforts focus on finding interesting patterns, in this paper, we take a novel approach called event summarization towards the understanding of the seemingly chaotic temporal data. Event summarization aims at providing a concise interpretation of the seemingly chaotic data, so that domain experts may take actions upon the summarized models. Event summarization decomposes the temporal information into many independent subsets and finds well fitted models to describe each subset.
2983 en Machine Learning for Stock Selection In this paper, we propose a new method called Prototype Ranking (PR) designed for the stock selection problem. PR takes into account the huge size of real-world stock data and applies a modified competitive learning technique to predict the ranks of stocks. The primary target of PR is to select the top performing stocks among many ordinary stocks. PR is designed to perform the learning and testing in a noisy stocks sample set where the top performing stocks are usually the minority. The performance of PR is evaluated by a trading simulation of the real stock data. Each week the stocks with the highest predicted ranks are chosen to construct a portfolio. In the period of 1978-2004, PR’s portfolio earns a much higher average return as well as a higher risk-adjusted return than Cooper’s method, which shows that the PR method leads to a clear profit improvement.
2984 en IMDS: Intelligent Malware Detection System The proliferation of malware has presented a serious threat to the security of computer systems. Traditional signature-based antivirus systems fail to detect polymorphic and new, previously unseen malicious executables. In this paper, resting on the analysis of Windows API execution sequences called by PE files, we develop the Intelligent Malware Detection System (IMDS) using Objective Oriented Association (OOA) mining based classification. IMDS is an integrated system consisting of three major modules: PE parser, OOA rule generator, and rule based classifier. An OOA Fast FPGrowth algorithm is adapted to efficiently generate OOA rules for classification. A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of King Soft Corporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system outperform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan, as well as previous data mining based detection systems which employed Naive Bayes, Support Vector Machine (SVM) and Decision Tree techniques.
2985 en iLink: Search and Routing in Social Networks - Part 2 The growth of Web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks. This paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration. We propose a general interaction model for the underlying social networks and then a specific model (iLink) for social search and message routing. A key contribution here is the development of a general learning framework for making such online peer production systems work at scale. The iLink model has been used to develop a system for FAQ generation in a social network (FAQtory), and experience with its application in the context of a fullscale learning-driven workflow application (CALO) is reported. We also discuss methods of adapting iLink technology for use in military knowledge sharing portals and other message routing systems. Finally, the paper shows the connection of iLink to SQM, a theoretical model for social search that is a generalization of Markov Decision Processes and the popular Pagerank model.
2992 en Interview with Jon Kleinberg This interview was made at the KDD 2007 Conference where we cought up with John Kleinber who was one of the **invited speakers**. We discussed **his popularity at Cornell** **University **and and how students affectionately call him "Rebel King", how he **published his recent text book on algorithms **with coauthor Eva Tardos and what are his **future plans**...
2993 en Interview with Pavel Berkhin What has Pavel Berkhin to say about his beginings as a researcher, his first and last algorithm, the industry and in general the KDD Conference, since he is this years chairman of the event.
3000 en Hierarchical Mixture Models: a Probabilistic Analysis Mixture models form one of the most widely used classes of generative models for describing structured and clustered data. In this paper we develop a new approach for the analysis of hierarchical mixture models. More specifically, using a text clustering problem as a motivation, we describe a natural generative process that creates a hierarchical mixture model for the data. In this process, an adversary starts with an arbitrary base distribution and then builds a topic hierarchy via some evolutionary process, where he controls the parameters of the process. We prove that under our assumptions, given a subset of topics that represent generalizations of one another (such as baseball&#160;- sports&nbsp;- base), for any document which was produced via some topic in this hierarchy, we can efficiently determine the most specialized topic in this subset, it still belongs to. The quality of the classification is independent of the total number of topics in the hierarchy and our algorithm does not need to know the total number of topics in advance. Our approach also yields an algorithm for clustering and unsupervised topical tree reconstruction. We validate our model by showing that properties predicted by our theoretical results carry over to real data. We then apply our clustering algorithm to two different datasets: (i) “20 newsgroups” [19] and (ii) a snapshot of abstracts of arXiv [2] (15 categories, 240,000 abstracts). In both cases our algorithm performs extremely well.
3001 en Information distance from a question to an answer We provide three key missing pieces of a general theory of information distance [3, 23, 24]. We take bold steps in formulating a revised theory to avoid some pitfalls in practical applications. The new theory is then used to construct a question answering system. Extensive experiments are conducted to justify the new theory.
3002 en Statistical Change Detection for Multi-Dimensional Data This paper deals with detecting change of distribution in multi-dimensional data sets. For a given baseline data set and a set of newly observed data points, we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set. We define a test statistic that is strictly distribution-free under the null hypothesis. Our experimental results show that the density test has substantially more power than the two existing methods for multi-dimensional change detection.
3003 en Learning the Kernel Matrix in Discriminant Analysis via Quadratically Constrained Quadratic Programming The kernel function plays a central role in kernel methods. In this paper, we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in Regularized Kernel Discriminant Analysis (RKDA), which performs linear discriminant analysis in the feature space via the kernel trick. Previous studies have shown that this kernel learning problem can be formulated as a semidefinite program (SDP), which is however computationally expensive, even with the recent advances in interior point methods. Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a Quadratically Constrained Quadratic Programming (QCQP) formulation for the kernel learning problem, which can be solved more efficiently than SDP. While most existing work on kernel learning deal with binary-class problems only, we show that our QCQP formulation can be extended naturally to the multi-class case. Experimental results on both binary-class and multiclass benchmark data sets show the efficacy of the proposed QCQP formulations.
3008 en Scalable Look-Ahead Linear Regression Trees The motivation behind Look-ahead Linear Regression Trees (LLRT) is that out of all the methods proposed to date, there has been no scalable approach to exhaustively evaluate all possible models in the leaf nodes in order to obtain an optimal split. Using several optimizations, LLRT is able to generate and evaluate thousands of linear regression models per second. This allows for a near-exhaustive evaluation of all possible splits in a node, based on the quality of fit of linear regression models in the resulting branches. We decompose the calculation of the Residual Sum of Squares in such a way that a large part of it is pre-computed. The resulting method is highly scalable. We observe it to obtain high predictive accuracy for problems with strong mutual dependencies between attributes. We report on experiments with two simulated and seven real data sets.
3009 en Estimating Rates of Rare Events at Multiple Resolutions We consider the problem of estimating occurrence rates of rare events for extremely sparse data, using pre-existing hierarchies to perform inference at multiple resolutions. In particular, we focus on the problem of estimating click rates for (webpage, advertisement) pairs (called impressions) where both the pages and the ads are classified into hierarchies that capture broad contextual information at different levels of granularity. Typically the click rates are low and the coverage of the hierarchies is sparse. To overcome these difficulties we devise a sampling method whereby we analyze a specially chosen sample of pages in the training set, and then estimate click rates using a two-stage model. The first stage imputes the number of (webpage, ad) pairs at all resolutions of the hierarchy to adjust for the sampling bias. The second stage estimates click rates at all resolutions after incorporating correlations among sibling nodes through a tree-structured Markov model. Both models are scalable and suited to large scale data mining applications. On a real-world dataset consisting of 1/2 billion impressions, we demonstrate that even with 95% negative (non-clicked)events in the training set, our method can effectively discriminate extremely rare events in terms of&#160; heir click propensity.
3010 en Predictive Discrete Latent Factor Models for Large Scale Dyadic Data We propose a novel statistical method to predict large scale dyadic response variables in the presence of covariate information. Our approach simultaneously incorporates the effect of covariates and estimates local structure that is induced by interactions among the dyads through a discrete latent factor model. The discovered latent factors provide a predictive model that is both accurate and interpretable. We illustrate our method by working in a framework of generalized linear models, which include commonly used regression techniques like linear regression, logistic regression and Poisson regression as special cases. We also provide scalable generalized EM-based algorithms for model fitting using both "hard" and "soft" cluster assignments. We demonstrate the generality and efficacy of our approach through large scale simulation studies and analysis of datasets obtained from certain real-world movie recommendation and internet advertising applications.
3011 en A Scalable Modular Convex Solver for Regularized Risk Minimization A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as `1 and `2 penalties. At present, our solver implements 20 different estimation problems, can be easily extended, scales to millions of observations, and is up to 10 times faster than specialized solvers for many applications. The open source code is freely available as part of the ELEFANT toolbox.
3022 en Introduction to the Panel Since the 1989 workshop on knowledge discovery in databases, the field has seen sustained growth and interest and has attained significant maturity. The main objectives of this panel will be to reflect on the successes and failures in the field of data mining over the last eighteen years and to examine what insights we can take with us as we move forward.
3023 en Successes, Failures and Learning From Them At an abstract level, the theme of the field is concerned with extracting actionable and interpretable knowledge from data in as efficient a manner as possible. The primary purpose of this panel, in the context of this underlying theme, is to consider the following questions. What have been the major successes and breakthroughs that we as a field can point to with pride? What have been the critical mistakes or mis-steps that have been taken along the way? And finally, what can we hope to learn from both our successes and mistakes and how can this knowledge be used to determine how to focus our efforts in the future?
3024 en Successes, Failures and Learning From Them Over the last eighteen years, the field of knowledge discovery and data mining has matured considerably. Although the field has evolved as a result of synergistic co-operation among researchers in databases, artificial intelligence, statistics and systems, it has maintained its own identity. From a single workshop in 1989, the field can now lay claim to at least 5 major conferences and numerous symposium devoted to its central theme.
3025 en Successes, Failures and Learning From Them 
3026 en Successes, Failures and Learning From Them Another topic of interest here is to highlight some of the classic mistakes made in the field. Topics of interest here could range from the use of non-representative training data to the ignorance of population drift when modeling time-varying data, from not accounting for errors in data or labels in the model to an over reliance on a single technique for the task on hand and from asking the wrong question in the context of the application driver to sampling without care. A related topic here might be to think about the role of benchmark datasets and algorithms, and reflect on the general importance and requirement for repeatable and reproducible results.
3027 en Successes, Failures and Learning From Them Over the last eighteen years, while there have clearly been successful deployment of knowledge discovery and data mining solutions, there have also undoubtedly been mistakes and failures. This aspect of the panel discussion will examine the low-lights (important mistakes and failures)&#160;with the end goal of trying to learn from them.
3028 en Debate The terms "success" and "failure" often convey fuzzy semantics that are open to interpretation. As part of the discussion, it is expected that panelists will offer their thoughts on the aforementioned questions while defining their interpretation of these terms in the context of particular domains. After an initial round of discussions by the panelists, the floor will then be opened to an interactive session with the audience. Finally, panelists will be asked to conclude their presentations with their outlook on how one can learn from the successes and failures of the past 15+ years and what in their opinion are the critical opportunities for the field in the future.
3029 en San Jose jazz festival During the day the KDD 2007 attendees were in the conference rooms, but at night everyone was outside on the streets to listen to jazz on the San Jose Jazz Festival which celebrates: \\ - bringing jazz legends to Silicon Valley \\ - bringing music to schools \\ - supporting local musicians \\ - promoting emerging musicians and new jazz forms
3030 en Privacy Preserving DataMining The rapid growth of the Internet over the last decade has been startling. However, efforts to track its growth have often fallen afoul of bad data --- for instance, how much traffic does the Internet now carry? The problem is not that the data is technically hard to obtain, or that it does not exist, but rather that the data is not shared. Obtaining an overall picture requires data from multiple sources, few of whom are open to sharing such data, either because it violates privacy legislation, or exposes business secrets. The approaches used so far in the Internet, e.g., trusted third parties, or data anonymization, have been only partially successful, and are not widely adopted. The paper presents a method for performing computations on shared data without any participants revealing their secret data. For example, one can compute the sum of traffic over a set of service providers without any service provider learning the traffic of another. The method is simple, scalable, and flexible enough to perform a wide range of valuable operations on Internet data.
3031 en Winning The DARPA Grand Challenge The DARPA grand challenge, technical details enabling Sebastian Thrun's&#160;win, and an introduction to the next phase called "The Urban Grand Challenge"
3032 en Rules, Race, and Mel Gibson 2006 Slavoj Zizek talking about the explicit, truth, rules, politics, Mel Gibson, society, race, racism, antisemitism; lecturing and developing a psychoanalysis of culture and societies. Public open lecture for the students of the European Graduate School EGS, Media and Communication Studies department program, Saas-Fee, Switzerland, Europe, 2006,
3033 en Learning and Recognizing Visual Object Categories Over the past few years there has been substantial progress in the development of techniques for recognizing generic categories of objects in images, such as automobiles, bicycles, airplanes, and human faces. Much of this progress can be traced to two underlying technical advances: # detectors for locally invariant features of an image, and # the application of techniques from machine learning. Despite recent successes, however, there are some fundamental concerns about methods that rely heavily on feature detection, because the local image evidence used in detection decisions is often highly ambiguous due to the absence of contextual information. We are taking a different approach to learning and recognizing visual object categories, in which there is no separate feature detection stage. In our approach, objects are modeled as local image patches with spring-like connections that constrain the spatial relations between patches. Such models are intuitively natural, and their use dates back over 30 years. Until recently such models were largely abandoned due to computational challenges that are addressed by our work. Our approach can be used to learn models from weakly labeled training data, without any specification of the location of objects or their parts. The recognition accuracy for such models is better than when using techniques based on feature detection that encode similar forms of spatial constraint.
3035 en Everything is Miscellaneous David Weinberger's new book covers the breakdown of the established order of ordering. He explains how methods of categorization designed for physical objects fail when we can instead put things in multiple categoreis at once, and search them in many ways. This is no dry book on taxonomy, but has the insight and wit you'd expect from the author of The Cluetrain Manifesto, Small Pieces Loosely Joined, and a former writer for Woody Allen.
3036 en Human Computation Tasks like image recognition are trivial for humans, but continue to challenge even the most sophisticated computer programs. This talk introduces a paradigm for utilizing human processing power to solve problems that computers cannot yet solve. Traditional approaches to solving such problems focus on improving software. I advocate a novel approach: constructively channel human brainpower using computer games. For example, the ESP Game, described in this talk, is an enjoyable online game -- many people play over 40 hours a week -- and when people play, they help label images on the Web with descriptive keywords. These keywords can be used to significantly improve the accuracy of image search. People play the game not because they want to help, but because they enjoy it. I describe other examples of "games with a purpose": Peekaboom, which helps determine the location of objects in images, and Verbosity, which collects common-sense knowledge. I also explain a general approach for constructing games with a purpose.
3037 en Reverse engineering techniques to find security bugs: A case study of the ANI Alex Sotirov is a vulnerability engineer at determina. He will discuss some latest techniques in reverse engineering software to find vulnerabilities. Particularly, he'll discuss his technique that lead him to find the ANI bug (a critical new bug in WinXP and Vista). Alex will describe the tools he uses for reverse engineering and show how he reverse engineered ANI Bug. He will continue to discussed Windows security mechanisms (ASLR, /GS) and describe how ANI exploit bypasses them.
3038 en Statistical Aspects of Data Mining (Stats 202) This is the Google campus version of Stats 202 which is being taught at Stanford this summer. I will follow the material from the Stanford class very closely. That material can be found at [[http://www.stats202.com/|www.stats202.com]]. The main topics are exploring and visualizing **data**, association analysis, classification, and clustering. The textbook is Introduction to **Data** **Mining** by Tan, Steinbach and Kumar. Googlers are welcome to attend any classes which they think might be of interest to them.
3041 en Debunking third-world myths with the best stats you've ever seen You've never seen data presented like this. With the drama and urgency of a sportscaster, [[http://www.ted.com/index.php/speakers/view/id/90|Hans Rosling]] debunks myths about the so-called "developing world" using extraordinary animation software developed by his Gapminder Foundation. The Trendalyzer software (recently acquired by Google) turns complex global trends into lively animations, making decades of data pop. Asian countries, as colorful bubbles, float across the grid -- toward better national health and wealth. Animated bell curves representing national income distribution squish and flatten. In Rosling's hands, global trends -- life expectancy, child mortality, poverty rates -- become clear, intuitive and even playful. (Recorded February 2006 in Monterey, CA. Duration: 20:35) - More TEDTalks at [[http://www.ted.com/]] ;//"Rosling believes that making information more accessible has the potential to change the quality of the information itself." ://Business Week Online//
3042 en Aesthetic Science: Understanding Preferences for Color and Spatial Composition 
3043 en The Implications of OpenID OpenID is an emerging standard that provides simple, decentralised authentication for the Web. OpenID follows the Unix philosophy, solving one small problem rather than attempting to tackle the many larger challenges posed by online identity. This talk will explore the implications of OpenID, and explore the best practices required to take advantage of this new technology while avoiding the potential pitfalls. Speaker:\\ Simon Willison is a consultant on OpenID and client- and server-side Web development, and a co-creator of the Django Web framework. Before going frelance Simon worked on Yahoo!'s Technology Development team, and prior to that at the Lawrence Journal-World, an award winning local newspaper in Kansas. \\ Simon maintains a popular Web development weblog at [[http://simonwillison.net/]]
3044 en The Next Fifty Years of Science The scientific method which provides us with so many technological goodies does not resemble the science of 1600. Ever since Bacon, science has undergone a slow evolution. Landmarks in the history of the scientific method are the invention of libraries, indexes, citations, controlled experiments, peer review, placebos, double blind experiments, randomization, and search among others. At the core of the scientific method is the structuring of information. In the next 50 years, as the technologies of information and knowledge accelerate, the nature of the scientific process will change even more than it has in the last 400 years. We can't predict what specific inventions will arise in the next 50 years, but based on long-term trends in epistemic tools, I believe we can speculate on how the scientific method itself -- that is, how we know -- will change in the next five decades.
3045 en Faith, Evolution, and Programming Languages Faith and evolution provide complementary--and sometimes conflicting--models of the world, and they also can model the adoption of programming languages. Adherents of competing paradigms, such as functional and object-oriented programming, often appear motivated by faith. Families of related languages, such as C, C++, Java, and C#, may arise from pressures of evolution. As designers of languages, adoption rates provide us with scientific data, but the belief that elegant designs are better is a matter of faith. This talk traces one concept, second-order quantification, from its inception in the symbolic logic of Frege through to the generic features introduced in Java 5, touching on features of faith and evolution. The remarkable correspondence between natural deduction and functional programming informed the design of type classes in Haskell. Generics in Java evolved directly from Haskell type classes, and are designed to support evolution from legacy code to generic code. Links, a successor to Haskell aimed at AJAX-style three-tier web applications, aims to reconcile some of the conflict between dynamic and static approaches to typing.
3047 en Introduction to bioinformatics I will start by giving a general introduction into Bioinformatics, including basic biology, typical data types (sequences, structures, expression data and networks) and established analysis tasks. In the second part, I will discuss the problem of predictive sequence analysis with Support Vector Machines (SVMs). I will introduce a series of kernels suitable for different analysis tasks. Furthermore I will discuss the basic data structures needed for large scale learning and how to combine kernels for heterogeneous data. In the third part, I will focus on Hidden Markov models and discriminative alternatives like Conditional Random Fields and Hidden Markov SVMs suitable for segmentation tasks frequently appearing in Bioinformatics. In the last part I will present three applications in greater detail: A large margin alignment algorithm, computational gene finding and the identification of polymorphisms from resequencing arrays.
3048 en Introduction to kernel methods This lecture given by Mr. Smola is combined with Mr. Bernhard Schoelkopf and will encopass Part 1, Part 5, Part 6 of the complete lecture. \\ Part 2, 3 and 4 of this lecture can be found here [[mlss07_scholkopf_intkmet|//at Bernhard Schoelkopf's// "%title"]]
3049 en Introduction to kernel methods This lecture given by Mr. Bernhard Schölkop is combined with Mr. Smola and will encompass Part 2, Part 3, Part 4 of the complete lecture. Part 1 , 5, 6 of this lecture can be found here at [[mlss07_smola_intkmet|//Alex Smola's// "%title"]]
3050 en Bayesian inference and Gaussian processes 
3051 en Interview with Usama Fayyad [[http://videolectures.net/usama_fayyad/|Dr. Usama Fayyad]] is** [[http://yhoo.client.shareholder.com/press/management.cfm|Yahoo!'s Chief Data Officer and Executive Vice President, Research & Strategic Data Solutions.]]** Dr. Usama Fayyad is responsible for Yahoo!'s overall data strategy, architecting Yahoo!'s data policies and systems, prioritizing data investments, and managing the Company's data analytics and data processing infrastructure. Here he discusses on son=me contemporary topics such as academy and industry cooperation, privacy policy and his beginings as a young researcher and his first algorithms. ; In this interview the Videolectures.Net team spoke to him about his starts as a **young researcher**, the distinction between researchers and engeneers in data mining or whether he felt as a **scientific businessman**. It was very interesting to her what was his opinion on **privacy policy**, if he does remember his **first algorithm **and his **message to the community**. :// "It is my fundamental belief that the most elegant theoretical problems are tipically embedded in the most mundane real applications, so you really dont have to think too fancy or hard, all you have to do is you have to embedd yourself in a real problem and try to solve the real problem with real constraints."//
3053 en KDD-07 Best Paper Awards 
3054 en Student Travel Awards 
3055 en KDD Cup Winners 
3056 en SIGKDD Service and Innovation Awards 
3058 en Welcome from the Program Chairs This proceedings is the published record of the Thirteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-07) held in San Jose, California on August 12–15, 2007. The KDD-07 conference provides a forum for novel research results and important applications in the area of data mining and knowledge discovery. The vibrancy, excitement and breadth of the field are reflected by the strong lineup of research papers, invited talks, tutorials and workshops at the conference. The conference and the proceedings represent the efforts of a large number of people. We would like to thank the Industrial and Government Applications Track Chairs, the members of the Organizing Committee, the members of the Program Committee (including the Research Track Senior Program Committee), the external reviewers, and the student volunteers who helped out at the conference. These individuals contributed many hours of their time to serve their scientific community and help make the conference as successful as it is. We would also like to thank the ACM staff and the conference sponsors for their support.
3059 en ACM SIGKDD Data Mining Practice Prize Winners 
3060 en Temporal Causal Modeling with Graphical Granger Methods The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of “Granger causality”, based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.
3061 en Graphical models An introduction to directed and undirected probabilistic graphical models, including inference (belief propagation and the junction tree algorithm), parameter learning and structure learning, variational approximations, and approximate inference. \\ - Introduction to graphical models: (directed, undirected and factor graphs; conditional independence; d-separation; plate notation) \\ - Inference and propagation algorithms: (belief propagation; factor graph propagation; forward-backward and Kalman smoothing; the junction tree algorithm) \\ - Learning parameters and structure: maximum likelihood and Bayesian parameter learning for complete and incomplete data; EM; Dirichlet distributions; score-based structure learning; Bayesian structural EM; brief comments on causality and on learning undirected models) \\ - Approximate Inference: (Laplace approximation; BIC; variational Bayesian EM; variational message passing; VB for model selection) \\ - Bayesian information retrieval using sets of items: (Bayesian Sets; Applications) \\ - Foundations of Bayesian inference: (Cox Theorem; Dutch Book Theorem; Asymptotic consensus and certainty; choosing priors; limitations)
3062 en Opening of the 9th Machine Learning Summer School 
3063 en Lost in Translation -- Solving biological problems with machine learning We demonstrate the application of machine learning methods to problems from biology, chemistry, and pharmacy, nameley the prediction of protein subcellular localization, prediction of chromatiographic separation of oligo nucleotides, and the prediction of percutaneous drug absorption. For these examples, we show how translating the primary data into problem-specific features is essential for solving classification and regression problems.
3064 en Learning Mental Associations as a means to build Organizational Memories Office workspace reveals collections of documents structured along directories, bookmarks and email folders. The respective taxonomies represent conceptual implicit knowledge generated by the user about his/her role, tasks, and interests. Starting from that, learning methods can be applied to generate "electronic mental models" allowing to understand the user's image of events, people, documents, appointments, etc. and the associative relationships among them. As a result, a workspace is transformed into a vivid personal memory assistant by which a user is supported in her/his perspective information categorization and retrieval. Moreover, users may publish parts of the resulting profiles to signal their competences to their colleagues fostering peer-to-peer-collaboration.
3065 en Kernel Methods for Dependence and Causality 
3066 en Convex Optimization The lectures will provide an introduction to the theory and applications of convex optimization.nn;The emphasis will be on results useful for convex modeling, i.e., recognizing and formulating convex optimization problems in practice.n: - The first lecture will introduce some of the fundamental theory of convex sets and functions.n: - In lecture 2 we will discuss general properties of convex optimization problems and define important standard classes (linear and quadratic programming, second-order cone programming, semidefinite programming) and their applications.n: - In lecture 3 the material will be illustrated with various examples.
3067 en Statistical learning theory - Learning Theory: Foundations and Goals \\ - Learning Bounds: Ingredients and Results \\ - Implications: What to conclude from bounds \\
3068 en Dirichlet Processes: Tutorial and Practical Course **The Bayesian approach** allows for a coherent framework for dealing with uncertainty in machine learning. By integrating out parameters, Bayesian models do not suffer from overfitting, thus it is conceivable to consider models with infinite numbers of parameters, aka Bayesian nonparametric models. An example of such models is the Gaussian process, which is a distribution over functions used in regression and classification problems. Another example is the Dirichlet process, which is a distribution over distributions. Dirichlet processes are used in density estimation, clustering, and nonparametric relaxations of parametric models. It has been gaining popularity in both the statistics and machine learning communities, due to its computational tractability and modelling flexibility. In the tutorial I shall introduce Dirichlet processes, and describe different representations of Dirichlet processes, including the Blackwell-MacQueen? urn scheme, Chinese restaurant processes, and the stick-breaking construction. I shall also go through various extensions of Dirichlet processes, and applications in machine learning, natural language processing, machine vision, computational biology and beyond. In the practical course I shall describe inference algorithms for Dirichlet processes based on Markov chain Monte Carlo sampling, and we shall implement a Dirichlet process mixture model, hopefully applying it to discovering clusters of NIPS papers and authors.
3069 en Topics in image and video processing 
3070 en Regularisation in Image Analysis 
3073 en Opening - The 5th International Workshop on Mining and Learning with Graphs There have been several workshops on mining and learning from graphs in recent years such as last year's MLG and its forerunner MGTS workshop series on Mining Graphs, Trees and Sequences. These were successful, but were tied to the conference of one research community. Nowadays there seems to be a surge of interest in mining and learning from structured data across several communities. Most researchers, however, only have exposure to one or two communities, and no clear understanding of the relative advantages and limitations of different approaches has yet emerged. We believe this is an ideal time for a workshop that allows active researchers in this area to discuss and debate the unique challenges of mining and learning from structured data. The MLG 2007 workshop will thus concentrate on mining and learning with structured data in general and its many appearances and facets such as interpretations, graphs, trees, sequences. Specifically, we seek to invite researchers in Statistical Relational Learning, Kernel Methods for Structured Inputs/Outputs, Graph Mining, (Multi-) Relational Data Mining, Inductive Logic Programming, among others.
3075 en Learning and Charting Chemical Space with Strings and Graphs: Challenges and Opportunities for AI and Machine Learning Informatics methods and computers have not yet become as pervasive in chemistry as they have in physics and biology. Drawing analogies from bioinformatics, key ingredients for progress in chemoinformatics are the availability of large, annotated databases of compounds and reactions, data structures and algorithms to efficiently search these databases, and computational methods to predict the physical, chemical, and biological properties of new compounds and reactions. We will describe how graph-based methods play a key role in the development of: (1) a large public database of compounds and reactions (ChemDB) and the underlying algorithms and representations; (2) machine learning kernel methods to predict molecular properties; and (3) the applications of these methods to drug screening/design problems and the identification of new drug leads against a major disease.
3076 en ProbLog and its Application to Link Mining in Biological Networks ProbLog is a recently introduced probabilistic extension of Prolog [De Raedt, Kimmig, Toivonen, IJCAI 07]. A ProbLog program defines a distribution over logic programs by specifying for each clause the probability that it belongs to a randomly sampled program, and these probabilities are mutually independent. The semantics of ProbLog is then defined by the success probability of a query in a randomly sampled program. It has been applied to link mining and discovery in a large biological network. In the talk, I will also discuss various learning settings for ProbLog and link mining, in particular, I shall present techniques for probabilistic local pattern mining, probabilistic explanation based learning [Kimmig, De Raedt, Toivonen, ECML 07] and theory compression from examples [De Raedt et al, ILP 96]. This is joint work with Angelika Kimmig, Hannu Toivonen, Kate Revoredo and Kristian Kersting.
3077 en Graph Identification Within the machine learning community, there has been a growing interest in learning structured models from input data that is itself structured. Graph identification refers to methods that transform an observed input graph into an inferred output graph. Examples include inferring organizational hierarchies from social network data and identifying gene regulatory networks from protein-protein interactions. The key processes in graph identification are entity resolution, link prediction, and collective classification. I will overview algorithms for these tasks and discuss the need for integrating the results to solve the overall problem collectively.
3078 en Learning Graph Matching As a fundamental problem in pattern recognition, graph matching has found a variety of applications in the field of computer vision. In graph matching, patterns are modeled as graphs and pattern recognition amounts to finding a correspondence between the nodes of different graphs. There are many ways in which the problem has been formulated, but most can be cast in general as a quadratic assignment problem, where a linear term in the objective function encodes node compatibility functions and a quadratic term encodes edge compatibility functions. The main research focus in this theme is about designing efficient algorithms for solving approximately the quadratic assignment problem, since it is NP-hard. In this paper, we turn our attention to the complementary problem: how to estimate compatibility functions such that the solution of the resulting graph matching problem best matches the expected solution that a human would manually provide. We present a method for learning graph matching: the training examples are pairs of graphs and the “labels” are matchings between pairs of graphs. We present experimental results with real image data which give evidence that learning can improve the performance of standard graph matching algorithms. In particular, it turns out that linear assignment with such a learning scheme may improve over state-of-the-art quadratic assignment relaxations. This finding suggests that for a range of problems where quadratic assignment was thought to be essential for securing good results, linear assignment, which is far more ef icient, could be just sufficient if learning is performed. This enables speed-ups of graph matching by up to 4 orders of magnitude while retaining state-of-the-art accuracy.
3081 en Online Learning 
3085 en Department of Knowledge Technologies Core business of JSI KT Dep is research, technology development and deployment of state-of-the-art analytic solutions on large real-life scenarios.
3087 en Department of Communication Systems The Department of Communication Systems is concerned mainly with the research, development and design of next generation networks and wireless access systems, and the development of new algorithms for parallel and distributed computing and computer simulations. Other research activities include the development of software tools for testing, modeling and simulation of communication systems, provision of security services in communication networks, digital signal processing in medicine, etc. With its research and development activities the department has been actively involved in national and international projects including Framework Programme and Structural Funds projects.
3088 en Computer Systems Department Computer Systems Department at Jozef Stefan Institute is concerned primarily with the design automation of computing structures and systems. Within this broad area, we are concentrating particularly on metaheuristic approach to engineering design and logistics problems as well as system design and test.
3089 en Department of Intelligent Systems Development of methods and techniques of intelligent computer systems, with applications in the areas of event-driven security and supervisory systems for near-real-time and mission critical operation and network communication systems.
3090 en Welcome and Presentation of Center for Knowledge Transfer in Information Technologies * Centre for knowledge transfer in information technologies performs educational, promotional and infrastructural activities and provides direct exchange of information and experience between researchers and the users of their research results. * We develop and prepare carefully designed educational events, such as: seminars, workshops, conferences and summer schools. * Because of our experiences in European projects we have decided to offer the service to the industries and organizations for consulting, pre-evaluating and helping prepare EU projects proposal as well as support by the project implementation. * We have prepared a number of training web portals with more than 2700 hours of recorded tutorials from different domains of knowledge available at **[[http://videolectures.net/]]**
3091 en IRC IRENE, AREA Science Park * Assistance to industry, in particular SMEs, in the definition of technology needs * Promotion, through the IRC Network, of innovative ideas at European level * Identification of potential technology partners * Enhance of Transnational Technology Transfer Agreements * Support to R&D projects under the European Research Framework Programme
3092 en active-media-group.com srl We are specialized in EDI (electronic data interchange) online solutions: with our product DERWID® we are present on the Italian, Austrian, German and Lichtenstein’s markets. Extensive know-how in cross-platform software development, system integratin, EDI and growing Linux-market are the basis of our competence. Solutions provided in dedicated projects as well as standard tools for different requirements can be offered.
3093 en D-Level srl D-Level is a supplier of Open-Source software platforms and solutions (the Devil Framework) for collecting, integrating, correlating, controlling and visualizing information produced and consumed by hardware and software technologies involved in modern working processes.
3094 en EIDON S.p.A. EIDON is an engineering and contract research company, working in partnership with enterprises to provide cutting edge technological support for product and process innovation. Established in 1979, EIDON was one of the first centers in Italy to provide technological support services of this nature.The company designs and develops ICT solutions for the global, automated management and control of production processes, including those distributed throughout the country. EIDON is recognised by MUR (Italian Ministry of University and Research) as an excellence laboratory in the fields of computer science and electronics.
3095 en ELIMOS srl System integrator, real-time Software development, Digital TVCC systems and centralization, ANPR system, domotic
3096 en Emaze Networks S.p.A. Emaze Networks is an innovative provider of services and products in the Information **Security field**. Emaze customers are Financial and Insurance institutes, Industry and Services companies, R&amp;D departments, Telecommunication and Utility companies.
3097 en GME (general micro electronics) srl Gme electronic engineers have been managing electronic product design and manufacturing for big European companies since 1993. We are capable of developing a wide range of electronic products, home automation (zigbee, lonworks, konnex), wireless, wifi, rfid, with all disciplines required.
3098 en Sicom test S.r.l. The mission of the Company is to perform measurements and tests concerning telecommunication mobile terminal products. Sicom services are including Functional tests, Quality of service (QoS), SAR measurements, Network operator acceptance, Global Certification Forum tests and railways R-GSM tests.
3099 en T-CONNECT srl T-Connect is engaged in R&D of wireless applications on 3G devices integrated with Location Based Services for the Mobile Business. The company works with a 3G mobile carrier for consulting services on functional and inter-operability (IOT) testing on wireless systems (3G mobile networks, Wi-Fi and DVB-H).
3100 en Testability Snc Development, Deployement, Installation and Startup of: * Production Testing Lines for Electronic devices and equipment * Automated Test Benches * Mechanical Toolings for Electronics, i.e. fixtures * Replicas of existing Production/Testing lines * Technology transfer
3101 en Wego s.r.l. Wego works in the e-government sector. Its activities consist of consulting and software development for public administration entities, and especially of:n * business process and administrative proceedings engineering;n * front end solutions development;n * electronic document management (EDM).
3102 en IRC Slovenia, Jo?ef Stefan Institute * Helping local industry specify its new technological needs (technological audits) and with the help of IRC network trying to identify partners to provide these new technologies. * Helping local industry identify which of its technologies are suitable for transfer to other regions or industries and promoting these innovative ideas across Europe through the Innovation Relay Centres network. * Providing assistance in the negotiation process between the provider and the receiver of the technology. * Advising on related aspects of research exploitation, such as patenting and licensing. * Informing about relevant Community and national financial support schemes for innovation.
3103 en andEuros d.o.o. The andEuros company is active in electronics and software research and development to provide complete solutions on system architecture, sensors, sensor networks, mixed signal hardware designs, distributed software and hardware platforms, embedded systems, FPGA/VHDL system-on-chip design, powerline communications and sensor protocols.
3104 en IGEA d.o.o. – GIS Development, Consulting and Services * GIS Development, Consulting and Services (GIS system implementation, data services, cartography, educational and training services ...) * IT Projects (software engineering, programming, system administration ...) * Research and Development * Project management of complex projects
3105 en IKS d.o.o. * Implementing computer vision methods in various fields (video surveillance, traffic control, industrial quality inspection, medical diagnostics) * Development of database applications * Algorithm development and testing
3106 en INDATA d.o.o. Research and development of applied electronics: * Microprocessor applicaiton for building energy management * Building management systems components based on EN14908/ANSI 709.3 standard * Hardware implementation of SSGL protocol
3107 en ISKRA ZA??ITE d.o.o. Research, production and implementation the products on: * Surge protection devices in Low Power Distribution Systems for Power suply Systems; Telephone Exchanges and Terminals; Base Stations, Oile, Gass and wather pipe lines * Data transmission Systems * External Lighting protection Base Stations, Oile, Gass and wather pipe line stations, Medium Voltage Surge Arestors * Integration and Enngenering for complete solutions in Telecommunication Networks, Surge and Over voltage protection solutions * Reserch, production and implementation on Telecommunication Access and Sensor Systems
3108 en Machine learning and finance 
3109 en NOMEN d.o.o. * CORE BUSINESS: BIOMETRIC SECURITY * Other Security Fields: video&amp;audio surveliance, close protection, education * Trainings for security personnel, border security guards, anti terror seminars…
3110 en SETCCE – Security Technology Competence Centre E-commerce products and services (globally used and W3C-compliant digital signature components; products for electronic invoicing process management, and outsourced services; products and services for trusted electronic archiving; CA’s) IT consulting services (company-wide business process de-materialization and optimization; PKI-related security policies; application of digital signatures; ambiental intelligence; pervasive systems etc.)
3111 en Sequential Monte Carlo methods Parts 4 and 5 of this lecture are presented in [[mlss07_davy_smcmc|//Manuel Davy's// "%title"]]
3112 en Stochastic Information Processing in Sensor Networks: Challenges, Some Solutions, and Open Problems 
3113 en Students performing "Easy" on the last day of the MLSS 
3114 en Closing remarks 
3115 en Interview about past, present, future of MLSS In this interview the Videolectures.Net team spoke to Bernhard Schölkopf at the MLSS 2007 in Tuebingen. We were interested how he sees the **social part of the school**, if he **still attends school with the same enthusiasm**, if the talks were too narrow and specialised or widely comprehensable and if **they should invite speakers from different fields such as psychology**...
3116 en A discussion about ML 
3117 en Efficient Closed Pattern Mining in Strongly Accessible Set Systems Many problems in data mining can be viewed as a special case of the problem of enumerating the closed elements of an independence system w.r.t. some specific closure operator. We consider a generalization of this problem to strongly accessible set systems and arbitrary closure operators. For this more general problem setting, the closed sets can be enumerated with polynomial delay if deciding membership in the set system and computing the closure operator can be solved in polynomial time. We discuss potential applications in graph mining.
3118 en Support Vector Machines for Collective Inference Interdependent training instances violate the common assumption of independently drawn examples and render classical learning algorithms an inappropriate choice. Collective inference approaches explicitly incorporate these dependencies by translating the examples into a graph where two training instances are connected if their values depend on each other. We present a support vector approach for collective inference allowing for arbitrary dependencies in the data and report on empirical results. Since exact inference for large graphs is infeasible, we integrate an approximate decoding technique based on loopy belief propagation into the optimization problem. We empirically compare versions of the procedure that are based on exact (using the Hugin algorithm) and approximate decoding (loopy belief propagation and others) in terms of accuracy and execution time.
3120 en Opening of the PMNP 2007 in Sheffield 
3121 en The Cost of Learning Directed Cuts Classifying vertices in digraphs is an important machine learning setting with many applications. We consider learning problems on digraphs with three characteristic properties: (i) The target concept corresponds to a directed cut; (ii) the total cost of finding the cut has to be bounded a priori; and (iii) the target concept may change due to a hidden context. For one motivating example consider classifying intermediate products in some process, e.g., for manufacturing cars or the control flow in software, as faulty or correct. The process can be represented by a digraph and the concept is monotone: Typical faults that appear in an intermediate product will also be present in later stages of the product. The concept may depend on a hidden variable as some pre-assembled parts may vary and the fault may occur only for some charges and not for others. In order to be able to trade off between the cost of having a faulty product and the costs needed to find the cause of the fault, tight performance guarantees for finding the bug are needed.
3123 en Speeding up Graph Edit Distance Computation with a Bipartite Heuristic In the present paper we aim at speeding up the computation of exact graph edit distance. We propose to combine the standard tree search approach to graph edit distance computation with the suboptimal procedure. The idea is to use a fast but suboptimal bipartite graph matching algorithm as a heuristic function that estimates the future costs. The overhead for computing this heuristic function is small, and easily compensated by the speed-up achieved in tree traversal. Since the heuristic function provides us with a lower bound of the future costs, it is guaranteed to return the exact graph edit distance of two given graphs.
3124 en Gene Regulatory Network Inference: In Silico Hypotheses and Experimental Validation The literature is replete with various approaches to extracting gene regulatory networks from microarray profiling data. Although many of these methods have produced networks which appear biologically plausible, based on circumstantial evidence from the literature, very little work has been done on validating the model networks experimentally. In this paper we present new results from a microarray time series study of adaptation to cold and successive re-adaptation to optimal temperatures in E. coli. Model networks were inferred from the data using the variational Bayesian state space modelling approach of Beal et al. (2005). Analysis of the biological implications of these network models is still on-going, but preliminary analysis has already revealed some promising novel biological hypotheses relating to the transcriptional response of bacterial cells adapting to the temperature shift. Our model places a number of genes at the higher level of the hierarchy (“hubs”) in the temperature shifted network, including hns and hybC. Encouragingly, the model network reveal some of the known regulatory interactions in the literature. The model also indicates that hns downregulates genes involved in aerobic metabolism and upregulates genes involved in anaerobic metabolism. This immediately suggests the hypothesis that hns plays a key role in regulating a switch between aerobic and anaerobic metabolism during the temperature adaptation. The experimental verification of this hypothesis is extremely simple. The hns- mutant exhibits the phenotype of growing at 10oC but stops growing if switched from 10oC to 37oC. Time series microarray data collected from this mutant strain should directly address the question of whether the expression of genes involved in aerobic/anaerobic metabolism during re-adaptation to 37oC is dependent on the expression of hns. Regulatory interactions which are either confirmed or not confirmed by this experiment will be used to define Bayesian priors for iterative retraining of the state space model by including the time series data collected from the perturbed system. We present results from a full cycle of this iterative procedure.
3125 en Least squares estimation of a transcription regulation model The way transcription factors regulate the activity of their target genes is of much interest. Several authors have recently used model based computational approaches to infer concentrations of transcription factor proteins from high throughput gene expression data ([1-3,6-7]. Here, I present an approach explicitly formulated to model periodic biological phenomena, and a least squares framework for parameter estimation of such a model. Such a computational strategy can be used to infer levels of transcription factor activities at the protein level using genes that are regulated by single transcription factors, and then to decipher “transcriptional logic” of genes under regulation by the comboined actions of multiple transcription factors.
3126 en Reverse engineering gene and protein regulatory networks using graphical models: A comparative evaluation study One of the major goals in systems biology is to infer the architecture of biochemical pathways and regulatory networks from postgenomic data, such as microarray gene expression and cytometric protein expression data. Various reverse engineering Machine Learning methods have been proposed in the literature, and it is important to understand their relative merits and shortcomings. In the talk the learning performances of three different graphical models machine learning methods, namely Relevance networks, Gaussian Graphical Models, and Bayesian networks, are cross-compared on real cytometric protein data and simulated data from the RAF signalling pathway. Relevance networks are based on pairwise association scores and straightforward to implement. But the inference is not done in the context of the whole system and there is no possibility to distinguished between direct and indirect associations. Both shortcomings are addressed by Gaussian graphical models, where the partial correlation between two variables, conditional on all the other domain variables, is employed as association score. Bayesian networks are more flexible probabilistic graphical models for conditional dependence and independence relations. Bayesian networks are based on directed acyclic graphs and can be exploited to analyse interventional data for identifying putative causal interactions. The empirical results were obtained by applying the shrinkage estimator of Schaefer and Strimmer (2005) to compute the inverse covariance matrix for Gaussian Graphical Models, and Bayesian network inference was done by sampling BNs from the posterior distribution with order Markov chain Monte Carlo (MCMC), as proposed by Friedman and Koller (2003). The experimental results were obtained by analysing data from the RAF protein signalling network reported in Sachs et al. (2005); which describes the interaction of eleven phosphorylated proteins and phospholipids in human immune system cells. Thereby it was distinguished between real cytometric protein activity measurements reported in Sachs et al. (2005) and synthetically generated data as well as between pure observational and interventional data. Observational data are obtained by passively monitoring the system without any interference while interventional data are obtained by actively manipulating variables, e.g. using gene knock-out experiments. Detailed results of this empirical study have been published in Werhli et al. (2006) and Grzegorczyk (2007). The three main findings can be summarized as follows. First, exclusively on Gaussian observational data, Bayesian networks and Gaussian graphical models were found to outperform Relevance networks. Second, for observational data no significant difference between Bayesian networks and Gaussian Graphical models was observed. Third, only for interventional data Bayesian networks clearly performed superior to the other two approaches.
3127 en Support Computation for Mining Frequent Subgraphs in a Single Graph Defining the support (or frequency) of a subgraph is trivial when a database of graphs is given: it is simply the number of graphs in the database that contain the subgraph. However, if the input is one large graph, it is surprisingly difficult to find an appropriate support definition. In this paper we study the core problem, namely overlapping embeddings of the subgraph, in detail and suggest a definition that relies on the non-existence of equivalent ancestor embeddings in order to guarantee that the resulting support is anti-monotone. We prove this property and describe a method to compute the support defined in this way.
3128 en General Graph Refinement with Polynomial Delay Of many graph mining algorithms an essential component is its procedure for enumerating graphs such that no two enumerated graphs are isomorphic. All frequent subgraph miners require such a component [14, 5, 1, 6], but also other data mining algorithms, such as for instance [7] require such a procedure, which is often called a “refinement operator” or a “join operator” depending on the enumeration (or can- didate generation) strategy. Consequently, in the data min- ing literature a huge amount of algorithms have been pre- sented for enumerating graphs without isomorphisms. None of these algorithms, however, have been shown to run with polynomial delay, i.e. when enumerating the graphs, even without accessing any data, in the worst case it may take exponential time to list the next graph. From a theoreti- cal perspective, this is a disappointing result, as Goldberg [3, 4] showed already in the early nineties that enumerating all connected graphs without isomorphs can be done with O(n6) polynomial delay. In this paper, we address this prob- lem. We show that there is a graph enumeration algorithm that can be used in graph mining algorithms and has O(n5) polynomial delay. Thus, we not only propose an algorithm to enumerate (the interesting) parts of the class of connected graphs, but also improve the earlier bound on graph enumer- ation that was shown by Goldberg. At the same time, our algorithm is general enough to be also applicable for other types of graphs than connected graphs. For instance, it can also be used for enumerating unconnected graphs, planar graphs, outerplanar graphs, and many other types of graphs, with polynomial delay. The datastructure that is produced by the algorithm allows membership queries to be executed in polynomial time: for any given graph, independent of its representation, we can determine in polynomial time if it is part of a set of enumerated subgraphs. In particular, in our algorithm it is not necessary to compute a canonical form. However, if this is desirable, we can use our algorithm to enumerate graphs in several canonical forms, for instance, the DFS and BFS canonical forms used in gSpan [14] and MoFA [1].
3129 en Improving frequent subgraph mining in the presence of symmetry The difficulty of the frequent subgraph mining problem arises from the tasks of enumerating the subgraphs and calculating their support in the dataset. If the dataset graphs have additional information in the form of labels, these problems can be solved quite easily. However, if the dataset graphs are unlabeled or only have a few labels, then the complexity of these problems greatly reduces the number and sizes of the dataset graphs that can be managed. Thus far, researchers working on the frequent subgraph mining problem have given little attention to such datasets, and current algorithms tend to do poorly on them. Yet, there are many applications which deal with this type of data, mainly in the fields of compute vision where the data is structured as 2D or 3D meshes [8], or communication/transportation networks where the information is mostly topological.
3130 en DIGDAG, a first algorithm to mine closed frequent embedded sub-DAGs Although tree and graph mining have attracted a lot of attention, there are nearly no algorithms devoted to DAGmining, whereas many applications are in dire need of such algorithms. We present in this paper DIGDAG, the first algorithm capable of mining closed frequent embedded sub- DAGs. This algorithm combines efficient closed frequent itemset algorithms with novel techniques in order to scale up to complex input data.
3131 en Abductive Stochastic Logic Programs for Metabolic Network Inhibition Learning We revisit an application developed originally using Induc- tive Logic Programming (ILP) by replacing the underlying Logic Pro- gram (LP) description with Stochastic Logic Programs (SLPs), one of the underlying Probabilistic ILP (PILP) frameworks. In both the ILP and PILP cases a mixture of abduction and induction are used. The abductive ILP approach used a variant of ILP for modelling inhibition in metabolic networks. The example data was derived from studies of the e®ects of toxins on rats using Nuclear Magnetic Resonance (NMR) time-trace analysis of their bio°uids together with background knowledge representing a subset of the Kyoto Encyclopedia of Genes and Genomes (KEGG). The ILP approach learned logic models from non-probabilistic examples. The PILP approach applied in this paper is based on a gen- eral approach to introducing probability labels within a standard sci- enti¯c experimental setting involving control and treatment data. Our results demonstrate that the PILP approach not only leads to a signi¯- cant decrease in error accompanied by improved insight from the learned result but also provides a way of learning probabilistic logic models from probabilistic examples.
3132 en An Efficient Sampling Scheme For Comparison of Large Graphs As new graph structured data is being generated, graph comparison has become an important and challenging problem in application areas such as molecular biology, telecommunications, chemoinformatics, and social networks. Graph kernels have recently been proposed as a theoretically sound approach to this problem, and have been shown to achieve high accuracies on benchmark datasets. Different graph kernels compare different types of subgraphs in the input graphs. So far, the choice of subgraphs to compare is rather ad-hoc and is often motivated by runtime considerations. There is no clear indication that certain types of subgraphs are better than the others. On the other hand, comparing all possible subgraphs has been shown to be NP-hard, thus making it practically infeasible. These difficulties seriously limit the practical applicability of graph kernels. In this article, we attempt to rectify the situation, and make graph kernels applicable for data mining on large graphs and large datasets. Our starting point is the matrix reconstruction theorem, which states that any matrix of size 5 or above can be reconstructed given all its principal minors. By applying this to the adjacency matrix of a graph, we recursively define a graph kernel and show that it can be efficiently computed by using the distribution of all size 4 subgraphs of a graph. This distribution, we argue, is similar to a sufficient statistic of the graph, especially when the graph is large. Exhaustive enumeration of these subgraphs is prohibitively expensive, scaling as O(n4). But, by bounding the deviation of the empirical estimates of the distribution from the true distribution, it suffices to sample a fixed number of subgraphs. Incidentally, our bounds are stronger than those found in the bio-informatics literature for similar techniques. In our experimental evaluation, our graph kernel outperforms state-of-the-art graph kernels both in times of time and classification accuracy.
3133 en Fast Inference in Infinite Hidden Relational Models Relational learning is an area of growing interest in machine learning (Dzeroski & Lavrac, 2001; Friedman et al., 1999; Raedt & Kersting, 2003). Xu et al. (2006) introduced the infinite hidden relational model (IHRM) which views relational learning in context of the entity-relationship database model with entities, attributes and relations (compare also (Kemp et al., 2006)). In the IHRM, for each entity a latent variable is introduced. The latent variable is the only parent of the other entity attributes and is a parent of relationship attributes. The number of states in each latent variable is entity class specific. Therefore it is sensible to work with Dirichlet process (DP) mixture models in which each entity class can optimize its own representational complexity in a self-organized way. For our discussion it is sufficient to say that we integrate a DP mixture model into the IHRM by simply letting the number of hidden states for each entity class approach infinity. Thus, a natural outcome of the IHRM is a clustering of the entities providing interesting insight into the structure of the domain.
3134 en Inferring vertex properties from topology in large networks Network topology not only tells about tightly-connected “communities,” but also gives cues on more subtle properties of the vertices. We introduce a simple probabilistic latent-variable model which finds either latent blocks or more graded structures, depending on hyperparameters. With collapsed Gibbs sampling it can be estimated for networks of 106 vertices or more, and the number of latent components adapts to data through a Dirichlet process prior. Applied to the social network of a music recommendation site (Last.fm), reasonable combinations of musical genres appear from the network topology, as revealed by subsequent matching of the latent structure with listening habits of the participants. The advantages of the generative nature of the model are explicit handling of uncertainty in the sparse data, and easy interpretability, extensibility, and adaptation to applications with incomplete data.
3135 en A Polynomial-time Metric for Outerplanar Graphs (Extended Abstract) In the chemoinformatics context, graphs have become very popular for the representation of molecules. However, a lot of algorithms handling graphs are computationally very expensive. In this paper we focus on outerplanar graphs, a class of graphs that is able to represent the majority of molecules. We define a metric on outerplanar graphs that is based on finding a maximum common subgraph and we present an algorithm that runs in polynomial time. Having an efficiently computable metric on molecules can improve the virtual screening of molecular databases significantly.
3136 en Weighted Substructure Mining for Image Analysis In web-related applications of image categorization, it is desirable to derive an interpretable classification rule with high accuracy. Using the bag-of-words representation and the linear support vector machine, one can partly fulfill the goal, but the accuracy of linear classifiers is not high and the obtained features are not informative for users. We propose to combine item set mining and large margin classifiers to select features from the power set of all visual words. Our resulting classification rule is easier to browse and simpler to understand, because each feature has richer information. As a next step, each image is represented as a graph where nodes correspond to local image features and edges encode geometric relations between features. Combining graph mining and boosting, we can obtain a classification rule based on subgraph features that contain more information than the set features. We evaluate our algorithm in a web-retrieval ranking task where the goal is to reject outliers from a set of images returned for a keyword query. Furthermore, it is evaluated on the supervised classification tasks with the challenging VOC2005 data set. Our approach yields excellent accuracy in the unsupervised ranking task and competitive results in the supervised classification task.
3137 en A Universal Kernel for Learning Regular Languages We give a universal kernel that renders all the regular languages linearly separable. We are not able to compute this kernel efficiently and conjecture that it is intractable, but we do have an efficient ?-approximation
3138 en Mining, Indexing, and Searching Graphs in Large Data Sets Recent research on pattern discovery has progressed from mining frequent itemsets and sequences to mining structured patterns including trees, lattices, and graphs. As a general data structure, graph can model complicated relations among data with wide applications in Web, social network analysis, and bioinformatics. However, mining and searching large graphs in graph databases is challenging due to the presence of an exponential number of frequent subgraphs. In this talk, we present our recent progress on developing efficient and scalable methods for mining and searching of graphs in large databases. We introduce gSpan and CloseGraph, two efficient methods for mining frequent graph patterns in graph databases. Then we introduce constraint-based graph mining methods. Further, we introduce a graph indexing method, gIndex, and a graph approximate searching method, grafil, both taking advantages of frequent graph mining to construct a compact but highly effective graph index and perform similarity search with such indexing structures. These methods not only facilitate mining and querying graph patterns in massive datasets but also claim broad applications in other fields, including DB/OS systems and software engineering.
3143 en Independence ratios of nearly planar graphs With purposeful imprecision, a graph is said to be nearly planar if it resembles in some essential way a planar graph. Classic measures of near planarity include the thickness, the crossing number, and the genus. Modern versions of nearly planar graphs include the graphs embedded on surfaces with large width (shortest noncontractible cycle). Geometric graph theory has given us a different notion of locally planar as well as two definitions of quasi-planar graphs. This talk will begin with a retrospective look at independence ratios of locally planar graphs. We consider contrasts between results about the independence ratio and those about the chromatic number. We will then proceed to results and open questions about independence ratios of various classes of nearly planar graphs.
3144 en Honeycomb tori and Cayley graphs on generalized dihedral groups We investigate a family of graphs known to some people as honeycomb tori. We establish that they all are Cayley graphs on generalized dihedral groups. We then look at hamiltonicity properties for this family of graphs.
3145 en Graphs with extremal energy tend to have a small number of distinct eigenvalues The sum of the absolute values of the eigenvalues of a graph is called the energy of the graph. We study the problem of finding graphs with extremal energy within specified sets of graphs. We develop some tools for treating such problems and obtain some partial results. In particular, we show that in many cases the expected extremal graphs with a small number of distinct eigenvalues do not exist and that actual extremal graphs could have a large number of distinct eigenvalues. Zigzag and central circuit structure
3146 en Chirality, genera and simplicity of orientably-regular maps An orientably-regular map is a cellular embedding of a graph or multigraph in an orientable-surface, with the property that the group of orientation- and incidencepreserving automorphisms has a single orbit on the arcs (ordered edges) of the embedding. I will describe a recent computer-assisted determination of all such maps on surfaces of genus 2 to 100, which revealed some interesting patterns, leading to new discoveries about gaps in the genus spectrum of examples that (a) are chiral, and/or (b) have the property that the map or its topological dual has simple underlying graph. The last part is joint work with Jozef Siran and Tom Tucker.
3147 en Zigzag and central circuit structure of two-faced plane graphs A zigzag in a k-valent plane graph G is a circuit of edges, such that any two, but not three consecutive edges belong to the same face. A railroad in G is a circuit of evengonal faces, such that any face is adjacent to its neighbors on opposite edges. Boundary circuits of a railroad are two ”parallel” zigzags if k = 3, or, in a 4-valent graph, two such central circuits. We consider the zigzag and railroad structure of two-faced 3- and 4-valent plane graphs (generalizations of Platonic polyhedra) and their connections with other problems.
3148 en On automorphism groups of vertex-transitive graphs One of the most fundamental questions one can ask about a vertex-transitive graph  is what is the full automorphism group Aut() of ? This is usually a difficult question, as is evidenced by determining Aut() should allow for the solution of many other problems concerning . For example, is  edge-transitive, 1/2-transitive, or normal? Additionally, one should be able to solve the isomorphism problem for . In this talk, we will report on recent progress in determining Aut(), for  in certain classes of vertex-transitive graphs, especially Cayley graph of certain abelian groups.
3149 en Geometry of partial cube graphs Partial cubes are graphs defined by a geometric structure: the graph vertices can be placed on the vertices of a hypercube in such a way that graph distance equals Hamming distance. We survey recent developments in the theory of these graphs that relate them in other ways to geometric structures: lattice embeddings, hyperplane arrangements, systems of translated quadrants in the plane, and flip graphs of triangulations.
3150 en Erdös-Ko-Rado theorems I will show that this theorem has a natural proof using linear algebra, and that this approach also applies to situations where sets are replaced by objects such as subspaces, permutations or partitions.
3151 en Small polyhedral models of the torus, the projective plane and the Klein bottle Models of these manifolds have been studied at least since the work of Moebius, with increasing depth and many results in more recent times. The models range from purely combinatorial to various types of geometric representations, such as by topological complexes, by planar-faced polyhedra (convex or nor necessarily convex), or by smooth manifolds. The talk will give a survey of available results, and then concentrate on what seems to be a new direction – models that admit as faces selfintersecting polygons. One of the unexpected results is that in some cases such models are simpler and more readily visualized than the more traditional ones, and that in other cases they are the only possible ones. The understanding of the role of selfintersecting polygons as faces sheds light, among other things, on the relations between the Platonic solids and the Kepler-Poinsot regular polyhedra. Many open problems remain, both in the traditional framework and in the new one.
3152 en Geometric intersection graphs Geometric intersection graphs are intensively studied both for their practical motivations and interesting theoretical properties. Many classes allow elegant characterizations, for many of them optimization problems NP-hard in general can be solved in polynomial time. We will present a survey of recent results and old problems in this area, including questions related to colorability, maximum clique or representations of planar and co-planar graphs. Computational complexity of recognition of many of intersection defined classes of graphs will be one of the main topics.
3153 en On Hurwitz theory: enumerating branched surface coverings With a chronological review of Hurwitz theory, we survey some known results on the enumeration of the equivalence classes of several types of branched coverings of a surface. In particular, relations with the enumeration of the equivalence classes of several types of graph coverings and enumerating the isomorphism classes of branched orientable surface coverings of a nonorientable surface will be mentioned. Also we discuss a similar problem for branched coverings having prescribed branched types.
3154 en Famous and lesser known problems in “elementary” combinatorial geometry and number theory Which problems attain great notoriety and which are delegated to collect dust on a shelf? “Elementary” problems tend to attract attention because they are very easy to understand and look “solvable”. It is a mystery to me why some attract a lot of attention while others lie hibernating waiting for some new fresh ideas. In their recent interesting book Research Problems in Discrete Geometry (Springer, New York 2005) P. Brass, W. Moser, J. Pach wrote: “Although Discrete Geometry has a rich history extending more than 150 years, it abounds in open problems that even a high-school student can understand and appreciate. Some of these problems are notoriously difficult and are intimately related to deep questions in other fields of mathematics. But many problems, even old ones, can be solved by a clever undergraduate or a high- school student equipped with an ingenious idea and the kinds of skills used in a mathematical olympiad.”
3155 en Partial cubes and other l1-graphs Partial cubes are isometric subgraphs of the hypercube graphs, while l1-graphs are graphs embeddable in a hypercube up to a scale. These two classes of graphs have been focus of much study in recent years. In the talk we will discuss recent structure results and a Euler-type inequality for partial cubes, which is a joint work with S. Klavˇzar. We will also review the classification of l1-embeddable fullerene graphs (joint work with M. Marcusanu) and related results.
3156 en Distance-regular graphs and the quantum affine algebra Uq(bsl2) Combinatorial objects, such as graphs, can often be used to construct representations of abstract algebras. In this talk we will consider a graph possessing a high degree of regularity, known as a distance-regularity. For this graph we define an algebra generated by the adjacency matrix and a certain diagonal matrix. There exists a set of elements in this algebra that, under a minor assumption, satisfy some attractive relations. Using these relations we obtain a representation of the quantum affine sl2 algebra.
3157 en Infinite planar tessellations We survey problems involving planar embeddings of locally finite, infinite graphs, especially graphs that have only one infinite component when any finite subgraphs is deleted. Problems considered concern separating double rays, geodetic double rays, facial walks, rate of growth, and vertex-, edge- and face-homogeneity.
3158 en Graph methods and geometry of data In recent years graph-based methods have seen success in different machine learning applications, including clustering, dimensionality reduction and semi-supervised learning. In these methods a graph is associated to a data set, after which certain aspects of the graph are used for various machine learning tasks. It is, however, important to observe that such graphs are empirical objects corresponding to a randomly chosen set of data points. In my talk I will discuss some of our work on using spectral graph methods for dimensionality reduction and semi-supervised learning and certain theoretical aspects of these methods, in particular, when data is sampled from a low-dimensional manifold.
3159 en Learning gene regulatory networks in Arabidopsis Thaliana Gene regulatory networks govern the functional development and biological processes of cells in all organisms. Genes regulate each other as part of a complex system, of which it is vitally important to gain an understanding. For example, discovery of the complete gene regulatory networks in humans would allow the identification of genes which cause disease, and could be used for drug discovery to identify genes interacting with compounds of interest. Similarly in plants knowledge of the gene regulatory networks would allow the development of stress (drought/salt/temperature) resistant crops. Learning large gene regulatory networks with thousands of genes with any certainty from microarray data is extremely challenging. This research aims to build around known networks from the literature on gene regulation, and assesses which other genes are likely to play a regulatory role or be in the same regulatory pathways. The gene regulatory networks are modelled with a Bayesian network. The gene expThe use of large scale public microarray data appears to be a very useful starting point for informing future experiments in order to determine gene regulatory networks.ression levels are quantised and a greedy hill climbing search method is used within a network structure learning algorithm. The inclusion of extra genes with the best explanatory power into the model has been demonstrated to be robust. Large sets of microarray experiments are used in this analysis, specifically 2466 NASC Arabidopsis thaliana microarrays containing gene expression levels of over twenty thousand genes in a number of experimental conditions. Initial investigation of this data is very promising. We have learned gene transcription sub-networks (see Figure 1) regulated by the plant’s circadian clock. The network shown was generated from microarray data without the use of any prior information, and yet the method managed to identify the strong causal relationships between clock components (TOC1, LHY, ELF3, ELF4, CCA1) and to link these to further key regulators of important processes (e.g. ZAT, myb and GATA transcription factors).
3160 en A theory of similarity functions for learning and clustering Kernel methods have proven to be very powerful tools in machine learning. In addition, there is a well-developed theory of sufficient conditions for a kernel to be useful for a given learning problem. However, while a kernel function can be thought of as just a pairwise similarity function that satisfies additional mathematical properties, this theory requires viewing kernels as implicit (and often difficult to characterize) maps into high-dimensional spaces. In this talk I will describe a more general theory that applies to more general similarity functions (not just legal kernels) and furthermore describes the usefulness of a given similarity function in terms of more intuitive, direct properties of the induced weighted graph. An interesting feature of the proposed framework is that it can also be applied to learning from purely unlabeled data, i.e., clustering. In particular, one can ask how much stronger the properties of a similarity function should be (in terms of its relation to the unknown desired clustering) so that it can be used to *cluster* well. Investigating this question leads to a number of interesting graph-theoretic properties, and their analysis in the inductive setting uses regularity-lemma type results of [FK99,AFKK03]. This work is joint with Maria-Florina Balcan and Santosh Vempala.
3161 en Convergence of the graph Laplacian application to dimensionality estimation and image segmentation Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. We will present the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a nonuniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. We will give two applications of these theoretical results.
3162 en ProBic: identification of overlapping biclusters usinf Probabilistic Relational Models, applied to simulated gene expression data Biclustering is an increasingly popular technique to identify regulatory modules that are linked to biological processes. A bicluster is defined as a subset of genes which have a similar expression profile for a subset of conditions in the context of gene expression data. We describe a novel method, called ProBic, to simultaneously identify a series of overlapping biclusters in gene expression data within the framework of Probabilistic Relational Models (PRMs) [1;2]. PRMs are a relational extension to Bayesian Networks and allow for the integration of relational data within a unified probabilistic framework. A PRM model describes a joint probability as in Bayesian networks but with additional constraints on the conditional probability functions. We propose a novel PRM based biclustering model, in which gene expression data can be considered as relational data. The classes are Gene, Condition and Expression. Both the classes Gene and Condition have a vector attribute Bicluster containing a series of bicluster-id’s. These vectors represent which biclusters exist for a gene or condition and are initially unknown. Condition has an extra attribute ID, which is a unique number for each condition. Expression has an attribute Level containing the expression value and two reference slots which point to the gene and condition for which the level was measured. Expression.Level is conditionally dependent on Gene.Bicluster, Condition.Bicluster and Condition.ID. The conditional dependency is modeled as a set of Gaussian distributions with conjugate priors. The ProBic model naturally deals with missing values (in fact, there are no ‘missing’ values in this model) and robust sets of biclusters are obtained due to explicit modeling of noise. The maximum likelihood solution is approximated using an Expectation-Maximization strategy. ProBic was applied to simulated gene expression data sets and all the biclusters were successfully identified. Various noise settings and different overlap models (average, sum, product) have been explored. Our results show that PRM models can be used to identify overlapping biclusters in an efficient and robust manner, naturally dealing with missing values and noise.
3163 en Probabilistic graph partitioning We consider the problem of Graph Partitioning for applications in Web Mining and Collaborative Filtering. Our approach is based on predicting the presence/absence of a directed link based on a form of probabilistic mixture model. Being based on a generative model of directed graphs, we are able to apply an approximate Bayesian treatment to automatically select an appropriate number of partitions. We will discuss an application in Collaborative Filtering and comment on relations to mixed membership models, Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis.
3164 en Estimating parameters and hidden states in biological networks with particle filters Abstract. Identifying biological networks requires to develop models able to capture their dynamics and statistical learning methods to estimate their parameters from time-series measurements. In particular, Ordinary Differential Equations (ODE's) are a rich family of quantitative models, but their estimation remains a bottleneck for reverse engineering, especially when the biological processes are nonlinear and partially observed. In a recent work [5], we have proposed a state-space model, derived from ODE's used in Systems Biology, that can encompass regulatory networks, metabolic networks or signaling pathways. For this model, we have derived a Bayesian estimation procedure for both parameters and hidden variables based on nonlinear filtering and a particular approximation scheme: the Unscented Kalman Filter (UKF). Despite satisfactory results, the UKF approximation possesses some limitations such as few theoretical results and a limited range of applications. We propose then to use Sequential Monte Carlo methods (SMC), also known as particle filters [2], which are now standard methods for filtering nonlinear and non Gaussian processes. SMC methods provide a nonparametric approximation of the filtering probability discretely supported by the so-called particles whose convergence properties have been intensively studied [1, 2]. In this work, we develop a SMC approach for the Bayesian estimation of the (kinetic) parameters and hidden states, by considering the parameters as additional hidden states with no evolution. Despite the generality of SMC methods, the deterministic evolution of the hidden variables implies a fast degenerescence of standard algorithms, e.g. bootstrap filter. To overcome this problem, we use a solution proposed by Liu and West [4], which relies on an adapted kernel smoothing of the particle approximation. The method is illustrated on the Repressilator, an ODE proposed for a gene regulatory network[3], and on an ODE for the JAK-STAT signaling pathway [6]. Experimental results show that particle filters provide similar results to UKF for the parameter estimation and lower Mean Square Error for the state estimation, while offering a greater versatility.
3165 en Evolution of protein complexes and protein interaction networks There is an abundance of data on protein interactions and protein complexes, both from conventional smallscale experiments collected over the decades, including threedimensional structures, and more recently by largescale functional genomics experiments. We can now draw on the information available about protein interactions in order to study the evolution of interactions. We have shown that interactions, just like individual proteins, frequently emerge by duplication and divergence. The duplication of a protein that engages in proteinprotein interactions raises issues about the stoichiometry and equilibrium of protein complexes when the quantity of one component increases. Nevertheless, our results indicate that most interactions and complexes have evolved by stepwise duplications of individual proteins engaged in interactions. We show that duplicated complexes retain the same overall function, but have different binding specificities and regulation, revealing that duplication is associated with functional specialization[1,2]. From analysis of crystal structures of proteins as well as the domain architectures of multidomain proteins, it is clear that physical interactions between identical or homologous domainsand protein chains are extremely common [3,4]. How have this particular class of interactions evolved, and
3166 en Prediction on a graph We will discuss the problem of robust online learning over a graph. Consider the following game for predicting the labeling of a graph. ”Nature” presents a vertex v1; the ”learner” predicts the label of the vertex ˆy1; nature presents a label y1; nature presents a vertex v2; the learner predicts ˆy2; and so forth. The learner’s goal is minimize the total number of mistakes. If nature is adversarial, the learner will always mispredict; but if nature is regular or simple, there is hope that a learner may make only a few mispredictions. Thus, a methodological goal is to give learners whose total mispredictions can be bounded relative to the ”complexity” of nature’s labeling. In this talk, we consider the ”label cut size” as a measure of the complexity of a graph’s labeling, where the size of the cut is the number of edges between disagreeing labels. We will give bounds which depend on the cut size and the (resistance) diameter of the graph.
3167 en Inferring ancestral states of the bZIP transcription factor interaction network As whole-genome protein interaction network datasets become available for a wide range of species, evolutionary biologists have the opportunity to address some of the unanswered questions surrounding the evolution of these complex systems. Protein interaction networks from divergent organisms may be compared to investigate how gene duplication, deletion and ‘re-wiring’ processes may have shaped the evolution of their contemporary structures [1,2]. However, current approaches to aligning observed networks from multiple species are generally lacking the phylogenetic context necessary for meaningful conclusions to be drawn regarding network evolution. Here we show how probabilistic modeling can provide a platform for the quantitative analysis of multiple protein interaction networks. We apply this technique to the reconstruction of ancestral networks for the bZIP family of transcription factors [3] and find that excellent agreement is obtained with an alternative, sequence-based method for the prediction of leucine zipper interactions [4]. Further analysis shows our probabilistic method to be significantly more robust to the presence of noise in the observed network data than a simple parsimony-based approach [5]. In addition, the integration of evidence over multiple species means that the same method may be used to improve the quality of noisy interaction data for extant species. This is the first time that ancestral states of a protein interaction network have been reconstructed using an explicit probabilistic model of network evolution. We anticipate that it will form the basis of more general methods for probing the evolutionary history of biochemical networks.
3168 en Mixture models on graphs One of the most fundamental challenges in the analysis of 'omics data sets is clustering the relevant quantities (gene transcripts, protein levels, etc.) into distinct groups. One of the simplest instances occurs when comparing data obtained from two different conditions, where the basic task is to assess whether a quantity is upregulated, downregulated or unregulated. This task has traditionally been addressed using t-statistics or, from a probabilistic point of view, mixture models, with one mixture representing one of the three states of regulation. This approach tacitly assumes the various measurements to be independently drawn from the same mixture distribution. However, it is well known that biological quantities (genes, enzymes, etc.) are not independent, but they are linked in an often very complex network of interactions at various levels. It is therefore reasonable to use available network structure (and weighting) information in order to obtain a more accurate inference of the expression state. This can also be found useful in finding suitable subnetworks that exhibit coherent behaviours, giving rise to testable biological predictions. In this contribution, we introduce a probabilistic model that implements mixture models on a graph. The graph structure is encoded in a set of conditional prior distributions over the latent class memberships. This formulation leads naturally to a Gibbs sampling approach. We present preliminary results on synthetic and real data where gene expression is modelled as a mixture of a Gaussian and two exponential distributions.
3169 en Bayesian Inference of transcription factor activity - an application to the fission yeast cell cycle When modeling genetic regulatory interactions, it is often assumed that the mRNA expression of a transcription factor is a reliable proxy for the regulatory activity of that transcription factor. There are many examples where this assumption does not hold due to post-transcriptional and translational modifications of the transcription factor protein. As true transcription factor activity is very difficult to measure, methods to infer it are becoming increasingly common and it is likely that will become increasingly important when building models of regulatory interactions. Previously, we have shown how Bayesian techniques, particularly Markov- Chain Monte-Carlo based sampling, can enable us to make inferences regarding the activity of transcription factors based on the transcript levels of their targets. However, in that work, we only looked at simple regulatory interactions where one transcription factor acted individually on a set of target genes. In this work, we investigate extending this model to the more general (and common) case of multiple transcription factors working together. As an example application, we use data from a small regulatory network from the fission yeast cell cycle in which several transcription factors are known to work together to produce the desired response, and for which plentiful experimental data are available.
3170 en Stochastic estimation of fluxes in metabolic networks The qualitative and quantitative information conveyed by metabolic networks are important for regulating the metabolism of an organism to achieve desired targets. One approach to quantification is the 13C tracer experiment which aims to provide information on metabolic fluxes. The flux estimation problem is addressed in steady state and dynamic conditions in this presentation. The problem formulation in the steady state leads to a latent variable model structure which is utilised in applying the stochastic estimation framework to solve the flux quantification problem. A natural algorithm to solve this problem is the expectationmaximisation algorithm which is applied first. This is extended to the Markov Chain Monte Carlo algorithm to account for nonGaussian measurement noise. Finally, a sequential Monte Carlo filter is used to determine the fluxes under dynamic conditions. Results are presented for the central metabolism of Cornybacterium Glutamicum in the steady state and using a simulated metabolic network for the dynamic case.
3171 en Stochastic Parameter Estimation in Biochemical Signalling Pathways It is common when modelling biochemical networks to use qualitative information such as the general ODE model structure so as to proceed in parameter estimation while at the same time retaining the basic model structure the best represents the biochemical process governing the cell. This is not the case however when the population of the available molecules from each of the participating species is very small (small copy number) deeming necessary the introduction of complex stochastic modelling techniques that make use of chemical master equations to simulate the trajectories of the states (species concentration) of the system [7]. Gene expression is stochastic by nature [7][5] and as a consequence gene regulatory and signal transduction networks follow a similar behaviour. Most importantly, a large number of gene expression data sets examined in yeast, mouse and human cells follow a Pareto-like distribution model skewed by many low-abundance transcripts, covering a large variety of eukaryotic cells [2]. It is therefore apparent that a stochastic modelling strategy should be structure so as to accommodate the specific needs of the system....
3172 en A comparison of hypothesis testing methods for ODE models of biochemical systems In this talk we present a comparison of different methods of testing alternative hypotheses expressed using ODE models of biochemical systems. We investigated applicability, limitations and stability of a range of hypotheses testing methods including maximum likelihood based information criteria, local deterministic approximations around maximum a posteriori estimates (Laplace approximations) for computing marginal likelihoods, importance sampling based marginal likelihood estimators, and a path sampling estimator built upon the principles of thermodynamic integration. We demonstrate that in the cases where models are linear in the parameter space, Laplace approximations provide a fast and stable estimate of the marginal likelihoods required for computing Bayes factors. This estimate, however, fails when the models have non-trivial parameter posteriors. We reject common importance sampling estimators as they produce very unstable estimates in practical cases (relative errors of the estimates vary from 40% to 600% depending on the particular example used). We demonstrate that the annealed importance sampling estimator of the marginal likelihoods and path sampling methods produce very good estimates even in non-trivial cases (relative error within 1%-8%). Maximum likelihood information criteria often produce the correct ordering of the hypotheses. These methods, however, do not produce a quantitative measure of model preference (odds) and sometimes even fail, preferring a more complex model over the true one, and there is no general method to detect such a failure. The study is performed over realistically sized ODE models of biochemical systems using simulated data sets.
3173 en Debate 
3174 en Frequent graph mining - what is the question? The objective of data mining is to find regularities, or interesting patterns in large data sets, such as business transactions. More recently, there has been great interest in extending this work to structured data, such as graphs. The domain could be a database of molecular graphs, or the web graph, and the question could be to find subgraphs which occur frequently in the data. Algorithms usually list frequent subgraphs or other patterns. There are many different formulations of this problem. At this stage of the development of the field, it appears to be of some interest to put together a general picture of the different variants. In this talk we present an attempt towards this direction.
3175 en Transductive Rademacher complexities for learning over a graph Recent investigations indicate the use of a probabilistic ”learning” perspective of tasks defined on a single graph, as opposed to the traditional algorithmical ”computational” point of view. This note discusses the use of Rademacher complexities in this setting, and illustrates the use of Kruskal’s algorithm for transductive inference based on a nearest neighbor rule.
3176 en Strings, graphs, invariants Strings play an important role in various sciences, from computer science, linguistics, social sciences, to various natural sciences, including bioinformatics. Strings, words, or finite sequences are mainly studied in formal language theory and form the basis of logic, mathematics and theoretical computer science. Although strings have a simple linear structure, we may associate a number of invariants to them in particular, via various graphs. This non-technical talk will explain some of these features and survey some of the recent work of the present authors. 123
3177 en On graphical representation of proteins We will review a selection of graphical representations of proteins and explore their mathematical properties. In particular, we will consider highly condensed representations of proteins by ”magic circle”, representation by star-like graphs and spectral-like representations and will consider calculations of some of acompanying invariants. Finally we will outline graphical approaches to protein alignment.
3178 en Graph complexity for structure and learning The talk will consider ways of bounding the complexity of a graph as measured by the number of partitions satisfying certain properties. The approach adopted uses Vapnik Chervonenkis dimension techniques. An example of such a bound was given by Kleinberg et al (2004) with an application to network failure detection. We describe a new bound in the same vein that depends on the eigenvalues of the graph Laplacian. We show an application of the result to transductive learning of a graph labelling from examples.
3179 en Semidefinite ranking on graphs We consider the problem of ranking the vertices of an undirected graph given some preference relation. This ranking on graphs problem has been tackled before using spectral relaxations in [1]. Their approach is strongly related to the spectral relaxation made in spectral clustering algorithms. One problem with spectral relaxations that has been found in clustering is that even on simple toy graphs the spectral solution can be arbitrarily far from the optimal one [2]. It has recently been shown that semidefinite relaxations offer in many cases better solutions than spectral ones for clustering [3] and transductive classification [4]. We therefore investigate semidefinite relaxations of ranking on graphs.
3180 en Random walk graph kernels and rational kernels Random walk graph kernels (Gartner et al., 2003 [5]; Borgwardt et al., 2005 [1]) count matching random walks, and are defined using the tensor product graph. Loosely speaking, rational kernels (Cortes et al., 2004, 2003, 2002 [4, 3, 2]) use the weight assigned by a transducer to define a kernel. The kernel is shown to be positive semi-definite when the transducer can be written as a composition of two identical transducers. In our talk we will establish explicit connections between random walk graph kernels and rational kernels. More concretely, we show that composition of transducers is analogous to computing product graphs, and that rational kernels on weighted transducers may be viewed as generalizations of random walk kernels to weighted automata. In order to make these connections explicit we adapt slightly non-standard notation for weighted transducers, extensively using matrices and tensors wherever possible. We prove that under certain conditions rational kernels are positive semi-definite. Our proof only uses basic linear algebra and is simpler than the one presented in Cortes et al., 2004[4].
3182 en Extracting Semantic Relations from Query Logs In this paper we study a large query log of more than twenty million queries with the goal of extracting the semantic relations that are implicitly captured in the actions of users submitting queries and clicking answers. Previous query log analyses were mostly done with just the queries and not the actions that followed after them. We first propose a novel way to represent queries in a vector space based on a graph derived from the query-click bipartite graph. We then analyze the graph produced by our query log, showing that it is less sparse than previous results suggested, and that almost all the measures of these graphs follow power laws, shedding some light on the searching user behavior as well as on the distribution of topics that people want in the Web. The representation we introduce allows to infer interesting semantic relationships between queries. Second, we provide an experimental analysis on the quality of these relations, showing that most of them are relevant. Finally we sketch an application that detects multitopical URLs.
3183 en Multiscale Topic Tomography Modeling the evolution of topics with time is of great value in automatic summarization and analysis of large document collections. In this work, we propose a new probabilistic graphical model to address this issue. The new model, which we call the Multiscale Topic Tomography Model (MTTM), employs non-homogeneous Poisson processes to model generation of word-counts. The evolution of topics is modeled through a multi-scale analysis using Haar wavelets. One of the new features of the model is its modeling the evolution of topics at various time-scales of resolution, allowing the user to zoom in and out of the time-scales. Our experiments on Science data using the new model uncovers some interesting patterns in topics. The new model is also comparable to LDA in predicting unseen data as demonstrated by our perplexity experiments.
3184 en A Concept-based Model for Enhancing Text Categorization Most of text categorization techniques are based on word and/or phrase analysis of the text. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes more to the meaning of its sentences than the other term. Thus, the underlying model should indicate terms that capture the semantics of text. In this case, the model can capture terms that present the concepts of the sentence, which leads to discover the topic of the document. A new concept-based model that analyzes terms on the sentence and document levels rather than the traditional analysis of document only is introduced. The concept-based model can effectively discriminate between non-important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed model consists of concept-based statistical analyzer, conceptual ontological graph representation, and concept extractor. The term which contributes to the sentence semantics is assigned two different weights by the concept-based statistical analyzer and the conceptual ontological graph representation. These two weights are combined into a new weight. The concepts that have maximum combined weights are selected by the concept extractor. A set of experiments using the proposed concept-based model on different datasets in text categorization is conducted. The experiments demonstrate the comparison between traditional weighting and the concept-based weighting obtained by the combined approach of the concept-based statistical analyzer and the conceptual ontological graph. The evaluation of results is relied on two quality measures, the Macro-averaged F1 and the Error rate. These quality measures are improved when the newly developed concept-based model is used to enhance the quality of the text categorization
3185 en Expertise modeling for matching papers with reviewers An essential part of an expert-finding task, such as matching reviewers to submitted papers, is the ability to model the expertise of a person based on documents. We evaluate several measures of the association between a document to be reviewed and an author, represented by their previous papers. We compare language-model-based approaches with a novel topic model, Author-Persona-Topic (APT). In this model, each author can write under one or more "personas," which are represented as independent distributions over hidden topics. Examples of previous papers written by prospective reviewers are gathered from the Rexa database, which extracts and disambiguates author mentions from documents gathered from the web. We evaluate the models using a reviewer matching task based on human relevance judgments determining how well the expertise of proposed reviewers matches a submission. We find that the APT topic model outperforms the other models.
3186 en SCAN: A Structural Clustering Algorithm for Networks Network clustering (or graph partitioning) is an important task for the discovery of underlying structures in networks. Many algorithms find clusters by maximizing the number of intra-cluster edges. While such algorithms find useful and interesting structures, they tend to fail to identify and isolate two kinds of vertices that play special roles&#160;- vertices that bridge clusters (hubs) and vertices that are marginally connected to clusters (outliers). Identifying hubs is useful for applications such as viral marketing and epidemiology since hubs are responsible for spreading ideas or disease. In contrast, outliers have little or no influence, and may be isolated as noise in the data. In this paper, we proposed a novel algorithm called SCAN (Structural Clustering Algorithm for Networks), which detects clusters, hubs and outliers in networks. It clusters vertices based on a structural similarity measure. The algorithm is fast and efficient, visiting each vertex only once. An empirical evaluation of the method using both synthetic and real datasets demonstrates superior performance over other methods such as the modularity-based algorithms.
3187 en Development of NeuroElectroMagnetic Ontologies (NEMO): A Framework for Mining Brain Wave Ontologies Event-related potentials (ERP) are brain electrophysiological patterns created by averaging electroencephalographic (EEG) data, time-locking to events of interest (e.g., stimulus or response onset). In this paper, we propose a generic framework for mining and developing domain ontologies and apply it to mine brainwave (ERP) ontologies. The concepts and relationships in ERP ontologies can be mined according to the following steps: pattern decomposition, extraction of summary metrics for concept candidates, hierarchical clustering of patterns for classes and class taxonomies, and clustering-based classification and association rules mining for relationships (axioms) of concepts. We have applied this process to several dense-array (128-channel) ERP datasets. Results suggest good correspondence between mined concepts and rules, on the one hand, and patterns and rules that were independently formulated by domain experts, on the other. Data mining results also suggest ways in which expert-defined rules might be refined to improve ontology representation and classification results. The next goal of our ERP ontology mining framework is to address some long-standing challenges in conducting large-scale comparison and integration of results across ERP paradigms and laboratories. In a more general context, this work illustrates the promise of an interdisciplinary research program, which combines data mining, neuroinformatics and ontology engineering to address real-world problems.
3188 en Exploiting Duality in Summarization with Deterministic Guarantees Summarization is an important task in data mining. A major challenge over the past years has been the efficient construction of fixed-space synopses that provide a deterministic quality guarantee, often expressed in terms of a maximum-error metric. Histograms and several hierarchical techniques have been proposed for this problem. However, their time and/or space complexities remain impractically high and depend not only on the data set size n, but also on the space budget B. These handicaps stem from a requirement to tabulate all allocations of synopsis space to different regions of the data. In this paper we develop an alternative methodology that dispels these deficiencies, thanks to a fruitful application of the solution to the dual problem: given a maximum allowed error, determine the minimum-space synopsis that achieves it.&#160;These complexity advantages offer both a spaceefficiency and a scalability that previous approaches lacked. We verify the benefits of our approach in practice by experimentation.
3189 en Webpage Understanding: an Integrated Approach Recent work has shown the effectiveness of leveraging layout and tag-tree structure for segmenting webpages and labeling HTML elements. However, how to effectively segment and label the text contents inside HTML elements is still an open problem. Since many text contents on a webpage are often text fragments and not strictly grammatical, traditional natural language processing techniques, that typically expect grammatical sentences, are no longer directly applicable. In this paper, we examine how to use layout and tag-tree structure in a principled way to help understand text contents on webpages. We propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model. In this model, semantic labels of page structure can be leveraged to help text content understanding, and semantic labels of the text phrases can be used in page structure understanding tasks such as data record detection. Thus, integration of both page structure and text content understanding leads to an integrated solution of webpage understanding. Experimental results on research homepage extraction show the feasibility and promise of our approach.
3190 en Knowledge Discovery of Multiple-topic Document using Parametric Mixture Model with Dirichlet Prior Documents, such as those seen onWikipedia and Folksonomy, have tended to be assigned with multiple topics as a meta-data. Therefore, it is more and more important to analyze a relationship between a document and topics assigned to the document. In this paper, we proposed a novel probabilistic generative model of documents with multiple topics as a meta-data. By focusing on modeling the generation process of a document with multiple topics, we can extract specific properties of documents with multiple topics. Proposed model is an expansion of an existing probabilistic generative model: Parametric Mixture Model (PMM). PMM models documents with multiple topics by mixing model parameters of each single topic. Since, however, PMM assigns the same mixture ratio to each single topic, PMM cannot take into account the bias of each topic within a document. To deal with this problem, we propose a model that considers Dirichlet distribution as a prior distribution of the mixture ratio. We adopt Variational Bayes Method to infer the bias of each topic within a document. We evaluate the proposed model and PMM using MEDLINE corpus. The results of F-measure, Precision and Recall show that the proposed model is more effective than PMM on multiple-topic classification. Moreover, we indicate the potential of the proposed model that extracts topics and document-specific keywords using information about the assigned topics.
3191 en Tracking Multiple Topics for Finding Interesting Articles We introduce multiple topic tracking (MTT) for iScore to better recommend news articles for users with multiple interests and to address changes in user interests over time. As an extension of the basic Rocchio algorithm, traditional topic detection and tracking, and single-pass clustering, MTT maintains multiple interest profiles to identify interesting articles for a specific user given user-feedback. Focusing on only interesting topics enables iScore to discard useless profiles to address changes in user interests and to achieve a balance between resource consumption and classification accuracy. Also by relating a topic’s interestingness to an article’s interestingness, iScore is able to achieve higher quality results than traditional methods such as the Rocchio algorithm. We identify several operating parameters that work well for MTT. Using the same parameters, we show that MTT alone yields high quality results for recommending interesting articles from several corpora. The inclusion of MTT improves iScore’s performance by 9% in recommending news articles from the Yahoo! News RSS feeds and the TREC11 adaptive filter article collection. And through a small user study, we show that iScore can still perform well when only provided with little user feedback.
3192 en Content-based Document Routing and Index Partitioning for Scalable Similarity-based Searches in a Large Corpus We present a document routing and index partitioning scheme for scalable similarity-based search of documents in a large corpus. We consider the case when similarity-based search is performed by finding documents that have features in common with the query document. While it is possible to store all the features of all the documents in one index, this suffers from obvious scalability problems. Our approach is to partition the feature index into multiple smaller partitions that can be hosted on separate servers, enabling scalable and parallel search execution. When a document is ingested into the repository, a small number of partitions are chosen to store the features of the document. To perform similarity-based search, also, only a small number of partitions are queried. Our approach is stateless and incremental. The decision as to which partitions the features of the document should be routed to (for storing at ingestion time, and for similarity based search at query time) is solely based on the features of the document. Our approach scales very well. We show that executing similarity-based searches over such a partitioned search space has minimal impact on the precision and recall of search results, even though every search consults less than 3% of the total number of partitions.
3193 en Mining Favorable Facets The importance of dominance and skyline analysis has been well recognized in multi-criteria decision making applications. Most previous studies assume a fixed order on the attributes. In practice, different customers may have different preferences on nominal attributes. In this paper, we identify an interesting data mining problem, finding favorable facets, which has not been studied before. Given a set of points in a multidimensional space, for a specific target point p we want to discover with respect to which combinations of orders (e.g., customer preferences) on the nominal attributes p is not dominated by any other points. Such combinations are called the favorable facets of p. We consider both the effectiveness and the efficiency of the mining. A given point may have many favorable facets. We propose the notion of minimal disqualifying condition (MDC) which is effective in summarizing favorable facets. We develop efficient algorithms for favorable facet mining for different application scenarios. The first method computes favorable facets on the y. The second method pre-computes all minimal disqualifying conditions so that the favorable facets can be looked up in constant time. An extensive performance study using both synthetic and real data sets is reported to verify their effectiveness and efficiency.
3194 en Weighting versus Pruning in Rule Validation for Detecting Network and Host Anomalies For intrusion detection, the LERAD algorithm learns a succinct set of comprehensible rules for&#160;detecting anomalies,&nbsp;which could be novel attacks. LERAD validates the learned rules on a separate held-out validation set and removes rules that cause false alarms. However, removing rules with possible high coverage can lead to missed detections. We propose to retain these rules and associate weights to them. We present three weighting schemes and our empirical results indicate that, for LERAD, rule weighting can detect more attacks than pruning with minimal computational overhead.
3195 en Cost-effective Outbreak Detection in Networks Given a water distribution network, where should we place sensors to quickly detect contaminants? Or, which blogs should we read to avoid missing important stories? These seemingly different problems share common structure: Outbreak detection can be modeled as selecting nodes (sensor locations, blogs) in a network, in order to detect the spreading of a virus or information as quickly as possible. We present a general methodology for near optimal sensor placement in these and related problems. We demonstrate that many realistic outbreak detection objectives (e.g., detection likelihood, population affected) exhibit the property of “submodularity”. We exploit submodularity to develop an efficient algorithm that scales to large problems, achieving near optimal placements, while being 700 times faster than a simple greedy algorithm. We also derive online bounds on the quality of the placements obtained by any algorithm. Our algorithms and bounds also handle cases where nodes (sensor locations, blogs) have different costs. We evaluate our approach on several large real-world problems, including a model of a water distribution network from the EPA, and real blog data. The obtained sensor placements are provably near optimal, providing a constant fraction of the optimal solution. We show that the approach scales, achieving speedups and savings in storage of several orders of magnitude. We also show how the approach leads to deeper insights in both applications, answering multicriteria trade-off, cost-sensitivity and generalization questions.
3196 en Joint Optimization of Wrapper Generation and Template Detection Many websites have large collections of pages generated dynamically from an underlying structured source like a database. The data of a category are typically encoded into similar pages by a common script or template. In recent years, some value-added services, such as comparison shopping and vertical search in a specific domain, have motivated the research of extraction technologies with high accuracy. Almost all previous works assume that input pages of a wrapper induction system conform to a common template and they can be easily identified in terms of a common schema of URL. However, we observed that it is hard to distinguish different templates using dynamic URLs today. Moreover, since extraction accuracy heavily depends on how consistent input pages are, we argue that it is risky to determine whether pages share a common template solely based on URLs. Instead, we propose a new approach that utilizes similarity between pages to detect templates. Our approach separates pages with notable inner differences and then generates wrappers, respectively. Experimental results show that our proposed approach is feasible and effective for improving extraction accuracy.
3197 en Detecting Anomalous Records in Categorical Datasets We consider the problem of detecting anomalies in high arity categorical datasets. In most applications, anomalies are defined as data points that are ’abnormal’. Quite often we have access to data which consists mostly of normal records, along with a small percentage of unlabelled anomalous records. We are interested in the problem of unsupervised anomaly detection, where we use the unlabelled data for training, and detect records that do not follow the definition of normality. A standard approach is to create a model of normal data, and compare test records against it. A probabilistic approach builds a likelihood model from the training data. Records are tested for anomalousness based on the complete record likelihood given the probability model. For categorical attributes, bayes nets give a standard representation of the likelihood. While this approach is good at finding outliers in the dataset, it often tends to detect records with attribute values that are rare. Sometimes, just detecting rare values of an attribute is not desired and such outliers are not considered as anomalies in that context. We present an alternative definition of anomalies, and propose an approach of comparing against marginal distributions of attribute subsets. We show that this is a more meaningful way of detecting anomalies, and has a better performance over semi-synthetic as well as real world datasets.
3198 en Constraint-Driven Clustering Clustering methods can be either data-driven or need-driven. Data-driven methods intend to discover the true structure of the underlying data while need-driven methods aims at organizing the true structure to meet certain application requirements. Thus, need-driven (e.g. constrained) clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks, privacy preservation, and market segmentation. However, the existing methods of constrained clustering require users to provide the number of clusters, which is often unknown in advance, but has a crucial impact on the clustering result. In this paper, we argue that a more natural way to generate actionable clusters is to let the application-specific constraints decide the number of clusters. For this purpose, we introduce a novel cluster model, Constraint-Driven Clustering (CDC), which finds an a priori unspecified number of compact clusters that satisfy all user-provided constraints. Two general types of constraints are considered, i.e. minimum significance constraints and minimum variance constraints, as well as combinations of these two types. We prove the NP-hardness of the CDC problem with different constraints. We propose a novel dynamic data structure, the CD-Tree, which organizes data points in leaf nodes such that each leaf node approximately satisfies the CDC constraints and minimizes the objective function. Based on CD-Trees, we develop an efficient algorithm to solve the new clustering problem. Our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm.
3199 en A Spectral Clustering Approach to Optimally Combining Numerical Vectors with a Modular Network We address the issue of clustering numerical vectors with a network. The problem setting is basically equivalent to constrained clustering by Wagstaff and Cardie [20] and semisupervised clustering by Basu et al. [2], but our focus is more on the optimal combination of two heterogeneous data sources. An application of this setting is web pages which can be numerically vectorized by their contents, e.g. term frequencies, and which are hyperlinked to each other, showing a network. Another typical application is genes whose behavior can be numerically measured and a gene network can be given from another data source. We first define a new graph clustering measure which we call normalized network modularity, by balancing the cluster size of the original modularity. We then propose a new clustering method which integrates the cost of clustering numerical vectors with the cost of maximizing the normalized network modularity into a spectral relaxation problem. Our learning algorithm is based on spectral clustering which makes our issue an eigenvalue problem and uses k-means for final cluster assignments. A significant advantage of our method is that we can optimize the weight parameter for balancing the two costs from the given data by choosing the minimum total cost. We evaluated the performance of our proposed method using a variety of datasets including synthetic data as well as real-world data from molecular biology. Experimental results showed that our method is effective enough to have good results for clustering by numerical vectors and a network.
3200 en Dynamic hybrid clustering of bioinformatics by incorporating text mining and citation analysis To unravel the concept structure and dynamics of the bioinformatics field, we analyze a set of 7401 publications from the Web of Science and MEDLINE databases, publication years 1981–2004. For delineating this complex, interdisciplinary field, a novel bibliometric retrieval strategy is used. Given that the performance of unsupervised clustering and classification of scientific publications is significantly improved by deeply merging textual contents with the structure of the citation graph, we proceed with a hybrid clustering method based on Fisher’s inverse chi-square. The optimal number of clusters is determined by a compound semiautomatic strategy comprising a combination of&#160; istancebased and stability-based methods. We also investigate the relationship between number of Latent Semantic Indexing factors, number of clusters, and clustering performance. The HITS and PageRank algorithms are used to determine representative publications in each cluster. Next, we develop a methodology for dynamic hybrid clustering of evolving bibliographic data sets. The same clustering methodology is applied to consecutive periods defined by time windows on the set, and in a subsequent phase chains are formed by matching and tracking clusters through time. Term networks for the eleven resulting cluster chains present the cognitive structure of the field. Finally, we provide a view on how much attention the bioinformatics community has devoted to the different subfields through time.
3201 en Enhancing Semi-Supervised Clustering: A Feature Projection Perspective Semi-supervised clustering employs limited supervision in the form of labeled instances or pairwise instance constraints to aid unsupervised clustering and often significantly improves the clustering performance. Despite the vast amount of expert knowledge spent on this problem, most existing work is not designed for handling high-dimensional sparse data. This paper thus fills this crucial void by developing a Semi-supervised Clustering method based on spheRical KmEans via fEature projectioN (SCREEN). Specifically, we formulate the problem of constraint-guided feature projection, which can be nicely integrated with semi-supervised clustering algorithms and has the ability to effectively reduce data dimension. Indeed, our experimental results on several real-world data sets show that the SCREEN method can effectively deal with high-dimensional data and provides an appealing clustering performance.
3203 en BoostCluster: Boosting Clustering by Pairwise Constraints Data clustering is an important task in many disciplines. A large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints. However, these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints. We present a boosting framework for data clustering, termed as BoostCluster, that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints. The key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised. The proposed framework addresses this problem by dynamically generating new data representations at each iteration that are, on the one hand, adapted to the clustering results at previous iterations by the given algorithm, and on the other hand consistent with the given side information. Our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering&#160; algorithms (Kmeans, partitional SingleLink, spectral clustering), and its performance is comparable to the state-of-the-art algorithms for data clustering with side information.
3204 en Assisting Translators in Indirect Lexical Transfer We present the design and evaluation of a translator’s amenuensis that uses comparable corpora to propose and rank non-literal solutions to the translation of expressions from the general lexicon. Using distributional similarity and bilingual dictionaries, the method outperforms established techniques for extracting translation equivalents from parallel corpora.
3205 en Nonlinear Adaptive Distance Metric Learning for Clustering A good distance metric is crucial for many data mining tasks. To learn a metric in the unsupervised setting, most metric learning algorithms project observed data to a lowdimensional manifold, where geometric relationships such as pairwise distances are preserved. It can be extended to the nonlinear case by applying the kernel trick, which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space. In this paper, we propose a novel unsupervised Nonlinear AdaptiveMetric Learning algorithm, called NAML, which performs clustering and distance metric learning simultaneously. NAML first maps the data to a high-dimensional space through a kernel function; then applies a linear projection to find a low-dimensional manifold where the separability of the data is maximized; and finally performs clustering in the low-dimensional space. The performance of NAML depends on the selection of the kernel function and the projection. We show that the joint kernel learning, dimensionality reduction, and clustering can be formulated as a trace maximization problem, which can be solved via an iterative procedure in the EM framework. Experimental results demonstrated the efficacy of the proposed algorithm.
3206 en A Framework for Simultaneous Co-clustering and Learning from Complex Data For difficult classification or regression problems, practitioners often segment the data into relatively homogenous groups and then build a model for each group. This two-step procedure usually results in simpler, more interpretable and actionable models without any loss in accuracy. We consider problems such as predicting customer behavior across products, where the independent variables can be naturally partitioned into two groups. A pivoting operation can now result in the dependent variable showing up as entries in a “customer by product” data matrix. We present a modelbased co-clustering (meta)-algorithm that interleaves clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models. This algorithm provably converges to a local minimum of a suitable cost function. The framework not only generalizes co-clustering and collaborative filtering to model-based coclustering, but can also be viewed as simultaneous co-segmentation and classification or regression, which is better than independently clustering the data first and then building models. Moreover, it applies to a wide range of bi-modal or multimodal data, and can be easily specialized to address classification and regression problems. We demonstrate the effectiveness of our approach on both these problems through experimentation on real and synthetic data.
3207 en Joint Cluster Analysis of Attribute and Relationship Data Without Priori Specification of the Number of Clusters In many applications, attribute and relationship data are available, carrying complementary information about real world entities. In such cases, a joint analysis of both types of data can yield more accurate results than classical clustering algorithms that either use only attribute data or only relationship (graph) data. The Connected k-Center (CkC) has been proposed as the first joint cluster analysis model to discover k clusters which are cohesive on both attribute and relationship data. However, it is well-known that prior knowledge on the number of clusters is often unavailable in applications such as community identification and hotspot analysis. In this paper, we introduce and formalize the problem of discovering an a-priori unspecified number of clusters in the context of joint cluster analysis of attribute and relationship data, called Connected X Clusters (CXC) problem. True clusters are assumed to be compact and distinctive from their neighboring clusters in terms of attribute data and internally connected in terms of relationship data. Different from classical attribute-based clustering methods, the neighborhood of clusters is not defined in terms of attribute data but in terms of relationship data. To efficiently solve the CXC problem, we present JointClust, an algorithm which adopts a dynamic two-phase approach. In the first phase, we find so called cluster atoms. We provide a probability analysis for this phase, which gives us a probabilistic guarantee, that each true cluster is represented by at least one of the initial cluster atoms. In the second phase, these cluster atoms are merged in a bottom-up manner resulting in a dendrogram. The final clustering is determined by our objective function. Our experimental evaluation on several real datasets demonstrates that JointClust indeed discovers meaningful and accurate clusterings without requiring the user to specify the number of clusters.
3209 en Introduction to the Workshop 
3210 en Foundations of Statistical Learning Theory : Empirical Infe-rence in high-dimention spaces 
3211 en Information Theo-retic and Alge-braic Methods for Network Anomaly Detection The tutorial will discuss two central issues: (i) Information Theoretic principles and algorithms for extracting predictive statistics in distributed networks and (ii) algebraic and spectral methods for network anomaly detection. The first part will deal with the concept of predictive information - the mutual information between the past and future of a process, its sub-extensive properties, and algorithms for estimating it from data.We will argue that the information theoretic predictability quantifies the complexity of a process and provides effective ways for detecting anomalies and surprises in the process. Using the Information Bottleneck algorithms one can extract approximate sufficient statistics from the past to the future of the process and use them as anomaly detectors on multiple time scales. In the second part we will discuss ways for analyzing network activity using spectral methods (distributed PCA and network Laplacian analysis) for identifying regular temporal patterns of connected network components. By combining the two approaches, we will suggest new techniques for network anomaly detectors for security.
3212 en Data stream management and mining The course provides an introduction to the data stream management and mining field. The following points are treated: (1) applications which motivated these new developments (telecommunications, computer networks, stock market, security, ...), (2) new concepts related to data streams (structure of a stream, timestamps, time windows, ...), (3) main features of data stream management systems, (4) adaptations of data mining algorithms to the case of streams, (5) solutions to summarize data streams.
3213 en Mining Massive Data Sets Today, the amount of data coming from all possible sources is enormous and growing at a fast pace due, in large part, to the ubiquitous Web and its increasing presence in our everyday life; but also to emails, cell phones, credit cards, retail, finance ... These data serve all sorts of functions : from query and search, to extracting information, providing services as well as managing security. Many fields are involved : statistics, data mining, text mining, data streams, search, social networks ... There is no lack of sophisticated techniques produced by academic activity, where challenges mostly deal with novelty, accuracy, and scalability of algorithms. However, in real-world applications, challenges are quite different : scalability (usually one or two orders of magnitude more than in academic publications), ease-of-use and capability to integrate efficient techniques into working systems in a transparent way, while always producing value for the customer. Real-world solutions are complex and usually need to integrate many technical components, from the various fields mentioned before: it thus becomes important to assess how these fields can complement one another. In the first part of the talk, I will present the challenges of real-world data mining applications. I will introduce the general Statistical Learning Theory framework and discuss some of the technical issues involved (large dimension data sets, missing data, outliers, non-i.i.d. structured data, unlabelled data ...) In the second part, I will show, taking examples from the implementation in KXEN and applications developed, how a theoretical framework (Structural Risk Minimization [1]) can be used to solve some of the challenges met in the real-world. I will finally describe some open practical issues which will require further theoretical investigation.
3214 en User logs processing using machine learning techniques User modeling is progressively becoming an important and generic component of many applications and services. The mains reasons that explain this phenomenon are the tasks increasing complexity and the wide variety of users. \\ nformation systems, hypermedia, websites, and application software are becoming more and more complex, hence difficult to use efficiently. Also, the amount of on-line information available to a user through Internet is huge and is still increasing everyday so that recovering information is becoming harder and harder. Finally, together with the huge development of Internet, more and more on-line commercial websites and services are proposed to Internet users. The aim and interest of user modeling consists in these situations in helping the user to efficiently use the systems he is offered and to retrieve the information he is looking for by filtering the information according to his will and needs. Furthermore, while many software, hypermedia, websites and services are potentially used by a variety of users, these systems have been traditionally developed in a “one size fits all” manner. Consequently, they are often not adapted to most of the users, with various knowledge, preferences, and needs. In this context user modeling allows personalizing such systems, their content or presentation, in order to fit the individual.
3215 en Learning using Many Examples The statistical learning theory suggests to choose large capacity models that barely avoid over-fitting the training data. In that perspective, all datasets are small. Things become more complicated when one considers the computational cost of processing large datasets. Computationally challenging training sets appear when one want to emulate intelligence: biological brains learn quite efficiently from the continuous streams of perceptual data generated by our six senses, using limited amounts of sugar as a source of power. Computationally challenging training sets also appear when one want to analyze the masses of data that describe the life of our computerized society. The more data we understand, the more we enjoy competitive advantages. – The first part of the tutorial clarifies the relation between the statistical efficiency, the design of learning algorithms and their computational cost. – The second part makes a detailed exploration of specific learning algorithms and of their implementation, with both simple and complex examples. – The third part considers algorithms that learn with a single pass over the data. Certain algorithms have optimal properties but are often too costly. Workarounds are discussed. – Finally, the fourth part shows how active example selection provides greater speed and reduces the feedback pressure that constrain parallel implementations.
3216 en Large-Scale Semi-Supervised Learning Labeling data is expensive, whilst unlabeled data is often abundant and cheap to collect. Semi-supervised learning algorithms that can use both types of data can perform significantly better than supervised algorithms that use labeled data alone. However, for such gains to be observed, the amount of unlabeled data trained on should be relatively large. Therefore, making semi-supervised algorithms scalable is paramount. In this work we discuss several recent techniques for improving the scaling ability of these algorithms.
3217 en Summarizing Data Stream's History This article presents data mining algorithms whose goal is to build summaries designed to summarize the whole history of one or several data streams so that selected parts of that history may be studied later.
3218 en Emergent patterns in large social systems Although they are composed of hundreds, thousands, or even millions of heterogeneous individuals, large-scale social systems manifest consistent macroscopic patterns in their network structure and interaction dynamics. An understanding of these patterns can not only shed light on certain aspects of individual behavior, but also provide insight into matters such as identification of key communities or individuals, the relation between social structure and group performance, the dynamics of information propagation, the resilience of networks to attack, and many others.
3219 en Evolving Networks Most real networks often evolve through time: changes of topology can occurnif some nodes and/or edges appear and/or disappear, and even if the topologynstays static, the types or weights of node and edges can also change. Mobilendevices with wireless capabilities (mobile phones, laptops, etc.) are a typicalnexample of evolving networks where nodes or users are spread around in thenenvironment and connections between users can only occur if they are near eachnothers. This who-is-near-who network is going to evolve every time users movenand communication services such as the spread of any information will deeplynrely on the mobility and on the characteristics of the underlying network.nWe will present here some results focusing on three key problems - measuring,ndescribing and modeling evolving networks - using a typical evolving networknwhere 41 sensors had been distributed to participants of a conference, whichnwhere asked to keep the sensor at all time. Each sensor was able to detect andnrecord the presence of others sensors within their radio range which gives someninformation on the proximity of participants
3220 en Diffusion and Cascading Behaviour in Networks Diffusion is a process by which information, viruses, ideas and new behaviorn spread over the network. For example, adoption of a new technology begins onn a small scale with a few “early adopters”, then more and more people adopt itn as they observe friends and neighbors using it. Eventually the adoption of then technology may spread through the social network as an epidemic “infecting”n most of the network. As it spreads over the network it creates a cascade. Cascadesn have been studied for many years by sociologists concerned with the diffusion ofn innovation; more recently, researchers have investigated cascades for selectingn trendsetters for viral marketing, finding inoculation targets in epidemiology, andn explaining trends in blogspace.
3221 en Mining Networks through Visual Analytics: Analysts are faced with massive collections gathering documents, events and actors from which they try to make sense, searching data to locate patterns and discover evidence. Visual and interactive exploration of data has now established as a fruitful strategy to tackle the problem posed by this abundance of information. The Visual Analytics initiative promotes the use of Information Visualization to support analytical reasoning through a sense-making loop based on which the analysis incrementally builds hypotheses.
3222 en Inference and Learning with Networked Data In many applications we would like to draw inferences about entities that are interconnected in complex networks. For example, calls, emails, IM, and web pointers link people into huge social networks. However, traditional statistical and machine learning classification methods assume that entities are independent of each other. I start by discussing various applications of "classification" (scoring) in networked data, from fraud detection to counterterrorism to network-based marketing. I then discuss four characteristics of networked data that allow improvements-- sometimes substantial--over traditional classification: (i) models can take into account "guilt by association," (ii) inference can be performed "collectively," whereby inferences on linked entities mutually reinforce each other, (iii) characteristics of linked entities can be incorporated in models, and (iv) models can incorporate specific identifiers, such as the identities of particular individuals, to improve inference. I present results demonstrating the effectiveness of these techniques.
3223 en Ontologies and Machine Learning We address the problem of constructing light-weight ontology from social network data. As an example we use social network of a mid size research institution obtained based on e-mail communication. The main contribution is an architecture consisting from five major steps that enable transformation of the data from a given e-mail transactions recordings to an ontology estimating the structure of the organization. Once having a set of sparse vectors, we apply an approach to semi-automated ontology construction as implemented in the OntoGen tool. The experiments and illustrative evaluation show that our approach is useful and applicable in real life situations where the goal is to model social structures based on communication records.
3224 en Ontogen Software Demo We address the problem of constructing light-weight ontology from social network data. As an example we use social network of a mid size research institution obtained based on e-mail communication. The main contribution is an architecture consisting from five major steps that enable transformation of the data from a given e-mail transactions recordings to an ontology estimating the structure of the organization. Once having a set of sparse vectors, we apply an approach to semi-automated ontology construction as implemented in the OntoGen tool. The experiments and illustrative evaluation show that our approach is useful and applicable in real life situations where the goal is to model social structures based on communication records.
3226 en EEG/fMRI correlation analysis. A data and model driven approach 
3227 en Of bursts and blobs - or: How to link EEG, neuronal spikes and fMRI 
3228 en From functional elements to networks in fMRI 
3229 en Multimodal Imaging: EEG-fMRI integration 
3230 en New BCI approaches: Selective Attention to Auditory and Tactile Stimulus Streams 
3231 en The Machine Learning Approach to Brain-Computer Interfacing - Part 1 
3232 en Symbolic Dynamics of Neurophysiological Data 
3233 en Attention improves object representations in cortical activities 
3234 en Matching pursuit and unification in EEG analysis 
3235 en Amplitude and phase patterns in encephalographic signals - multivariate approaches for movement-related brain activity 
3236 en EEG Coupling, Granger Causality and Multivariate Autoregressive Models 
3237 en Multimodal Imaging: MEG-NIRS integration 
3238 en Exploiting temporal delays in interpreting EEG/MEG data in terms of brain connectivity 
3239 en The Machine Learning Approach to Brain-Computer Interfacing - Part 2 
3241 en ECOLEAD, visions and goals 
3242 en AI methods and VE 
3244 en Ontologies, semantic web and VE 
3245 en The Long Road from Text to Meaning Computers have given us a new way of thinking about language. Given a large sample of language, or corpus, and computational tools to process it, we can approach language as physicists approach forces and chemists approach chemicals. This approach is noteworthy for missing out what, from a language-user's point of view, is important about a piece of language: its meaning. I shall present this empiricist approach to the study of language and show how, as we develop accurate tools for lemmatisation, part-of-speech tagging and parsing, we move from the raw input -- a character stream -- to an analysis of that stream in increasingly rich terms: words, lemmas, grammatical structures, Fillmore-style frames. Each step on the journey builds on a large corpus accurately analysed at the previous levels. A distributional thesaurus provides generalisations about lexical behaviour which can then feed into an analysis at the ‘frames' level. The talk will be illustrated with work done within the ‘Sketch Engine' tool. For much NLP and linguistic theory, meaning is a given. Thus formal semantics assumes meanings for words, in order to address questions of how they combine, and WSD (word sense disambiguation) typically takes a set of meanings (as found in a dictionary) as a starting point and sets itself the challenge of identifying which meaning applies. But, since the birth of philosophy, meaning has been problematic. In our approach meaning is an eventual output of the research programme, not an input.
3246 en Hidden Topic Markov Models Algorithms such as Latent Dirichlet Allocation (LDA) have achieved significant progress in modeling word document relationships. These algorithms assume each word in the document was generated by a hidden topic and explicitly model the word distribution of each topic as well as the prior distribution over topics in the document. Given these parameters, the topics of all words in the same document are assumed to be independent. In this work, we propose modeling the topics of words in the document as a Markov chain. Specifically, we assume that all words in the same sentence have the same topic, and successive sentences are more likely to have the same topics. Since the topics are hidden, this leads to using the well-known tools of Hidden Markov Models for learning and inference. We show that incorporating this dependency allows us to learn better topics and to disambiguate words that can belong to different topics. Quantitatively, we show that we obtain better perplexity in modeling documents with only a modest increase in learning and inference complexity. //Joint work with Michal Rosen-Zvi and Yair Weiss.//
3247 en Gears and the Mashup Problem Mashups are the most interesting innovation in software development in decades. Unfortunately, the browser's security model did not anticipate this development, so mashups are not safe if there is any confidential information in the page. Since virtually every page has at least some confidential information in it, this is a big problem. Google Gears may lead to the solution. Speaker: Douglas Crockford Douglas Crockford is the world's foremost living authority on JavaScript. He is an architect with Yahoo's Ajax Strike Force. He is the founder of two startups, and was Director of Technology at Lucasfilm Ltd., Director of New Media at Paramount, and a researcher at Atari and SRI.
3248 en The ‘Last Lecture’ of Randy Pausch Almost all of us have childhood dreams: for example, being an astronaut, or making movies or video games for a living. \\ Sadly, most people don't achieve theirs, and I think that's a shame. I had several specific childhood dreams, and I've actually achieved most of them. More importantly, I have found ways, in particular the creation (with Don Marinelli), of CMU's ([[http://etc.cmu.edu/|Entertainment Technology Center]]), of helping many young people actually **achieve** their childhood dreams. This talk will discuss how I achieved my childhood dreams (being in zero gravity, designing theme park rides for Disney, and a few others), and will contain realistic advice on how **you** can live your life so that you can make your childhood dreams come true, too.
3249 en Three Beautiful Quicksorts This talk describes three of the most beautiful pieces of code that I have ever written: three different implementations of Hoare's classic Quicksort algorithm. \\ # The first implementation is a bare-bones function in about a dozen lines of C. \\ # The second implementation starts by instrumenting the first program to measure its run time; a dozen systematic code transformations proceed to make it more and more powerful yet more and more simple, until it finally disappears in a puff of mathematical smoke. It therefore becomes the most beautiful program I never wrote. \\ # The third program is an industrial-strength C library Qsort function that I built with Doug McIlroy. A theme running through all three implementations is the power of elegance and simplicity. \\ (This talk expands my Chapter 3 in Beautiful Code, edited by Oram and Wilson and published by O'Reilly in July, 2007.)
3250 en Topics in Biology - Organismal Biology 
3253 en Evolving Systems One of the important research challenges today is to develop new theoretical methods, algorithms, and implementations of systems with a higher level of flexibility and autonomy, we can say with higher level of intelligence. These systems have to be able to evolve their structure and knowledge on the environment and ultimately – evolve their intelligence. To address the problems of modelling, control, prediction, classification and data processing in a dynamically changing and evolving environment, a system must be able to fully adapt its structure and adjust its parameters, rather than use a pre-trained and a fixed structure. That is, the system must be able to evolve, to self-develop, to self-organize, to self-evaluate and to self-improve. The talk will concentrate on the problems and results the author encountered during last several years of research in this emerging area as well as on the approach to on-line identification of a particular type of fuzzy models – so called Takagi-Sugeno fuzzy models including some applications, in particular to mobile robots, mobile communications, process modelling and control, on-line evolving classification intelligent (inferential) sensors.
3254 en Graphical Models In the last decade probabilistic graphical models -- in particular Bayes networks and Markov networks -- became very popular as tools for structuring uncertain knowledge about a domain of interest and for building knowledge-based systems that allow sound and efficient inferences about this domain. The lecture gives a brief introduction into the core ideas underlying graphical models, starting from their relational counterparts and highlighting the relation between independence and decomposition. Furthermore, the basics of model construction and evidence propagation are discussed, with an emphasis on join/junction tree propagation. A substantial part of the lecture is then devoted to learning graphical models from data, in which quantitative learning (parameter estimation) as well as the more complex qualitative or structural learning (model selection) are studied.
3255 en Data Quality In industry the term „Quality“ used in the context of “Quality Control or Assurance” of products - and later services - has a history of about one hundred years. It is used in an ISO norm as “Suitability for use relative to a given objective of usage”. Looking at “Products” and “Processes” one distinguishes between “Quality of Design” and “Quality of Performance”. “Data Quality” is a term which is used at Statistical Offices and supranational Organizations (OECD, UN NAGroup etc.) for about the same time. It became popular in computer science twenty years ago, when data quality problems related to data warehousing, ETL, data cleansing , data mining and data integration were detected. Data Quality is mostly defined as above, i.e. fitness for use given an objective of data processing on a specific domain. For example, the objective may be web-mining where semi-structured data is to be integrated. Evidently, the term “data quality” has many various facets. Stepwise refining the granularity starting from several data sources to a single value of an attribute (variable) one can differ between multi-sources or data bases, single databases (on the schema or data level), records and values. For instance, on the data level errors, outliers, null-values (missing values), inconsistent (incoherent) values or simply semantic misuse of data are of concern while on the schema level integrity constraints may be violated. All these factors may lead to low data quality.
3256 en Compact and Understandable Descriptions of Mixtures of Bernoulli Distributions Finite mixture models can be used in estimating complex, unknown probability distributions and also in clustering data. The parameters of the models form a complex representation and are not suitable for interpretation purposes as such. In this paper, we present a methodology to describe the finite mixture of multivariate Bernoulli distributions with a compact and understandable description. First, we cluster the data with the mixture model and subsequently extract the maximal frequent itemsets from the cluster-specific data sets. The mixture model is used to model the data set globally and the frequent itemsets model the marginal distributions of the partitioned data locally. We present the results in understandable terms that reflect the domain properties of the data. In our application of analyzing DNA copy number amplifications, the descriptions of amplification patterns are represented in nomenclature used in literature to report amplification patterns and generally used by domain experts in biology and medicine.
3257 en Multiplicative Updates for L1-Regularized Linear and Logistic Regression Multiplicative update rules have proven useful in many areas of machine learning. Simple to implement, guaranteed to converge, they account in part for the widespread popularity of algorithms such as nonnegative matrix factorization and Expectation-Maximization. In this paper, we show how to derive multiplicative updates for problems in L1-regularized linear and logistic regression. For L1–regularized linear regression, the updates are derived by reformulating the required optimization as a problem in nonnegative quadratic programming (NQP). The dual of this problem, itself an instance of NQP, can also be solved using multiplicative updates; moreover, the observed duality gap can be used to bound the error of intermediate solutions. For L1–regularized logistic regression, we derive similar updates using an iteratively reweighted least squares approach. We present illustrative experimental results and describe efficient implementations for large-scale problems of interest (e.g., with tens of thousands of examples and over one million features).
3258 en Learning to align: a statistical approach We present a new machine learning approach to the inverse parametric sequence alignment problem: given as training examples a set of correct pairwise global alignments, find the parameter values that make these alignments optimal.We consider the distribution of the scores of all incorrect alignments, then we search for those parameters for which the score of the given alignments is as far as possible from this mean, measured in number of standard deviations. This normalized distance is called the ‘Z-score’ in statistics. We show that the Z-score is a function of the parameters and can be computed with efficient dynamic programs similar to the Needleman-Wunsch algorithm.We also show that maximizing the Z-score boils down to a simple quadratic program. Experimental results demonstrate the effectiveness of the proposed approach.
3259 en Transductive Reliability Estimation for Kernel Based Classifiers Estimating the reliability of individual classifications is very important in several applications such as medical diagnosis. Recently, the transductive approach to reliability estimation has been proved to be very efficient when used with several machine learning classifiers, such as Naive Bayes and decision trees. However, the efficiency of the transductive approach for state-of-the art kernel-based classifiers was not considered. In this work we deal with this problem and apply the transductive reliability methodology with sparse kernel classifiers, specifically the Support Vector Machine and Relevance Vector Machine. Experiments with medical and bioinformatics datasets demonstrate better performance of the transductive approach for reliability estimation compared to reliability measures obtained directly from the output of the classifiers. Furthermore, we apply the methodology in the problem of reliable diagnostics of the coronary artery disease, outperforming the expert physicians’ standard approach.
3260 en Fast Clustering based on Kernel Density Estimation The Denclue algorithm employs a cluster model based on kernel density estimation. A cluster is defined by a local maximum of the estimated density function. Data points are assigned to clusters by hill climbing, i.e. points going to the same local maximum are put into the same cluster. A disadvantage of Denclue 1.0 is, that the used hill climbing may make unnecessary small steps in the beginning and never converges exactly to the maximum, it just comes close. We introduce a new hill climbing procedure for Gaussian kernels, which adjusts the step size automatically at no extra costs. We prove that the procedure converges exactly towards a local maximum by reducing it to a special case of the expectation maximization algorithm. We show experimentally that the new procedure needs much less iterations and can be accelerated by sampling based methods with sacrificing only a small amount of accuracy.
3261 en Visualising the Cluster Structure of Data Streams The increasing availability of streaming data is a consequence of the continuing advancement of data acquisition technology. Such data provides new challenges to the various data analysis communities. Clustering has long been a fundamental procedure for acquiring knowledge from data, and new tools are emerging that allow the clustering of data streams. However the dynamic, temporal components of streaming data provide extra challenges to the development of stream clustering and associated visualisation techniques. In this work we combine a streaming clustering framework with an extension of a static cluster visualisation method, in order to construct a surface that graphically represents the clustering structure of the data stream. The proposed method, OpticsStream, provides intuitive representations of the clustering structure as well as the manner in which this structure changes through time.
3262 en Relational Topographic Maps We introduce relational variants of neural topographic maps including the self-organizing map and neural gas, which allow clustering and visualization of data given as pairwise similarities or dissimilarities with continuous prototype updates. It is assumed that the (dis-)similarity matrix originates from Euclidean distances, however, the underlying embedding of points is unknown.Batch optimization schemes for topographic map formations are formulated in terms of the given (dis-)similarities and convergence is guaranteed, thus providing a way to transfer batch optimization to relational data.
3263 en A Support Vector Machine Approach to Dutch Part-of-Speech Tagging Part-of-Speech tagging, the assignment of Parts-of-Speech to the words in a given context of use, is a basic technique in many systems that handle natural languages. This paper describes a method for supervised training of a Part-of-Speech tagger using a committee of Support Vector Machines on a large corpus of annotated transcriptions of spoken Dutch. Special attention is paid to the decomposition of the large data set into parts for common, uncommon and unknown words. This does not only solve the space problems caused by the amount of data, it also improves the tagging time. The performance of the resulting tagger in terms of accuracy is 97.54 %, which is quite good, where the speed of the tagger is reasonably good.
3264 en Towards Adaptive Web Mining: Histograms and Contexts in Text Data Clustering We present a novel approach to the growing neural gas (GNG) based clustering of the high-dimensional text data. We enhance our Contextual GNG models (proposed previously to shift the majority of calculations to context-sensitive, local sub-graphs and local sub-spaces and so to reduce computational complexity) by developing a new, histogram-based method for incremental model adaptation and evaluation of its stability.
3265 en Does SVM Really Scale Up to Large Bag of Words Feature Spaces? We are concerned with the problem of learning classification rules in text categorization where many authors presented Support Vector Machines (SVM) as leading classification method. Number of studies, however, repeatedly pointed out that in some situations SVM is outperformed by simpler methods such as naive Bayes or nearest-neighbor rule. In this paper, we aim at developing better understanding of SVM behaviour in typical text categorization problems represented by sparse bag of words feature spaces. We study in details the performance and the number of support vectors when varying the training set size, the number of features and, unlike existing studies, also SVM free parameter C, which is the Lagrange multipliers upper bound in SVM dual. We show that SVM solutions with small C are high performers. However, most training documents are then bounded support vectors sharing a same weight C. Thus, SVM reduce to a nearest mean classifier; this raises an interesting question on SVM merits in sparse bag of words feature spaces. Additionally, SVM suffer from performance deterioration for particular training set size/number of features combinations.
3266 en Incremental Learning with Multiple Classifier Systems Using Correction Filters for Classification Classification is a quite relevant task within data mining area. This task is not trivial and some difficulties can arise depending on the nature of the problem. Multiple classifier systems have been used to construct ensembles of base classifiers in order to solve or alleviate some of those problems. One of the most current problems that is being studied in recent years is how to learn when the datasets are too large or when new information can arrive at any time. In that case, incremental learning is an approach that can be used. Some works have used multiple classifier system to learn in an incremental way and the results are very promising. The aim of this paper is to propose a method for improving the classification (or prediction) accuracy reached by multiple classifier systems in this context.
3267 en Combining Bagging and Random Subspaces to Create Better Ensembles Random forests are one of the best performing methods for constructing ensembles. They derive their strength from two aspects: using random subsamples of the training data (as in bagging) and randomizing the algorithm for learning base-level classifiers (decision trees). The base-level algorithm randomly selects a subset of the features at each step of tree construction and chooses the best among these. We propose to use a combination of concepts used in bagging and random subspaces to achieve a similar effect. The latter randomly select a subset of the features at the start and use a deterministic version of the base-level algorithm (and is thus somewhat similar to the randomized version of the algorithm). The results of our experiments show that the proposed approach has a comparable performance to that of random forests, with the added advantage of being applicable to any base-level algorithm without the need to randomize the latter.
3268 en Two Bagging Algorithms with Coupled Learners to Encourage Diversity In this paper, we present two ensemble learning algorithms which make use of boostrapping and out-of-bag estimation in an attempt to inherit the robustness of bagging to overfitting. As against bagging, with these algorithms learners have visibility on the other learners and cooperate to get diversity, a characteristic that has proved to be an issue of major concern to ensemble models. Experiments are provided using two regression problems obtained from UCI.
3269 en Relational Algebra for Ranked Tables with Similarities: Properties and Implementation The paper presents new developments in an extension of Codd’s relational model of data. The extension consists in equipping domains of attribute values with a similarity relation and adding ranks to rows of a database table. This way, the concept of a table over domains (i.e., relation over a relation scheme) of the classical Codd’s model extends to the concept of a ranked table over domains with similarities. When all similarities are ordinary identity relations and all ranks are set to 1, our extension becomes the ordinary Codd’s model. The main contribution of our paper is twofold. First, we present an outline of a relational algebra for our extension. Second, we deal with implementation issues of our extension. In addition to that, we also comment on related approaches presented in the literature.
3270 en A New Way to Aggregate Preferences: Application to Eurovision Song Contests Voting systems have a great impact on the results of contests or elections. Simple methods are actually used, whereas they do not provide most accurate results. For example, in the Eurovision Song Contest, the winner may not be the most preferred candidate. Condorcet criterion, which consists in preserving most of the individual votes in the final ranking, seems intuitively the most relevant. In this paper, we propose a new ranking method founded on Condorcet voting count principle which minimizes the number of pairwise inversions of the individual preferences. We propose a two-step method: computing the cycles among vote preferences and removing a minimal set of pairwise preferences to erase all the cycles and turn the votes into a partial order as close as possible to a total order. Finally, we evaluate the impact of our ranking procedure on the last 30 Eurovision Song Contests.
3271 en Noise Filtering and Microarray Image Reconstruction Via Chained Fouriers Microarrays allow biologists to determine the gene expressions for tens of thousands of genes simultaneously, however due to biological processes, the resulting microarray slides are permeated with noise. During quantification of the gene expressions, there is a need to remove a gene’s noise or background for purposes of precision. This paper presents a novel technique for such a background removal process. The technique uses a gene’s neighbour regions as representative background pixels and reconstructs the gene region itself such that the region resembles the local background. With use of this new background image, the gene expressions can be calculated more accurately. Experiments are carried out to test the technique against a mainstream and an alternative microarray analysis method. Our process is shown to reduce variability in the final expression results.
3272 en Soft Topographic Map for Clustering and Classification of Bacteria In this work a new method for clustering and building a topographic representation of a bacteria taxonomy is presented. The method is based on the analysis of stable parts of the genome, the so-called “housekeeping genes”. The proposed method generates topographic maps of the bacteria taxonomy, where relations among different type strains can be visually inspected and verified. Two well known DNA alignement algorithms are applied to the genomic sequences. Topographic maps are optimized to represent the similarity among the sequences according to their evolutionary distances. The experimental analysis is carried out on 147 type strains of the Gammaprotebacteria class by means of the 16S rRNA housekeeping gene. Complete sequences of the gene have been retrieved from the NCBI public database. In the experimental tests the maps show clusters of homologous type strains and presents some singular cases potentially due to incorrect classification or erroneous annotations in the database.
3273 en Making Time: Pseudo Time-Series for the Temporal Analysis of Cross Section Data The progression of many biological and medical processes such as disease and development are inherently temporal in nature. However many datasets associated with such processes are from cross-section studies, meaning they provide a snapshot of a particular process across a population, but do not actually contain any temporal information. In this paper we address this by constructing temporal orderings of cross-section data samples using minimum spanning tree methods for weighted graphs. We call these reconstructed orderings pseudo time-series and incorporate them into temporal models such as dynamic Bayesian networks. Results from our preliminary study show that including pseudo temporal information improves classification performance. We conclude by outlining future directions for this research, including considering different methods for time-series construction and other temporal modelling approaches.
3274 en Recurrent Predictive Models for Sequence Segmentation Many sequential data sets have a segmental structure, and similar types of segments occur repeatedly. We consider sequences where the underlying phenomenon of interest is governed by a small set of models that change over time. Potential examples of such data are environmental, genomic, and economic sequences. Given a target sequence and a (possibly multivariate) sequence of observation values, we consider the problem of finding a small collection of models that can be used to explain the target phenomenon in a piecewise fashion using the observation values. We assume the same model will be used for multiple segments. We give an algorithm for this task based on first segmenting the sequence using dynamic programming, and then using k-median or facility location techniques to find the optimal set of models. We report on some experimental results.
3275 en Sequence Classification Using Statistical Pattern Recognition Sequence classification is a significant problem that arises in many different real-world applications. The purpose of a sequence classifier is to assign a class label to a given sequence. Also, to obtain the pattern that characterizes the sequence is usually very useful. In this paper, a technique to discover a pattern from a given sequence is presented followed by a general novel method to classify the sequence. This method considers mainly the dependencies among the neighbouring elements of a sequence. In order to evaluate this method, a UNIX command environment is presented, but the method is general enough to be applied to other environments.
3276 en A Partial Correlation-Based Algorithm for Causal Structure Discovery with Continuous Variables We present an algorithm for causal structure discovery suited in the presence of continuous variables. We test a version based on partial correlation that is able to recover the structure of a recursive linear equations model and compare it to the well-known PC algorithm on large networks. PC is generally outperformed in run time and number of structural errors.
3277 en Fuzzy Logic Based Gait Classification for Hemiplegic Patients In this study a fuzzy logic classification system was used first to discriminate healthy subjects from patients rather than classifying those using Brunnstrom stages. Decision making was performed in two stages: feature extraction of gait signals and the fuzzy logic classification system which is used Tsukamato-type inference method. According to our signal feature extraction studies, we focused on temporal events and symetrical features of gait signal. Developed system has six inputs while four of them for temporal features evaluation rule block and two of them symmetrical features evaluation rule block. Our simulation test results showed that proposed system classify correctly 100% of subjects as patient and healthy elderly. The correlation coefficient was found 0.85 for classification to subjects to correct Brunnstrom stages. The results show that classifying patients becomes increasingly difficult linearly according to hemiplegia’s severity.
3278 en Traffic Sign Recognition Using Discriminative Local Features Real-time road sign recognition has been of great interest for many years. This problem is often addressed in a two-stage procedure involving detection and classification. In this paper a novel approach to sign representation and classification is proposed. In many previous studies focus was put on deriving a set of discriminative features from a large amount of training data using global feature selection techniques e.g. Principal Component Analysis or AdaBoost. In our method we have chosen a simple yet robust image representation built on top of the Colour Distance Transform (CDT). Based on this representation, we introduce a feature selection algorithm which captures a variable-size set of local image regions ensuring maximum dissimilarity between each individual sign and all other signs. Experiments have shown that the discriminative local features extracted from the template sign images enable simple minimum-distance classification with error rate not exceeding 7%.
3279 en Novelty Detection in Patient Histories: Experiments with Measures Based on Text Compression Reviewing a patient history can be very time consuming, partly because of the large number of consultation notes. Often, most of the notes contain little new information. Tools facilitating this and other tasks could be constructed if we had the ability to automatically detect the novel notes. We propose the use of measures based on text compression, as an approximation of Kolmogorov complexity, for classifying note novelty. We define four compression-based and eight other measures. We evaluate their ability to predict the presence of previously unseen diagnosis codes associated with the notes in patient histories from general practice. The best measures show promising classification ability, which, while not enough to serve alone as a clinical tool, might be useful as part of a system taking more data types into account. The best individual measure was the normalized asymmetric compression distance between the concatenated prior notes and the current note.
3280 en Parameter Learning for Bayesian Networks with Strict Qualitative Influences We propose a new method for learning the parameters of a Bayesian network with qualitative influences. The proposed method aims to remove unwanted (context-specific) independencies that are created by the order-constrained maximum likelihood (OCML) estimator. This is achieved by averaging the OCML estimator with the fitted probabilities of a first-order logistic regression model. We show experimentally that the new learning algorithm does not perform worse than OCML, and resolves a large part of the independencies.
3281 en Tree Augmented Naive Bayes for Regression Using Mixtures of Truncated Exponentials: Application to Higher Education Management In this paper we explore the use of Tree Augmented Naive Bayes (TAN) in regression problems where some of the independent variables are continuous and some others are discrete. The proposed solution is based on the approximation of the joint distribution by a Mixture of Truncated Exponentials (MTE). The construction of the TAN structure requires the use of the conditional mutual information, which cannot be analytically obtained for MTEs. In order to solve this problem, we introduce an unbiased estimator of the conditional mutual information, based on Monte Carlo estimation. We test the performance of the proposed model in a real life context, related to higher education management, where regression problems with discrete and continuous variables are common. This work has been supported by the Spanish Ministry of Education and Science, project TIN2004-06204-C03-01 and by Junta de Andalucía, project P05-TIC-00276.
3282 en Conditional Classification Trees Using Instrumental Variables The framework of this paper is supervised learning using classification trees. Two types of variables play a role in the definition of the classification rule, namely a response variable and a set of predictors. The tree classifier is built up by a recursive partitioning of the prediction space such to provide internally homogeneous groups of objects with respect to the response classes. In the following, we consider the role played by an instrumental variable to stratify either the variables or the objects. This yields to introduce a tree-based methodology for conditional classification. Two special cases will be discussed to grow multiple discriminant trees and partial predictability trees. These approaches use discriminant analysis and predictability measures respectively. Empirical evidence of their usefulness will be shown in real case studies.
3283 en Robust Tree-Based Incremental Imputation Method for Data Fusion Data Fusion and Data Grafting are concerned with combining files and information coming from different sources. The problem is not to extract data from a single database, but to merge information collected from different sample surveys. The typical data fusion situation formed of two data samples, the former made up of a complete data matrix X relative to a first survey, and the latter Y which contains a certain number of missing variables. The aim is to complete the matrix Y beginning from the knowledge acquired from the X. Thus, the goal is the definition of the correlation structure which joins the two data matrices to be merged. In this paper, we provide an innovative methodology for Data Fusion based on an incremental imputation algorithm in tree-based models. In addition, we consider robust tree validation by boosting iterations. A relevant advantage of the proposed method is that it works for a mixed data structure including both numerical and categorical variables. As benchmarking methods we consider explicit methods such as standard trees and multiple regression as well as an implicit method based principal component analysis. A widely extended simulation study proves that the proposed method is more accurate than the other methods.
3284 en Visualizing Sets of Partial Rankings Partial rankings are totally ordered subsets of a set of items. They arise in different applications, such as clickstream analysis and collaborative filtering, but can be difficult to analyze with traditional data analysis techniques as they are combinatorial structures. We propose a method for creating scatterplots of sets of partial rankings by first representing them in a high-dimensional space and then applying known dimensionality reduction methods. We compare different approaches by using quantitative measures and demonstrate the methods on real data sets from different application domains. Despite their simplicity the proposed methods can produce useful visualizations that are easy to interpret.
3285 en A Partially Supervised Metric Multidimensional Scaling Algorithm for Textual Data Visualization Multidimensional Scaling Algorithms (MDS) allow us to visualize high dimensional object relationships in an intuitive way. An interesting application of the MDS algorithms is the visualization of the semantic relations among documents or terms in textual databases. However, the MDS algorithms proposed in the literature exhibit a low discriminant power. The unsupervised nature of the algorithms and the ’curse of dimensionality’ favor the overlapping among different topics in the map. This problem can be overcome considering that many textual collections provide frequently a categorization for a small subset of documents. In this paper we define new semi-supervised measures that reflect better the semantic classes of the textual collection considering the a priori categorization of a subset of documents. Next the dissimilarities are incorporated into the Torgerson MDS algorithm to improve the separation among topics in the map. The experimental results show that the model proposed outperforms well known unsupervised alternatives.
3286 en Landscape Multidimensional Scaling We revisit the problem of representing a high-dimensional data set by a distance-preserving projection onto a two-dimensional plane. This problem is solved by well-known techniques, such as multidimensional scaling. There, the data is projected onto a flat plane and the Euclidean metric is used for distance calculation. In real topographic maps, however, travel distance (or time) is not determined by (Euclidean) distance alone, but also influenced by map features such as mountains or lakes. We investigate how to utilize landscape features for a distance-preserving projection. A first approach with rectangular cylindrical mountains in the MDS landscape is presented.
3288 en Entropy Properties of a Decision Rule Class in Connection with machine learning abilities Many methods of Machine Learning are based on the idea of empirical risk minimisation. It is to find a decision rule or a model from some set which most perfectly fits the data presented in the training set. This idea is based on the large number law: empirical risk converges to real risk, if the training set is large enough. But if the class of decision rules or models is too large (in some sense) one meets the problem of oferfitting, the model perfectly corresponds to the data presented in the training set, but shows large errors on new data. It is due to the fact that only uniform convergence of empirical risk to the real risk guarantees closeness of the optimal model behaviour on the training set and on the new data. We introduce the notion of entropy of a decision rule class over a fixed sample sequence as log of the number of possible classifications of the sequence by the rules of the class. Maximum entropy over sequences of a fixed length l determines sufficient condition of the uniform convergence and corresponding estimates. But only average entropy H(l) behaviour determine necessary and sufficient condition of the uniform convergence. The condition is that H(l) / l (average entropy per symbol) should go to zero when the sequence length goes to infinity. If the condition does not hold then there exists a set of objects with non zero probability measure, such that almost all sequences of arbitrary finite length from this set may be divided in all possible ways by the rules of the class. One can easily see, that in this case overfitting is inevitable. Similar results are found for real dependencies instead of decision rules.
3289 en Ground Facts, Rules and Probabilistic Inference for Cyc One aspect of Cyc is a very large, logic-based knowledge base that includes, inter-alia, large amounts of background knowledge over a wide variety of domains, but it is more than that; the Cyc project is an attempt to move towards general artificial intelligence by supporting automated reasoning about a very wide variety of real-world concerns. To support that goal, Cyc also encompasses, obviously enough, and inference engine able to reason over a large, contextual, knowledge base, but it also includes components for interpreting and producing natural language, acquiring knowledge and responding to user queries, and for interfacing with other software. Applying logic to representation of general knowledge, /at scale/, and using it in the production of intelligent behaviors has been difficult enough; unfortunately it is becoming clear that doing so using traditional logics is probably not sufficient, either for satisfying a long term goal of supporting general intelligence, or even for shorter term goals, like recognizing, interpreting, and elaborating descriptions of piracy events. In this talk, I'll briefly describe what Cyc is, and has been, and how it is growing, touch on an early approach to abductive reasoning and classification in a traditional logical framework, and some difficulties with that approach, and then describe recent, very initial work training the Markov Logic networks based on ground facts and rules within the millions of axioms of the Cyc KB. Finally I'll sketch a vision for a system that truly integrates both sound, deductive reasoning, and the bounded unsoundness of probabilistic classification, induction, abduction and deduction.
3290 en Really Achieving Your Childhood Dreams ***“The brick walls are there for a reason: they are not there to keep us out, they let us prove how badly we want something.”***
3291 en CERN's 27km Big Bang machine Presentation of the [[http://lhc.web.cern.ch/lhc/|Large Hadron Collider]] and of the [[http://atlasexperiment.org/|Atlas Experiment]], happening under your feet here in Geneva.
3292 en Wikipedia: a social innovation ? 
3293 en Outdoctrination: Society, Children, Technology and Self Organisation in Education 
3294 en Distorted Morality Speech at Harvard University about America's war on terror
3295 en PVC and cooperation from AI perspective 
3296 en Virtual organizations management 
3297 en ICT infrastructure 
3298 en Text and web data mining: Tutorial I 
3299 en Text and web data mining: Tutorial II 
3300 en Distributed planning and scheduling 
3302 en Distributed SW engineering 
3303 en Virtual reality for VE 
3304 en VR: practical experience in CAVE laboratory 
3305 en AIESEC PVC ECOLEAD case study 
3306 en E4 presentation 
3307 en The Graphing Calculator Story It's midnight. I've been working sixteen hours a day, seven days a week. I'm not being paid. In fact, my project was canceled six months ago, so I'm evading security, sneaking into Apple Computer's main offices in the heart of Silicon Valley, doing clandestine volunteer work for an eight-billion-dollar corporation. \\ [[http://www.pacifict.com/Story/|...more at www.pacifict.com/Story/]]
3308 en ToolEast presentation 
3309 en Agent technologies for VE + SW demonstrations: MAS Tutorial 
3310 en Knowledge technologies for network organisations This lecture presents the current research work at JSI on Knowledge technologies and potentials these technologies have to solve problems in Networked organisations. Some real prototypes and solutions has been presented as well as some visionary plans.
3311 en Modelling competences 
3314 en The State of the Oceans Jeremy Jackson, marine ecologist and environmental advocate, professor of oceanography at the Scripps Institution of Oceanography, describes how overfishing, habitat destruction, global warming and other human-induced activities have contributed to a crisis in the health of the world's oceans
3315 en How the Body Fights Infection Miniaturized battles are waged continuously by heroic micro-warriors that protect us from viruses, bacteria, fungi and parasites. Join Dr. Richard Locksley for a look at how these unseen victories (and occasional defeats) are played out, and how vaccination stacks the deck in our favor. \\
3322 en Nanowires and Nanocrystals for Nanotechnology Nanowires and nanocrystals represent important nanomaterials with one-dimensional and zero-dimensional morphology, respectively. Here I will give an overview on the research about how these nanomaterials impact the critical applications in faster transistors, smaller nonvolatile memory devices, efficient solar energy conversion, high-energy battery and nanobiotechnology
3323 en Creative Commons Italy 
3324 en Creative Commons Music Performance 
3325 en WTC Lecture - collapse of WTC Buildings BYU Physics professor and founder of SCHOLARS FOR 9/11 TRUTH Steven E **Jones** presents his presentation on the collapse of **WTC** Buildings 1,2&#160;and 7 on 9/11. A very informative and scientific presentation that raises serious questions about the official account of the collapse of the World Trade Center Towers and Building 7.&nbsp;&nbsp;
3326 en An Economic Response To Unsolicited Communication 
3327 en The Paradox of Choice - Why More Is Less 
3328 en NASA's "Beyond Einstein" Program: Exploration at the Limits of Space & Time Albert Einstein's General Theory of Relativity predicted results that were so incredible that even he did not accept them: space is expanding from a Big Bang, space itself contains an energy that is pulling the Universe apart from within, and deep chasms of gravity called black holes actually exist. Astonishingly, all of these wild ideas are now known to be true. But now we need to build on Einstein's work to take the next step -- to study the underlying physics of the very phenomena that came out of his theories. NASA's Beyond Einstein program consists of a series of space missions, large and small, that push Einstein's theories to their limits, using increasingly more sensitive probes. The two flagship missions now in development, Constellation-X and LISA, will explore extremes of space, measuring X-rays and gravitational waves. The smaller missions, the Einstein probes, will target specific science questions such as "What is Dark Energy?" and "What powered the Big Bang?"
3330 en Cost-effective Outbreak Detection in Networks Which blogs should we read to avoid missing important information? Where should we place sensors in a water distribution network to quickly detect contaminants? These seemingly different problems share common structure: Outbreak detection can be modeled as a problem of selecting nodes (blogs, sensor locations, ...) in a network, in order to detect the spreading of a virus or information as quickly as possible. We present a general methodology for near optimal sensor placement in these and related problems. We demonstrate that many realistic outbreak detection objectives (e.g., detection likelihood, population affected) exhibit the property of “submodularity’’. We exploit submodularity to develop an efficient algorithm that scales to large problems, provably achieving near optimal placements, while being 700 times faster than a simple greedy algorithm. We evaluate our approach on several large real-world problems, including a model of a water distribution network, and real blog data. We also show how the approach leads to deeper insights in both applications, answering multicriteria trade-off, cost-sensitivity and generalization questions. Joint work with: Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne VanBriesen and Natalie Glance Recepient of best student paper award at ACM SIGKDD ‘07 conference.
3331 en Explanation of SVM's behaviour in text classification We are concerned with the problem of learning classification rules in text categorization where many authors presented Support Vector Machines (SVM) as leading classification method. Number of studies, however, repeatedly pointed out that in some situations SVM is outperformed by simpler methods such as naive Bayes or nearest-neighbor rule. In this paper, we aim at developing better understanding of SVM behaviour in typical text categorization problems represented by sparse bag of words feature spaces. We study in details the performance and the number of support vectors when varying the training set size, the number of features and, unlike existing studies, also SVM free parameter C, which is the Lagrange multipliers upper bound in SVM dual. We show that SVM solutions with small C are high performers. However, most training documents are then bounded support vectors sharing a same weight C . Thus, SVM reduce to a nearest mean classifier; this raises an interesting question on SVM merits in sparse bag of words feature spaces. Additionally, SVM suffer from performance deterioration for particular training set size/number of features combinations.
3332 en Energy Efficient Transistors Major technological shifts in solid-state device technology have typically been associated with improvements in device energy efficiency. This was true in the transition from vacuum tubes to bipolar transistors in the 1950s, and then again from bipolar transistors to MOSFETs in the 1970s. The next technological revolution will similarly stem from an improvement in energy efficiency at the device or at the circuit level. This lecture will explore ways in which new materials and transistors can extend the performance of electronic systems.
3333 en Introduction to Active Networks The goal of active networking is to create communication networks that reposition static, low-level network operation into dynamic, differentiated, and adaptable behavior. This allows communication hardware to be more fully used given that its operation can be tailored to specific application requirements. This also enables a more flexible and survivable communication network. Active networking decouples the network protocol from its transport by allowing easy insertion of protocols on top of the transport layer. Active networking also minimizes requirements for global agreement; it does not require years of standards negotiation to introduce new protocols. Active networking enables on-the-fly experimentation given easy insertion of new protocols and network applications, thus enabling the rapid deployment of new services and applications. The mechanism for implementing an active network is to enable communication packets to carry network code as well as data. This code may be installed on the fly into low-level network devices as the packet flows throughout the network. Ultimately, the essence and fundamental uniqueness of active networking is the flexibility introduced by a tight integration of code and data in service of the communication network. Both code and data flow within, and change the operation of, the network. Legacy networks have focused on improving the flow of data based on the fundamentals of analyses such as Shannon’s fundamental insights into entropy as well as analyses in support of moving bits, such as queuing theory. With the tighter integration of code and data in an active network, a broader view of information encompassing both code and data in the form of Kolmogorov complexity is required. In this form of analysis, there is a focus on code and data as a combined entity. Active packets in the Kolmogorov complexity framework may vary from static data, as in legacy networks, to pure executable code; as code, the packets act as small executable models of information. This presentation delves into some of these issues.
3336 en Future of Technology by Google chairman and CEO Eric Schmidt 
3337 en Interview with Santiago Calatrava Santiago Calatrava, Architect; Visuals of Milwaukee Art Museum, City of Science, and various buildings/bridges
3338 en Stanford Experts on Climate Change and Carbon Trading Dr. Schneider is one of the world's leading scientific experts of climate change (his name is cited on all those climate change charts and graphs we've seen so far). Dr. Heller has extensive experience with policy and negotiations surrounding climate change and sustainable development. Professor Heller also recently served as Sergey's host at the recent UN Climate Change Conference meeting in Montreal where Prof. Heller proved his indepth knowledge of thenuances of legislative works, such as the Kyoto Protocol, and the mechanisms that are currently being employed.
3342 en The complete meeting 
3343 en The International involvement of the JSI Institute 
3344 en The International involvement of Jonneum Research and the JSI Institute 
3345 en About the Cooperation 
3346 en About the Cooperation of Jonneum Research and the JSI Institute 
3347 en Debate 
3349 en Wireless Tech & Regulatory Reality: Policy and Fantasy in the 21st Century Sascha Meinrath has been described as a "[[http://www.savetheinternet.com/=coalition|community Internet pioneer]]" and an "[[http://infodev-study.oplan.org/the-study/1-background-and-introduction/copy4_of_1-background-and-introduction|entrepreneurial visionary]]" and is a well-known expert on [[http://en.wikipedia.org/wiki/Wireless_community_network|community wireless networks]] (CWNs) and [[http://en.wikipedia.org/wiki/Municipal_broadband|municipal broadband]]. Leading news sources, including [[http://www.economist.com/science/displayStory.cfm?story_id=3535732|the Economist]], [[http://www.nytimes.com/2006/09/27/technology/circuits/27fon.html?ex=1160539200&en=53c38adbd350e304&ei=5070|the New York Times]], [[http://www.thenation.com/blogs/edcut?pid=77928|the Nation]], and [[http://www.npr.org/templates/story/story.php?storyId=4834612|National Public Radio]], often cite Sascha's work in covering issues related to CWNs. Sascha is the Research Director for the [[http://www.newamerica.net/|New America Foundation's]] [[http://www.spectrumpolicy.org/|Wireless Future Program]]. Additionally, he coordinates the [[http://www.oswc.net/|Open Source Wireless Coalition]], a global partnership of open source wireless integrators, researchers, implementors and companies dedicated to the development of open source, interoperable, low-cost wireless technologies. He is a regular contributor to [[http://www.muniwireless.com/|MuniWireless.com]], the leading source for municipal wireless news and information, and a regular contributor to [[http://govtech.net/digitalcommunities|Government Technology's Digital Communities]], the online portal and comprehensive information resource for the public sector. Sascha has also worked with [[http://www.freepress.net/|Free Press]], the [[http://www.caida.org/|the Cooperative Association for Internet Data Analysis (CAIDA)]], the [[http://www.acornactivemedia.com/| Acorn Active Media Foundation]], the [[http://www.ethoswireless.com/|Ethos Group]], and the [[http://www.cuwin.net/|CUWiN Foundation]]. Sascha holds a Bachelor's Degree from [[http://www.yale.edu/|Yale University]] and a Master's Degree from the [[http://www.psych.uiuc.edu/|University of Illinois at Urbana-Champaign]], both in psychology. He is a Telecommunications Fellow at the University of Illinois in the [[http://www.comm.uiuc.edu/icr/|Institute for Communications Research]], where he is finishing his PhD on community empowerment and the impacts and interactions of participatory media, wireless communications, and emergent technologies. more >>> [[http://www.saschameinrath.com/|saschameinrath.com]]
3353 en Sequential Monte Carlo methods continued Parts 1, 2 and 3 of this lecture are presented in [[mlss07_doucet_smcm|//Arnaud Doucet's// "%title"]]
3354 en My Turing Machine or Yours? The title of this talk is “Your Turing Machine or Mine?”. What I am alluding to with this title is the universality of a universal Turing machine, and at the same time, to the fact that there are many different universal Turing machines with somewhat different properties. Universal Turing machines are both universal and individual, in different senses. A universal Turing machine is one that can emulate (or imitate) any other Turing machine, and thus in a sense can undertake to compute any of a very large class of computable functions. But there are an indefinitely large number of universal Turing machines that can be defined.
3355 en Linguistically-informed, statistically-driven induction of morphology **Problem:** induction of morphology from unannotated text. **Main idea: **knowledge of linguistic and statistical properties of morphology allows for a simple induction algorithm. Develops ideas from previous work: Goldsmith (2001), Schone &amp; Jurafsky (2000), Yarowsky &amp; Wicentowski (2000, 2004).
3356 en Incrementally learning an Incremental parser 'What is the relation between the language surface statistics and its hidden syntactic structure?' \\ To explore this, the author presents an unsupervised parser. Some of the properties of natural language that are used: Tree structures are skewed. Humans process language incrementally. Words have the Zipfian distribution. Bootstrapping in learning.
3357 en Linguistic Relevance of Unsupervised Data-Oriented Parsing Is Empiricist Language Acquisition Possible? \\ Can we acquire language by constructing analogies with previous input?
3358 en A New Way to look at Networking Today's research community congratulates itself for the success of the internet and passionately argues whether circuits or datagrams are the One True Way. Meanwhile the list of unsolved problems grows. Security, mobility, ubiquitous computing, wireless, autonomous sensors, content distribution, digital divide, third world infrastructure, etc., are all poorly served by what's available from either the research community or the marketplace. I'll use various strained analogies and contrived examples to argue that network research is moribund because the only thing it knows how to do is fill in the details of a conversation between two applications. Today as in the 60s problems go unsolved due to our tunnel vision and not because of their intrinsic difficulty. And now, like then, simply changing our point of view may make many hard things easy.
3359 en Unsupervised Learning of Syntactic Structure Probabilistic models of language. ''Everybody knows that language is variable'' - Sapir (1921). \\ Probabilistic models give precise descriptions of a variable, uncertain world. The choice for language isn’t a dichotomy between rules and neural networks. Probabilistic models can be used over rich linguistic representations. They support inference and learning. There’s not much evidence of a poverty of the stimulus preventing them being used.
3360 en Words in puddles of sound Words in a “sea of sound” (Saffran, 2001) \\ Discovering words from continuous speech; with no reliable cues to word boundaries (Jones, 1918; Liberman et al., 1967); where words are realised variably (Pollack & Pickett, 1964).
3361 en Simulating Language Acquisition Many models of Language Acquisition focus on learnability problem: Aim to show that certain problematic constructions can be learnt. Often use artificial input (toy grammars). Output does not map onto child speech: error on output vector larger for ungrammatical items; unclear how this relates to actual error rates. Implications outside the phenomenon under investigation can be unclear.
3362 en Latent Semantic Grammar Induction Supervised Grammar Induction works great but is expensive, time consuming and area specific. Unsupervised Grammar Induction is a more general solution, can explore knowledge required: OS/hidden syntax, projectivity, prior distributions, semantics.
3363 en Memory-Based models of inflectional morphology acquisition and processing Old and recent results using Memory-Based Learning in inflectional morphology (Pinker: fruit fly of psycholinguistics). Memory-based learning with TiMBL: learning is storage. Cases: English and Dutch past tense; more interesting: German plural and Dutch plural. Computational psycholinguistics methodology.
3364 en A Bayesian approach to the Poverty of the stimulus Shown that given reasonable domain-general assumptions, an unbiased rational learner could realize that languages have a hierarchical structure based on typical child-directed input. Can use this paradigm to explore the role of recursive elements in a grammar: the “winning” grammar contains additional non-recursive counterparts for complex NPs; perhaps language, while fundamentally recursive, contains duplicate non-recursive elements that more precisely match the input?
3365 en Grammatical Inference Vs. Grammar Induction: the Data or the Method? Why study the algorithms and not the grammars. Learning in the exact setting. Learning in a probabilistic setting.
3366 en Using Minimum Description Length to make Grammatical Generalizations What should Syntactic Theory Explain? \\ Which sentences are grammatical and which are not or how to transform observed sentences into a grammar.
3367 en Learning Phonotactic Constraints from Continuous Speech Native listeners of Dutch use knowledge of legal and illegal sound sequences in their language to detect word boundaries in continuous speech (McQueen, 1998). Such knowledge can be modeled within the framework of Optimality Theory (OT) by a set of phonotactic constraints. While many researchers within OT assume that constraints are innate, this paper aims to show that phonotactic constraints can be learned from data.
3368 en From Unsegmented Speech to Lexical Categories Using Phoneme Distributions Advantages of Computational Modeling of Language - Mediates between theory and data: evaluation, exploration, existence proofs; Requires making underlying assumptions explicit: can illuminate complex relationships between variables not easily grasped intuitively; make behaviorally testable predictions; No human subjects approval required!
3369 en A Bayesian approach to Word Segmentation: Theoretical and Experimental results Word segmentation. One of the first problems infants must solve when learning language. Infants make use of many different cues - phonotactics, allophonic variation, metrical (stress) patterns, effects of coarticulation, and statistical regularities in syllable sequences. Statistics may provide initial bootstrapping - used very early (Thiessen & Saffran, 2003); language-independent.
3370 en Bayesian models of cross-situational word learning 
3371 en An annotated hierarchies model of syntactic categories The role of syntactic categories (part-of-speech tags) in linguistic description: hierarchical organization of categories; factors in syntactic categorization. Presents a computational model for induction of hierarchical structure from relational and feature data. Presents the results of the application of the model to relational and feature data obtained from natural and artificial corpora.
3372 en Modeling semantic plausibility and the influence of visual context during on-line sentence comprehension Area of research: the processes that underlie the human capacity to understand language. \\ How does the human language processor work? Architectures, mechanisms. \\ How can we model it computationally? Understanding: Competence; Behaviour: Performance. \\ Interaction of language with other cognitive systems and the environment.
3373 en Ecology of bird populations in the Pacific Flyways Hear it straight from California experts about the threat of a global bird-flu epidemic. The following research scientists and public-health experts from UC Davis and other Northern California organizations addressed the gamut of common concerns at an Avian Influenza Symposium held in Davis, Calif., on April 15, 2006. Organized by the Yolo Audubon Society, the program particularly emphasized the role that wild birds might play in spreading avian influenza to humans.
3374 en Biology in Four Dimensions How do early birds get up in time to catch the worms? A clock in our brains helps us maintain daily, or circadian, rhythms. Dr. Joseph S.&#160;Takahashi discusses the natural history of biological rhythms and explains how he and other scientists have unraveled the complex workings of the body’s clocks. Biological clocks are key to understanding jet lag, various sleep disorders, and why teenagers have a hard time rising early.
3376 en Science in very high magnetic fields: NMR investigations of exotic quantum spin states After a short introduction presenting Grenoble High Magnetic Field Laboratory and its high-field NMR facility, we will illustrate possibilities of high-field NMR in Solid State Physics on several examples of low-dimensional, quantum, antiferromagnetic spin systems. We further focus on two types of 2D, dimer, spin systems, which give raise to very different ground states under an applied magnetic field. One example is the "Shastry-Sutherland" compound SrCu2(BO3)2, which exhibits magnetization plateaus at fractional values of the saturation magnetization. In this compound plateaus appear because the kinetic energy of the triplet excitations is strongly reduced by frustration, so that the triplets can crystallize into a commensurate super-lattice. NMR signature of such a super-lattice in the 1/8 magnetization plateau of SrCu2(BO3)2 is a unique observation of this type of magnetization plateau created by spontaneous breaking of translational symmetry [1]. We shall also discuss some new results on the magnetic ground states at fields above the 1/8 plateau (i.e. above 28.4 T), which seemed to be a candidate for a supersolid phase. Another type of 2D spin system is represented by the so-called "Han purple" compound, BaCuSi2O6, for which there is no magnetic frustration, and in which a (2D) Bose-Einstein condensation of triplet excitations occurs above 23.35 T. We shall present a microscopic picture of this complicated high field phase, in which NMR data reveal that the average boson density in the condensate is strongly modulated along the direction perpendicular to the 2D planes, with a density ratio for every second plane nA/nB
3379 en Bayesian Inference of Mechanistic Systems Models Using Population MCMC We demonstrate how Population Markov Chain Monte Carlo techniques may be used to sample from the complex posterior distributions which arise when estimating parameters over nonlinear mechanistic mathematical models of biological processes given noisy data. Further, we show how the samples obtained may be employed, using a Power Posteriors method, to accurately calculate the marginal likelihoods and Bayes factors over such models.
3380 en Parameter estimation of ODE's using Support Vector Regression and Qualitative Constraints Dynamical systems used for the modeling of biological networks (such as gene regulatory networks or metabolic networks) are generally based on Ordinary Differential Equations (ODE`s). Although nonlinear ODE`s are commonly used in Systems Biology, their identification and estimation from real data remain a difficult task because of the high number of parameters to estimate compared with the relatively few number of observations.
3381 en Reconstructing hidden protein activity by nonparameteric methods Most of the cellular processes are not accessible to direct measurements. For example, the level of protein activities can often only be assessed indirectly by their effects on gene expression or the modification of other proteins. The exact form of the functional relationships describing such interactions are unknown as well. Nevertheless, some progress can be made by combining factor analysis or dynamical models with nonparametric regression methods, which don't impose any form on reconstructed functions. On the other hand, identifiability becomes are major problem and needs to be balanced against flexibility. I will report on our experience with simulated and experimental data in exploring the limits of reconstructing hidden protein activities and interaction networks.
3382 en A study of MCMC and deterministic approximations for hypothesis testing using ODE models In this talk we present a comparison of different methods of testing alternative hypotheses expressed using ODE models of biological systems. We investigated applicability, limitations and stability of a range of hypotheses testing methods including maximum likelihood based information criteria, local deterministic approximations around maximum a posteriori estimates (Laplace approximations) for computing marginal likelihoods, importance sampling based marginal likelihood estimators, and a path sampling estimator built upon the principles of thermodynamic integration.
3383 en From Physiologically Based Pharmacokinetic Modelling toward System Biology Pharmacokinetic (PK) modelling has had since the beginning a systemic approach to the description of chemicals absorption, distribution, metabolism and excretion in or from the body. The earliest PK models were physiological and mechanistic in the sense that they started from a mathematical description of the body as organs of given composition and properties, linked by blood flow. For a while, the lack of fast methods for solving differential equation systems led to the formulation and use of empirical and much simpler compartmental models. Interest in physiologically based (PBPK) modelling has persisted, however, in toxicology where data are sparse, and where transpositions from animals to humans are best grounded in physiology and biochemistry. Advances in Bayesian numerical analysis now allow rigorous inferences for PBPK models, including in the context of complex data structures (e.g., hierarchical or "population" models).
3384 en Gene regulatory network reconstruction by Bayesian integration of prior knowledge and/or different experimental conditions There have been various attempts to improve the reconstruction of gene regulatory networks from microarray data by the systematic integration of biological prior knowledge. Our approach follows the Bayesian paradigm where the prior knowledge is expressed in terms of energy functions, from which a prior distribution over network structures is obtained in the form of a Gibbs distribution. The hyperparameters of this distribution represent the weights associated with the prior knowledge relative to the data. We have derived and tested an MCMC scheme for sampling networks and hyperparameters simultaneously from the posterior distribution, thereby automatically learning how to trade off information from the prior and the data. We have extended this approach to a Bayesian coupling scheme for learning gene regulatory networks from a combination of related data sets that were obtained under different experimental conditions and are therefore potentially associated with different active subpathways.
3385 en Weak noise approximate inference for diffusion models The modelling of the Stochastic Kinetics of biochemical networks by stochastic differential equations (SDE) has been successfully used as a basis for statistical inference for such models. Since Monte Carlo based inference can be time consuming for SDEs, we suggest a different approximate approach. The idea is that a diffusion model applies well to chemical kinetics, when the number of molecules of each type is large. In this limit, also the number uctuations are small leading to a small diffusion term compared to the drift. This suggests the application of a weak noise expansion.
3386 en Variational Inference for Markov Jump Processes Markov jump processes (MJPs) underpin our understanding of many important systems in science and technology. They provide a rigorous probabilistic framework to model the joint dynamics of groups (species) of interacting individuals, with applications ranging from information packets in a telecommunications network to epidemiology and population levels in the environment. These processes are usually non-linear and highly coupled, giving rise to non-trivial steady states (often referred to as emerging properties). Unfortunately, this also means that exact statistical inference is unfeasible and approximations must be made in the analysis of these systems. A traditional approach, which has been very successful throughout the past century, is to ignore the discrete nature of the processes and to approximate the stochastic process with a deterministic process whose behaviour is described by a system of non-linear, coupled ODEs. This approximation relies on the stochastic fluctuations being negligible compared to the average population counts. There are many important situations where this assumption is untenable: for example, stochastic fluctuations are reputed to be responsible for a number of important biological phenomena, from cell differentiation to pathogen virulence. Researchers are now able to obtain accurate estimates of the number of macromolecules of a certain species within a cell, prompting a need for practical statistical tools to handle discrete data.
3387 en Mathematical Modeling of Cell Signalling Pathways In recent years, the analysis of cell signalling systems through data-based models in ordinary differential equations (ODE) or other paradigms (e.g. stochastic models) has emerged as an invaluable tool to understand the underlying complexity of the protein interactions happening in cellular signal transduction. Compared with other biochemical systems, the modelling of cell signalling systems faces additional difficulties related to the challenges quantifying protein-protein processes but also to the lack of complete information about the topology of the considered network interactions. Since in most of the metabolic systems the complete network of interactions is (virtually) perfectly established, in cell signalling systems the real structure of the pathways is an open question to be elucidated either in parallel or through mathematical modelling based analysis. In this talk we discuss the use of power-law models (advantages and challenges) in biochemical systems. We also show how pre-existent biological knowledge and quantitative data can be integrated through mathematical modelling to validate hypothesis about the structure of signalling pathways.
3388 en Towards a Polymer Model of Genetic Recombination Homologous recombination (HR) refers to the shuffling of genetic material during cellular division e.g., meiosis and mitosis, and in response to environmentally-induced genetic damage. HR occurs preferentially over certain regions, and is notably influenced by physical compartmentalization due to chromosome packing. Although a number of experimental techniques exist for identifying such effect they are generally costly and time-consuming. Here we suggest a Bayesian approach to recom- bination with a mechanistic prior such that we may infer posterior distributions for recombination frequencies and efficiencies between regions where experimental data may be absent.
3389 en A software tool for Bayesian inference of ODE Biochemical Models An extendable software platform was developed to enable model parameter inference and model comparison of ODE models of biochemical systems. The software accepts ODE models defined using either SBML language, experimental data from plain files or spreadsheets, and performs model parameter inference using Markov Chain Monte Carlo methods. The software also implements several methods for computing marginal likelihoods, which are required for model comparison.
3391 en Introduction, Basic Notions in Graph Theory At the beginning examples and applications of configurations are shown.nBasic definitions in graph theory follow.
3392 en Isomorphism, Matrices and Graph Invariants; Subgraphs and Connectivity in Graphs 
3393 en Graph Colorings and Matchings 
3394 en Basic and Advanced Operations on Graphs 
3395 en Group Actions and Cayley Graphs 
3396 en Money As Debt Paul Grignon's 47-minute animated presentation of "Money as Debt" tells in very simple and effective graphic terms what money is and how it is being created. It is an entertaining way to get the message out. The Cowichan Citizens Coalition and its "Duncan Initiative" received high praise from those who previewed it. I recommend it as a painless but hard-hitting educational tool and encourage the widest distribution and use by all groups concerned with the present unsustainable monetary system in Canada and the United States. More at [[http://www.moneyasdebt.net/|moneyasdebt.net]]
3398 en Text Mining in Biological Texts 
3399 en Semi-supervised Learning for Text Classification 
3401 en A Graphical Model of the Local Structure of Proteins 
3402 en Modeling Human Category Learning 
3403 en A Kernel-based Nonlinear Approach for Time Series Forecast 
3404 en Catching Up Faster in Bayesian Model Selection and Model Averaging 
3405 en Probabilistic Counting Algorithms for Massive Data Streams 
3407 en Transductive Rademacher Complexity and its Applications 
3408 en Ensemble Systems for Incremental and Nonstationary Learning 
3409 en Supervised Learning on Matrices with the Dual Spectral Regularization 
3410 en Discovering the Truth by Conducting Experiments 
3414 en Automatic Techniques for Extracting Semantic Data (from text and media) Using the results of a number of large European projects Fabio will outline the main issues for acquiring ontological knowledge from text and media at large scale. A lot of the work described here is based on collaborations with the jet engine company Rolls Royce. This session will give attendees insights into how information extraction techniques can support the population of ontologies with data.
3417 en Polynitrogen Chemistry Podan bo pregled dela na podro?ju kemije polidu?ikovih spojin in spojin z visoko vsebnostjo du?ika. Prikazana bo sinteza in karakterizacija novih polidu?ikovih ionov N5+, N5?, N3NOF+, in N7O+. Opisani bodo napori za kombinacijo N5+ kationa z visoko energijskimi anioni, kot so N3?, NO3?, ClO4?, B(N3)4? in P(N3)6?.
3418 en Potential and limitations of minimally supervised botstrapping The detection of relation instances is a central functionality for the extraction of structured information from unstructured textual data and for gradually turning texts into semi-structured information. With respect to the acquisition of the classifiers or detection grammars, the existing approaches fall in three large categories: * detection by classifiers/grammars acquired through intellectual human labor * detection by classifiers/grammars acquired through supervised learning * detection by classifiers/grammars acquired through unsupervised or minimally supervised learning. In the talk we will provide examples for the classes of approaches and summarize their respective advantages and disad¬vantages. We will argue that different relation detection tasks require different methods or even different combinations of methods. One empirically promising and theoretically attractive line of research is the learning of extraction rules from seeds. Several minimally supervised approaches have been investigated that accomplished rather decent results with a minimum of effort. The learning algorithms are not domain dependent. The seed-based bootstrapping approaches are theoretically pleasing because the learned patterns and rules are modular and transparent. They can be reused in new applications and they can be a valuable resource for (computational) linguistic investigation. We will explain several bootstrapping methods, most of them starting with patterns as seeds and some with event seeds. We will also describe our own approach of bootstrapping (Xu et al. 2007) a radical extension of Xu et al. (2006). In this approach, learning starts from a small set of n-ary relation instances as "seeds" in order to auto-ma¬ti¬cally learn pattern rules from parsed data, which then can extract new instances of the n-ary relation and its projections. After a fruitful period of skillful trial and error, there seems to be the right time now for a more systematic investigation of the alternative approaches to relation detection. In addition to tables of recall and precision values for competing methods, we urgently need explanations, i.e. causal theories explaining the virtues and shortcomings of alternative techniques with respect to properties of domains and text data. We describe one theory of this kind based on experimental evidence and explanatory insight. The advocated scientific methodology will enable optimal choices for specific tasks, effectively reduce the number of promising combinations of methods for future investigation, and guide the search for completely new approaches.
3419 en Supervised reconstruction of biological networks The inference or reconstruction of various biological networks, including regulatory, signalling or metabolic pathways, from large-scale heterogeneous data is currently an active research subject with several important applications in systems biology. While several approaches proposed so far cast this problem as inferring a graph de novo from genomic data, I will argue in this talk that the network of interest is often partially known and that the reconstruction process should use this partial knowledge to guide the inference of the missing edges. I will then review how this paradigm leads naturally to various supervised machine learning algorithms for graph inference, and illustrate the relevance of the approach through several examples of successful prediction of missing enzymes in metabolic networks.
3421 en Identification of functional modules based on transcriptional regulation structure 
3422 en Welcome and description of EURANDOM 
3423 en About the previous Chairs of EURANDOM 
3424 en How Roberto Fernandez came to EURANDOM 
3425 en Random processes, partially ordered fields, gibbs fields These probabilistic objects are all defined in terms of families of probability kernels satisfying appropriate consistency relations. This common feature can be exploited to develop parallel theories in which concepts and techniques can be shared or imported. The talk will (i) present the main lines of this parallel treatment; (ii) discuss the resulting body of comparable properties (extremality and mixing, limit states, Dobrushin uniqueness criteria); (iii) review instances of interplay between the theories (phase transitions, uniqueness criteria), and (iv) present some remaining questions and open problems
3427 en How I was right even when I was wrong For the past several years I have warned people not to ask me to predict the future, because my predictions are usually wrong. Undaunted by failure, in this talk I will try to predict the future of the semantic web based on a very personal view of its history, the history of the internet, web, semantic web, and AI, and the mistakes I've made predicting where and how they would be valuable.
3428 en POWERSET - Natural Language and the Semantic Web The **Semantic Web** promises to revolutionize access to information by adding machine-readable semantic information to content which is normally interpretable only by people. In addition, it will also revolutionize access to services by adding semantic information to create machine-readable service descriptions. This ambitious vision has been slow to take off because of a chicken and egg problem. Markup is required before people will build applications, applications are required before it is worth the hard work of doing markup. **Natural language processing (NLP)** has advanced to the point where it can break the impasse and open up the possibilities of the Semantic Web. First, NLP systems can now automatically create annotations from unstructured text. This provides the data that semantic web applications require. Second, NLP systems are themselves consumers of semantic web information and thus provide economic motivation for people to create and maintain such information. For example, a new generation of natural language search systems, as illustrated by Powerset, can take advantage of semantic web markup and ontologies to augment their interpretation of underlying textual content. They can also expose semantic web services directly in response to natural language queries.
3440 en Utilizing ISP-P2P collaboration to enhance trust in P2P systems 
3441 en 6S: Collaborative Web search via an adaptive peer network 
3442 en Beyond Web 2.0 
3443 en Self-mapping Networks 
3444 en Emergent Networks as Distributed Reputation System 
3445 en Efficient and Decentralized Page Rank Approximation in P2P Networks with Malicious Agents 
3446 en Globalization and Changes in Life Courses in Modern Societies Research Goals: To which extent has globalization influenced life courses in modern societies? How do different domestic institutions filter these transformations? How do actors respond to globalization? How does globalization affect social inequality? What do we mean by globalization?
3449 en Social Dynamics in Age of the Web People have moved to the web: For many of their social and commercial interactions; To satisfy their information and entertainment needs; To generate content on a massive scale. (follows) The emergence of an online collective intelligence that can be tapped. What do these trends portend? And how can we benefit from them?
3450 en Computer Modeling of Genome Evolution during Sympatric Speciation In the natural, even large and panmictic populations, the inbreeding coefficient could be relatively high, because individuals are looking for sexual partners in the relatively short distances. We have simulated the evolution of such population on the square lattice. Results of simulations were compared to the human population and its genetic pool. In particular, we have analyzed distribution of recombination spots along the human chromosomes using the method of cumulative detrended walks. Frequency of recombination in all human chromosomes (excluding chromosome Y which has no partner for recombination) is higher in the subtelomeric regions (both ends of chromosomes) and relatively lower in the middle part of chromosomes. This resembles the distribution of accepted recombination events in the bitstring representing haplotypes in the computer simulated populations. In our simulations, individuals before reproduction have to produce gametes connected with some probability of recombination between haplotypes (bitstrings). We consider that recombination event is accepted if gamete produced by this recombination succeeded in forming the surviving zygote. Under high inbreeding and low frequency of intragenomic recombinations the middle parts of bitstrings in one genome complemented each other while at the ends the defective alleles are eliminated by purifying Darwinian selection. Such conditions are characteristic for populations after sympatric speciation. In conclusion, the middle part of chromosomes is more conserved and its structure seems to be responsible for belonging to higher taxons, while at the ends of chromosomes genes responsible for the intra species biodiversity are located.
3451 en Publish or Perish Publish-or-Perish Phenomenon - Evaluations of scientists depend on number of papers, positions in lists of authors, and journals’ impact factors. In Japan, Spain and elsewhere, such assessments have reached formulaic precision. But bureaucrats are not only wholly responsible for these changes - we scientists have enthusiastically colluded. What began as someone else’s measure has become our (own) goal… P.A. Lawrence, The politics of publication, Nature 422, 259 (2003).
3452 en Log-periodic Oscillations due to Discrete Effects in Complex Networks Authors show how discretization affects two major characteristics in complex networks: internode distances (measured as the shortest number of edges between network sites) and average path length, and as a result there are log-periodic oscillations of the above quantities. The effect occurs both in numerical network models as well as in such real systems as coauthorship, language, food, and public transport networks. Analytical description of these oscillations fits well numerical simulations. They consider a simple case of the network optimization problem, arguing that discrete effects can lead to a nontrivial solution.
3453 en Workshop Introduction A “Critical Infrastructure” (CI) is a large scale infrastructure which if degraded, disrupted or destroyed, would have a serious impact on health, safety, security or well-beings of citizens or the effective functioning of governments and/or economy. (W. Schmitz, 2006)
3454 en Integrated Risk Reduction of Information-based Infrastructure Systems Goal: Mitigate the danger of blackouts in communication and power supply networks Support of the “strategic assessment” of the current situation taking into account the dependency / interdependency structure. “Identified risks can be accepted or not with regard to the goal ?” Support of the “strategic decision making” based on the assessment of the current situation. Critical infrastructures (CIs) have undergone drastic changes in the last decades. The ubiquitous use of ICT has pervaded all traditional infrastructures, rendering them more intelligent, increasingly interconnected, complex, interdependent, and therefore more vulnerable. Critical infrastructures are vital backbones of modern societies and are increasingly depending on IT and communication networks. Due to the increasing IT-penetration CIs are more and more connected with each other with advantages and disadvantages. Due to this interconnection critical infrastructures can provide their services more cost-efficiently. On the other side in case of disturbances their behaviour can not ever be mastered as the large blackouts in USA and Europe have shown. Increasing complexity and manifold conventional and emerging threats jeopardise the system of mutual dependent critical infrastructures. CIP gains a high importance and has to be understood as a holistic process considering technologies and persons acting within this system of systems.
3457 en 'Lies, Damn Lies, and Statistics': A Critical Assessment of Preferential Attachment-type Network Models of the Internet Basic Question: Do the available Internet-related connectivity measurements and their analysis support the sort of claims that can be found in the existing complex networks literature? Key Issues: What about data hygiene? What about statistical rigor? What about model validation? Author discusses some of the main problems and challenges associated with measuring, inferring, and modeling various types of Internet-related connectivity structures. To this end, he uses some known examples to illustrate the need to understand the process by which Internet connectivity measurements are obtained, explore the sensitivity of inferred graph properties to known ambiguities in the data, be more critical with respect to the dominant, preferential attachmenttype network modeling paradigm, and be more serious/ambitious when it comes to model validation. Ignoring any of these issues is bound to produce results that are best described by the well-known aphorism "lies, damned lies, and statistics."
3460 en Modelling Interdependencies of Critical Infrastructures under Natural Disasters - A Case of Supply, Communication, and Transportation Infrastructures This paper introduces the methodological challenge of identifying and quantifying the interdependencies among several critical infrastructures. First, interdependency structures during a natural disaster are modeled based on past events, considering supply (electricity, water, and gas), communication (internet and telephone), and transportation infrastructures (road networks). Interdependencies are defined with respect to physical, functional, and socio-economic interrelationships. A quantification strategy is then introduced based on empirical surveys and economic models. As a case study, the developed model is applied to the 2004 Mid-Niigata earthquake, which severely damaged infrastructure systems in the northern mountainous region of Japan.
3463 en A Configurable Hardware Framework for a Trusted Computing Base: Application to the Power Grid Objectives: Develop enabling technology to provide customizable level of trust to a significant critical infrastructure as exemplified by the Power Grid computing and communication systems. The focus is on design methods and runtime techniques to achieve application-specific level of reliability and security, while delivering optimal and timely performance.
3464 en Simulation Experiments: Emerging Instruments for CIP Critical infrastructures (CIs) have undergone drastic changes in the last decades. The ubiquitous use of ICT has pervaded all traditional infrastructures, rendering them more intelligent, increasingly interconnected, complex, interdependent, and therefore more vulnerable. Critical infrastructures are vital backbones of modern societies and our society is fully dependent on them. Besides the traditional threats like natural disasters, man made disasters and terrorist attacks the complexity of the system of interdependent infrastructures has to be considered as an additional threat. The increasing IT-penetration leads to interdependencies and feedbacks which make more and more difficult to distinguish cause and effect. That means, critical infrastructures have to be designed as fault-tolerant systems. Redundancy and selfhealing are approved design principles. But such design principles cause increasing costs, and the private sector complying with the rules of shareholder value has to prevent costs. On the other hand security of critical infrastructures has a high societal importance. That means new business models and security technologies have to be introduced that fulfil the interests of the private enterprises and the state. In addition novel security technologies like Modelling and Simulation (M&S) have to be developed and used in order to protect critical infrastructures as well as to train people responsible for safety and security.
3465 en Artificial Regulatory Networks Evolution 
3466 en Candidate gene prioritization by genomic data fusion The overwhelming amount of biological data makes the assignment of candidate genes to diseases and biological pathways a formidable challenge. We present ENDEAVOUR, a generally applicable computational methodology to prioritize candidate genes based on their similarity to case-specific reference gene sets. Unlike previous methods, ENDEAVOUR is capable of flexibly utilizing multiple data sets from diverse sources. It allows the modular incorporation of de novo generated data sets and integrates distinct prioritizations into a global ranking by applying order statistics. We first validate the overallperformance in a statistical cross validation of 29 diseases and 3 biological pathways. We validate a novel candidate for DiGeorge syndrome in a zebrafish model and present several new candidates for congenital heart disease. We extend the basic ENDEAVOUR methodology using data from multiple species (human, mouse, rat, drosophila and C. elegans). We also present an alternative machine learning methodology for gene prioritization using kernel methods for novelty detection that outperforms our previous results.
3467 en Towards a semi-automatic functional annotation tool based on decision tree techniques 
3468 en Machine Learning Techniques to Identify Putative Genes Involved in Nitrogen Catabolite Repression in the Yeast Saccharomyces cerevisiae 
3469 en Interview with Bernardo A. Huberman Bernardo Huberman is a Senior HP Fellow and Director of the Information Dynamics Lab at Hewlett Packard Laboratories. He received his Ph.D. in Physics from the University of Pennsylvania, and is currently a Consulting Professor in the Department of Applied Physics at Stanford University. He originally worked in condensed matter physics, ranging from superionic conductors to two-dimensional superfluids, and made contributions to the theory of critical phenomena in low dimensional systems. He was one of the discoverers of chaos in a number of physical systems, and also established a number of universal properties in nonlinear dynamical systems. His research into the dynamics of complex structures led to his discovery of ultradiffusion in hierarchical systems.
3470 en Interview with Denis Noble In the interview we can find out that Denis Noble, professor of Cardio-Vascular Phisiology, built one of the first models of the heart and recently wrote a very provocative book called "Music of Life" about systems biology. The Videolectures.Net team spoke to him in Dresden at the European Conference on Complex Systems. We asked him **about his lecture at the conference, DNA, molecules, meaning of life** and how does **the organism works**. When asked if he moved from applications to philosophy, he answers positively.\\ //"Use my book to get rid of some mistakes you may have made by using metaphores of other people, use my metaphores as a contrast, go up the ladder but then throw these away too! Leave yourself, think yourself what you really think biology is about."//
3471 en Welcome Since June 1st, 2007, Dirk Helbing is Professor of Sociology, in particular of Modeling and Simulation at ETH Zurich. Before, he worked as Managing Director of the Institute for Transport & Economics at Dresden University of Technology, where he was appointed full professor in 2000. Having studied Physics and Mathematics in Göttingen, his master thesis dealt with the nonlinear modelling and multi-agent simulation of observed self-organization phenomena in pedestrian crowds. Two years later, he finished his Ph.D. at Stuttgart University on modelling social interaction processes by means of game-theoretical approaches, stochastic methods and complex systems theory, which was awarded two research prizes. After having completed his habilitation on traffic dynamics and optimization in 1996, he received a Heisenberg scholarship. Both theses were printed by international publishers. Apart from this, Helbing has (co-)organized several international conferences and (co-)edited proceedings or special issues on material flows in networks and cooperative dynamics in socio-economic and traffic systems. He has given 250 talks and published more than 200 papers, including several contributions to journals like Nature, Science, or PNAS, which were discussed by the public media (newspapers, radio, and TV) more than 200 times. He collaborates closely with international scientists. For example, he worked at the Weizmann Institute in Israel, at Xerox PARC in Silicon Valley, at INRETS in Paris and the Collegium Budapest - Institute for Advanced Study in Hungary, where he is now a member of the external faculty.
3472 en EU Framework Program 7 Wolfgang Boch is working with the European Commission for more than 15 years on Information and Communication Technologies (ICT), within the EU Framework Programmes for Research. In January 2007, he has been appointed Head of Unit for "Future and Emerging Technologies – Proactive Initiatives" in the context of Framework Programme VII (2007–2013). FET-Proactive aims to nurture the roots of innovation in Europe. It supports long-term and foundational research in ICT, in particular, radical interdisciplinary and multi-disciplinary explorations (such as bio-nano-info-cogno) of new and alternative approaches towards the development of new scientific foundations and technological breakthroughs. The FET proactive Initiatives are granted by the European Union a total funding of 120 M€ for the years 2007–2008. The FET-scheme acts as the pathfinder and incubator for new ideas and themes for long-term research in the area of information and communication technologies. His background is in Electrical Engineering and Informatics with a focus on feedback and control Systems. He holds a Master's Degree in Electrical Engineering from the University of Karlsruhe, Germany. Prior to joining the European Commission he worked for 10 years in R&D in the German aerospace and avionics industry. In the European Commission he has held previous positions as Head of Unit on ICT research related to Grid Technologies and Telematics Applications for the Environment.
3473 en Universal Access to Human Knowledge (Or Public Access to Digital Materials) The goal of universal access to our cultural heritage is within our grasp. With current digital technology we can build comprehensive collections, and with digital networks we can make these available to students and scholars all over the world. The current challenge is establishing the roles, rights, and responsibilities of our libraries and archives in providing public access to this information. With these roles defined, our institutions will help fulfill this epic opportunity of our digital age.
3474 en Towards Structured Output Prediction of Enzyme Function 
3475 en Integration of genome-wide data to infer genetic networks To comprehend biology as a system, one needs to analyze the structure and dynamics of cell components as modules rather than isolated part. Progress in technological devices, analytical methods and biological models are required to decipher molecular networks and eventually analyze the cell as a system. Clustering analysis of gene expression profiles allows the analysis of “ correlation” between genes and biological conditions. However it is yet restrictive as it does not reveal the causality of regulatory relationships. Besides it is very difficult to infer molecular networks from expression profiling only, as the only accessible information is the steady-state concentration of mRNA. This information is necessary but not sufficient to characterize the structure of transcriptional network and analyze its dynamic and functional properties. Modeling of transcriptional networks should take into account information such as RNA concentrations, cis-acting sequences, transcriptional activity and so forth, since each variable carries unique biological information. However due to limitation in accurate and highly parallel measuring technologies, these data are not routinely accessible. We have developed innovative bioarrays to measure with sufficient accuracy, parallelism and throughput relevant data to infer transcriptional networks. For instance, we are manufacturing DNA array containing promoter regions (human) to perform ChIP on chip analysis in order to localize for a given transcription factor, all putative binding sites onto the genome. One can then return to DNA arrays to confirm hypothesis generated from ChIP on chip data. We are also developing cell microarrays to characterize, genome wide, upstream regulators for a given gene on one hand and transcriptional activity on the other hand. Technological breakthroughs in micro and nanotechnologies to generate comprehensive and relevant data are thus as critical as innovation in analytical methods for deciphering transcriptional regulatory networks and developing system biology.
3476 en Mixture model of regressions for ChIP-chip experiment analysis 
3477 en A Marginalized Variational Bayesian Approach to the Analysis of Array Data 
3478 en Identification of overlapping biclusters using probabilistic relational models 
3479 en Discovering Common Sequence Variation in Arabidopsis thaliana In order to characterize natural sequence variation in 20 strains of the model plant Arabidopsis thaliana, whole-genome resequencing with high-density oligonucleotide arrays was performed in collaboration with Perlegen Sciences Inc. Array data were analyzed with a combination of existing model-based (MB; Hinds et al., Science, 2005) and novel machine learning (ML) methods. For the identification of single nucleotide polymorphisms (SNPs) we developed an algorithm based on support vector machines. Training and evaluation was done on published alignments (Nordborg et al., PLoS Biology, 2005). At the same false discovery rates (FDR) as MB, the ML algorithm identifies significantly more true SNPs, especially in regions of high polymorphism density and/or low hybridization quality. The union of SNP predictions from both methods contains on average 143,572 SNPs per strain at a FDR of 2.8% (648,570 non-redundant SNPs). Furthermore, a machine learning algorithm was developed to detect polymorphic regions containing insertions, deletions and variational hotspots, where SNP detection algorithms typically fail to identify individual SNPs. It discovers the approximate location of a substantial additional proportion of polymorphisms (54% of deleted nucleotides and 33% of insertion sites). With a combination of all three methods 74% of SNPs can be directly called or are contained in a polymorphic region prediction (Zeller et al., in preparation). We examined the patterns of and forces shaping sequence variation in Arabidopsis (Clark et al., Science, 2007): e.g. significant differences were observed between gene families, and genes mediating interaction with the biotic environment harbor exceptional polymorphism levels.
3480 en Gene-based bin analysis of genome-wide association studies 
3481 en Supervised Attribute Relevance Determination for Protein Identification in Stress Experiments 
3482 en Need of Systems Approach for Biological Explanation of Anti-Learnable Signatures 
3487 en The Mathematics of Emergence and Flocking Stephen J. Smale, a professor emeritus of mathematics at the University of California, Berkeley, who has contributed to a broad range of mathematical fields, has been named a recipient of the 2007 Wolf Foundation Prize in Mathematics, one of an array of prestigious prizes awarded yearly by the Israeli foundation. Though retired from UC Berkeley since 1994, Smale continues to explore new fields, such as learning theory - the mathematical description of nerve connections in the brain that give rise to intelligence and learning; flocking, the tendency of group behavior to look coordinated, as with a flock of birds or a school of fish; and the mathematics of data mining. (UC Newsroom, University of Califonia, 2007-01- 19)
3488 en Principle of Systems Biology illustrated using the Virtual Heart Highest systems property:  “The living organism does not really exist in the milieu extérieur but in the liquid milieu intérieur … a complex organism should be looked upon as an assemblage of simple organisms … that live in the liquid milieu intérieur.”
3489 en The Origin of Rhythmic Behavior in Regulated Biological Systems 
3490 en Science mapping with asymmetric co-occurence analisys We propose new innovative methods in order to reconstruct paradigmatic fields thanks to simple statistics over a scientific content database. We first define an asymmetric paradigmatic proximity between concepts which provides hierarchical structure over the set of concepts. We propose to implement overlapping categorization to describe paradigmatic fields as sets of concepts that may have several different usage and introduce a 2D embedding to represent these sets in a structured way. This enables to have a micro, meso and macro scale approach to our set of concepts. Concepts can also be dynamically clustered providing a high-level description of the evolution of the paradigmatic fields. We apply our set of methods on a case study from the Complex Systems Community through the mapping of the dynamics of more than 400 Complex Systems Science concepts indexed in a database of of several millions of journal papers.
3491 en Slower is faster: Reduction of consensus times through social inertia in the Voter Model In this paper, we investigate the role of heterogeneity added to the Voter Model. In our model, voters are equipped with an individual inertia to change opinion, which depends on the persistence time of a voter’s current opinion. In the simplest case, inertia-values are linearly increasing with persistence time by a slope ?, but our findings are qualitatively valid for other monotonously increasing functions, too. Here, in contrast to the Voter Model, voters change their individual behavior over time and the system builds up heterogeneity. The unexpected outcome of this dynamics is a non-monotonous development of average consensus times TC on the slope ?. Up to a value ?c, TC decreases systematically with increasing ?, i.e. systems with higher average inertia reach the final attractor state faster. For slopes larger than ?c, consensus times increase and can exceed the reference time of the Voter Model. These results are obtained only by considering a heterogeneity of voters that evolves through the described ageing of the voters, as we find monotonously increasing consensus times in a control setting of homogeneous inertia values. In the paper, we present the dynamical equations for a simplified mean field model, that give insight into the complex dynamics leading to the observed slower-is-faster effect.
3492 en On the Relevance of Extremes vs. Means in Organization Science Scalability is a key element of complexity science. Many complex systems tend to be selfsimilar across levels—the same dynamics work at multiple levels. They are explained by scaling laws. Scalability results from what Mandelbrot calls fractal geometry. A cauliflower is an obvious example. Fractals often show Pareto distributions and are signified by power laws. Researchers find organization-related power laws in intrafirm decisions, consumer sales, salaries, size of firms, movie profits, director interlocks, biotech networks, and industrial districts, for example. Power laws signify Pareto distributions, which show “fat tails,” (nearly) infinite variance, unstable means, and unstable confidence intervals. Pareto distributions are alien to most quantitative organizational researchers, who are trained in Gaussian statistics and are trained to go to great lengths to configure their data to fit the requirements of linear regression, normal distributions, and related statistical methods. While normal distributions, and related current quantitative methods are still relevant for a significant portion of organizational research, power laws signify that Pareto distributions, fractals, and underlying scale-free theories are increasingly pervasive and valid characterizations of organizational dynamics. Where true, researchers ignoring power law effects risk drawing false conclusions and promulgating useless advice to practitioners. This because what is important to most managers are the extremes they face, not the averages. The implications for organization science, however, go beyond extreme events. Tools do not exist in a theoretical vacuum. The adoption of normal distribution statistics carries a heavy baggage of assumptions. Reliance on linearity, randomness, gradualism, and equilibrium influences how theories are built, how legitimacy is conferred, and how research questions are formulated. We begin with findings about 80 kinds of power laws. Then, we present sixteen scale-free theories that apply to organizations. Next, we discuss research implications. Then, we discuss implications in terms of the basic predictor function, y = f(x) + ?. How does basic thinking about prediction, data, statistics, and the error term have to change?
3493 en Visual Analysis of Controversy in User-generated Encyclopedias Wikipedia is a large and rapidly growing Web-based collaborative authoring environment, where anyone on the Internet can create, modify, and delete pages about encyclopedic topics. A remarkable property of some Wikipedia pages is that they are written by up to thousands of authors who may have contradicting opinions. We define a network of authors revising other authors and present methods for its visual analysis that yield interesting insight into the structures of controversy by highlighting the dominant authors of a page, the roles they play, and the alters they confront.
3494 en Network Structure of Folksonomies Folksonomies can be viewed as three mode graphs or as graphs made up of nodes (tags, users, resources) connected by hyper-edges. I shall report on some network statistical properties of a folksonomy graph based on data collected for the del.icio.us system. Moreover, by introducing a suitable distance between resources based on tag co-occurrence, I shall show that folksonomies embed a meaningful semantic clusterization of resources.
3495 en Social Web Search This talk will present two research projects under way in the Network and agents Network (NaN), which study ways of leveraging online social behavior for better Web search. GiveALink.org is a social bookmarking site where users donate their personal bookmarks. A search and recommendation engine is built from a similarity network derived from the hierarchical structure of bookmarks, aggregated across users. 6S is a distributed Web search engine based on an adaptive peer network. By learning about each other, peers can route queries through the network to efficiently reach knowledgeable nodes. The resulting peer network structures itself as a small world that uncovers semantic communities and outperforms centralized search engines.
3496 en Linked - But how? The growing interest in social sites in scientific research as well as in business has led to a sharp increase in the analysis of such sites. The tools and methods to technically analyze such sites are developed and are being enhanced. One major question though seems to be left mostly unattended: If one takes the relationship data in a site like e.g. MySpace, what is the ‘real’ interpretation of the social network analysis? There is no clear definition as to the meaning of a link. Is it frienship, acquaintance, awareness? When analyzing the dynamics of such a site, the question of time dependent relevance of links is added to this problem. In this talk these points will be adressed with the goal of bringing into the focus of the discussion the content driven interpretation of the analysis of social sites.
3497 en Cloasing & Discussion Discussion and wrap-up led by Bettina Hoser with the presenting participants of the Social Websites - Complex Dynamics and Structure Satellite Conference.
3498 en Complexity in Human Activity: Conflicts, Currencies and Crimes 
3500 en Opening 
3501 en Chorus Plans and Progress, Meeting Objectives - Part 1 
3502 en Chorus Plans and Progress, Meeting Objectives - Part 2 
3503 en Quaero (French governmental initiative) - Quaero … Ergo Sum Presentation of an industrial research program on Multimedia Search and Navigation - Part 1 
3504 en Quaero (French governmental initiative) - Quaero … Ergo Sum Presentation of an industrial research program on Multimedia Search and Navigation - Part 2 
3505 en THESEUS Project -Structure and Activitiesof Core Technology Cluster (CTC) 
3506 en THESEUS Status Report 
3507 en iAd (Norwegian Initiative) - Part 1 
3508 en iAd (Norwegian Initiative) - Part 2 
3509 en MultimediaN (Dutch Initiative) - Part 1 
3510 en MultimediaN (Dutch Initiative) - Part 2 
3511 en IM2 (Swiss Initiative) 
3512 en Mundo AV (Spanish Initiative) - Part 1 
3513 en Mundo AV (Spanish Initiative) - Part 2 
3514 en Mundo AV (Spanish Initiative) - Part 3 
3515 en Open Session - European National Initiatives: Challenges, communalities, difficulties, targeted/expected impact, success criteria? What could be done together? 
3516 en An Introduction to OWL Starting with an overview of the requirements for defining and using ontologies on the Web Sean will then outline the main concepts and issues associated with the Web ontology languaages, RDF, RDFS and OWL. By the end of this presentation attendees will have gained a basic understanding of the principles underlying OWL.
3517 en Ontology Engineering Methodologies Using the insights gained in a decade of research Asun will describe the main principles and phases underlying the construction of ontologies, including acquisition, conceptualization, evaluation and integration. By the end of this presentation attendees will have an overview of the main steps involved in creating an industrial strength ontology.
3518 en Ontology Engineering Design Patterns Building upon Asun's presentation here Aldo will focus on Ontology Design. In particular, Aldo will apply the software engineering notion of design patterns to the design of ontologies. This session will give attendees a number of insights into the differences between good and bad ontology design.
3519 en Semantic Web Services In this session John and David will first give an overview of the problem and vision for applying Semantic Web technologies to Web Services, then they will describe a number of the most important approaches and finally Semantic Web Service applications. By the end of this session attendees will gained an understanding of the main issues in Semantic Web Services and an overview of OWL-S, SAWSDL and WSMO.
3520 en Web Service Modelling Ontology In this session John and David will first give an overview of the problem and vision for applying Semantic Web technologies to Web Services, then they will describe a number of the most important approaches and finally Semantic Web Service applications. By the end of this session attendees will gained an understanding of the main issues in Semantic Web Services and an overview of OWL-S, SAWSDL and WSMO.
3521 en OWL-S 
3522 en SAWSDL 
3523 en Semantic Web Applications In the final session Enrico will first describe the Semantic Web as it exists today and then outline the main requirements and issues surrounding the creation of Semantic Web applications.
3525 en Dynamics of Real-world Networks In our recent work we found interesting and unintuitive patterns for time evolving networks, which change some of the basic assumptions that were made in the past. The main objective of observing the evolution patterns is to develop models that explain processes which govern the network evolution. Such models can then be fitted to real networks, and used to generate realistic graphs or give formal explanations about their properties. In addition, our work has a wide range of applications: we can spot anomalous graphs and outliers, design better graph sampling algorithms, forecast future graph structure and run simulations of network evolution. Another important aspect of this research is the study of "local" patterns and structures of propagation in networks. We aim to identify building blocks of the networks and find the patterns of influence that these block have on information or virus propagation over the network. Our recent work included the study of the spread of influence in a large person-to-person product recommendation network and its effect on purchases. We also model the propagation of information on the blogosphere, and propose algorithms to efficiently find influential nodes in the network. Further work will include three areas of research. We will continue investigating models for graph generation and evolution. Second, we will analyze large online communication networks and devise models on how user characteristics and geography relate to communication and network patterns. Third, we will extend the work on the propagation of influence in recommendation networks to blogs on the Web, studying how information spreads over the Web by finding influential blogs and analyzing their patterns of influence. ; : [[http://www.cs.cmu.edu/~jure/thesis/]] ; Thesis Committee: : Christos Faloutsos (Chair) : Avrim Blum : John Kleinberg (Cornell University) : John Lafferty
3532 en Structural adaptive smoothing: Images, fMRI and DWI The talk presents a class of structural adaptive smoothing methods developed at WIAS. The main focus will be on the Propagation-Separation (PS) approach proposed in Polzehl and Spokoiny (2006). The method allows to simultaneously identify regions of homogeneity with respect to a prescribed model (structural assumption) and to use this information to improve local estimates. This is achieved by an iterative procedure. The name Propagation-Separation is a synonym for the two main properties of the algorithms. In case of homogeneity, that is if the prescribed model holds with the same parameters within a large region, the algorithm essentially delivers a series of nonadaptive estimates with decreasing variance and propagates to the best estimate from this series. Separation means that, as soon as in two design points i and j significant differences are detected between estimates, observations in j will not be used to estimate the parameter in j. We establish some theoretical {nonasymptotic} results on properties of the new algorithm. We present how this approach can be adjusted to different imaging modalities, ranging from denoising of greyvalue and color images, to the analysis of data from functional Magnetic Resonance Imaging (fMRI) and Diffusion Weigted Imaging (DWI) experiments.
3534 en Computation of the MLE for bivariate interval censored data I will discuss the new R-package 'MLEcens', which computes the nonparametric maximum likelihood estimator (MLE) for the distribution function of bivariate interval censored data. The computation of the MLE consists of two steps: a parameter reduction step and an optimization step. I will discuss algorithms for both steps. I will also illustrate the R-package using several examples.
3536 en Stochastic chains with variable length memory and the algorithm Context Stochastic chains with variable length memory define an interesting family of stochastic chains of infinite order on a finite alphabet. The idea is that for each past, only a finite suffix of the past, called "context", is enough to predict the next symbol. The set of contexts can be represented by a rooted tree with finite labeled branches. The law of the chain is characterized by its tree of contexts and by an associated family of transition probabilities indexed by the tree. These models were first introduced in the information theory literature by Rissanen (1983) as an universal tool to perform data compression. Recently, they have been used to model up scientific data in areas as different as biology, linguistics and music. Originally called "finite memory source" or "tree machines", these models became quite popular in the statistics literature under the name of "Variable Length Markov Chains" coined by Buhlmann and Wyner (1999). In my talk I will present some of the basic ideas, problems and examples of application in the field. I will focus on the algorithm Context which estimates the tree of contexts and the associated family of transition probabilities defining the chain.
3541 en Denoising and Dimension Reduction in Feature Space The talk presents recent work that interestingly complements our understanding the VC picture in kernel based learning. Our finding is that the relevant information of a supervised learning problem is contained up to negligible error in a finite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved using only linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efficient implicit representations of the data for supervised learning problems. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classification. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to aid in model selection, and to (3) denoise in feature space in order to yield better classification results. We complement our theoretical findings by reporting on applications of our method to data from gene finding and brain computer interfacing. This is joint work with Claudia Sannelli and Joachim M. Buhmann
3542 en Wedgelet Partitions and Image Processing In many applications of Image Processing it is crucial to dispose of efficient tools for extraction, analysis and representation of geometrical contents in natural images. These latter can be modelled by classes of bivariate functions, regular on a finite number of regions separated by smooth boundaries. It is by now a well-established fact that the usual two-dimensional tensor product wavelet bases are not optimal for approximating such classes. In the last ten years, several methods have been suggested as a remedy. Among them, wedgelets representations over quadtree structures represent a contour-based approach which allows an efficient digital implementation while capturing mainly geometric features of natural images. We discuss some algorithmic aspects due to the discrete nature of the method, leading to a fast computation of optimal solutions. As a possible application we present a new scheme for digital image compression based on these methods. The main ingredient for the design of an efficient coding scheme is to consider spatial redundancies between neighbouring atoms of the representation, relatively to the properties of the target regularity class. Joint work with Mattia Fedrigo, Felix Friedrich and Hartmut Führ.
3543 en Complex Systems Art Performance The tasks of the artist in between this self-organizing process is firstly in the preparation of the used objects which are mostly liquids. The objects are put into physical relation in certain amounts and in certain configurations. This is equivalent to the initial state of the self-organizing. Proceeding from this initial state develops the process of creating a form, while the artist can influence the self-organization by regulations, like for example adding new objects or physical reconfigurations of existing objects.
3544 en The Foundations of the Economics of Innovation During the last forty years, economics of innovation has emerged as a distinct area of enquiry at the crossing of the economics of growth, industrial organization, regional economics and the theory of the firm, becoming a well identified area of competence in economics specializing not only in the analysis of the effects of the introduction of new technologies, but also and mainly in understanding technological change as an endogenous process. As the result of the interpretation, elaboration and evolution of different fields of analysis in economic theory, innovation is viewed as a complex, path dependent process characterized by the interdependence and interaction of a variety of heterogeneous agents, able to learn and react creatively with subjective and procedural rationality.
3545 en Landscape Dynamics of El Farol Attendies Arthur’s paradigm of the El Farol bar for modeling bounded rationality and inductive behavior is undertaken. The memory horizon available to the agents and the selection criteria they utilize for the prediction algorithm are the two essential variables identified to represent the heterogeneity of agent strategies. The latter is enriched by including various rewarding schemes during decision making. Though the external input of comfort level is not explicitly coded in the algorithm pool, it contributes to each agent’s decision process. Playing with the essential variables, one can maneuver the overall outcome between the comfort level and the endogenously identified limiting state. The distribution of algorithm clusters significantly varies for shorter agent memories. This in turn affects the long-term aggregated dynamics of attendances. We observe that a transition occurs in the attendance distribution at the critical memory horizon where the correlations of the attendance deviations take longer time to decay to zero. A larger part of the crowd becomes more comfortable while the rest of the bar-goers still feel the congestion for long memories. Agents’ confidence on their algorithms and the delayed feedback of attendance data increase the overall collectivity of the system behavior.
3546 en Interview with Mark Newman Professor of Physics at the University of Michigan. Holds a joint appointment in the Department of Physics and the Center for the Study of Complex Systems. Also a member of the External Faculty of the Santa Fe Institute. Research interests: Networks and graph theory; Phase transitions and critical phenomena; Monte Carlo methods; Glassy spin systems; Prehistoric evolution and extinction.
3547 en Interview with Stephan Mertens Assistant professor (Privatdozent) for theoretical physics at Otto-von-Guericke University. Also external professor at the Santa Fe Institute. Current active fields of research: Random Number Generation; Computational Complexity and Statistical Mechanics; Low autocorrelated binary sequences.
3548 en Interview with Armando Bazzani Professor of Mathematical Physics, from the Department of Physics, at the University of Bologna. Interested in application of physical method to social system, especially to mobility problem in new cities.
3549 en Web Spam Detection Web spam can significantly deteriorate the quality of search engine results. Thus there is a large incentive for commercial search engines to detect spam pages efficiently and accurately. This talk presents spam detection systems that combine link-based and content-based features, and use the topology of the Web graph by exploiting the link dependencies among the Web pages.
3550 en Machine Learning for Intrusion Detection Intrusion detection is one of core technologies of computer security. The goal of intrusion detection goal is identication of malicious activity in a stream of monitored data which can be network trac, operating system events or log entries. A majority of current intrusion detection systems (IDS) follows a signature-based approach in which, similar to virus scanners, events are detected that match specic pre-dened patterns known as \signatures". The main limitation of signature-based IDS is their failure to identify novel attacks, and sometimes even minor variations of known patterns. Besides, a signicant administrative overhead is incurred by the need to maintain signature databases. Machine learning oers a major opportunity to improve quality and to facilitate administration of IDS. Supervised learning can be used for automatic generation of detectors without a need to manually dene and update signatures. Anomaly detection and other unsupervised learning techniques can detect new kinds of attacks provided they exhibit unusual character in some feature space. In our contribution, kernel and distance based learning algorithms for network intrusion detection will be presented. The two essential parts of our approach are online learning algorithms and feature extraction. The major requirements on the algorithmic part are linear run-time, online learning and data type abstraction. Simple but eective anomaly detection algorithms will be presented that satisfy these requirements (1). Feature extraction algorithms can be reduced to computation of similarity measures between sequential objects. In order to access the feature from the application-layer network protocols, in which most of modern remote exploits operate, similarity measures are computed directly over byte streams of TCP connections. Algorithms and data structures will be presented that allow e- cient computation of similarity measures in linear time with very low run-time constants and memory consumption (2)
3551 en Learning with structured data - structured outputs We focus on the prediction of structured outputs. A classical example is sequence labeling with applications in speech, vision, natural language or biology. Beyond sequences, the prediction of structured data, like trees, lattices or graphs also occurs in many domains. Structured prediction is usually considered as an extension of multi-class classification. It is considered as a challenging problem since the size of the output space increases drastically with the number of potential dependencies between output variables. Several methods have been recently proposed in the ML community in order to overcome this complexity and the domain is still largely open. We will provide a review of these methods and discuss there potential and limitations. These different ideas will be illustrated with Natural language processing and text mining applications.
3552 en Automatic detection and aggregation of name variants from large multi-lingual document collections Most of the Named Entity Recognition software will recognize Vladimir Putin, Wladimir Putin and Vladimir Poutine as being named entities. But, for some application, it is necessary to mark them as being all variants of the same person. In the Joint Research Centre we try to handle this problem of merging name variants that we gathered in a News corpus as part of the Europe Media Monitor (EMM) system. Such highly multi lingual system (35,000 news article per day in more than 30 languages) is quite likely to use various spellings to refer to the same person. As example, during 15 hours EMM found 10 variants referring to Abdullah Gul (Turkish foreign minister): Abdullah Gül / Abdullah Gul / ???????? ??? / Abdulá Gül / Abdullah Guel / Abdullah Gulas / ?????? ??? / ?? ??????? / ??? ???? ??? / ????????? ???. Our approach consists of extracting names from multilingual corpus, and then to merge very similar names in our repository. The various steps are: - guessing new names using language specific light resources - storing names in a repository - lookup for known names. Including some variants for languages that decline proper names. For example in Polish we can face the following sentence containing a declension of the proper name Tony Blair: Brown tymczasem wybra? skromne wakacje w ojczy?nie chc?c wyra?nie odró?ni? si? od Tony’ego Blaira. - compute similarity of names, including names written in different alphabets in order to merge two variants as belonging to the same person. In NewsExplorer the system automatically add 450 new names per day, 10% are automatically recognized as being variant of existing person names, 9% are possible variants to be validated by an expert. Those figures highlight the importance of such system when dealing with multi-lingual information. Various collected variants of person names gathered are available on our public website: http://press.jrc.it/NewsExplorer
3553 en Predicting the Outcome of a Game Optimization of many complex systems is often viewed as a black-box optimization problem. Such problems are often difficult to solve using conventional techniques, for a variety of reasons, such as the absence of derivatives, mixed data types, and so on. Techniques such as Genetic Algorithms, Estimation of Distribution Algorithms such as MIMIC and the CE method, and more recently, mathematically rigorous approaches such as Probability Collectives have been used for black-box optimization. It turns out that many of these techniques fall under the category of Monte Carlo Optimization. In this technique, we present a brief statistical analysis of Monte Carlo Optimization (MCO), and show that it is identical to Parametric Machine Learning (PL). Owing to this identity, we can use PL techniques to improve the performance of MCO. Then, we present a new version of the black-box optimization technique of Probability Collectives., and demonstrate the use of PL techniques to improve its optimization performance.
3554 en Repair Strategies for Minimizing the Risk of Cascading Failures in Electricity Networks In industrialized countries, a reliable supply of electrical energy is taken for granted. In order to guarantee a stable energy supply also in case of non-serious failures of electrical equipment, the underlying electric power network has been designed with redundancies. Nevertheless, major blackouts of the transmission grid occur all over the world. They are typically caused by a sequence of cascading failures and may be evidence of a critically loaded transmission system. Furthermore, the increasing trade in electricity { a consequence of the liberalization of the energy markets { has led to an additional load of the existing infrastructure. As a consequence, the reliable operation and maintenance of the electric power grid at minimum cost is an increasingly demanding task. The goal of our work is to develop repair strategies that minimize the risk of cascading failures. A power operator has generally not enough time to repair failed lines once a cascade has started, because cascading failures typically evolve in time scales of seconds and minutes. Consequently, we focus on repair strategies during normal operation. Even during normal operation, there are typically some lines that are not operating due to random failures or maintenance work. While the system may still have enough capacity to transmit the power demand, the average loading, and therefore the blackout risk, increases with every failed line.
3555 en Evolution of the Topology of High-voltage Electricity Network The electricity network represents an example of an evolving complex system. The first local network consisted of only few components, but within several decades they have evolved into a highly connected continental system. The growth of the network is influenced by various factors such as economy, demography, politics and technological developments. In this paper we follow the growth of the French electricity transmission network from its establishment in 1960 until the year 2000. The data set contains 21 maps (for different years) of the 400KV power lines. We used this data to extract information related to the network's growth rate such as the number of lines, number of lines intersections (nodes) and line's length in each map. Firstly we compare the results data sets with several economic and demographic indicators in order to indentify factors which correlate with the growth rate of the electricity networks. We further eveluate topological properties such as node connectivity degree, information centrality, betweenness centrality and robustness and observe their respective change in the course of time. Our result might be helpful for more efficient and fault tolerant network design.
3556 en Chromodynamics of Cooperation in Finite Populations In tag-based models for cooperation individuals recognize each other via arbitrary signals, so-called tags. Cooperators use a tag until they are discovered by defectors, who then destroy cooperation based on this tag by exploiting cooperators. If many tags are available, then cooperators can always establish new signals of recognition. This leads to a constant chase: Cooperators establish new signals for cooperation, while defectors constantly try to find out about these signals. Tag based cooperation is intimately related to the green beard effect. While conventional wisdom leads to the breakdown of cooperation based on such green beards, it can be shown that green beards can lead to cooperation if they are worn by red queens.
3557 en Equilibrium Transitions in Stochastic Evolutionary Games We analyze the long-run behaviour of stochastic dynamics in well-mixed populations and in spatial games with local interactions. We review results concerning the effect of the number of players and the noise level on the stochastic stability of Nash equilibria. To address the problem of equilibrium selection in spatial games with many players, we introduce a concept of ensemble stability. The standard stochastic stability describes a long-run behaviour of systems with a fixed number of players in the zero-noise limit. On the contrary, the ensemble stability is concerned with a fixed (but nevertheless low) noise level in the limit of the infinite number of players. We present examples of games in which when the number of players increases or the noise level decreases, a population undergoes a transition between its equilibria. In particular, it may happen that a risk-dominant and Pareto-efficient strategy, which is stochastically stable, in the long run is played with an arbitrarily small probability if the noise level is low and the number of players is big enough.
3558 en Mobility Promotes and Jeopardizes Biodiversity in Rock-Paper-Scissors Games Counterintuitive to a naive understanding of Darwinian evolution, where among two interacting species one is expected to be fitter than the other and therefore outcompetes it, a surprising biodiversity exists within the earth's ecosystems. Rock-paper-scissors games, where three strategies cyclically dominate each other, have emerged as a fruitful metaphor for the explanation of biodiversity. In this talk we discuss populations spatially coevolving with local cyclic dominance, and show that they are capable of preserving coexistence of all subpopulations, and in this way ensuring biodiversity. We find that the individuals' mobility competes with the locality of interactions (cyclic dominance) such that biodiversity gets lost above a certain mobility threshold. Below this critical value, all subpopulations coexist forming fascinating moving patterns composed of entangled spirals, which we describe analytically.
3559 en Filtering Multi-Lingual Terrorist Content with Graph-Theoretic Classifi-cation Tools Since the web is increasingly used by terrorist organizations, the ability to automatically detect multi-lingual terrorist-related content is extremely important. In this talk, we present an efficient detection methodology based on the recently developed graph-based web document representation models. Evaluation is performed on corpora in English and Arabic languages.
3560 en Recognition and Disambiguation of geographical references in text We present an approach to recognise geographical references in multilingual documents and to disambiguate homographic place names. The purpose of this work is to provide users with meta-information about documents (or whole collections) and to present their geographical coverage visually. The approach has been implemented for sixteen languages and is a fully operational part of the online news analysis system NewsExplorer (http://press.jrc.it/NewsExplorer) and further JRC applications.
3561 en Cooperation in Diffusive Spatial Games Random diffusion is shown to be an important mechanism on fostering cooperative behavior among simple agents (memoryless, unconditional cooperators or defectors) living on a spatially structured environment. In particular, under the Prisoner's Dilemma framework, when allowing the agents to move with the simple ``always-move'' rule, we find that cooperative behavior is not only possible but may even be enhanced. In addition, for a broad range of densities, mobile cooperators can more easily invade a population of mobile defectors, when compared with the fully viscous, immobile case. Thus, such simple mobility pattern may have played a fundamental role both in the onset and development of cooperative behavior, paving the way to more complex, individual and group, motility rules.
3562 en Using linguistic information as features for text categorization We report on some experiences using linguistic information as additional features in a classical Vector Space Model[10]. Extracted information of every word like the Part Of Speech and stem, lexical root have been combined in different ways for experimenting on a possible improvement in the classification performance and on several algorithms, like SVM [3], BBR [] and PLAUM [6]. Automatic Text Classification, or Automatic Text Categorization as is also known, tries to related documents to predefined set of classes. Extensive research has been carried out on this subject [11] and a wide range of techniques are appliable to solve this task: feature extraction [5], feature weighting, dimensionality reduction [4], machine learning algorithms and more. Besides, the classification task can be either binary (one out of two possible classes to select), multi-class (one out of set of possible classes) or multi-label (a set of classes from a larger set of potential candidates). In most cases, the latter two can be reduced to binary decisions [1], as the used algorithm does in our experiments [8]. In order to verify the contribution of the new features, we have combined them to be included into the vector space model by preprocessing the Reuters- 215781 collection, a well known set of data by the research community devoted to text categorization problems [2].
3563 en Hierarchical Meanfield Theory of Evolutionary Games on Structured Populations Evolutionary games played out in populations structured by spatial embedding or more general networks of interaction have been shown to have fundamentally different dynamics and outcomes compared to those taking place in well mixed ones. Recent experimental and theoretical work - published largely in interdisciplinary journals such as Nature and PNAS - has demonstrated that longstanding open problems in biology, sociology, and the economic sciences (ranging from the maintenance of diversity to the evolution of altruism and reciprocity) can only be understood if we look beyond well mixed populations and take into account the effects of spatial structure. The question of how one goes about choosing the relevant model to describe population structures, however, stands unanswered. Models where individuals are confined to the sites of some lattice or the nodes of some random graph have proved highly sensitive to seemingly minor changes in the implementation of the dynamics and the details of the underlying topology of interactions. In our paper we introduce a minimal model of population structure that is described by two distinct hierarchical levels of interaction. We derive the dynamics governing the evolution of such a system starting from fundamental individual level stochastic processes and find that the simple and straightforward hierarchical application of the mean-field approximation (the assumption of being well mixed) at both levels surprisingly unveils a new level of dynamical complexity. We believe that such minimal structure is more relevant in a wide range of natural systems, than more subtle setups with a delicate dependence on the details and symmetries of the model. We show that such minimal structure is sufficient for the emergence of effects previously only observed for explicit spatial embedding and demonstrate the potential of our model to identify robust effects of population structure on the dynamics and outcome of evolutionary games.
3569 en The Evolution of Cooperation: Simple Games and Complex Societies 
3570 en Social Networks from the Perspective of Physics 'In the history of public speaking, there have been many famous denials. One sunny day in 1880, Karl Marx declared: 'I am not a Marxist'. On a less auspicious occasion in 1973, Richard Nixon insisted 'I am not a crook'. Neither Marx’ nor Nixon’s audience gave much credence to their denials, and you too may respond with disbelief when I tell you that 'I am not a networker'. (Marc Granovetter, Connections, 1990) ... 'Instead, the slogan of the day will be 'We are all networkers now'.
3571 en Efficiency and Stability of Evolving Innovation Networks We present a model of network evolution in which the formation/deletion of links is coupled with the dynamics of the state variable of the nodes of the network. The model is originally intended to describe an economic network of agents engaged in knowledge production through knowledge exchange, although the framework can be generalized to other evolving networks. With respect to previous works, we improve by investigating how rich structures can emerge or collapse, depending on the individual rule of link formation/deletion used by selfish and boundedly rational agents. We characterize the emerging network topologies in terms of their efficiency and stability. The model reproduces qualitatively the main stylized facts of innovation networks, namely that they are sparse, locally dense and heterogeneous in degree.
3572 en Academic Employment Networks and Departmental Rank Academic prestige rankings are stagnant over time despite departmental changes. Departments might maintain their prestige through hiring and training patterns. This paper uses data from professors' employment histories as of January 2007 to define a retrospective employment network and uses that network to describe the relationship between departments' centrality in the hiringnetwork and academic prestige. This paper tests whether that relationship depends on whether or not we consider PhD training as part of the network and on department size. In addition, the analysis shows that the relationship is robust to variations in graph definitions and measurement techniques.
3573 en Socioeconomic Networks with Long-range Interactions In well networked communities, information is often shared informally among an individual's direct and indirect acquaintances. Here we study a model previously proposed by Jackson and Wolinsky to account for communicating information and allocating goods in socioeconomic networks. The model defines a utility function of node i which is a weighted sum of contributions from all nodes accessible from i. First, we show that scale-free networks are more efficient than Poisson networks for the range of average degree typically found in real world networks. We then study an evolving network mechanism where new nodes attach to existing ones preferentially by utility. We found the presence of three regimes: scale-free (rich-get-richer), fit-get-rich, and Poisson degree distribution. The fit-get-rich regime is characterized by a decrease in average path length.
3574 en Efficient Simulation of Complex Reaction Networks Complex reaction networks commonly appear in natural systems. Some examples are chemical, ecological, metabolic and gene-expression networks.These networks can be described by graphs, in which the nodesrepresent reactive species and the edges represent reactions.Computer simulations of these networks are commonly done using rate equations,which provide the time dependent concentrations of thereactive species as well as the reaction rates.These equations are easy to construct and efficient to integrate numerically. However, they are applicable only whenthe populations of reactive species are large andfluctuations are negligible.In case that the reactive species appear in low copy numbers,stochastic methods, based on the master equation, are needed.However, the master equation is not feasible for complex networks,because the number of equations proliferatesexponentially with the number of reactive species.Here we present a new computational method, based on moment equations, which dramatically reduces the number of equations.It enables to efficiently simulate fluctuating reactionnetworks of any level of complexity.The method requires only one equation for each reactive speciesand one equation for each reaction,thus reducing the number of equtaions to the absolute minimum for a stochastic simulation. The reduction is achieved with no compromise in the accuracy of the results.
3575 en Hierarchical Analysis of Piecewise Affine Models of Gene Regulatory Networks We propose in this paper a method to hierarchically organize a certain type of piecewise affine differential system. This specific class of dynamical systems has been extensively studied for the past few years, as it provides a good framework to model gene regulatory networks. Using the hierarchical organization of a piecewise affine system, we present a technique to qualitatively analyze the asymptotic behavior of the whole system thanks to the analysis of several smaller subsystems. Specifically adapted to these networks, an algorithm of threshold elimination is presented, that refines in certain cases the hierarchical decomposition and therefore improves the analysis.
3578 en Information Transfer in Moving Animal Groups The movement of animal flocks give us one of the clearest examples of the concept of 'complexity'. Simple interactions between animals lead to patterns that are somehow regular but at the same time difficult to characterise. In this paper we discuss first these models and their predictions about the movement of flocks. We provide novel results about how information is transfered in these systems. We look at recent experiments on locusts and pigeons which show that at least some of the patterns seen in these groups can be explained by the phase transitions and bifurcations that arise from these models. In particular, we look at a phase transition in the marching of locusts and symmetry breaking in the decision-making of pigeons.
3580 en Honeybees Moving Home - The Effect of Swarm Size on Decision-making Honeybee swarms are faced with a challenging task: how to decide on a new home. The way in which such a decision is made is a prime example of decentralised decision-making: only a small subset of bees in the swarm are involved in the process, each acting upon local information. Previous work has used individual-based simulations to unravel how the behaviour of the bees engaged in decision-making results in the swarm collectively choosing the best possible nest site. Here we explore how the size of the swarm affects the accuracy of the decision-making process when the presence of a certain number of bees at one nest site (quorum) is used to end the deliberations. As soon as a quorum is reached bees involved in the decision-making process indicate to the other bees on the swarm that a decision has been made and the swarm will prepare for lift-off. Assuming a fixed quorum size, we show that the larger the swarm, the less likely it will be able to select the best nest site as the quorum will be reached early in the process. We also show that the speed of the decision-making process is reduced in small swarms especially when only sites of mediocre quality have been discovered. Our results therefore indicate that large swarms will often make sub-optimal decisions while small swarms will require more time to select a nest site but that small swarms select, on average, better quality sites.
3581 en Individual-based Model of Bacterial Mobility in Biofilms We propose a new individual-based model of mobile bacteria producing a biofilm. The model is based on the assumption that bacteria have a brownian motion and produce a polymeric substance that reduce their mobility by increasing local viscosity. We define the biofilm as the polymeric substance together with the bacteria it contains. Our simulation results show that bacteria mobility increases the heterogeneity and the complexity of the biofilm (that we evaluate with an entropy measure). The obtained biofilm patterns show good qualitative agreement with experimental observation of biofilm micrographs.
3582 en Firewalls in Atrial Myocytes Atrial myocoytes play a prominent role in the generation of heart beats. Their contraction is controlled by Calcium signals that emerge at the cellular periphery and then proceed centripetally to engage the force-generating myofilaments. Experiments have demonstrated that these initial signals need to overcome a barrier just below the cell membrane before they move inward. Since atrial myoctes lack transverse tubules that transmit external signals to the cell interior as e.g. in ventricular myocytes, such a firewall represents a crucial determinant of atrial dynamics. For instances, it allows atrial myocytes to fine tune their responses to a wide range of vital stimuli. Here, we present a computationally advantageous model to investigate the mechanisms that give rise to these graded centripetal signals. Our framework takes into account the three dimensional organisation of atrial myocytes, especially the spatially restricted release of Calcium from internal storage compartments. We employ a fire-diffuse-fire (FDF) model to examine the spatio-temporal patterns and to probe the dependence of wave propagation on physiologically relevant parameters. Mimicking an excitable medium, the FDF approach reflects the significance of noise in intracellullar Calcium dynamics. The explicit construction of the corresponding Green's function allows for a detailed analysis.
3583 en Endocytosis and Signaling: Competition of Rab Proteins Endocytosis and signaling are tightly linked. A dynamic network of subcellular compartments, termed endosomes, actively controls signal propagation, amplitude and timing by uptake and transport of membrane-associated signaling molecules. The small GTPase Rab5 is the central organizer of the protein machinery that assembles on early endosomes, providing a unique identity to the compartment that controls cargo sorting, transport and signaling potential. We interlink experimental and computational approaches to (I) unravel the design principles underlying the function and regulation of the endocytic pathway and to (II) understand the molecular links that govern endosomal control of signaling events. Our approach takes into account both newly identified Rab5 effectors as well as the perspective of an on-going genome-wide RNAi screening for novel endocytic transport regulators. We develop and analyze a mathematical core model of the kinetic interactions between the endosomal organizers Rab4, Rab5 and Rab7. The values of the model parameters are estimated from literature and FRAP experiments. Simulations and bifurcation analysis of the model reveal bistable behavior of Rab5 versus Rab7 dominance that reproduces the controled and robust transition from early to late endosomal compartments as observed by means of computational motion tracking of individual endosomes in fluorescence live cell imaging data.The modeled Rab5 module serves as a blueprint for the sequential assembly of the endocytic pathway, therewith contributing an important spatial aspect to systems biology. Our kinetic model explains the experimentally observed transitions between endosomal compartments as the result of cargo-regulated competition between Rab proteins.
3584 en Ventricular Fibrillation in the Human Heart. Why is it different from Fibrillation in the Dog and Pig Heart? Sudden cardiac death is one of the major health problems in the industrialised world, leading to over 300,000 mortalities in the US alone annually. In most cases, it is caused by a cardiac arrhythmia called ventricular fibrillation (VF). Under normal conditions, the coordinated contraction of the heart leads to an effective pumping of blood through the body. In contrast, during fibrillation coordination of contraction is completely lost, rendering the heart incapable of pumping around blood. Despite the huge socio-economical costs of VF and decades of research its causes and mechanisms still remainpoorly understood. In experimental studies into the mechanisms of VF, pig and dog hearts are considered the best model systems for the human heart given their comparable size. In such studies it is found that fibrillation is caused by highly disorganised electrical wave patterns consisting of 50 or more rotating spiral waves. It has been assumed that a similar organisation underlieshuman VF. However, recent clinical studies suggest that fibrillation inthe human heart may have a far more simple organisation.Modelling studies have played an important role and are playing an increasingly important role in cardiac arrhythmia research from the single ion channel to the whole heart level. However, on the whole heart level, most modelling studies thus far have used phenomenological models or small heart animal models to obtain qualitative insights in VF mechanisms and patterns. Instead, we use a detailed model of the human ventricles, to quantitatively study human VF and why it might be different from VF in the pig and dog heart. We indeed find that human VF has a significantly simpler organisation than VF in the pig and dog heart, with wave patterns consisting of around 10 spiral waves only. We then study the dependence of VF wave pattern complexity on various major parameters of our model (excitability, anisotropy, action potential duration (APD) restitution slope, minimum APD). We find that VF wave pattern complexity is most strongly dependent on minimum APD, a factor that is found to differ between human and pig and dog hearts. We thus propose that differences in minimum action potential duration cause the differences in wave pattern complexity during VF in the human and pig and dog hearts. Both the simpler spatial organisation of human VF and it's suggested cause may have important implications for treating and preventing this dangerous arrhythmia in humans.
3585 en Information Transfer in Calcium Signal Transduction Calcium ions act as second messenger in many cell types. They transfer extracellular signals (e.g. from hormones) to targets within the cell, like Ca2+-dependent enzymes or transcription factors. Since a number of different effectors and cellular targets exist, it has been suggested, that specific information is encoded in the amplitude, frequency and waveform of the Ca-signal and decoded again, later on, by cellular targets. After stimulation, the calcium concentration in the cytosol of hepatocytes, for example, can display complex dynamic behavior including spiking and bursting oscillations. Using the information-theoretic measure Transfer Entropy (Schreiber 2000) we studied the properties of this signal transduction under different conditions. Therefore, we coupled a simple Ca2+-dependent enzyme activation process to a model of calcium oscillations (Kummer 2000) and to experimentally measured calcium time series. We simulated the system stochastically to account for random fluctuations in the case of low particle numbers. To approximate the rate of information transfer we analyzed the resulting time series for different levels of activation and different numbers of particles using kernel density estimation.
3586 en Population-based adaptive Systems: An Implementation in NEW TIES In this paper we introduce the notion of Population-based Adaptive Systems (PAS)with 3-tier adaptation by evolutionary, individual, and social learning (EL, IL, SL).We discuss some important aspects of using combinations of these adaptivemechanisms in general. We pay special attention to using natural reproductionin EL and the consequences of this for EL+IL combinations. We also presentdetails on the NEW TIES system, partly as a running example, partly forits own sake as a concrete realisation of a 3-tier PAS that is available asa research platfrom for experimental studies.
3587 en Evolutionary Classification of Toxin Mediated Interactions The rock scissors paper game gives atemplate for the structural dynamics of toxin induced oscillations withinbacteria. The first player is the sensitive which is toxified by thesecond, the producer, bearing the metabolic costs for resistance and toxinproduction. The producer is in turn again out-competed by the Resistant, who has no metabolic costs for toxin production. At least the Sensitivewins again, because he has no expenses for resistance. This leads to acoexistive/cyclic dynamics in the population-density of these threespecies. We derive here a scenario for the evolutionary dynamics of three speciesinvolved in this RSP - game. Starting from basic biologicalprinciples we determine evolutionary stable states or pathways in the traitspace of bacteria. For the bacteriocin producing bacteria sample mechanisms aredemonstrated. We derive functional relations between the parameters ofour model necessary for evolutionary dynamics. The process usesadaptive dynamics and we analyze the stability type of the obtained singular point. We investigate this behaviorwith respect to the toxicity of the producer and the yield/intrinsic growthspeed relation of each species. The result can be generalizedto all kinds of toxin or disease interactions of three species. As targetdynamics we get Zeeman class 33, which is permanent and guarantees stablecoexistence of three species by population dynamics and also with respect toadaptive evolution. This implies that all toxin mediated interactions tend toa stable coexistive fixed point given reasonable biological relations betweenthe parameters.
3588 en Cooperation in Social Dilemmas The emergence and maintenance of cooperative behavior that is beneficial to others but costly to the individual represents a major conundrum in evolutionary biology. Punishment represents an efficient mechanism to stabilize and maintain cooperation in social dilemmas and is ubiquitous in animal and human societies - ranging from toxin producing microorganisms to law enforcement institutions - but it remains unresolved how initially rare and costly punishment behavior can gain a foothold and spread through the population. In nature, animals and humans often carefully select their interaction partners or adjust their behavioral patterns in response to them. In the simplest case they simply refuse to participate in risky enterprises. Such voluntary participation in social dilemmas is an efficient mechanism to prevent deadlocks in states of mutual defection and thus represents a potent promoter of cooperation but fails to stabilize it. However, the combined efforts of punishment and volunteering may change the odds in favor of cooperation. Interestingly, even though the combined efforts fail in infinite populations they nevertheless provide an efficient mechanism to stabilize cooperation (and punishment) under the stochastic dynamics of finite populations with mutation and selection. Thus, the freedom to withdraw leads to prosocial coercion. This implements Hardin's principle: ``mutual coercion mutually [and voluntarily] agreed upon''.
3589 en Evolutionary Dynamics in Finite Populations: Oscillations, Diffusion, and Drift Reversal Coevolutionary dynamics arises in a wide range from biological to social dynamical systems. For infinite populations, a standard approach to analyze the dynamics are deterministic replicator equations, however lacking a systematic derivation. In finite populations modelling finite-size stochasticity by Gaussian noise is not in general warranted [1]. We show that for the evolutionary Moran process and a Local update process, the explicit limit of infinite populations leads to the adjusted or the standard replicator dynamics, respectively [2]. In addition, the first-order corrections in the population size are given by the finite-size update stochasticity and can be derived as a generalized diffusion term of a Fokker-Planck equation [2]. This framework can be readily transferred to other microscopic processes, as the local Fermi process [3] or the inclusion of mutations in the process [4]. We explicitely discuss the differences for the Prisoner's Dilemma, and Dawkin's Battle of the Sexes, where we show that the stochastic update fluctuations in the Moran process lead to a finite-size dependent drift reversal [2]. [1] J.C. Claussen and A. Traulsen, Phys. Rev. E 71, 025101(R) (2005); [2] A. Traulsen, J.C. Claussen, C. Hauert, Phys. Rev. Lett, 95, 238701; [3] A.Traulsen, M.A.Nowak, J.M.Pacheco, Phys. Rev. E 74, 011909 (2006); [4] A. Traulsen, J.C. Claussen, C. Hauert, Phys. Rev. E 74, 011901 (2006).
3590 en Combined Problems of Cooperation and Coordination In game theory, much attention has been paid to symmetrical 2-players games with binary decisions of the players. Within this frame, questions of social cooperation and social dilemmas have mostly been attached to investigations of the Prisoner's Dilemma (PD) with T > R > P > S and 2R > T + S. In this context, the readiness of individuals to resist the temptation to defect is studied in various settings. These investigations aim at explaining the origin and stability of cooperation among selfish individuals. But what if the readiness to resist temptation is not enough to reach a desired outcome? Maybe there are more than one desired solutions and the individuals additionally have to coordinate their actions to realize one of them. In this work, I focus on game theoretical conflicts that exhibit a combination of cooperation and coordination problems in the same game. Examples are (i) the Turn-Taking Dilemma (Neill, 2003) and (ii) the Route Choice Game (Helbing et al., 2005; Stark et al., 2007). The first one, (i), is similar to the above described PD, but the second inequality is reversed to T + S > 2R. The Pareto-inefficient equilibrium, and, thereby, the cooperation dilemma remains the same, but the system optimal solution (maximal cumulative payoff) is shifted to the off diagonal of the bimatrix. When considering an iterated game, this leads to a non-trivial, temporal coordination problem as flipping between the upper right and the lower left solutions of the bimatrix would lead to the only Pareto-efficient solution of the supergame. The latter point also holds for the Route Choice Game with T > P > S > R and T + S > 2P, that represents the problem of efficient usage of networks with capacity-restricted links (traffic networks, data-communication networks). Of course, investigations regarding the performance of systems with this underlying conflict yield completely different results than those with a PD game underlying. However, currently there is very little work done in this direction. In this contribution, I will present my current research on this topic as well as empirical results of previous work. [1] Helbing, D.; Schönhof, M.; Stark, H.-U.; Holyst, J. A. (2005). How individuals learn to take turns: Emergence of alternating cooperation in a congestion game and the prisoner's dilemma. Adv. Complex Syst. 8, 87-116; [2] Neill, D. B. (2003). Cooperation and coordination in the turn-taking dilemma. In: TARK. pp. 231-244; [3] Stark, H.-U.; Helbing, D.; Schönhof, M.; Holyst, J. A. (2007). Alternating cooperation strategies in a route choice game: Theory, experiments, and effects of a learning scenario. In: A. Innocenti; P. Sbriglia (eds.), Games, Rationality, and Behaviour, Palgrave, MacMillan.
3591 en Chaos and Stability in Learning Random Two-person Games Game theory often assumes perfect rationality. All agents know all payoff structures. They assume their opponents play fully rationally. Outcomes: Nash equilibria. No player has an incentive to deviate unilaterally.
3592 en Competing Associations We study a six-species Lotka-Volterra type system on different two-dimensional lattices when each species has two superior and two inferior partners. The invasion rates from predator sites to a randomly chosen neighboring prey's site depend on the predator-prey pair, whereby cyclic symmetries within the two three-species defensive alliances are conserved. Monte Carlo simulations reveal an unexpected non-monotonous dependence of alliance survival on the difference of alliance-specific invasion rates. The invasion probabilities are supplemented by Gaussian noise and conditions are identified that warrant the largest impact of noise on the evolutionary process. Our findings are conceptually related to the coherence resonance phenomenon in dynamical systems via the mechanism of threshold duality that is not limited to predator-prey cyclical interactions, but may apply to models of evolutionary game theory as well, thus indicating its applicability in several different fields of research.
3593 en Structure and Dynamics in Complex Networks 
3594 en The "Real World" Web Search Problem There are numerous papers which present methods to address web-search related challenges such as relevance and ranking, query processing, and classication. Unfortunately, many of these methods are ineective in a large-scale commer- cial setting, despite statistically signicant experimental results. To help bridge this gap between academic and commercial settings, this lecture examines the components of large-scale commercial search engines, then proposes ve classes of problems encountered by researchers in this area - biases; bad or dierent assumptions about statistics, users, queries or web contents; insucient or miss- ing data; inconsistencies related to evaluations and objectives; and policies or external factors, including resource limitations. Using real stories and personal experiences, the lecture illustrates examples of these problems, along with a few proposed approaches to deal with or reduce their consequences or eects. In addition to the classes of problems, there are several fundamental prop- erties of the web that are often not considered suciently when performing experiments or dening problems, resulting in unrealistic experiments or ob- jectives. Even within a search engine, overlooking key properties such as the non-stationarity of the users and the web, can result in ineective evaluations, and may even lead to failed subsystems. Fortunately, very simple approaches can often be highly eective. This lec- ture helps put context on how commercial search engines work, what problems they face, what eective solutions require, and how evaluations and problem denitions could be changed to more eectively predict success in a commercial setting - while still retaining interest of researchers.
3595 en Complex Systems Society Information 
3596 en Detecting Money Laundering Actions Using Data Mining and Expert Systems Nowadays terrorism is one of the biggest troubles that almost every country faces. It mainly influences the economy and the well being of the citizens and this effect is relatively larger in the developed countries. Since the financial sources of terrorist groups can be regarded as black money, the solutions against the money laundering actions can be expected to identify the transactions of the terrorists. Then, blocking their accounts could slow down their actions if cannot stop. In many countries, the financial institutions are expected to inform compliance regulation bodies about any persons or transactions that they think suspicious. To cope with this necessity, various software packages for anti money laundering (AML) have been developed and are commercially available.
3597 en The Complexity of Computation 
3598 en Approximation algorithms for k-anonymity and privacy preservation in query logs The talk (i) reviews a number of topics related to the concept of kanonymity, (ii) discusses two information-theoretic measures for capturing the amount of information that is lost during the anonymization process, (iii) presents approximation algorithms for the k-anonymization problem and (iv) discusses topics of privacy preservation on query logs.
3599 en A Systems Perspective of Influenza Virus Replication in Cell Cultures 
3600 en Website Privacy Preservation for Query Log Publishing In this work we study privacy preservation for the publi- cation of search engine query logs. In particular, we introduce a new privacy concern, which is that of website privacy (or business privacy). We define the possible adversaries that could be interested in disclosing website information and the vulnerabilities found in the query log, from which they could benefit. We also detail anonymization techniques to protect website information, and explore the different types of attacks that an adversary could use. We then present a graph-based heuristic to validate the effectiveness of our anonymization method, and perform an experimental evaluation of this approach. Our experimental results show that the query log can be appropriately anonymized against a specific attack for website exposure, by only removing approximately 9% of the total volume of queries and clicked URLs.
3601 en Link Analysis and Text Mining : Current State of the Art and Applications for Counter Terrorism The information age has made it easy to store large amounts of data.The proliferation of documents available on the Web, on corporate intranets, on news wires, and elsewhere is overwhelming. However, while the amount of data available to us is constantly increasing, our ability to absorb and process this information remains constant. Search engines only exacerbate the problem by making more and more documents available in a matter of a few key strokes. Link Analysis is a new and exciting research area that tries to solve the information overload problem by using techniques from data mining, machine learning, Information Extraction, Text Categorization, Visualization and Knowledge Management.
3602 en Fitting mixtures of regression lines with the forward search: application to clustering and outlier detection The forward search is a method for detecting unidentified subsets and masked outliers and for determining their effect on models fitted to the data. This talk describes a semi-automatic approach to outlier detection and clustering through the forward search. We address challenging issues including selection of the number of groups. The performance of the algorithm is shown on several trade data sets relevant for fraud detection problems.
3603 en Feature selection, fundamentals and applications Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. This presentation will cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods. Most feature selection methods do not attempt to uncover causal relationships between feature and target and focus instead on making best predictions.We will examine situations in which the knowledge of causal relationships benefits feature selection. Such benefits may include: explaining relevance in terms of causal mechanisms, distinguishing between actual features and experimental artifacts, predicting the consequences of actions performed by external agents, and making predictions in non-stationary environments.
3604 en Geolocalisation in Cellular Telephone Networks Cellular telephone operators are currently required to provide caller localisation information for use by emergency services. Localisation fields appended to Call Detail Records can now also be exploited for commercial purposes, such as: new network services; subscription fraud detection; customer behaviour monitoring; health care monitoring; law enforcement and forensics; and national and international security. The talk reviews cellphone localisation technology via: 1. A survey of location based services and applications; 2. An overview of localisation techniques with an example; 3. A GSM trace-mobile demonstration of types of information available
3605 en Optimization in an Amoeboid System 
3606 en Consistency Principle in Biological Dynamical Systems We propose consistency principle between different levels, to understand a biological system. Three topics are discussed. First, as a result of consistency between molecule replication and cell reproduction, universal statistical laws on chemical abundances over cells are derived as also confirmed experimentally. They include power-law distribution of gene expressions,log-normal distribution of chemical abundances over cells, and embedding of the power law into the network connectivity. Second, as a result of consistency between gene and phenotype,a general relationship between phenotype fluctuations by genetic variation and isogenic phenotypic fluctuation by developmental noise is derived. Third, we touch upon the chaos mechanism for stem cell differentiation with autonomous regulation, as a result of consistency of cell reproduction and growth of cell ensemble.
3607 en Interacting Random Boolean Networks Random Boolean networks (RBN) have been extensively studied asmodels of genetic regulatory networks. While many studies have been devoted tothe dynamics of isolated random Boolean networks, which may considered asmodels of isolated cells, in this paper we consider a set of interacting RBNs,which may be regarded as a simplified model of a tissue or a monoclonal colony.In order to do so, we introduce a cellular automata (CA) model, where each cellsite is occupied by a RBN. The mutual influence among cells is modelled byletting the activation of some genes in a RBN be affected by that of some genes inneighbouring RBNs. It is shown that the dynamics of the CA is far from trivial.Different measures are introduced to provide indications about the overallbehaviour. In a sense which is made precise in the text, it is shown that the degreeof order of the CA is affected by the interaction strength, and that markedlydifferent behaviours are observed. We propose a classification of these behavioursinto four classes, based upon the way in which the various measures of order areaffected by the interaction strength. It is shown that the dynamical properties ofisolated RBNs affect the probability that a CA composed by those RBNs belongsto one of the four classes, and therefore also affects the probability that a higherinteraction strength leads to a greater, or a smaller, degree of order.
3608 en Evaluating Deterministic Policies in Two-player Iterated Games We construct a statistical ensemble of games, where in each independent subensemble we have two players playing the same game. We derive the mean payoffs per move of the representative players of the game, and we evaluate all the deterministic policies with finite memory. In particular,we show that if one of the players has a generalized tit-for-tat policy,the mean payoff per move of both players is the same, forcing the equalization of the mean payoffs per move of both players. In the case of symmetric, non-cooperative and dilemmatic games, we show that generalized tit-for-tat or imitation policies together with the condition of not being the first to defect, leads to the highest mean payoffs per move for the players. Within this approach, it can be decided which policies perform better than others.The Prisoner's Dilemma and the Hawk-Dove games have been analyzed,and the equilibrium states of the infinitely iterated games have been determined. The infinitely iterated Prisoner's Dilemma game can have Nash solutions only if players have deterministic policies.
3610 en Anticipative Control of Switched Queueing Systems The relevant dynamics of a queueing process can be anticipated by taking future arrivals into account. If the transport from one queue to another is associated with transportation delays, as it is typical for traffic or productions networks, future arrivals to a queue are known over some time horizon and, thus, can be used for an anticipative control of the corresponding flows.A queue is controlled by switching its outflow between 'on' and 'off' similar to green and red traffic lights, where switching to 'on' requires a non-zero setup time. Due to the presence of both, continuous and discrete state variables, the queueing process is described as a hybrid dynamical system. From this formulation, we derive one observable of fundamental importance: the green time required to clear the queue. This quantity allows to detect switching time points for serving platoons without delay, i.e., in a 'green' wave manner. Moreover, we quantify the cost of delaying the start of a service period or its termination in terms of additional waiting time. Our findings may serve as a basis for strategic control decisions.
3611 en Folding Funnels in Energy Landscapes Local minima and the saddle points separating them in the energy landscapeare known to dominate the dynamics of biopolymer folding. Here weintroduce a notion of a 'folding funnel' that is concisely defined interms of energy minima and saddle points, while at the same time conformingto a notion of a 'folding funnel' as it is discussed in the protein foldingliterature.
3612 en On the Quantum Complexity of Classical Words We show that classical and quantum Kolmogorov complexity of binary words agree up to an additive constant.Both complexities are defined as the minimal length of any (classical resp. quantum) computer program that outputsthe corresponding word.It follows that quantum complexity is an extension of classical complexity to the domain of quantum states. This is true even if we allow a small probabilistic error in the quantum computer's output. We outline a mathematical proof of this statement, based on some analytical estimates and a classical programfor the simulation of a universal quantum computer.
3613 en Simulating Dynamic ATM Network Effects using Cellular Automata The dynamical behaviors of groups of airspace sectors are not trivial to be analyzed withoutappropriate theoretical tools. In this paper, we suggest a discrete model based on cellular automata to express the air traffic dynamics and complexity in the controlled airspace. Discrete time simulations have been performed with random selected scenarios of traffic and with independent sector parameters to investigate the impact of availability of local sectors on the whole state of the airspace. Obtained results show the existence of a traffic threshhold that leads to an experimental saturation of airspace. In the test scenario of 900 sectors with fly overtime ranging from 5 to 10 minutes, and sector capacity ranging from 13 to 17 aircraft per time unit, the resulting traffic threshold was 18000 flights.
3614 en Science, Technology, and Applications of Swarm Robotics 
3615 en Distributive Adaptive Control: A Real-world Cognitive Architecture applied to Robots, Spaces and Avatares 
3616 en Biological Principles of Swarm Intelligence 
3617 en Flocking as a Synchronization Phenomenon with Logistic Agents In this paper, we intend to show that the flocking phenomenon observed in many animal species behaviors, may be modeled as a synchronization process occurring within entity states. Although flocking has been widely studied and simulated in Swarm Intelligence, few works mention synchronization as a key aspect of the problem and model it properly. This paper proposes a modeling in terms of a reactive multi-agent system composed of interacting logistic agents moving in an environment. This specific MAS called Logistic MAS (LMAS) takes actually inspiration from the coupled map lattice field, which provides also many tools toanalyse convergence and stability of the system. We develop our approach in boththeoretical and applied way to demonstrate its relevance.
3618 en Optimising the Topology of Complex Neural Networks In this paper, we study instances of complex neural networks, i.e. neural networks with complex topologies. We use Self-Organizing Map neural networks whose neighbourhood relationships are defined by a complex network, to classify handwritten digits. We show that topology has a small impact on performance and robustness to neuron failures, at least at long learning times. Performance may however be increased by artificial evolution of the network topology. In our experimental conditions, the evolved networks are more random than their parents, but display a more heterogeneous degree distribution.
3619 en Statistical techniques for fraud detection, prevention, and evaluation The talk begins by setting the context: fraud is defined and its breadth outlined; figures are given showing how significant fraud is; and different areas of fraud are examined, including health care fraud, banking fraud, and scientific fraud. The particular data analytic challenges of banking fraud are described and illustrated in detail. These include the fact that the classes are highly unbalanced (with typically no more than 1 in a 1000 transactions being fraudulent), that class labels may often be incorrect, that there will typically be delays in discovering the true labels, that the transaction arrival times are random, that the data are dynamic, and, perhaps most challenging of all, that the distributions are reactive, changing in response to the implementation of fraud detection systems. The role of mechanistic and empirical models in tackling these problems is described. Both have been widely used, and both have a contribution to make. Banking data, and in particular banking fraud data are examined in detail. Raw credit card transaction data have 70-80 variables per transaction, and this can be multiplied many-fold for behavioural data, as in fraud detection problems. Questions arise as to how to aggregate the data: should one try to classify individual transactions or should activity records be constructed? A fundamental aspect of any predictive problem in data analysis is the choice of an appropriate criterion for estimation and performance assessment. In the case of fraud, one needs, in particular, to combine both classification accuracy and timeliness of classification. This means that standard measures of classification performance, such as error rate, AUC, KS statistic, information value, etc, are not sufficient. Suitable measures and performance curves are described which combine these aspects and which are now being adopted by the industry. Various statistical (used here in John Chambers’s sense of ‘greater statistics’) approaches have been developed for fraud detection problems, and some are described and illustrated, using data from some of the banks which have been collaborating with us. In particular, we look at supervised classification and anomaly detection methods. Finally in the context of banking fraud, some of the deeper but very important conceptual issues are outlined, including the economic imperative, whether fraud is now becoming ‘acceptable’, and what exactly we learn from empirical comparisons, Scientific fraud is contrasted with banking fraud. They have rather different drivers. In particular, financial gain is generally irrelevant to scientific fraud, which makes it an unusual kind of fraud - although, of course, the impact can be even more serious. Several examples are given, from a range of disciplines. The role of data analytic tools in detecting scientific fraud, and the nature of such tools, is described
3620 en Plastic Card Fraud Detection using Peer Group Analysis Fraud detection describesmethods that attempt to identify fraudulent activity as quickly as possible. From a statistical methods perspective there are broadly two approaches to fraud detection [1]. These relate to whether we intend to detect known examples of fraudulent activity or whether we intend to detect novel forms of fraudulent behaviour. In the former case pattern matching techniques are used in the latter case anomaly detection techniques are deployed. Peer group analysis is an unsupervised method for monitoring behaviour over time [2] and it can be used for anomaly detection [3]. In the context of plastic card fraud detection, peer groups are built for each account, where a peer group is collection of other accounts that behave similarly. The subsequent behaviour of each account is measured in relation to its peer group. Should an account’s behaviour deviate strongly from its peer group then the account is flagged as anomalous and its recent transactions are flagged as potential frauds. This approach differs from the usual anomaly detection methods where each account’s current behaviour is measured in relation to its own past behaviour. We show how to apply peer group analysis to times series that consist of timealigned multivariate continuous data. The initial analysis comprises of a method to determine the peer group members for each time series. For this we need to compare time series [4], we describe one method that is useful for plastic card transaction data. Once we have the peer groups, the analysis then comprises of a method for tracking a time series with respect to its peer group. An anomaly is said to have occurred should the separation between the time series and its peer group exceed some externally set threshold. Account histories of plastic card transaction data are neither time aligned nor do they consist of purely continuous data. A transaction can occur at any time and each transaction has associated with it a record containing a large amount of information. This enables the card issuer to distinguish between the large number of possible transaction types that can occur. For example an account holder who checks their balance at an ATM (an example of a transaction where no money is transferred) or an account holder who purchases a rental car but was not present at the point of sale. We describe one way to time align different account transaction histories and to transform some pertinent information into continuous variables. We summarise experiments performed using peer group analysis on real credit card transaction data. In particular we examine the effect that missed fraudulent transactions have on the performance of the peer groups. We describe a method for robustifying against fraudulent transactions contaminating peer groups.We present our results using a new measure of performance that has been designed specifically for plastic card fraud [5]. Not all accounts can be tracked well enough by their respective peer groups to usefully identify anomalous behaviour.We describe a measure of peer group quality which we use to identify accounts that are more likely to be successfully analysed using peer groups.
3621 en Modeling rare events: online advertisement targeting using machine learning and data mining The Turn automatic targeting network provides advertisers a revolutionary option for online advertising campaigns (online advertising is a $16 billion industry in 2006 according to the Interactive Advertising Bureau). An advertiser simply inputs its ad into a self-serve console, and Turn does the rest. Unlike many ad networks operating today, Turn incorporates extensive industry expertise and innovative technology from the fields of machine learning, information science and statistics, to truly make online advertising risk-free, relevant, simple, effective, and most importantly, profitable. Unlike traditional ad networks where advertisers need teams of employees to manage manual targeting including selecting sites or selecting and optimizing hundreds of thousands of keywords, the Turn network automatically analyzes and targets ads. Turn’s technology dynamically selects and blends hundreds of variables such as past performance, brand strength, user profiles, action type and site categories to determine the best targets for each ad, thus eliminating guesswork, time and complexity. The Turn network is based on statistical technology that intelligently targets both text and graphical ads. By dynamically and automatically selecting and blending targeting variables, Turn can determine the best ad or group of ads for any situation. Turn offers true pay-forperformance with its unique bidded CPA model. Because advertisers pay for actions that they define, Turn eliminates the risk of worthless or fraudulent clicks. Whether an advertiser is paying for product purchases, site visits, leads, or email signups, the advertiser is in control of what they pay for and when they pay for it. In the context of this problem setting (with billions of ad impressions), this poster will address some key issues in modeling rare events using machine learning and data mining such as uncertainty, the regression versus classification dilemma and feature engineering.
3622 en Open Source Intelligence Open Source Intelligence can be defined as the retrieval, extraction and analysis of information from publicly available sources. Each of these three processes is the subject of ongoing research resulting in specialised techniques. Today the largest source of open source information is the Internet. Most newspapers and news agencies have web sites with live updates on unfolding events, opinions and perspectives on world events are published. Most governments monitor news reports to feel the pulse of public opinion, and for early warning and current awareness of emerging crises. The phenomenal growth in knowledge, data and opinions published on the Internet requires advanced software tools which allow analysts to cope with the overflow of information. Malicious use of the Internet has also grown rapidly particularly on-line fraud, illegal content, virtual stalking, and various scams. These are all creating major challenges to security and law enforcement agencies. The alarming increase in the use of the Internet by extremist and Terrorist groups has emerged. The number of terrorist linked websites has grown from about 15 in 1998 to some 4500 today. These sites use slick multimedia to distil propaganda whose main purpose is to 1) enthuse and stir up rebellion in embedded communities 2) instill fear in the “enemy” and fight psychological warfare. Anonymous communication between terrorist cells via bulletin boards, chat rooms and email is also prevalent. The Joint Research Centre has developed significant experience in Internet content monitoring through its work on media monitoring (EMM) for the European Commission. EMM forms the core of the Commissions daily press monitoring service, and has also been adopted by the European Council Situation Centre for their ODIN system. A new research topic at the JRC is Web mining and open source intelligence. This applies EMM technology to the wider Internet and not just to news sites. This applies advanced multi-lingual search techniques to identify potential web resources and the extraction and download of all the textual content. This is then followed by automatic change detection, the recognition of places, names and relationships, and further analysis of the resultant large bodies of text. These tools help analysts to process large amounts of documents and derive structured data easier to analyse. This talk will review 4 main topics: • Internet trends and the rapid rise of Web 2.0 user generated content • Information retrieval: Live content monitoring of multilingual news reports. Web scraping & RSS feed generation, Web Mining and content monitoring • Information Extraction: Topic filtering, Topic Clustering, multilingual named entity extraction, geocoding and geolocating text, event extraction, opinion mining. • Information Analysis: Social Network derivation, geospatial indexing and analysis, incident tracking databases, statistical trend analysis, threat monitoring and assessment.
3623 en The "Digital Territory" as a Complex System of Interacting Agents, Emergent Properties and Technologies In this paper we attempt to contribute to the integration of themathematics and the technological developments and demonstratetheir interplay in realizing the concept of a Digital Territory.We describe the main mathematical tools that can be exploited inthe study of properties that emerge as soon as a population sizereaches a certain threshold point. Our aim is to show thatnowadays we have reached a level of mathematical and technologicalmaturity sufficient to model and simulate any possible worldmodel.
3624 en Combining Information Retrieval and Information Extraction for Medical Intelligence Global epidemic and medical surveillance is an essential function of Public Health agencies, whose primary aim is to protect the public from major health threats. To perform this function effectively one requires timely and accurate medical information from a wide range of sources. In this work we present a system designed to monitor the disease epidemics by analyzing textual reports, mostly in the form of news, available on the Web. The system rests on two major components—MedISys, based on Information Retrieval (IR) technology, and PULS, an Information Extraction (IE) system. The Medical Information System, MedISys, is an automatic tool that gathers reports concerning Public Health from thousands of Internet sources world-wide in 32 languages, classifies them according to hundreds of categories, detects trends across categories and languages, and notifies users.MedISys compiles quantitative summaries of latest reports on a variety of diseases, bioterrorism, toxins, bacteria, hemorrhagic fevers, viruses, medicines, water contaminations, animal diseases, Public Health organisations, etc.3 The system categorises all documents according to about 200 classes of health threats, using pre-defined weighted boolean queries, or alerts. It uses statistical procedures to detect a sudden increase in the volume of articles in any of the classes. MedISys is part of the EuropeMediaMonitor (EMM) product family [2], developed at the EC’s Joint Research Centre (JRC), which also includes NewsBrief,4 a live news aggregation system, and NewsExplorer,5 a news summary and analysis system [1]. MedISys has already proved to be a useful and an effective tool, which attracts thousands of users daily. IE technology is a natural direction for further enhancing the functionality that MedISys offers. One reason for this is that IE is able to deliver information about specific incidents of the diseases, whereas IR returns entire matched documents (with an indication which alerts fired). Another reason is that IE could boost precision, since keyword-based queries may trigger on documents which are off-topic but happen to mention the alerts in unrelated contexts, while pattern matching in IE assures that the keywords appear in relevant contexts only.
3625 en Learning to Extract Security-related Event Information from Large News Collections Automatic Event Extraction from texts emerges as an im- portant and complex text mining task. Its goal is to detect description of events of a speci¯c type described in the text. For each event the Event Extraction system is expected to ¯nd the time, the location, the participants in this event and their roles, as well as other related circum- stances. In this talk we present a Machine Learning approach for learning of information extraction patterns, a method for semi-automatic lexical acquisition, and an information aggregation strategy implemented in a working prototype nexus which detects automatically security related events in clusters of news articles.
3626 en Chaotic Saddles in Spatiotemporal Complex Systems Chaotic transients such as chaotic saddles, strange repellers, semi-attractors, and super-transients have been observed in many deterministic systems (Kantz and Grassberger 1985; Rempel and Chian 2005; Chian, Rempel and Rogers 2006; Rempel and Chian 2007). Formulas can be derived to relate the average life time of the transient to dimensions of the chaotic transient, and to Lyapunov exponents of the flow on it.In this paper, we show that chaotic saddles are responsible for chaotic transients and intermittency in extended complex systems exemplified by a nonlinear regularized long-wave equation, relevant to fluid and plasma studies (Rempel and Chian 2007). Following a transition to spatiotemporal chaos via quasiperiodicity and temporal chaos, the intermittent time series displays random switching between regimes of temporal and spatiotemporal chaos. Before the transition to spatiotemporal chaos, we identify a spatiotemporal chaotic saddle responsible for chaotic transients that mimic the dynamics of the post-transition attractor and can be used to predict its behavior. After the transition to spatiotemporal chaos, we describe a method to identify temporal and spatiotemporal chaotic saddles responsible for the two intermittent regimes.A similar scenario has been observed in the Kuramoto-Sivashinsky equation. We suggest that this scenario can be readily found in extended dissipative dynamical systems that exhibit transient spatiotemporal chaos prior to the transition to sustained spatiotemporal chaos, which evolve from temporal chaos to spatiotemporal chaos via a crisis-like chaotic transition, e.g., pipe flows and nonlinear optical systems. In fact, this scenario has been observed in a model of ring of cardiac cells and plasma laboratory experiments of drift waves.
3627 en Testing Complexity Measures on Symbolic Dynamics of Coupled Tent Maps We evaluate new complexity measures on the symbolic dynamics of coupled tent maps. These measures embody the idea to quantify complexity in terms of k-th order statistical dependencies that cannot be explained by interactions between k-1 units. We demonstrate that these measures are able to identify complex dynamical regimes.
3628 en Security Applications of Web mining 
3629 en CiteSeerX & ChemXSeer: Lessons for Cyber-infrastructure and Web E-science or cyberinfrastructure have become crucial for scientific progress and open source systems have greatly facilitated design and implementation. CiteSeer, a search engine and digital library for academic documents in computer and information science, was one of the first cyberinfrastructure projects to show the promise of improved search and access for scientific information. For chemistry we propose the ChemXSeer (funded by NSF Chemistry) architecture, a portal for academic researchers in environmental chemistry, which integrates the scientific literature and search with experimental, analytical and simulation datasets.
3630 en Interview with Françoise Fogelman Soulié **Françoise Soulie Fogelman has over 30 years of experience in data mining and CRM both from an academic and a business perspective.** We talked with her at the MMDSS event in Italy, Gazzada. We asked her the following questions: *How do academia and industry go together with young researchers? *Privacy policy issues *Datamining dream come true *Advice to young researchers
3631 en Interview with Clive Best 
3632 en Conclusion remarks 
3633 en Web Click Network We analyze the traffic-weighted Web host graph obtained from a large sample of real Web users over about seven months. A number of interesting structural properties are revealed by this complex dynamic network, some in line with the well-studied boolean link host graph and others pointing to important differences. We find that while search is directly involved in a surprisingly small fraction of user clicks, it leads to a much larger fraction of all sites visited. The temporal traffic patterns display strong regularities, with a large portion of future requests being statistically predictable by past ones. Given the importance of topological measures such as PageRank in modeling user navigation, as well as their role in ranking sites for Web search, we use the traffic data to validate the PageRank random surfing model. The ranking obtained by the actual frequency with which a site is visited by users differs significantly from that approximated by the uniform surfing/teleportation behavior modeled by PageRank, especially for the most important sites. To interpret this finding, we consider each of the fundamental assumptions underlying PageRank and show that each is violated by actual user behavior. Joint work with Mark Meiss, Santo Fortunato, Alessandro Flammini, and Alessandro Vespignani.
3634 en Dynamic Visualization of Internet Topology Evolution 
3635 en State-topology Interplay in Epidemic Dynamics on an Adaptive Network 
3636 en New Insights on the Traceroute Process of Network Exploration Dynamical processes taking place on real networks define on them evolving subnetworks whose topology is not necessarily the same of the underlying one. We investigate the problem of determining the emerging degree distribution, focusing on a class of tree-like processes, such as those used to explore the Internet's topology. A general theory based on mean-field arguments is proposed, both for single-source and multiple-source cases, and applied to the specific example of the traceroute exploration of networks. Our results provide a qualitative improvement in the understanding of dynamical sampling and of the interplay between dynamics and topology in large networks like the Internet.
3637 en Emergent Opinion Dynamics on Endogenous Networks In recent years networks have gained unprecedented attention in studying a broad range of topics, among them in complex systems research. In particular, multi-agent systems have seen an increased recognition of the importance of the interaction topology. It is now widely recognized that emergent phenomena can be highly sensitive to the structure of the interaction network connecting the system's components, and there is a growing body of abstract network classes, whose contributions to emergent dynamics are well-understood. However, much less understanding have yet been gained about the effects of network dynamics, especially in cases when the emergent phenomena feeds back to and changes the underlying network topology. Our work starts with the application of the network approach to discrete choice analysis, a standard method in econometric estimation, where the classic approach is grounded in individual choice and lacks social network influences. In this paper, we extend our earlier results by considering the endogenous dynamics of social networks. In particular, we study a model where the behavior adopted by the agents feeds back to the underlying network structure, and report results obtained by computational multi-agent based simulations.
3639 en Battling Networks of Rival Social Movements 
3641 en Targeted Reinnervation for Improved Myoelectric Prosthesis Control Don't miss the "it" moment from Pop!Tech 2005, as the world's first non-fictional bionic man maneuvers his prosthetic arm using only his&#160;mind. Jesse Sullivan and his doctor, Todd Kuiken, move every heart in the room with indomitable spirit and astonishing bionics.
3642 en Gene, Organism and Environment: Bad Metaphors and Good Biology The standard metaphors used to describe DNA and development are examined, including the claim that DNA "makes" protein, that DNA is&#160;"self-replicating" and the organisms "adapt" to their environments. In this lecture by distinguished evolutionary geneticist Richard Lewontin, he explains that DNA is manufactured by the cell machinery, that proteins are folded by rules that are not related to DNA sequence and that organisms, rather than adapting to their environment, are actively engaging in constructing their own environments, so that organisms and environments co-evolve.
3643 en Our Lives, Our Facebooks Students at a large number of American colleges and universities have come to rely on The Facebook ([[http://facebook.com/]]) as a vital supplement to their social lives. A social connector website, Facebook serves the information needs of students who have perpetually in-flux social networks. As a result, frequency and penetration of student use is remarkable. In this presentation, a longitudinal analysis of Facebook use by freshmen at the University of North Carolina at Chapel Hill will be presented. Use patterns will be analyzed, with a special concentration on factors that contributed to the product's success.
3644 en Where Mind and Matter Meet Recent advances in cellular science are heralding an important evolutionary turning point. For almost fifty years we have held the illusion that our health and fate were preprogrammed in our genes, a concept referred to as genetic determinacy. Though mass consciousness is currently imbued with the belief that the character of one's life is genetically predetermined, a radically new understanding is unfolding at the leading edge of science. Cellular biologists now recognize that the environment, the external universe and our internal physiology, and more importantly, our perception of the environment, directly controls the activity of our genes. This video will broadly review the molecular mechanisms by which environmental awareness interfaces genetic regulation and guides organismal evolution.
3645 en Scaling Laws in Biology and Other Complex Systems Life is very likely the most complex phenomenon in the Universe manifesting an&#160;extraordinary diversity of form and function over an enormous range. Yet, many of its most fundamental and complex attributes scale with size in a surprisingly simple fashion. For example, metabolic rate (the power required to sustain the system) scales as approximately the 3/4-power of mass over 27 orders of magnitude from molecular levels up to the largest multicellular organisms. Similarly, time-scales, such as lifespans and growth-rates, increase with exponents which are typically simple powers of 1/4. It will be shown how these universal quarter-power scaling laws follow from fundamental generic principles embedded in the dynamics and geometry of underlying networks, leading to a general quantitative theory that captures essential features of many diverse biological systems. Examples will include animal and plant vascular systems, growth, cancer, aging and mortality, sleep, DNA nucleotide substitution rates. These ideas will be extended to discuss social organisations such as cities and firms: to what extent, if at all, can we think of these as very large organisms and therefore as an extension of **biology**? Analogues to metabolic rate and behavioral times in cities scale counter to their behaviour in **biology**. Driven by innovation and the creation of wealth this has dramatic implications for their growth, development, sustainability and pace of life which, left unchecked, potentially sow the seeds for their collapse.
3646 en Return to the RNAi World: Rethinking Gene Expression and Evolution While investigating the genetic workings of the microscopic worm, C. elegans, Mello and&#160;colleague Andrew Fire, PhD, of the Carnegie Institution of Washington, discovered RNAi, a natural but previously unrecognized process by which a certain form of RNA can be manipulated to silence—or interfere with—the expression of a selected gene. The discovery, published in the journal Nature in 1998, has had two extraordinary impacts on biological science. One is as a research tool: RNAi is now the state-of-the-art method by which scientists can knock out the expression of specific **genes** in cells, to thus define the biological functions of those **genes**. But just as important has been the finding that RNA interference is a normal process of genetic regulation that takes place during development. Thus, RNAi has provided not only a powerful research tool for experimentally knocking out the expression of specific **genes**, but has opened a completely new and totally unanticipated window on developmental gene regulation. RNAi is now showing promising in the clinic as a new class of gene-specific therapeutics.
3647 en Mysteries of the Human Genome The human genome, the hereditary material we pass on to our progeny, can be cast as a 3 billion letter string over a DNA alphabet of four. We currently understand 1.5% of this mass, mostly in the form of genes, DNA substrings that code for proteins, the quintessential constituents of every living cell. The remainder 98.5% of our genome was often deemed as "junk". This picture changed when the genome of related species became available. By comparison we are suddenly able to pinpoint the locations of a staggering one million additional human subsequences that must be important to the human cell. The functions of these regions remain largely unknown, while their sheer volume overwhelms any comprehensive experimental approach. Guided by experimental results for handfuls of these subsequences, computational approaches can be employed to tackle the tremendous challenge of understanding this data and providing key biological observations. In this talk, I will describe ultraconserved elements, some of the most perplexing regions within the human genome, and track down a phenomenon of turning genomic junk into gold. The talk will assume no prior knowledge in Molecular Biology.
3648 en How Bacteria Cause Disease Join Warren Levinson to learn about the various agents that cause infectious diseases: bacteria, viruses, fungi, protozoa and worms, with a focus on how bacteria are transmitted and cause disease, and how exotoxins and endotoxins cause symptoms of disease.
3649 en BacGrid: Simulations of Bacteria using the GRID Bacterial biofilms provide systems of the complexity needed to&#160;exemplify many of the generic features of multi-cellular behaviour, without such complexity at once becoming overwhelming. They are in addition of enormous environmental, industrial and medical importance. Many processes in biofilms operate at the macroscopic scale and are thus susceptible continuum modelling approaches. It is essential, however, that models incorporate in an appropriate way information about the micro-scale behaviour and their results must in turn be coupled back into the rules adopted in the cell-scale modelling, motivating the use of agent-based modelling. The simulation of such complex systems typically requires huge computing resources. The Grid provides an unrivalled technology for large scale distributed simulation and is exceptionally well suited to addressing the challenges raised by integrative-biology. In this paper we present BacGrid, a system for performing distributed simulation of bacteria using the High Level Architecture (HLA) and the Grid. We present the bacterial model and show results from initial experiments investigating the role of quorum sensing molecule (QSM) in the development of the bacterial colony. We go on to sketch out the design for the distribution of bacterial simulation components across the grid and indicate how this technology can be used to create large scale simulations. We conclude with a discussion of the current status of the system and our plans for future work and experiments.
3650 en The Origin of the Human Mind: Insights from Brain Imaging and Evolution UCSD cognitive scientist Martin Sereno takes you on a captivating exploration of the brain's structure and function as revealed through&#160;investigations with new advanced imaging techniques and understandings of evolution.
3651 en Evolution of the Human Species Eminent evolutionary biologist Christopher Wills takes you on an exploration of human evolutionary history and how it is derived from both&#160;the genetic and fossil records.
3652 en ISP-aided Biased Query Search for P2P Systems in a Testlab More than half of Internet traffic today is contributed by peer-to-peer (P2P) systems. P2P systems build their overlay topology largely agnostic of the Internet underlay, which often leads to traffic management challenges for Internet Service Providers (ISP) and potentially inefficient neighborhood selection for P2P nodes. To overcome this, we propose to use an oracle hosted by the ISPs, so that ISPs and P2P users can cooperate for improved performance. The oracle can be queried by P2P nodes while choosing neighbors for content search, and it will rank the possible neighbors of the querying node according to a locality indication, like the AS-hop distance. The ISP would gain by keeping traffic within its Autonomous System (AS) network, and the P2P node would experience improved performance like lesser delay and better bandwidth.In this paper, we evaluate the benefits of our scheme by performing experiments in a real Testlab consisting of routers, switches and computers running actual instances of P2P applications. We showhow we configure representative AS topologies for P2P networks using VLANs and trunking ports, andpresent experimental results with content search phase of a P2P network using different file sharing and search query distributions.
3654 en Uncovering Latent Structure in Valued Graphs: A Variational Approach As more and more network-structured datasets are available, the statistical analysis of valued graphs has become a common place. Looking for a latent structure is one of the many strategies used to better understand the behavior of a network. Several methods already exist for the binary case.We present a model-based strategy to uncover groups of nodes in valued graphs. This framework can be used for a wide span of parametric random graphs models. Variational tools allow us to achieve approximate maximum likelihood estimation of the parameters of these models. We provide a simulation study showing that our estimation method performs well over a broad range of situations.
3656 en Eigenmode of Decision-by-Majority Process on Complex Networks The nature of opinion formation dynamics in complex networks is investigated using eigenmode analysis. Opinion formation dynamics is modeled by a decision-by-majority process of spin-like variables located at vertices of complex networks. Hamiltonian of the system is defined, and estimated by the eigenvalue and eigenvector of the adjacency matrix constructed from several network models. Then, the eigenmodes of initial and final state of the dynamics are analyzed by numerical studies. It is shown that the magnitude of the largest eigenvector at the initial states are key determinant for the resulting dynamics. It is proved that the final state of the dynamics can be estimated by the eigenmodes of the initial state.
3657 en Generating Graphs with Predefined k-Core Structure The modeling of realistic networks is of great importance for complex systemsresearch. Previous procedures typically model the natural growth of networks byiteratively adding nodes, use geometric positioning information, define linkconnectivity with preference for nearest neighbors or already highly connectednodes, or combine several of these approaches. Our novel model is based on the well-know concept of 'k'-cores, originally introduced in social network analysis. Recent studies exposed the significant 'k'-core structure of several real world systems, e.g. the AS network of theInternet. We present two algorithms for generating networks which strictlyadhere to the sizes of a given 'k'-core structure but also exhibit adaptationto various use cases. We showcase this in a comparative evaluation with twowell-known AS network generators.
3658 en Randomness and Complexity in Networks Recently we showed that a simple model of network rewiring could besolved exactly for any time and any parameter value. We also showed that this model can be recast in terms of several well known models of statistical physics such as urn model and the voter model. We also noted that it has been applied to a wide range of problems. Here we consider various generalisations of this model and includesome new exact results.
3659 en Spectral Plot Properties: Qualitative Classification of Networks We introduce a tentative classification scheme for empirical networks based on global qualitative properties detected through the spectrum of the Laplacian of the graph underlying the network. Our method identifies several distinct types of networks across different domains of applications, indicates hidden regularity properties and provides evidence for processes like node duplication behind the evolution or construction of a given class of networks.
3660 en Connections between Random Boolean Networks and their Annealed Model Random Boolean Networks (RBN) with bias are considered.We show that the behaviour of the so called annealed modelis completely determined by the expectation of the average sensitivity. We proof that the annealed model is its ordered phase if and only if the corresponding quenched modelis in its ordered phase. As a side product we get an annealed analysis forRBNs with fixed bias.
3662 en Directed, Overlapping Clusters in Social Networks Recently an efficient search technique locating communities or networkmodules (densely connected groups of nodes) was introduced fordirected networks (Palla et al: New Journal of Physics 9, 186 (2007)). Here we investigate the centrality properties of directed module membersin social networks obtained from e-mail exchanges and fromsociometric questionnaires.Our results indicate that nodes in the overlaps betweenmodules play a central role in the studied systems. Furthermore,the two different types of networks show interesting differencesin the relation between the centrality measures and therole of the nodes in the directed modules.
3663 en Dynamics on and of Biological Networks: Case Studies on the Machinery of Life Gene regulation networks and other molecular networks that regulate the processes of life in the living cell are prototypes for dynamical networks that combine the aspects of both, transferring dynamical signaling on the one hand, and being structurally dynamical themselves on the other hand. Both phenomena, while living on vastly different timescales (that of molecular interactions versus the timescale of macroevolution), are closely interwoven and depend on each other. We will take a closer look at this interesting type of complex networks and I will review a few approaches and views from different angles.
3664 en Topographic Analysis of an Empirical Human Sexual Network Spreading of electronic viruses, among computers and mobile phones, typically depends on address/phone number lists. The network formed by these lists is not symmetric: the fact that A has B’s address does not ensure that B has A’s address. Thus the underlying network on which such spreading takes place is directed: the links are in general one-way. We present an extension of our analysis for spreading on undirected graphs, to the case of directed graphs. We find that some ideas from Web link analysis lead us to a concrete prediction: that the epidemic coverage changes qualitatively when the rate of infections from ”outside” the network exceeds a threshold rate. Specifically, for low rate of infections from outside, with high probability, only the giant component and its out-components are infected; while for above-threshold infection rate from outside, the whole graph is likely infected.
3665 en Evolution of Cooperation on Dynamical Graphs Population structure has been proposed as one of the mechanism promoting cooperation. Until recently, most studies assumed that the interaction network can be described by a regular graph. Recently, Ohtsuki et al. [1] have shown for a number of other interaction topologies that selection favours cooperation (i.e. the fixation probability of a single cooperator is higher than the fixation probability of a neutral mutant) in the prisoners dilemma game if the benefit (b) of the altruistic act divided by its cost (c) exceeds the average number of neighbours (k), that is, if b=c > k. They found this relation to be approximately valid in populations of different structure, in which interaction topology is described variously by regular, random regular, random, or scale-free graphs. Similarly, Santos et al. [4] have shown that the heterogeneous degree distribution of these other types of graphs generally facilitate the dominance of cooperative behaviour. Previous studies [1], [4], [5] have assumed that the graph is static during evolution. This assumption implies that a newborn individual (or accepted strategy-by-imitation) in a given position interacts with exactly the same individuals that were connected to every preceding individual at this position. Some recent papers studied the evolution of cooperation on dynamical networks. They either studied the fixation probability of a single cooperator among defectors in the case when graph dynamics is much faster than dynamics of evolution [2], or if the relative speed of graph and evolutionary dynamics were waried systematically they assumed that cooperators and defectors were in the same fraction initially in the population [3], [4]. Here we investigate how sensitive is the fixation probability of a single cooperator to the network dynamics, if dynamics is slow relative to the evolution.
3666 en From Protein-Protein to Domain-Domain Interactions and Back With the advent of domain interaction networks derived from 3D structures and experimentally determined large-scale protein interaction networks a number of interesting questions arise: Are interaction interfaces more conserved than the rest of the surfaces? Can the structure of protein interaction networks be linked to underlying domain interactions? Can motifs derived from domain interactions predict protein interactions? Can viral domains mimick native human protein interaction interfaces? There is an intricate relationship between domain-domain and protein-protein interactions, whose understanding helps to answer the above questions. In the talk, I will review how to construct networks derived from structural domains and how to complement large-experimental protein interaction networks with interactions extracted from literature. I will shed light onto the structure of these networks, how to identify functional modules, and how to predict interactions. The discussed techniques will be used to identify candidate targets in pancreas cancer and will show how viruses interfere with the apoptotic programme of their hosts.
3667 en A Social Network Approach to Unsupervised Induction of Syntactic Clusters for Bengali In this paper we describe some experiments on fully unsupervised induction of parts-of-speech tags for Bengali words from a raw text corpus. For this purpose, we construct the network of 5000 most frequent Bengali words, where nodes are the types and the weight on the edge between two types is indicative of their distributional similarity and cluster the network using the Chinese Whispers algorithm [1]. We also propose the concept of tag-entropy that measures the cohesiveness of the word clusters in terms of the lexical categories of the constituent words.
3668 en Rearrangement of Spirals into Target Patterns in the Course of Dictyostelium discoideum Aggregation The amoebae Dictyostelium discoideum has been frequently investigated as a suitable model system in the framework of biological pattern formation, signal transduction, cell differentiation and morgenesis (see [1] for a review). Under conditions of starvation populations of these amoebae aggregate due to chemical waves of cAMP. As a cAMP wave passes by a cell, it moves in the opposite direction, consequently towards the origin of the signal, as long as the cAMP-concentration is increasing over time. Formally this system my be regarded as a complex self-organizing network of cells, communicating through chemical medium. The spatio-temporal patterns of cAMP waves are a manifestation of the intrinsic properties of this active medium. Depending on the experimental conditions either spiral or circular-shaped waves of cAMP arise [2]. Sometimes a pattern of one type is transformed into the other one in the course of time. In experiments two different examples for transitions from one to the other type of spatio-temporal patterns were reported [3]. In the first case a continuous transition from the spiral type pattern to target waves was observed at the later stages of aggregation. In the second case the transition was induced by a spatially homogeneous cAMP pulse, when after resetting oscillating spots, bearing target patterns emerged instead of the originally present spirals. Similar results were reported by Lee et al. [4]. The biological significance of different types of cAMP waves for Dictyostelium discoideum morphogenesis and the mechanisms of transitions between them are still unclear. Thus the question arises: what are the internal cellular properties and the external experimental conditions, which lead to such a transition? We attempt to find the answer to this question applying a mathematical modelling approach.
3669 en Examining Higher Order Transformations for Scale-free Small World Graphs The degree distribution of scale-free Small World networks follows a power law. For random graph generators, its exponent is constrained by the construction mechanism, whereas in real-world data, different slopes can be observed. However, the degree distribution alone does not reveal much of the local structure of these graphs. Therefore, we propose a graph transformation we call ”higher order” transformation, which encodes the number of common neighbours two vertices share in its edge weights. Studying the degree distribution of secondand third order graphs and comparing it to natural language cooccurrence data, we find that the higher order transformation reveals differences that cannot be detected by only looking at traditional measures on the original graph.
3671 en Statistical Dynamics of Religions and Adherents We argue that religion is another degree of freedom to describe a population on an evolving network. We present a comprehensive analysis of 55+2 so called religion evolutions, as measured through their number of adherents. The Avrami-Kolmogorov differential equation which usually describes solid state transformations is used in each case in order to obtain the preferential attachment parameter introduced previously [1]. It is often found close to unity, indicating a smooth evolution. However large values suggest the occurrence of extreme cases which we conjecture are controlled by so called external fields. A few cases indicate the likeliness of a detachment process. Various cases are illustrated. It seems that religion is an as exciting, and even more physically interesting statistical physics subject than language [2] due to the presence of external fields and various time scales. Hamiltonian and Langevinian like description will be suggested. [1] M. Ausloos and F. Petroni, Statistical Dynamics of Religions and Adherents, Europhys. Lett.77 (2007) 38002 (4 pp); [2] D.M. Abrams, S.H. Strogatz, Modelling the dynamics of language death. Nature 424 (2003) 900 (3 col).
3672 en Social Networks and Ideological Movements in History: Burning and the Rise of English Protestantism There is a historical consensus that at the beginning of the reign of the Catholic Queen Mary (1553-58), the Protestant reforms instituted by Henry VIII in the 1530s and continued under Edward VI (1547-53) had engaged the support of only a tiny minority of the population. The restoration of Catholicism met with widespread approval.  But a mere six years later the re-introduction of Protestantism on the Edwardian model by Elizabeth I in 1559 met with virtually no protest.  A good historical case can be made that the persecution and burning of high-profile Protestants by Mary was an important factor in reversing public opinion. A network approach, in which society is envisaged as a scale-free network with each individual influenced in their religious beliefs by a small number of others to whom they pay attention yields the result that the burnings may well have been the decisive factor. The highly connected individuals here are of course the Protestant martyrs. England in 1559 had not become a nation of Protestant zealots, but sufficient people were impressed by the martyrs’ demeanour to acquiesce in the new faith. This analysis is supported by contemporary evidence that Protestant leaders under Mary stressed that executions were opportunities to display public fortitude and piety to influence people.
3673 en Do Language Change Rates Depend on Population Size? An earlier study (Nettle 1999b) concluded, based on computer simulations and some inferences from empirical data, that languages will change the more slowly the larger the population gets. We replicate this study using a more complete language model for simulations (the Schulze model combined with a Barab´asi-Albert network) and a richer empirical dataset (the World Atlas of Language Structures edited by Haspelmath et al. 2005). Our simulations show either a weak or stronger dependence of language change on population sizes depending on the parameter settings, and empirical data, like some of the simulations, show a weak dependence.
3674 en Global and Local Dynamics in Correlated Systems In this talk, we will show results concerning the characterization and visualization of correlations in financial systems by means of network analysis [1-4]. We will discuss results from the application of a new method which is able to construct complex graphs from cross-correlation matrices. Dynamical changes in the local topology and hierarchy of these graphs are detected and related to markets fluctuations [5]. [1] T. Aste, T. Di Matteo, S. T. Hyde, Physica A 346 (2005) 20-26; [2] M. Tumminello, T. Aste, T. Di Matteo, R. N. Mantegna, PNAS 102, n. 30 (2005) 10421 10426; [3] T. Aste and T. Di Matteo, Physica A 370 (2006) 156-161; [4] M. Tumminello, T. Aste, T. Di Matteo, and R. N. Mantegna, EPJB 55 (2007) 209-217; [5] F. Pozzi, T. Di Matteo, T. Aste, in preparation (2007).
3675 en Who needs Patterns? 
3676 en A framework for pattern analysis 
3677 en PAC-Bayes Analysis of Classification The lecture will introduce the PAC Bayes approach to the statistical analysis of learning. After some historical introduction, the key theorems will be covered. We will then consider some applications including for Support Vector Machines and novelty detection. A discussion of the status of the prior in the approach will lead to an investigation of how learning the prior can be used in practical applications. Discussions of further extensions of the approach will conclude the presentation.
3678 en Weighted Transducers and Rational Kernels 
3679 en Similarity and differences by finite automata 
3680 en On-line learning algorithms: theory and practice 
3681 en Clustering 
3682 en Probabilistic Graphical Models and Structured Prediction 
3683 en Magic Moments: Moment-based Approaches to Structured Output Prediction 
3684 en Pattern Discovery in Bioinformatics 
3685 en Support Vector Machines and Kernel Methods 
3687 en On The History of Ugliness In “History of Beauty,” Umberto Eco explored the ways in which notions of attractiveness shift from culture to culture and era to era. With ON UGLINESS, a collection of images and written excerpts from ancient times to the present, he asks: Is repulsiveness, too, in the eye of the beholder? And what do we learn about that beholder when we delve into his aversions? Selecting stark visual images of gore, deformity, moral turpitude and malice, and quotations from sources ranging from Plato to radical feminists, Eco unfurls a taxonomy of ugliness. As gross-out contests go, it’s both absorbing and highbrow.
3688 en Interview with Neil F. Johnson 
3690 en Interview with Adobe Chief Software Architect Kevin Lynch speaks about his begennings at Macintosh, how he startet in the business 25 years ago and how he named the Adobe breakthrough application Dreamweaver. The Videolectures.Net team encountered him at the W3C Video on the Web Workshop where among other things we asked him about his personal and professional dream come true, his vision of the video in the general...
3691 en Interview with the W3C Chief Executive Officer **Steve Bratt is the CEO of the Consortium. **Videolectures.Net sent a paper at the W3C Video on the Web Workshop which was held 12-13 December 2007, San Jose, California. There we met mr. Bratt and asked him a few questions after the workshop finished: *How is W3C handling video at this moment? *What will happen after this workshop? *Who is standardising the video on the web, big repositories or W3C? *What is the closest domain to video on the web, the semantic web? *How do you see research and video on the web? *The first thing you will do when you leave the building?
3692 en Interview with W3C Compound Document Specialist Doug Schepers has been programming since his early teens. He has focused on SVG for the past 6 years, developing SVG applications. He is a founding member of Vectoreal, an association of SVG professionals. You can reach him at svg at [[http://schepers.cc/|schepers.cc]]. If you want to read more about Doug, you can read his personal Web site, Schepers.cc. Videolectures.Net sent a paper at the W3C Video on the Web Workshop which was held 12-13 December 2007, San Jose, California. There we met Doug and asked him a few questions after the workshop finished: *Any comments on your presentation *What is W3C doing for the standardisation of the video on the web? *What did you miss on the workshop? *The current hot topic about the video on the web? *Personal and professional dream come true * First thing you will do after you leave this building?
3693 en An alternative modeling for biological signaling networks Biological signaling networks transmit and process extra-cellular information, triggering complex transformations that lead to different cellular responses. The overall cellular behavior is well grasped by differential equations; the challenge is to produce mathematically affordable microscopic models leading to these equations. The usual modeling approach, based on chemical kinetics, is hampered by the large number of assumptions needed. As an alternative, we propose a spin-flip dynamics defined by asymmetric mean-field interactions. This dynamics yield density-profile processes whose trajectories converge almost surely to the solutions of dynamical systems with possibly complex behavior.
3694 en Singular Diffusion Equations: Minimally Stochastic Solution Schemes Total variation (TV) and balanced forward-backward (BFB) diffusion are popular examples of singular diffusion processes: Finite extinction time, the experimentally observed tendency to create piecewise constant regions, and the absence of parameters makes them very interesting image processing tools. However, their appropriate numerical treatment is still a challenge. In this contribution a minimally stochastic approach to the underlying singular equations is presented. It relies on analytical solutions of two-pixel signals and stochastic rounding. This introduces regularisation via integer arithmetic and does not require any limits on the diffusivity. Experiments demonstrate the favourable performance of the proposed probabilistic method.
3695 en Locally Analytic Schemes for Diffusion Filtering of Images Nonlinear diffusion filtering has proven its value as a versatile tool for structure-preserving image denoising. Among the most interesting methods of this class are tensor-driven anisotropic diffusion as well as singular isotropic diffusion filters like total variation flow. For different reasons, devising good numerical algorithms for these filters is challenging. A spatial discretisation transforms nonlinear diffusion partial differential equations into systems of ordinary differential equations. Their investigation yields insights into the properties of diffusion-based algorithms but leads also to the design of new algorithms with favourable stability properties which are at the same time simple to implement. Moreover, interesting links to wavelet-based denoising methods are established in this way. The talk focusses on the construction and properties of locally (semi-)analytic schemes for nonlinear isotropic and anisotropic diffusion on 2D images, with extensions to the 3D case.
3696 en Adaptive procedures for FDR control in multiple testing Multiple testing is a classical statistical topic that has enjoyed a tremendous surge of interest in the past ten years, due to the growing domain of applications that are in demand for powerful and reliable procedures to this regard. For example, in bioinformatics it is often the case that multiple testing procedures are needed to process data in very high dimension where only a small number of sample points are available. In their 1995 seminal work, Benjamini and Hochberg first introduced the false discovery rate (FDR), a notion of type I error control that is particularly well suited to screening processes where a very high number of hypotheses has to be tested. It has since then been recognized as a de facto standard. We first review existing so-called "step-up" testing procedures with FDR control valid under several types of dependency assumptions on the joint test statistics, and show that we can recover (and extend) them by considering a very simple set-output point of view along with with what we call a "self-consistency condition" which is sufficient to ensure FDR control. We then proceed to consider adaptive procedures, where the estimation of the total proportion of true null hypotheses can lead to improved power. To this regard we introduce an algorithm that is almost always more powerful than an adaptive procedure proposed by Benjamini, Yekutieli and Krieger (2006).
3697 en Homeomorphic smoothing splines: monotonizing an unconstrained estimator in nonparametric regression 
3698 en MCMC, SMC,... What next ? The Monte Carlo method was initially developed for scientific computing in statistical physics during the early days of the computers. Due to the rapid progress in computer technology and the need for handling large datasets and complex systems, the past two decades have witnessed a strong surge of interest in Monte Carlo methods from the scientific community. Researchers ranging from computational biologist to signal \& image processing engineers and to financial econometricians now view Monte Carlo techniques as essential tools for inference. Besides using the popular Markov chain Monte Carlo strategies and adaptive variants of it, various sequential Monte Carlo strategies have recently appeared on the scene, resulting in a wealth of novel and effective inferential and optimization tools. In this talk, we will present what we believe to be the "state-of-the art" in Monte-Carlo simulations for inference and will try to identify the next challenges.
3699 en Contour Enhancement and Completion via Left Invariant Evolution Equations on SE(2) 
3700 en Shape constraints and algorithms We consider several problems in the areas of nonparametric regression and image analysis under shape constraints. The task is always to produce simple functions with small number of local extreme values, while multiresolution criteria ensure good approximation of the fitted functions. These strategies easily lead to minimisation problems that can be very difficult to solve, hence the design of efficient algorithms is crucial. One of the problems that we study is concerned with online data where fast processing is particularly important.
3712 en STM Manipulation of Atoms and Molecules Novel quantum structures can be realized by manipulating surface single atoms andn molecules. Recent efforts in manipulating these basic construction species by meansn of a low-temperature ultra high vacuum scanning tunneling microscope, combined withn a variety of tunneling spectroscopic methods, will be presented. The performedn experiments are important for both, fundamental understanding and construction ofn novel nano-devices. Described will be i.a. measurements of lateral forces requiredn to move individual atoms, realization of a multi-step single molecule switch, and ann hybrid device, composed of atoms and molecules.
3714 en Graphite - a new twist Carbon is an element that is unique in the variety, utility and individuality of its allotropes. Diamond and graphite each have several uniquely extreme properties that have been exploited in twentieth century science and technology. Against the curious landscape of the periodic table the discoveries of fullerenes (1985) and nanotubes (1991) stand out as substantial landmarks. Their beauty lies in creating isolatable molecular forms. In the somewhat messier world of crystal defects, similar topological concepts can be applied, leading to sheets which are buckled, folded, 'welded' together, or unified into one sheet. The description of these defects comes from the science of dislocations, and their structures can be deduced from first principles methods, such as density functional theory (DFT). A connector between sheets, similar to the ramp connecting the floors of a multistorey car park, is a prismatic screw dislocation dipole. A fold is a pile-up of basal dislocation dipoles. The identification and characterization of dislocations in graphite, gives insight into structures which have been overlooked in the science of graphite, especially radiation damage which occurs in reactor graphite subjected to energetic neutrons.
3717 en Wetting and Contact Lines of Micrometer-sized Ellipsoids We experimentally and theoretically investigated the wetting of a fluid interface on solid micrometer-sized ellipsoidal particles. The latter were obtained from uniaxial stretching of monodisperse polystyrene spheres (radius R = 5?m) and the aspect ratio, k, was varied from 1 to about 10. We have demonstrated that such ellipsoids at oil-water interfaces manifest long range capillary interactions of considerable energies, up to about 105 times the thermal energy. The contact line exhibits saddle-like deformations and has a quadrupolar symmetry: the interface is pulled down near the tips of the ellipsoid and pulled up near the middle of the particle. Two ellipsoids attract each other tip to tip or side by side but repel one another when in a side-to-tip configuration. These trends are indeed in line with the general rule according to which the interaction between capillary charges of the same sign is attractive while it is repulsive between charges of opposite signs. To interpret our experimental data, we numerically solved the partial wetting problem on a single ellipsoid and found that the contact line is indeed a saddle-shaped curve with a quadrupolar symmetry. Furthermore, comparisons of experimental and simulated data allowed us to determine contact angles and rather unexpectedly, the latter were found to decrease significantly with increasing ellipsoid aspect ratio.
3725 en YL Ventures As you will soon discover, YL Ventures is not just another venture capital fund. We are entrepreneurs just like you, having exited our last startup immediately prior to launching this fund. We have a proven track record as entrepreneurs, having founded three successfully-exited technology startups. All of our exits have been relatively quick and medium size – just like the strategy of YL Ventures. We are technologists like you who know your industry and can get to a deep understanding of your business within hours not weeks. We are young like you and we understand your needs and what motivates you, and our stated purpose is not just to generate significant returns for the fund, but also look after you and make sure that at the end of the day you get what you wanted. After all, we want you to come back and also refer your friends!
3726 en Interview with Yoav Andrew Leitersdorf - Venture Capital 
3728 en Online Learning of Music Preference We consider the problem of online learning in a changing environment under sparse user feedback. Specifically, we address the classification of music types according to a user’s preferences for a hearing aid application. The classifier, operating under limited computational resources, must be capable of adjusting to types of data not represented in the training set, and to changing user demands. The user provides feedback only occasionally, prompting the classifier to change its state. We propose an online learning algorithm capable of incorporating information from unlabeled data by a semi-supervised strategy, and demonstrate that the use of unlabeled examples significantly improves classification performance if the ratio of labeled points is small.
3729 en Linear Programming Boosting for Classiﬁcation of Musical Genre Classification of musical genre from raw audio files is a fairly well researched area of music research, and as such provides a good starting point for testing a new algorithm. The Music Information Retrieval Evaluation eXchange (MIREX) is a yearly competition in a wide range of machine learning applications in music. MIREX 2005 included a genre classification task, the winner of which [1] was an application of the multiclass boosting algorithm AdaBoost.MH [2]. It is believed that Linear Programming Boosting (LPBoost) is a more appropriate algorithm for this application due to the higher degree of sparsity in the solutions [3]. The present study aims to improve on the [1] result by using a similar feature set and the multiclass boosting algorithm LPBoost.MC. \\ References: [1] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and K. Bal´azs. Aggregate features and ADABOOST for music classification. Machine Learning, 65 (2-3):473–484, 2006. [2] R.E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37:297–336, 1999. [3] Ayhan Demiriz, Kristin P. Bennett, and John Shawe-Taylor. Linear programming boosting via column generation. Machine Learning, 46(1–3):225–254, 2002.
3730 en The Conditionally Independent Voice Model I will talk about two related topics. First I will introduce the conditionally independent voice model, which expresses music as a collection of voices that evolve independently from one another when conditioned on a process that describes some shared evolving attribute such as harmony. I will show an application to pitch spelling from MIDI, though I believe the model may find use in a variety of musical applications. Attempts to train this model automatically using traditional Baum-Welch type methods were not particularly successful, due, perhaps, to the inappropriateness of the marginal likelihood criterion. We introduce a method for directly minimizing the error rate on a test set, using computational ideas from POMDPs.
3731 en Information Dynamics and the Perception of Temporal Structure in Music It has often been observed that one of the more salient effects of listening to music to create expectations within the listener, and that part of the art of making music to create a dynamic interplay of uncertainty, expectation, fulfilment and surprise. It was not until the publication of Shannon's work on information theory, however, that the tools became available to quantify some of these concepts. Since then, there has been sporadic interest in the relationship between information theory and music and aesthetic perception in general. \\ In this talk, we will examine how a small number of \emph{time-varying} information measures, such as entropies and mutual informations, computed in the context of a dynamically evolving probabilistic model, can be used to characterise the temporal structue of a stimulus sequence, considered as a random process from the point of view of a Bayesian observer. \\ One such measure is a novel \emph{predictive information rate} which we conjecture may provide an explanation for the `inverted-U' relationship often found between simple measures of randomness (\eg entropy rate) and judgements of aesthetic value (Berlyne 1971). We explore these ideas in the context of Markov chains using both artificially generated sequences and two pieces of minimalist music by Philip Glass, showing that even an overly simple model (the Markov chain), when interpreted according to information dynamic principles, produces a structural analysis which largely agrees with that of an expert human listener. \\ We will also discuss how the same principles can be applied to models more complex than the fully observed Markov chain (in particular, hidden Markov models), by using online variational Bayesian methods to track the observer's (probabilistic) beliefs about unobserved variables.
3732 en Time-Frequency and Synchrony Analysis of Responses to Steady-State Auditory and Musical Stimuli from Multichannel EEG Brain responses to audio stimuli are analysed using data driven time-frequency analysis. This is achieved based on the electroencephalogram (EEG) recordings and with auditory chirps or music as the audio stimulus. The empirical mode decomposition (EMD) is applied to multichannel EEG recordings, and the insight into the brain responses is provided by the analysis of the dynamics of auditory steady-state responses (ASSR). The proposed approach is further illustrated on the analysis of EEG responses to classical music. A comprehensive synchrony analysis is provided based on the visualization of EMD and spectrogrammatching techniques. Simulation results illustrate the potential of the proposed approach in future brain computer/machine interfaces.
3733 en The Mental Representation of Music: A Neural Darwinist Perspective In this presentation, I review the perceptual research related to auditory representation for music. The research suggests that multiple representations exist concurrently in the auditory system, and that the dominant representation is shaped by the specific auditory environment. I note that the research is consistent with theories of competitive representations, such as Edelman's neural Darwinist approach. I propose that the difference in predictive accuracy for different representations provides the feedback mechanism by which competing representations are selected. Repercussions for cognitive modeling of music are discussed.
3734 en Measuring and Modeling Musical Expression Expressive timing and dynamics are an important part of musical meaning. For those skeptical of this claim I will play some examples of music with and without expressive timing and dynamics. I will then provide an overview of previous approaches to measuring and modeling musical expression in different contexts. One relatively unexplored task is that of analyzing music for which no musical score is available. I will describe why this is a different problem than score following and will argue for the importance of score-free models in working with improvisation and related tasks. I will provide some preliminary results employing a correlation-based model (Autocorrelation Phase Matrix; APM) to infer metrical trees in unscored music. Expressive timing and dynamics can then be measured with respect to these trees. A note for the audience: \\ The spirit of NIPS workshops is to bring new and perhaps even half-baked ideas to the table. In that spirit, my hope is that my talk is derailed into a discussion about measuring and modeling expressive timing and dynamics in unscored streams of MIDI or audio.
3735 en Project Presentation: Closing the Loop of Sound Evaluation and Design (CLOSED) Objectives: Despite being a promising and lively playground, sound design is not a solid discipline yet. We believe that the reason is to be found in the lack of design-oriented measurement and evaluation tools. \\ The CLOSED project aims at providing a functional-aesthetic sound measurement tool that can be profitably used by designers. At one end, this tool will be linked with physical attributes of sound-enhanced everyday objects; at the other end it will relate to user emotional response. The measurement tool will be made of a set of easy-to-interpret indicators, which will be related to use in natural context, and it will be integrated in the product design process to facilitate the control of sonic aspects of objects, functionalities, and services encountered in everyday settings. \\ The aim of the CLOSED project is to provide such concepts and tools, toward closing the loop of sound evaluation and design. \\ more on http://closed.ircam.fr/
3736 en Project Presentation: Emergent Cognition through Active Perception (EmCAP) EmCAP (Emergent Cognition through Active Perception) is a research project in the field of Music Cognition funded by the European Commission (FP6-IST). It started in October 2005 and will finish by September 2008. \\ Our goal is to investigate how complex cognitive behaviour in artificial systems can emerge through interacting with an environment, and how, by becoming sensitive to the properties of the environment, such systems can autonomously develop effective representations.
3737 en Hierarchical Bayesian Models for Audio and Music Processing In recent years, there has been an increasing interest in statistical approaches and tools from machine learning for the analysis of audio and music signals, driven partially by applications in music information retrieval, computer aided music education and interactive music performance systems. The application of statistical techniques is quite natural: acoustical time series can be conveniently modelled using hierarchical signal models by incorporating prior knowledge from various sources: from physics or studies of human cognition and perception. Once a realistic hierarchical model is constructed, many audio processing tasks such as coding, restoration, transcription, separation, identification or resynthesis can be formulated consistently as Bayesian posterior inference problems. \\ In this talk, we will review recent advances in various signal models for audio and music signal analysis. In particular, factorial switching state space models, Gamma-Markov random fields will be discussed. Some models admit exact inference, otherwise efficient algorithms based on variational or stochastic approximation methods can be developed. We will illustrate applications on music transcription, tempo tracking, restoration and source separation applications.
3738 en Hallucinations in Auditory Perception In this talk I want to review the need for richer architectures for auditory processing. Many experiments point to the tangled web of connections in the perceptual system, yet our engineering solutions remain almost exclusively bottom-up. How is it that we can provide context, so that our systems can solve musical analysis and auditory scene-analysis problems? I'll talk about notable systems that are successful "hallucinators."
3739 en An Auditory Model for the Detection of Perceptual Onsets and Beat Tracking in Singing We describe a biophysically motivated model of auditory salience and present results which show that the derived measure of salience can be used to successfully identify the position of perceptual onsets in a musical stimulus. We evaluate the method using a corpus of unaccompanied freely sung stimuli. We briefly show that perceptual onsets detected by the model are in good agreement with those identified by a combination of state-of-the-art algorithms and manual correction. We show that this continuousmeasure of salience can be used to track and predict rhythmic structure on the basis of its periodicity, thus avoiding the necessity for ad hoc decisions as to if, or when, an event has occurred.
3741 en Model Compression: Bagging your Cake and Eating it too (part 1) 
3742 en Model Compression: Bagging your Cake and Eating it too (part 2) 
3743 en Architecture Conscious Data Analysis: Progress and Future Outlook Over the past several years, architectural innovation in processor design has led to new capabilities in single-chip commodity processing and high end compute clusters. Examples include hardware prefetching, simultaneous multithreading (SMT), and more recently true chip multiprocessing. At the very high-end, systems area networking technologies like InfiniBand have spurred the development of affordable cluster-based supercomputers capable of storing and managing peta bytes of data. We contend that data mining and machine learning algorithms which often require significant computational, I/O and communication resources, stand to benefit from such innovations if appropriately leveraged. The challenges to do so are daunting. \\ First, a large number of state-of-the-art data mining algorithms grossly under-utilize modern processors, the building blocks of current generation commodity clusters. This is due to the widening gap between processor and memory performance and the memory and I/O intensive nature of these applications. Second, the emergence of multi-core architectures to the commodity market, bring with them further complications. Key challenges brought to the fore include the need to enhance available fine-grained parallelism and to alleviate memory bandwidth pressure. Third, parallelizing data mining algorithms on a multi-level cluster environment is a challenge given the need to share and communicate large sets of data and to balance the workload in the presence of data skew. \\ In this talk I will discuss progress made in the context of these challenges and attempt to demonstrate that ``architecture conscious" solutions are both viable and necessary. I will attempt to separate general methodologies and techniques from specific instantiations whenever it makes sense. We will conclude with a discussion on future outlook, both in the context of systems support for next generation algorithms as well as in terms of educational objectives brought to the fore in this context. \\ This is joint work with my graduate students Gregory Buehrer, Amol Ghoting and Shirish Tatikonda.
3744 en Who is Afraid of Non-Convex Loss Functions? The NIPS community has suffered of an acute convexivitis epidemic: \\ - ML applications seem to have trouble moving beyond logistic regression, SVMs, and exponential-family graphical models; \\ - For a new ML model, convexity is viewed as a virtue; \\ - Convexity is sometimes a virtue; \\ - But it is often a limitation. \\ ML theory has essentially never moved beyond convex models - the same way control theory has not really moved beyond linear systems.
3745 en Large Scale Learning with String Kernels In applications of bioinformatics and text processing, such as splice site recognition and spam detection, large amounts of training sequences are available and needed to achieve sufficiently high prediction performance on classification or regression tasks. Although kernel-based methods such as SVMs often achieve state-of-the-art results, training and evaluation times may be prohibitively large. When single kernel computation time is already linear (w.r.t. the input sequences) it seems difficult to achieve further speed ups. In this work we describe an efficient technique for computing linear combinations of string kernels using sparse data structures such as explicit maps, sorted arrays and suffix tries, trees or arrays [5]. As computing linear combinations of kernels make up the dominant part of SVM training and evaluation, speeding up their computation is essential. Considering the recently proposed and successfully used linear time string kernels, like the Spectrum kernel [2] and the Weighted Spectrum kernel [3] we show that one can accelerate SVM training by factors of 7 and 60 times, respectively, while requiring considerably less memory. Our method allows us to train string kernel SVMs on sets as large as 10 million sequences [4]. Moreover, using these techniques the evaluation on new sequences is often several thousand times faster, allowing us to apply the classifiers on genome-sized data sets with seven billion test examples [6]. The presented algorithms are implemented in our Machine Learning toolbox SHOGUN for which the source code is publicly available at http://www.shogun-toolbox.org. \\ References: [1] D. Gusfield. Algorithms on strings, trees, and sequences. Cambridge University Press, 1997. [2] C. Leslie, E. Eskin, and W. S. Noble. The spectrum kernel: A string kernel for SVM protein classification. In R. B. Altman, A. K. Dunker, L. Hunter, K. Lauderdale, and T. E. Klein, editors, Proceedings of the Pacific Symposium on Biocomputing, pages 564–575, Kaua’i, Hawaii, 2002. [3] G. R¨atsch and S. Sonnenburg. Accurate Splice Site Prediction for Caenorhabditis Elegans, pages 277–298. MIT Press series on Computational Molecular Biology. MIT Press, 2004. [4] S. Sonnenburg, P. Philips, G. Schweikert, and G. R¨atsch. Accurate splice site prediction using support vector machines. BMC Bioinformatics, 8, 2007. [5] S. Sonnenburg, G. R¨atsch, and K. Rieck. Large-scale learning with string kernels. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines, Neural Information Processing Series, pages 73–104. MIT Press, Cambridge, MA, 2007. [6] S. Sonnenburg, A. Zien, and G. R¨atsch. ARTS: Accurate Recognition of Transcription Starts in Human. Bioinformatics, 22(14):e472–480, 2006.
3746 en Speeding Up Stochastic Gradient Descent n order to tackle large-scale learning problems whose solution necessarily involves a large model with many tunable parameters, difficult non-convex optimization has to be performed efficiently. Computational complexity arguments strongly suggest that deep architectures will be necessary to represent the kind of complex functions that AI involves. Unfortunately, this involves difficult optimization problems and efficient approximate iterative optimization becomes key to obtain good generalization, and not so much the regularization techniques that have been so well studied in the last two decades. Furthermore, because of the size of the data sets involved in such tasks, it is imperative that computation scale no more than linearly with respect to the number of training examples. In many cases, the algorithm to beat is stochastic gradient descent, and the comparisons have to be made by looking at the curve of test error versus computation time. Following recent interest in online versions of second-order optimization methods, we present computational tricks that yield a linear time variant of natural gradient optimization. Another issue, that is particularly difficult to address in the optimization of multi-layer neural networks, is how to parallelize efficiently. SMP machines becoming cheaper and easier to use, we compare and discuss different strategies for exploiting parallelization of training for multi-layer neural networks, showing that naive approaches fail but those taking into account the communication bottleneck yield impressive speed-ups.
3747 en Stationary Features and Folded Hierarchies for Efficient Object Detection Most discriminative techniques for detecting instances from object categories in still images consist of looping over a partition of a pose space with dedicated binary classifiers. This strategy is inefficient for a complex pose, i.e., for fine-grained descriptions: i) fragmenting the training data, which is inevitable in dealing with high in-class variation, severely reduces accuracy; ii) the computational cost at high pose resolution is prohibitive due to visiting a massive pose partition. \\ To overcome data-fragmentation I will discuss a novel framework centered on pose-indexed, stationary features, which allows for efficient, one-shot learning of pose-specific classifiers. Such features assign a response to a pair consisting of an image and a pose, and are designed so that the probability distribution of the response is constant if an object is actually present. To avoid expensive scene processing, the classifiers are arranged in a hierarchy based on nested partitions of the pose, which allows for efficient search. The hierarchy is then "folded" for training: all the classifiers at each level are derived from one base predictor learned from all the data. The hierarchy is "unfolded" for testing: parsing a scene amounts to examining increasingly finer object descriptions only when there is sufficient evidence for coarser ones. I will illustrate these ideas by detecting and localizing cats in highly cluttered greyscale scenes. This is joint work with Francois Fleuret.
3748 en Efficient Machine Learning using Random Projections As an alternative to cumbersome nonlinear schemes for dimensionality reduction, the technique of random linear projection has recently emerged as a viable alternative for storage and rudimentary processing of high-dimensional data. We invoke new theory to motivate the following claim: the random projection method may be used in conjunction with standard algorithms for a multitude of machine learning tasks, with virtually no degradation in performance. Thus, random projections can been shown to result in both significant computational savings and provably good performance.
3749 en New Quasi-Newton Methods for Efficient Large-Scale Machine Learning The BFGS quasi-Newton method and its limited-memory variant LBFGS revolutionized nonlinear optimization, and dominate it to this day. Their application to large-scale machine learning, however, has been hindered by the fact that they assume a smooth, strictly convex, and deterministic objective function in a finite-dimensional vector space. Here we relax these assumptions one by one, and present (L)BFGS variants newly developed in our group that perform well on non-convex smooth, quasi-convex non-smooth, and non-deterministic objectives. Paradigmatic applications include parameter estimation in MLPs (non-convex smooth) and SVMs (convex non-smooth), and stochastic approximation of gradients (non-deterministic) for efficient online learning on large data sets. \\ We are also able to lift LBFGS to an RKHS for online SVM training. In all these cases our BFGS variants outperform previous methods on a wide variety of models and data sets, from toy problems to large-scale data-mining tasks.
3750 en Large-Scale Euclidean MST and Hierarchical Clustering We present new fast algorithms for performing the single-linkage hierarchical clustering method, a classical data mining method used heavily in bioinformatics and astronomy, given similarities which are metrics. We present experimental results that demonstrate significant speedup over previous algorithms on both synthetic and real data, including a dataset of 3 million astronomical observations and a dataset of protein folding trajectories. Additionally, our algorithms use considerably less storage than previous methods. More generally, our algorithm appears to be the fastest practical solution to the well-known Euclidean Minimum Spanning Tree problem.
3751 en Large Scale Sequence Labelling The general sequence labelling problem consists in processing an input sequence (xi) and producing an output sequence (yi) of discrete labels. Since the space of the possible output sequences is discrete, this can be viewed as a massive classification problem. \\ The notion of structured output prediction arises when one makes strong modelling assumption in order to learn the association with a reasonable number of examples. The conditional independence assumption states that a label it can be modelled as a function of the inputs (xt+i), i 2 I and the labels (yt+j), j 2 J for suitable choice of the sets I and J . The invariance assumption states that this function does not depend on t. The choice of sets I and J has a non trivial impact on the generalization performance and on the training and testing times.
3753 en Introduction to the Workshop In this workshop, we aim to highlight important problems and to gather ideas of how to address them. The target audience are practitioners, providing insight into and analysis of problems with certain methods or comparative studies of several methods, as well as theoreticians interested in characterizing the hardness of continuous distributions or proving relevant properties of an established method.
3754 en Infer.NET - Practical Implementation Issues and a Comparison of Approximation Techniques Infer.NET is an efficient, general-purpose inference engine developed at Microsoft Cambridge by Tom Minka, John Winn and others. It aims to be highly efficient, general purpose and extensible --- three normally contradictory goals. We have largely managed to achieve these goals using a compiler-like architecture, so that code is generated to perform the desired inference task. Infer.NET can apply one of a range of inference algorithms to a given probabilistic model, and so provides a useful framework for comparing the performance of different algorithms. In this talk, I will describe the capabilities and infrastructure of Infer.NET and give examples of applying both expectation propagation and variational message passing on the same model. I will also describe some failure cases that we have encountered for each algorithm.
3755 en Approximating the Partition Function by Deleting and then Correcting for Model Edges 
3756 en Variational Optimisation by Marginal Matching 
3757 en Improving on Expectation Propagation 
3758 en Large-scale Bayesian Inference for Collaborative Filtering The Netflix prize problem provides an excellent testing ground for machine learning. The problem is large scale and the data complex and noisy. It is therefore likely that relatively complex models with careful regularization are needed in order to get reasonable predictions. A Bayesian modeling approach seems ideal for the task if it is possible to scale it up to the size of the Netflix data set, where extremely high-dimensional Bayesian expectations will possibly have to be approximated. In this talk, an ordinal regression low-rank matrix decomposition model is presented. We use a variational Bayes (VB) inference algorithm to demonstrate that it is possible to make a large scale Bayesian algorithm. This model also highlight some of the general limitations of VB. The more accurate expectation propagation/expectation consistent (EP/C) inference cannot be applied to this bi-linear model without further approximations. We therefore propose a hybrid approach with EP/C inspired modifications of the VB algorithm. We compare the different variational approximations with a Laplace approximation, a MAP approximation and a Hamiltonian MCMC. In the latter one sample takes around 6 hours of computing time on a 1GHz processor, with fast C++ code, so there is a very clear case to be made for deterministic approximate inference. Another good feature of the Netflix data is the magnitude of the the test set which makes even small differences in the performance significant.
3759 en Perturbative Corrections to Expectation Consistent Approximate Inference Algorithms for approximate inference usually come without any guarantee for the quality of the approximation. Nevertheless, we often find cases where such algorithms perform extremely well on the computation of posterior moments when compared to time consuming (and in the limit exact) MC simulations or exact enumerations. \\ A prominent example is the Expectation Propagation (EP) algorithm when applied to Gaussian process classification. Can we understand when and why we can trust the approximate results or, if not, how we could obtain systematic improvements? \\ In this talk, we rederive the fixed point conditions of EP using the ideas of expectation consistency (EC) [1] and explicitly consider the terms neglected in the approximation. We will show how one can derive a formal (asymptotic) power series expansion for this correction and compute its leading terms. We will illustrate the approach for the case of GP classification and for networks of Ising variables. \\ [1] Expectation Consistent Approximate Inference, Manfred Opper and Ole Winther, JMLR 6, 2177 - 2204 (2005).
3760 en A Completed Information Projection Interpretation of Expectation Propagation This talk presents an interpretation of expectation propagation (EP) as a hybrid between two different iterated Bregman projections algorithms from the convex analysis and programming literature whose convergence behavior is well studied. It is suggested that convergence results for EP may be developed through this interpretation by adapting relevant convergence proofs for the related projections algorithms. Example convergence results for special cases of EP are derived through this connection, as well as through a connection between EP and the Gauss Seidel iterative solution method.
3762 en Learning, Information Extraction and the Web 
3763 en The Security of Mobile Agent Systems In [2] a model for mobility, called Petri hypernets, was presented. Hypernets offer a visual formalism to describe hierarchically structured dynamic agents. The agents take the form of Petri nets, which manipulate other agents as resources. In [4] a logic was proposed in which one can describe the temporal and the structural properties of agents in a hypernet. One can also analyse some properties of hypernets expressible as Petri net invariants thanks to the translation of hypernets to 1-safe Petri net systems given in [3]. Recently, see [1], we have addressed the problem of modelling complex, hybrid discrete agent systems in a modular way. The idea here is to obtain the view of the complete system from a number of simpler views, each devoted to a specific perspective. One view, for instance, could describe how some mobile agents can evolve in time — this view could be captured as a hypernet. Another could address other issues, for instance the rights of an agent to read some messages. The purpose of the talk is to describe how the approach can be used to model, specify and verify some security aspects of mobile agent systems.
3764 en Subgroup Discovery: Recent Biomedical Applications This talk presents recent advances in data mining, focusing on subgroup discovery and the ways to use subgroup discovery to generate actionable knowledge for decision support. Actionable knowledge is explicit symbolic knowledge, typically presented in the form of rules, that allow the decision maker to recognize some important relations needed to perform an appropriate action, such as planning a population screening campaign aimed at detecting individuals with high disease risk. Different subgroup discovery approaches are outlined, illustrated with case studies from medicine and functional genomics.
3765 en Neutrinos: Discovering a New Physics World The discovery of neutrino mass and mixing is one of the major discoveries in particle physics in the recent years. Non-zero neutrino masses and mixing are considered as the first direct evidence of new physics beyond the Standard model. Phenomenological consequences of this discovery as well as possible implications for fundamental theory will be discussed. Future programs of research and possible developments of the neutrino technologies will be outlined.
3767 en A compact high-brilliance SAXS/SWAXS/GISAXS instrument for laboratory use A compact modular laboratory unit for 2-D SAXS, SWAXS and GISAXS with high brilliance is presented, that facilitates nanostructure analysis of bulk materials, liquid crystals, (bio-) polymer or nanoparticle solutions, and thin solid films. The system, S3-MICRO, is based upon a combination of a point-focus microbeam delivery system (GeniX from Xenocs, Grenoble, France) working at a maximum power of 50 Watt with the Hecus S3-camera architecture with 1-D and 2-D detectors. With monochromatized Cu radiation, a SAXS resolution of Qmin &gt; 0.003 Å-1, corresponding to d-values of ~ 2000 Å, can be easily achieved. Due to the point-cross-section of the primary beam, desmearing of the scattering data is not required. The beam brilliance exceeds that of high-power rotating-anode SAXS systems by a factor of &gt; 3, and that of conventional line-focussing systems by two orders of magnitude. This allows to reduce measuring times down to seconds for many problems in nanostructure research, facilitating high-throughput (e.g in structural proteomics) and on-line process analytical applications. Due to the low electrical power (50 Watt) and the consequently minimal cooling requirements, the system design is extremely compact and economical, and therefore optimally suited for installation in multi-facility analytical laboratories, or in mobile test stations. The software package includes modules for system control (SWAXView, Labview ® based) and for on-line data analysis (EASYSWAX) supporting automatic multi-sample analysis and routine analytical applications in R&amp;D and QC.
3774 en Welcome address and Opening speach 
3776 en The rule of law and the role of the courts: The perception of law from within A proper understanding of the role of the courts in safeguarding the rule of law as an independent branch of government is a prerequisite for any viable public appreciation of the law in general and the judicial system in particular. This is notably and quite obviously the case with the perception of the role of the courts from the viewpoint of the two “political” branches of government as well as the general public, which will presumably be the focus of other talks in the conference; but it is also partly dependent upon the self-positioning of the judiciary, which will be the main focus (or the originating point) of this talk. This task may be addressed in several ways. At a principled level, it is the challenge of reconciling the (internal) need for an independent – or autonomous – judiciary with the (external) drive for its accountability. At a more concrete level, it is translated into individual choices that fall somewhere between the lines of judicial restraint and judicial activism, epitomized by the now slightly démodé “political question” doctrine. While the doctrine itself has somewhat disappeared from the legal debate in the recent years, the questions it posed remain just as valid today. Should judges be making policy? Where is the line to be made between what is the proper realm of adjudication and what goes beyond into improper interference with the other branches of government? Can judicial activism ever be defended as proper and, if so, to what extent and under what circumstances? May a certain increased level of judicial activism be justified in transitional societies, if and inasmuch as it is used to further the cause of judicial independence? The talk will address these questions with reference to recent examples from across the globe of their actuality and practical volatility, ranging from the relatively harmless academic questioning of Justice Ginsburg’s dissent in the recent Ledbetter v. Goodyear Tire &amp; Rubber case, calling upon Congress to “correct” the Supreme Court’s “parsimonious reading” of a certain legislation and trying to “propel legislative change”, to the critical case of the unfortunate illegitimate ousting of absurdly vast numbers of judges in Pakistan as an attempt of stifling judicial autonomy in anticipation of judicial decisions unfavorable to the government.
3777 en Measuring Public Confidence in European Courts This paper considers indicators of public confidence in the courts in nine continental European Union countries, considering ways this may be evaluated, factors contributing to or detracting from public confidence, and ways some European countries have addressed public interests and concerns. We draw on a European study that set out to explore ways of measuring the quality of justice. nnThe study developed a framework within which participating nations reported on their efforts to measure and improve the performance of their judicial systems, including the institutional framework, recruitment and training, means of evaluation, and measures of satisfaction and quality. The European research aimed to understand the quality measurement systems that had been used in the jurisdictions involved in the study, to assess their impact and the possibility of generalising any of them to other jurisdictions. nnIt was by no means assumed that quality was to be measured quantitatively. As will be seen in more detail shortly, a vast range of indicators was available, from financial data and information on appeals and their outcomes to public campaigns in the media and on the streets. While noting a tendency for judges and managers to value different types of information and to appeal to diverse sets of values, the researchers remained equally interested in all approaches. We return, below, to some further observations on the tensions between legal and managerial approaches and the possibility of their reconciliation. In the present paper we explore a variety of ways in which courts and other public institutions may be able to gather evidence of public confidence, or the lack of it, in order to make judgments as to their standing in the community. nnThroughout the study we have been particularly interested in measures that may lead courts to some action to improve or reform their performance, and here we report on those measures that appeared to enhance the capacity of the courts to earn public confidence. Let us consider what information we might be able to gain to better understand its quality, which may be of more interest than its quantity.
3781 en ECHR Caselaw on Media and Judiciary 
3786 en The first inorganic nanopods and nanobuds The discovery of WS2 and MoS2 nanotubes and inorganic fullerene-like nanoparticles (IF), shortly proceeded the discovery of carbon fullerenes (C60), nanotubes and onions. The WS2 and MoS2 nanomaterials have shown important applications as solid lubricants, electron devices, catalysts, super shock absorbers, etc. In tribology, on contrary to plate-like crystals, IF exhibit ultra-low friction and wear even in humidity. The mass production of non-agglomerated IF is an important challenge. We report on new production approaches of new forms of IF in macroscopic quantities. The synthesis is based on sulphurization of MoOx, WOx and MoxSyIz quasi one-dimensional precursors, which transform to MoS2 or WS2 based IF nanomaterials. The results of various characterisation methods reveal the possible mechanism of the formation of these new complex nanomaterials. W5O14 nanowires are synthesized by chemical transport reaction using NiI2 as a growth promoter. The light-blue crystals of metallic conductivity with specific resistivity of 27 ??cm have grown up typically to several millimetres in length and up to 200 nm in wideness. This rarely synthesized phase was reported as homogeneous phase only in 1978 by I.J. McColm et al., and in meantime declared as the compound, which may hardly exist. The W18O49 nanowires up to several millimetres in length are synthesized by a chemical transport reaction using iodine as a transport agent. The morphology of the wires can be controlled by the ratio between starting materials and by the growth conditions. By optimisation we can gain pure purple W18O49 phase with crystals several hundred nanometers wide or blue nanowire W18O49 phase with wideness bellow 200 nm. The first inorganic nanobuds – WS2 nanotubes decorated on outer surface with fullerene-like particles are synthesized by sulphurization of the W5O14 nanowires. The sulphurization takes place in a gas mixture of 1%H2, 1% H2S and 98 % Ar with a flow rate of 30 ml/min at 1050 K for 2 hours. The fullerene-like particles nucleate in surface corrugations of the nanowires and grow up by a diffusion process simultaneously with the transformation of nanowires to hollow multi-wall nanotubes. At slightly lower sulphurization temperature the material was transformed to WS2 nanotubes. The diameter of the tubes or nanobuds is smaller than the wideness of precursor nanowires revealing an exfoliation of the precursors. The sulphurized W18O49 nanowires transform to nanotubes decorated with flakes of WS2, but not forming the fullerenes-like particles. Nanowires based on Mo6S2I8 were also used as precursor crystals for sulphurization. We synthesized the Mo6S2I8 nanowires directly from elements at 1320 K. The prolonged reaction time (72 hours) resulted in several millimetre long needles having a diameter from several tens to a few hundred nanometers. The result of the sulphurization are the first MoS2 nanopods or “mama-tubes” - spherical MoS2 nanoparticles grown in the confined geometry of MoS2 nanotube reactors.
3787 en Nanostructuring of polycristalline gold thin films, deposited on glass, by means of ion beam Thin solid films appear most commonly in polycrystalline form, which means that they have higly constrained single-crystalline grains. Polycrystalline films are used in a large variety of devices, such as magnetic storage media, catalytic and thermal elements, protective coatings. It is thus desirable to extend to polycrystalline films, the approaches which have been developed for the self-organised formation of nanostructures on single-crystalline metal substrates. Ion beam sputtering can be used to modify surfaces on a nanoscale level, in most cases the result is formation of ripples on the surface. Substrates with well-defined vertical roughness, controlled orientation and periodicity can be achieved by varying macroscopic parameters that influence ripple formation, such as ion beam energy and ion beam dose. Thin Au films (150 nm thick) were deposited on glass microscope slides by two different deposition techniques: thermal and sputter deposition, thus resulting in different initial grain sizes and grain size distributions. The films were then ion beam sputtered in a sequence of different times to determine the evolution of the morphology and the role grain size plays in the morphological characteristics of ion beam sputtered thin films. Resulting morphology was then characaterised by FE-SEM imaging and by AFM, giving data on roughness, wavelength and underlaying grain size evolution. For comparison, commercially obtained gold films grown on mica, which had grain sizes in the order of a few hundred nm, were also included in the experiment.
3788 en Structural and electronic properties of molybdenum chalcohalide nanowires We combine ab initio density functional and quantum transport calculations based on the nonequilibrium Green’s function formalism to compare structural, electronic, and transport properties of Mo6S6-xIx nanowires with carbon nanotubes. We find systems with x=2 to be particularly stable and rigid, with their electronic structure and conductance close to that of metallic (13,13) single-wall carbon nanotubes. Mo6S6-xIx nanowires are conductive irrespective of their structure, more easily separable than carbon nanotubes, and capable of forming ideal contact to Au leads through thio groups.
3789 en Some topics in theoretical nanofriction As is natural for all properties determined by surfaces and interfaces rather than by bulk, adhesion and friction increase in importance when the objects that come into contact  and move relative to one another are nanosized. In this talk I propose to review some theory and simulation work recently done in our group on topics and models that are inspired by nanofriction.nnAmong them are:n * Some strange phenomena determined by the sliding of periodic but incommensurate systems.n * The effect of high temperatures and surface melting on the friction felt by an AFM tip.n * The negative differential friction predicted for high-speed sliding coaxial nanotubes.n * The diffusive-ballistic crossover predicted for the frictional slip time of gold nanoclusters on graphite.
3790 en Zero-bias conductance through coupled quantum dots Using three supplementary numerical methods: a) quantum Monte Carlo algorithm based on the constrained path method, b) variational approach, and c) numerical renormalization group technique we compute zero-temperature conductance through different interacting regions. Comparison of our results with those obtained with the essentially exact Bethe ansatz method reveals excellent agreement. We then extend our calculations to three quantum dots coupled in series as well as to multiple quantum dot systems coupled in parallel. We study the effect of various strengths of inter-dot overlap on the shape of Kondo plateaus that appear as a function of the gate voltage. Our results for conductance are further supplemented with calculations of various correlation functions in terms of the gate voltage. In the limit when the overlap between quantum dots is small, the system behaves as a two-channel Kondo model. We investigate a possibility for detecting a Non-Fermi liquid behavior in the system of weakly coupled quantum dots and discuss its experimental relevance. We also present the phase diagram containing different Kondo regimes.
3791 en Magnetic impurity formation in quantum point contacts A quantum point contact (QPC), a narrow region separating two wider electron reservoirs, is the standard building block of sub-micron devices, such as quantum dots - small boxes of electrons, and qubits - the proposed basic elements of quantum computers. As a function of its width, the conductance through a QPC changes in integer steps of G0 = 2e2/h, signalling the quantization of its transverse modes. Such measurements also reveal an additional shoulder at a value around 0.7 ? G0 which has become known as the 0.7 anomaly. Recently it has been suggested that this phenomenon can be explained if one invokes the existence of a magnetic impurity in the QPC at low densities. Here we report on our extensive density functional calculations that reveal the formation of an electronic state with a spin-1/2 magnetic moment in the channel as the density increases above pinch-off, under very general conditions.
3792 en An efficient, mixed semiclassical/quantum mechanical model to simulate planar and wire nano-transistors The design of miniaturized planar and nanowire field effect devices for CMOS-compatible nano-electronics poses new challenges in the field of nanostructure modelling. Pros and cons of innovative devices incorporating different channel materials (Si, Ge, strained-Si, III-V) and crystal orientations need to be assessed. Transistors with body thickness (TSi) and channel length (L) of few nanometres have been demonstrated [Uch02,Wak03], where strong quantization effects in the vertical (y) direction coexist with quasi-ballistic, far from equilibrium carrier transport in the lateral (x) direction. In the field of engineering applications, full quantum–mechanical modelling of realistic nano-transistors has been so far mainly restricted to ballistic transport [Lau04]. However, scattering in the channel is still remarkably important to predict the drain current, even for nano-devices with L ≈10 nm [Pal04]. Moreover, it is unclear if the complex full quantum treatment of the scattering would lead to manageable numerical models, and if the expected huge computation times will be paid off by improved accuracy. In this contribution, we report recent advances in the development of an efficient modelling framework capable to combine the accuracy of quantum mechanical simulations with a semiclassical treatment of carrier transport aimed at the accurate calculation of the main performance metrics of planar (and wire-like) devices for nano-electronic applications.
3793 en Optical Properties and Excitonic Effects in Mo-S-I Nanowires In this contribution we discuss the role of excitons in the dielectric function and in the optical properties of Mo-S-I nanowires. We make use of the “EXCITING” package, developed in Graz. The package is based on the solution of the two-particle Bethe-Salpeter equation (BSE) for the electron-hole pairs and includes the electron-hole corre-lations. We focus on the Mo6S6 nanowires despite the fact that so far only its close relative Mo6Se6 has been synthesized. Mo6S6 has no bridging anions and the dressing anions are all S atoms. 2. It has very promising, strongly anisotropic mechanical as well as electronic properties.
3794 en Can particles at the nanoscale result in better drug delivery systems? Due to the advances of molecular biotechnology and bioinformatics, there is a strong increase of new candidate drug molecules. However, their transport to the actual site of action and bioavailability are decisive issue. In order to enable adequate delivery of small or larg drug molecules some special formulation will be required. Current advances in material science and nanotechnology promise the development of new generations of drug carriers for these active substances for diagnostic and therapeutic purposes. Today are mostly investigated liposomes, nanoparticles, dendrimers, nanotubes etc. where size, geometry, porosity, dispersity, surface chemistry are highly defined. Carrier composition and structure promises control over biological fate. In the talk three examples studied in our lab will be used to illustrate these points: preparation of drug loaded solid lipid nanoparticles, interactions of nanoparticles with the cells and surfactant selection for adequate nanoparticles production that causes minimal toxicity to HEK 293 cells, while preventing nanoparticles aggregation. These observations indicate that in the design and development of novel nanoparticles for drug delivery a combination of factors such as composition, size and surface properties influence bioavailability, uptake into the cells and toxicity. Current work focuses on systematic variation of these parameters to develop carrier system with designed function.
3795 en Formulation of PLGA nanoparticles for intracellular delivery of protein drug Design and formulation of advanced drug delivery systems (DDS), such as nanoscale carriers, presents an attractive research area in the field of drug formulation. A vast contribution is expected in delivery of biopharmaceuticals as is clearly recognized that inadequate delivery is the single most important factor delaying their application in clinical practise. In spite of some successful guidelines, formulation of protein drugs in DDS requires step-by step strategy and methods differing from those used for classical pharmaceutical drugs since proteins are the most delicate ones in term of retaining their biological function. A model protein drug cystatin was selected in our work, having high potential for inactivating cysteine proteases, enzymes involved in processes of tumour invasion and metastasis. Nanoparticles was used as carrier system with the aim to increase the bioavailability of the protein drug by protecting it from premature degradation in biological environment and faciliting its intracellular delivery. Cystatin was incorporated in poly(lactide-co-glycolide) (PLGA) nanoparticles by the water-in-oil-in-water emulsion solvent diffusion method. To preserve its biological activity an optimized technique was developed, adjusting physical and chemical parameters of processes during nanoparticle production. Cystatin-loaded NPs had size of 300-350 nm diameter, and contained 1.6 % (w/w) of cystatin, retaining 85% of its starting activity. To follow cellular uptake of nanoparticles, cystatin was labelled with fluorescent dye (Alexa Fluor 488) prior to its encapsulation into NPs. Image analysis showed rapid internalization of NPs into MCF-10A neoT cells as the fluorescence spots were detected after treatment with NPs. On the other hand, labelled free cystatin was internalised very slowly, suggesting that NPs facilitate the delivery of its cargo into the cells. Cystatin, delivered by NPs, also exerted its inhibitory activity on intracellular target cathepsin B, suggesting that its integrity was preserved throughout the processes of formulation and delivery. On the other hand, free cystatin did not impart proteolityic activity of cathepsin B, when tested under the same conditions using the substate, specific for intracellular cathepsin B. Our results show that protein drug can be formulated in the active form into PLGA NPs, when suitably selecting the process parameters of NP-production. NPs are also able to facilitate delivery of protein drug into the cells, enabling its activity on the intracellular target
3796 en Nanoscale functionalities for biopharmaceutical drug delivery One of our recent research challenges has been exploitation of the active principle of protein aggregation, involving metal coordination of specifically designed protein analogues. Specifically designed analogues of tumour necrosis factor alpha (TNF-alpha) rich in histidines served as model proteins. LK801 is a TNF-alpha analogue with double histidine mutation (Glu107HisGly108HisTNF-alpha) in the tip region of the bell-shaped molecule. Due to the symmetrical trimeric structure, histidine residues in the tip region form an almost planar cluster of six well accessible histidines resulting in strong binding to Immobilized Metal Affinity Chromatography (IMAC), which was used for efficient single step purification. IMAC was also used for preparation of His10-TNF, a TNF-alpha analogue with His10 tag and amino acid sequence responsible for enterokinase cleavage added to the native N-terminus and H7dN6TNF analogue, which has a tag comprised of seven histidines on N-terminus. Two types of inorganic nanoparticles containing metal ions were prepared, zinc phosphate nanoparticles by precipitation, and zinc modified silica by adsorption of zinc ions on commercially available silica nanoparticles. For the proof of concept, in preliminary experiments bovine serum albumin (BSA) was used for binding. BSA contains naturally surface exposed histidines and was also expected to coordinatively bind to nanoparticles with metal ions. When such particles were exposed to low pH buffers or buffers containing imidazole, release of BSA was confirmed by SDS-PAGE analysis, thus proving the reversibility of metal-specific binding. In the next steps, histidine rich TNF analogues were used for binding to Zn-phosphate nanoparticles and release studies were performed under different conditions. We also measured biological activity of TNF-alpha analogues prior to binding and later after the release from inorganic nanoparticles. The controlled formation of TNF-alpha analogues nanoparticles was tried by protein self-assembly using metal ions (Zn2+). First experiments were performed with only the addition of zinc ions and later we performed experiments using Zn2+ and different biocompatible chelates (phytic acid and 1,4,8,11-Tetraazacyclotetradecane-1,4,8,11-tetraacetic acid). Upon administration of above mentioned protein aggregates to the testing animals, an increased immune response is anticipated. In the case of TNF-alpha analogues, an enhanced formation of antibodies against TNF-alpha would be advantageous serving as a basis for developing new drugs for chronic diseases associated with pathogenically elevated TNF-alpha levels (rheumatoid arthritis, Crohn disease, psoriasis, etc.). His10-TNF analogue appears especially interesting, since it exhibits very low in vitro cytotoxic activity. Upon formation of nanostructures, a significantly diminished number of accessible receptor binding sites and consequently even more reduced cytotoxicity is expected, leading to safe formation of anti-TNF-alpha antibodies.
3797 en Novel routes to nano-materials for Li-ion batteries Advanced Li-ion batteries providing enhanced storage capacities and improved power performances are currently required not only by the fast-growing market of portable electronics, but also by emerging electric or hybrid-electric vehicles. We are investigating two novel techniques for this purpose: Spark Discharge Generation (SDG) and Electrostatic Spray Reductive Precipitation (ESRP). SDG uses a physical “top down” approach that relies on the atomization of two metal rods via a sudden spark. Two cylindrical rods are connected to high voltage and parallel to a variable capacitance. The capacitors are periodically charged to the break-down voltage of the system determined by the gap between the rods. Through the high temperature of the generated spark, electrode material is rapidly evaporated, and the vapour condenses to form nano-sized metallic particles. In addition, an unconventional densification technique, called Magnetic Pulse Compaction (MPC), is being used for self-manufacturing metal or alloy rods to be atomized. ESRP is a physical-chemical technique relying on a combined “top-down” and “bottom-up” approach, which bridges aerosol generation with chemical precipitation in order to form nanoparticles. Electro-Spraying of liquids consists in the creation of charged aerosols by applying a high voltage between a nozzle, through which the liquid to be sprayed is fed, and a counter-electrode. Interesting properties of this phenomenon are the narrow size distribution of the emitted droplets, as well as tuning the droplet size by controlling experimental parameters. Moreover, high net surface charge on the generated droplets causes repulsive interaction, preventing droplet coalescence. These beneficial aspects have been exploited in combination with a well-known technique for the synthesis of metallic and alloyed particles, namely Reductive Precipitation (RP) of metal chlorides by NaBH4. Dissolved metal chlorides precursors are forced to flow by a syringe pump which provides a constant supply to the nozzle with a controlled flow rate. Under the influence of the high electric field small charged droplets are formed and attracted towards a ring-shaped counter electrode, which is placed in the reductive solution. In this way, the droplets containing precursor metal ions are driven into the reductive bath, where they are immediately reduced to their zero-valent state. Primary particles with size in the range of 2-5 nm can be obtained by proper selection of the experimental parameters.
3798 en Manipulation of Nanoscale Charged Polar States in Manganites The complicated interplay among charge, spin and lattice degrees of freedom in manganites is believed to induce the unexpected magnetic and transport phenomena, such as the colossal magnetoresistance (CMR). Manganites display also a variety of useful multiferroic properties such as colossal magnetocapacitance effect and high dielectric constant. In multiferroics ferromagnetic order can be controlled by an electric field, or ferroelectric order can be controlled by a magnetic field. Among them, La1-xSrxMnO3 is the most attractive candidate for multiferroic applications because of a combination of desirable properties. In this work we report the observation of the high contrast of electric field induced charged polar states after the local application of the electric field to the surface of samples via several Scanning Probe Microscopy (SPM) techniques in La0.9Sr0.1MnO3 and La0.89Sr0.11MnO3 single crystals. The electric-field-induced contrast is observed in Kelvin mode (KFM) confirming local modification of the surface properties of manganites. Piezoelectric effect of the induced states is assessed using Piezoresponse Force Microscopy (PFM). These results are complemented by the measurements of piezoresponse hysteresis and surface potential hysteresis loops at some area in standard pulse dc mode. The induced polar charged states relax with characteristic time constant of about 80 hours at room temperature, which exceeds Maxwell relaxation time by many orders of magnitude. The mechanisms of the observed phenomena are discussed along with the possible instrumentation effects. The origin of the effect can be related to the nanoscale charge and spin dynamic inhomogeneities appearing in manganites due to a delicate balance of charge, lattice and magnetic order. The injection of the additional charge carriers in the induced area promotes the appearance of the polar charged states. The long relaxation time for the induced charged state may be explained by the existence of the intrinsic inhomogeneous states. All these results show that the existence of the stable areas with the increased charge concentration is possible and thus it confirms the tendency towards charge segregation in manganites.
3799 en Self-catalysed growth of GaAs nanowires by MBE Semiconductor nanowires (NWs) growth is typically assisted by a metal particle, called the catalyst. The use as the catalyst of a material different from the components of the wire may change the semiconductor properties due to the diffusion of the catalyst in the nanowire body during the growth. Moreover, the most commonly used catalyst is Au, a metal that is incompatible with the Si technology. For the above mentioned reasons it is therefore of importance to develop a technology leading to a catalyst-free growth of semiconductor nanowires. Here we report preliminary results on catalyst-free growth of GaAs NWs by molecular beam epitaxy. The GaAs NWs have been grown on cleaved edges of Si wafers, with no catalyst pre-deposition. The growth lasted 30 minutes and has been performed at 580 ??T ??620 °C. Two kinds of nanowires have been obtained. The NWs of the first type are as long as 5 ?m with a section diameter in the range of tens of nm and have a spherical particle at their end tip. Energy dispersive X-ray spectroscopy (EDS) demonstrates that the spherical particle is composed of Ga, and that the NW body is GaAs. The NWs density depends on the crystallographic orientation of the facets that compose the cleaved edges of the Si-wafers. The second type of NWs are generally characterised by a smaller aspect ratio, clearly faceted lateral and tip surfaces, and no metallic particle on their tip. EDS curves reveal that they are completely made of GaAs. The EDS results suggest that a Ga induced self-catalysed growth occurred on specific surface locations where Ga droplets formed and were trapped during the first stages of the GaAs deposition. Work is in progress to understand the growth process and in particular to understand whether the droplet-less NWs grow through a different process or the absence of a Ga droplet is due to its lost during growth.
3800 en Carbon nanotubes added hexagonal WO3 films A world-wide concern for enviromental safety that demands monitoring the emission of hazardous gases into the atmosphere, combined with recent advances in wireless senzor networks is increasing the need of low-power gas sensors and low-cost. Among metal oxides, between theirselves the tungsten oxides are among the most used materials in electro-, photo- and gasochromic applications. In this work, a soft chemical nanocrystalline processing route has been demonstrated for the preparation of hexagonal tungsten oxides by the acidic precipitation of Na2WO4.2H2O solution at temperatures as low as 120 °C and 330 °C. The structural properties of films were investigated by TEM Philips CM-20. The sensing properties of films were measured to gaseous ammonia at various temperatures. We founded the correlation between structure and gas sensing properties of WO3 films. The average size of WO3.1/3 H2O crystallites is ~ 50 ÷ 100 nm. After the head treatment of films, the average size of WO3 crystallites decreased to ~ 30 ÷ 50 nm. The electron diffraction of the film confirmed the phase change from orthorhombic to hexagonal one. The gas measurements were performed by direct injection using a gas serynge, and the arrows in the graphics indicate the total amount of gas present in the measurement chamber after successive injections.
3801 en Preparation of the Bi12SiO20 thin films by the sol-gel method Bismuth silicon oxide Bi12SiO20 (BSO), which has a sillenite structure, is a piezoelectric, electro-optic, photo-refractive, and optically active material. Recently, the sillenites have begun to be considered for use as dielectric in the field of electronics. They are used as new material in LTCC (Low-temperature co-fired ceramic) technology. Because of the low sintering temperature, good chemical and dielectric properties of the “bulk” BSO1 we decided to prepare thin films of this material. In this paper we will report on the preparation of BSO thin films using sol-gel method on various substrates. The aim of work was to achieve pure BSO thin films with good control over their microstructure and thickness.
3802 en Photosensitive titanium oxide sols and gels for solar energy conversion and storage Due to the potential applications in the field of environmental protection, the photochemistry of TiO,,2,, is a fast growing area both in terms of research and commercial activity. Beside to the white pigment properties of rutile and anatase (e.g. paints and cosmetic products), titanium dioxide is used in heterogeneous catalysis and photocatalysis (water purification, air cleaning), in photoelectrochemical solar cells for the production of hydrogen and electricity, as an active layer in the design of electrochromic devices, as a gas sensor, as a corrosion-protective coating, in ceramics and in electric devices such as varistors, to name few. In such applications, the performance of titanium oxide could be optimized with specific nanostructural control over the morphology of the material. Under UV irradiation, an electron-hole pair is generated in TiO,,2,, then technological devices are based on chemical reactions or induced electron transfers. Due to the large band gap of TiO,,2,, (3.2eV), only 10% of the solar spectrum is used. A major objective for future work is the development of a semiconductor photocatalyst film which is able to utilize visible as well as UV light. Nanoscience has the potential to provide entirely new classes of materials with capabilities that transcend these limitations and generate the performance breakthroughs required for a viable economy based on sustainable energy. Recent investigations in this area allowed us to synthesize novel photo-sensitive titanium oxide sols and gels by controlling the condensation of titanium species in non aqueous solvents. Depending on different parameters such as concentration in Ti^^4+^^, ageing or thermal treatment, to emphasise few, the structuration of the inorganic framework leads to various layered structure. The adsorbed organic species control the growth of the nano-objects present in the sol or gel. Due to the enhanced surface area to volume ratio, these nanostructured sols and gels produce singular photo-electrochemical properties that are drastically different from their bulk counterparts. When irradiated, these materials can absorb photons with a lower energy than the bandgap energy of the original semiconductor, and thus a significant increase in the limiting efficiency of conventional solar cells is expected. These new materials are principally characterized by a partially occupied intermediate band, isolated from the valence and conduction bands. Our purpose is to use these intermediate band materials as sensitizers in both third generation photoelectrochemical solar cells and photo-batteries (ultracapacitors).
3803 en HAADF-STEM imaging: from qualitative to quantitative interpretation of atomic resolution HAADF-STEM images In our presentation an overview on qualitative and quantitative HAADF-STEM technique will be given and illustrated by examples of characterization of various inorganic ceramic materials, such as CaTiO3-La(Mg,Ti)O3 solid solution, GaN blue laser diode, bulk CaTiO3 and AO-doped SrTiO3 (A=Sr,Ca,Ba). The specimens for the HAADF-STEM observations were prepared by high-energy and/or low-energy ion milling and were observed in a FEG JEOL-2010F (Cs=0.48 mm). The probe semi-angle was 10 mrad. The inner and outer annular angles of the HAADF detector were 100 and 220 mrad, respectively. The HAADF-STEM image simulations were carried out using a calculation scheme developed by Watanabe. Our results showed that differences as small as 2% in the average atomic number Z can readily be detected by HAADF-STEM imaging. In qualitative interpretation of atomic-resolution HAADF-STEM images we compared intensity ratios between different atom columns or used intensity profiles to show the difference in the chemical composition between individual atom columns. In this way we could qualitatively interpret the ordering and/or partial ordering of solute atoms in bulk materials, evaluate the occupancy of atom columns in special structures and study the segregation of impurities along grain boundaries. Quantitative interpretation required image simulations and matching of the processed experimental images with the calculated ones. However, in order to calculate HAADF-STEM images the exact structure of the observed structural phenomena should be known, i.e., the positions of the atoms, in order to create proper supercells for calculations. The realistic values of the Debye-Waller factor should also be used in calculations. After image calculations appropriate matching algorithms with the experimental images were applied in order to determine the best fit between calculated and experimental image.
3804 en Structural properties of magnetic nanoparticles The basic magnetic properties of nanoparticles have been intensively studied and the influence of the small (nano) size on the magnetic properties is generally well understood. However, the magnetic properties of mixed-oxide nanoparticles depend on the size of particles not only directly, but also indirectly, through the influence of the small size on the structure of nanoparticles. It is known that the structure of nanoparticles is more flexible compared to the “bulk” structure. Usually, it adapts to the small size and the large surface-to-volume ratio resulting in distribution of atoms over different lattice sites that is significantly different to that of the bulk material. Additionally, defects are usually present in the structure of the nanoparticles. Different aspects of the deviations in the structure of nanoparticles from the ideal “bulk” state will be discussed in the cases of two structural types of magnetic nanoparticles: spinel ferrites (ZnFe2O4, Mn0.5Zn0.5Fe2O4, CoFe2O4) and hexagonal ferrite (hexaferrite, BaFe12O19). In the spinels, the deviation of the nanoparticles crystal structure from the bulk situation expresses itself mainly with different distribution of the constituting cations over two different lattice sites existing in the structure (tetrahedrally coordinated A sites and octahedrally coordinated B sites). In the case of hexaferrites, the exchange of two cations, Ba and Fe over different lattice positions is not likely. A hexaferrites' crystal structure can be described as a stacking sequence of two basic blocks, a spinel block S, containing Fe cations, and the block R, containing Fe and Ba cations. Here, the adaptation of the crystal structure to the small nanoparticle size by the change in the stacking of the two structural blocks is expected. The flexibility in the nanoparticles crystal structure results in a flexibility of the nanoparticles’ chemical composition, allowing large compositional deviations from the bulk stoichiometry without losing the single-phase structure. Depending on the method used for their synthesis, the nanoparticles also differ in the state of their crystallinity. The small spinel ferrite nanoparticles of controlled sizes could be relatively simply prepared already at low temperatures. In this work, two methods were used: a co-precipitation in reversed microemulsions and a thermal decomposition of the corresponding oleates. In contrary to spinel ferrites, relatively high temperatures are needed for the formation of hexaferrites making their controlled synthesis in the form of the small nanoparticles relatively difficult. Special hydrothermal methods were used for their synthesis in this work. The structure of the nanoparticles has been studied using X-ray diffractometry (XRD), high-resolution electron microscopy (HREM) coupled with energy-dispersive X-ray spectroscopy (EDX), and X-ray absorption spectroscopy (EXAFS, XANES) measured at E4 beamline of synchrotron radiation laboratory HASYLAB at DESY, Hamburg (project II-04-065 EC) under EU Contract RII3-CT-2004-506008 (IA-SFS).
3805 en The nanostructure of silicon thin films for solar cells A typical thin-film silicon solar cell is deposited on a glass substrate covered with a conductive transparent metal oxide. The active part of a solar cell consists of three to six silicon layers, each with a thickness of ten to several hundred nanometers, deposited in a layer-by-layer fashion. In these structures layers with different individual optical gaps is stacked together, in order to cover as much of the solar spectrum as possible. By changing the structure of the material, going from pure anorphous to monocrystalline, it is possible to obtain the variation in optical gap using the same material. Silicon in the form of nanocrystals drags in that sense particular attention in last decade. For any practical use, it is important to n know size and size distribution of nano particles in this kind of structure. n A series of multilayered silicon thin films was prepared by the decomposition of silane gas, diluted with hydrogen, in a radio-frequency glow discharge. Films with nanocrystallites (nc-Si) of different sizes were processed by varying the silane-to-hydrogen ratio. The nanostructures of the silicon thin films were studied by Raman spectroscopy (RS) and high-resolution transmission electron microscopy (HRTEM). Raman spectrum of the microcrystalline Si shows one intensive sharp band at 521 cm-1. For crystallites smaller than 30 nm this band (transversal optical - TO mode) is broadened and its position is shifted to lower frequencies. The shift is dependent on the average crystallite sizes. The size of the nanocrystallites in the investigated samples was estimated from the shift of the TO mode in the Raman spectra of the nc-Si after the deconvolution of the spectra. The volume fraction of the crystalline phase can be estimated from the ratio of the integrated intensities of the crystalline TO and the amorphous TO modes after the deconvolution of the Raman spectra. Since the deconvolution procedure influences the accuracy of obtained result, various methods of deconvolution were applied. Therefore, for the calculation of the Raman shifts and the integrated intensities, the spectra are frequently fitted as the sum of amorphous (Gaussian) contributions and crystalline contribution (Voight). We further deconvoluted the spectra, but using a somewhat different procedure than that one usually described in the literature. Since the spectra of the multilayered nc-Si thin films can be deconvoluted to the modes belonging to the a-Si and the TO mode of the nc-Si crystalline fraction, we first removed the amorphous contribution directly by subtracting the experimental spectra of completely a-Si from the spectra of our multilayered nc-Si samples. The TO band in the nc-Si appears as asymmetric and broad, which suggests the coexistence of smaller and larger crystals, so we deconvoluted the TO mode of the nc-Si into two components. The two components can be assigned as one belonging to the small crystallites, and the other to the larger crystallites.
3806 en Characterization and Properties of Novel Oxygen Contained Hollandite VO1.52(OH)0.77 Nanorods Synthesized by Nonaqueous Sol-Gel Route Using the nonaqueous sol-gel route based on benzyl alcohol pathway, a new compound VO1.52(OH)0.77 in the form of nanorods has been synthesized. Comprehensive structural investigations have been carried out using complementary neutron and synchrotron powder X-ray diffraction as well as different electron microscopy techniques: SEM, (HR)TEM, EDX, SAED, EELS for determination of material's morphology, crystallinity and oxidation state of vanadium. The data show that the structure can be described as hollandite-type containing only oxygen in the 2?2 channels along the c-axis, with hydrogen attached to the one octahedral-coordinating oxygen, forming thus OH? group. The nanorods are single-crystalline, up to 500 nm long and 105 nm in diameter with the growth direction along the axis. The shape of OK edge and L3/L2 white-line ratio confirmed that the vanadium oxidation state is between 3+ and 4+ which is also deduced from the charge neutrality analysis being +3.81. Temperature dependent DC conductivity measurement evidenced Arrhenius behavior and semiconducting ground state with a band gap of 0.64 eV (cooling mode). Ab initio density-functional calculations with local spin density approximation including orbital potential (LSDA+U) predicts a direct band gap of 0.64 eV and a high degree of hybridization between O 2p and V 3d orbitals. The temperature dependence of magnetic susceptibility follows the Curie-Weiss law above 150 K. The extracted effective magnetic moment per vanadium of 1.95 B is consistent with the mixture of 3+ and 4+ oxidation states with predominant fraction of 4+.
3807 en FT-IR microspectroscopy: a powerful tool for spatially resolved studies on supports for solid phase organic synthesis Solid Phase Organic Synthesis (SPOS) is an efficient technique for the synthesis of fine chemicals and for the development of compounds libraries through combinatorial approaches. SOPS performances can be optimized carefully tuning the chemo-physical properties of polymeric supports, usually porous beads, with particular reference to the distribution of the reaction products into the beads, which gives information on the pore accessibility, on the efficiency of the reactant’s diffusion process into pores and on the load capacity of the bead. Differently from optical transparent polymers, for which all parameters of interest can be quantified by two-photon microscopy, opaque supports are difficult to characterize by conventional analytical techniques. We propose a new method to systematically study parameters affecting performance of opaque supports for SPOS, based on FT-IR microspectroscopy on thin bead slides in transmission mode (All data collected with Bruker VERTEX 70 interferometer coupled with Hyperion 3000 IR microscope). Opaque amino-methacrylate beads with different pore diameters, Synbeads by Resindion s.r.l (Mitsubishi Chem. Corp., Milan, Italy), are acylated via chemical coupling with 3-nitropropionic acid at different reactions times. Functionalized and non-functionalized beads, cut in 5 microns slices, are chemically imaged with Focal Plane Array detector (FPA, 64X64 pixel) coupled with a IR conventional source, allowing a fast chemical imaging of the nitro functional group presence and distribution within the bead. In order to achieve a better S/N ratio and then more accurate details on nitro-group distribution, selected bead sections are also mapped at 5 microns spatial resolution along their diameter using a Mercury-Cadmium-Telluride (MCT) single-point detector operated with Synchrotron Radiation (SR). The combined approached proposed has the main advantage to be useful for each type of SPOS support material and allows, by FT-IR imaging, an easy and fast access to data as bead functionalization and functional group distribution. More accurate quantitative relations between bead polymer type, its degree of polymerization, bead pore dimension and mean porosity, supported reaction and synthesis condition can be achieved exploiting the high brightness of SR source and the major sensitivity of MCT detector.
3808 en Can high Tc superconductivity be explained with the BCS model? An optical approach 
3810 en The roles of Rule of Law and Judiciary in the German society The roots reach to the European tradition, to the English Magna Charta of 1215, to the Act of Settlement of 1701, to the enlightenment of the 18th century and to the liberty movements. The independence of judges is said to be one of the achievements of the Prussian kingdom of Frederick 2nd. When building his castle of Sanssouci he was annoyed by a the rattling of a mill that spoilt the silence of the park in front of his residence.. He tried to buy it from the miller. When the man refused to sell it because he did not want to lose the basis of his living the king angrily threatened him with expropriation. nnBut the brave man is said to have replied: If it were not for the Kammergericht (Court of Appeal), Your Majesty! The story has always made good public relation for the Rule of Law in Prussia, and the the mill is still to be seen. Unfortunately it is only a legend. The king was entitled to give the judges orders or to make a decision in their place. Frederick did so 33 times. Only in his political testament he recommended to his successors not to do that any more. Even if the judges really were independent in Prussia , another question was what kind of cases belonged to their jurisdiction. Maybe their jurisdiction applied to all civil and all criminal matters. nAs far as the state was concerned, however, judges were entitled to decide only on its civil (fiscal) matters (the business the monarch had with his subjects, civil contracts etc).. But not on the legality of the administrative procedure or the results of it. Very late in the 19th century they established a court that could deal with these matters which turned out to be a forerunner of our modern administrative judiciary. Much later than that were born the constitutional courts. nnThe people of the 19th century were preoccupied with getting constitutions from their monarchs; they did not bother so much yet about having it judged by an independent judiciary. As a matter of fact, constitutional matters were not tried earlier than after the foundation of the first German Republic in 1919. A special court for constitutional matters was first established after the 2nd world war in Western Germany with the Federal Constitutional Court in Karlsruhe...
3814 en Course introduction 
3815 en Introduction to data modelling This first presentation introduces the basic principles of data modelling together with linear in the parameters models.
3816 en The Multi-layer Perceptron This presentation describes the multilayer perceptron and practical issues in data modelling.
3817 en Functional Analysis in Data Modelling This second presentation describes the basic theory of functional analysis used in kernel methods.
3818 en Introduction to Support Vector Machines This first presentation introduces support vector machines.
3819 en Kernel Based Methods This second presentation covers more general kernel methods including training, model selection and practical aspects.
3820 en Dimensionality Reduction This presentation discusses methods for dimensionality reduction.
3821 en Semi-supervised learning This presentation is an introduction to semi-supervised learning.
3822 en Hierarchical Clustering This presentation describes approaches to hierarchical clustering.
3823 en Bayesian methods for data Modelling His presentation introduces the basic ideas of Bayesian methods for data modelling.
3824 en Graphical models This presentations provide an introduction to graphical models together with more advanced topics on inference, propagation and learning structure.
3825 en Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care This presentation describes the application of data modelling using Kalman filters to premature baby monitoring.
3826 en Learning with Gaussian Processes This presentation describes the basic foundations and advanced theory of Gaussian processes.
3827 en Application of Data Modelling to Fault Detection in an Internal Combustion Engine This presentation describes the application of data modelling to fault detection in an internal combustion engine.
3828 en Applications of Machine Learning to the Game of Go This presentation, based on his PhD from the University of Cambridge, describes a number of applications of machine learning to the game of Go.
3829 en Modelling Genomic Data This presentation describes work on the modelling of genomic data.
3830 en Panel discussion – Future Challenges in Data Modelling 
3831 en Applications to Machine Vision This presentation describes novel approaches to spatial inference problems in vision and image processing. Markov random field models are described for image restoration, foreground segmentation, graph cutting and stereo matching.
3832 en Probabilistic Explanation Based Learning 
3833 en Efficient Computation of Recursive Principal Component Analysis 
3834 en Mining Queries 
3835 en Additive Groves of Regression Trees 
3837 en Experiences of the Scottish judiciary 
3838 en Experiences of the Bulgarian judiciary 
3839 en Bridged Refinement for Transfer Learning 
3840 en Experiences of the Croatian judiciary 
3841 en Transparent Courts, Responsible Media, Society Served: Finding a Workable Way 
3842 en Putting Things in Order: On the Fundamental Role of Ranking in Classification and Probability Estimation 
3843 en Classification of Web Documents Using a Graph-Based Model 
3844 en The "uneasy" relationship between media and justice: towards a theoretical explanation In this paper, I examine how political and social theory could be used to make sense of the uneasy relationship between media, news media in particular, and the justice system. To speak of an ‘uneasy’ relationship in this context raises two fundamental questions, namely whether this relationship is always and inevitably going to be difficult, and what kind of harm may ultimately result from persistently distorted and/or negative media portrayals of justice. Liberal theory sees the media as a watchdog or ‘Fourth Estate’ in democracy which aims for the truth in order to expose malpractice or abuse of power. nnHowever, the liberal narrative also allows for the fact that this ideal is seldom attained, often because news media are thrown off course by commercial imperatives which result in sensationalist, superficial and distorted reporting. At its worst, this is seen as something which could lead to a serious erosion of the rule of law. However, contrasting the liberal ideal and its discontents with Niklas Luhmann’s theory of autopoiesis, I also examine a different argument, namely that as law and the media are closed and self-referential systems, they both produce accounts of the social which are inevitably distorted. nnFurthermore, precisely because law is a closed and autonomously working system, autopoietic theory allows us to reach the conclusion that law is exceptionally well equipped to deal with extraneous pressures such as those generated by a very inquisitive or sensation-driven press.
3846 en Joint Inclusion Memorandum - Macedonia 
3852 en Learning to Classify Documents with Only a Small Positive Training Set 
3856 en Structured Prediction: Maximum Margin Techniques Traditionally there has been a mismatch between the requirements of nontrivial applications and the prediction tools offered by machine learning. Applications such as natural language processing, optical character recognition, and path planning are often implemented in terms combinatorial inference algorithms, such as parsing algorithms, Viterbi decoding, and A* planning. These inference algorithms necessarily utilize the inherent structure of the problem to efficiently navigate an exponential number of target elements such as the set of all parse trees for a sentence, the set of possible words of a particular length, or the set of all paths between two points in a graph. On the other hand, research into supervised learning techniques in machine learning and statistics has focused primarily on regression and classification algorithms which at best handle only a handful of classes. These techniques cannot be applied directly to most applications. Typically, engineers are required to meticulously define learnable subproblems by inducing independence assumptions which are often strongly violated in practice.  In recent years, however, the advent of conditional random fields, and then maximum margin structured classification, has changed the way the machine learning community views these problems. Researchers have found ways in which the inherent structure in the problems can be used to directly train these combinatorial inference procedures. Dubbed structured prediction, this class of algorithms utilizes the same implicit structural properties that make the inference algorithms efficient.  In this presentation, after introducing structured prediction at a high level, I will cover in detail one of the two most cited formalisms of structured prediction: maximum margin structured classification. With a particular emphasis placed on functional gradient techniques, I will present a number of algorithms for solving these problems along with their results on various applications and a discussion of relative trade-offs.
3857 en Using the Web to Reduce Data Sparseness in Pattern-based Information 
3858 en Site-Independent Template-Block Detection 
3859 en Introduction to Graph Labelling Workshop and Web spam Challenge 
3860 en A Fast Method to Predict the Labeling of a Tree Given an n vertex weighted tree with (structural) diameter SG and a set of ` vertices we give a method to compute the corresponding `?` Gram matrix of the pseudoinverse of the graph Laplacian in O(n + `2SG) time. We discuss the application of this method to predicting the labeling of a graph. Preliminary experimental results on a digit classification task are given.
3861 en Diffusion Learning Machines 
3862 en Beyond String Search: Fast and Accurate Retrieval of Entities and Dependencies 
3863 en Fighting Spam under Attack: Some Notes from the Field 
3865 en Nonparametric Relational Learning with Applications to Decision Support and Bioinformatics and with a Perspective for the Semantic Web 
3866 en Discovering and Tracking User Communities 
3867 en Semi-Supervised Learning: A Comparative Study for Web Spam and Telephone User Churn We compare a wide range of semi-supervised learning techniques both for Web spam filtering and for telephone user churn classification. Semisupervised learning has the assumption that the label of a node in a graph is similar to those of its neighbors. In this paper we measure this phenomenon both for Web spam and telco churn. We conclude that spam is often linked to spam while honest pages are linked to honest ones; similarly churn occurs in bursts in groups of a social network.
3868 en Web Spam Challenge 2007 Track II - Secure Computing Corporation Research To discriminate spam Web hosts/pages from normal ones, text-based and link-based data are provided forWeb Spam Challenge Track II. Given a small part of labeled nodes (about 10%) in aWeb linkage graph, the challenge is to predict other nodes’ class to be spam or normal.We extract features from link-based data, and then combine them with text-based features. After feature scaling, Support Vector Machines (SVM) and Random Forests (RF) are modeled in the extremely high dimensional space with about 5 million features. Stratified 3-fold cross validation for SVM and out-of-bag estimation for RF are used to tune the modeling parameters and estimate the generalization capability. On the small corpus for Web host classification, the best F-Measure value is 75.46% and the best AUC value is 95.11%. On the large corpus for Web page classification, the best F-Measure value is 90.20% and the best AUC value is 98.92%.
3869 en France Telecom RD Submissions 
3870 en Efficient AUC Optimization for Classification 
3871 en Flexible Grid-Based Clustering 
3872 en Polyp Detection in Endoscopic Video using SVMs 
3873 en A Density-Biased Sampling Technique to Improve Cluster Representativeness 
3874 en Expectation Propagation for Rating Players in Sports Competitions 
3875 en Efficient Closed Pattern Mining in Strongly Accessible Set Systems 
3876 en Discovering Emerging Patterns in Spatial Databases: a Multi-Relational Approach 
3877 en Mining Large Graphs: Laws and Tools 
3878 en Discovering and Tracking User Communities 
3879 en Discovering and Tracking User Communities 
3880 en Approximation and Inference using Latent Variable Sparse Linear Models A variety of Bayesian methods have recently been introduced for performing approximate inference using linear models with sparse priors. We focus on four methods that capitalize on latent structure inherent in sparse distributions to perform: (i) standard MAP estimation, (ii) hyperparameter MAP estimation (evidence maximization), (iii) variational Bayes using a factorial posterior, and (iv) local variational approximation using convex lower bounding. All of these approaches can be used to compute Gaussian posterior approximations to the underlying full distribution; however, the exact nature of these approximations is frequently unclear and so it is a challenging task to determine which algorithm and sparse prior are appropriate. Rather than justifying prior selections and modeling assumptions based on the credibility of the full Bayesian model as is sometimes done, we base evaluations on the actual cost functions that emerge from each method. To this end we discuss a common objective function that encompasses all of the above and then briefly assess its properties with respect to three representative applications: (i) finding maximally sparse signal representations, (ii) predictive modeling (e.g., RVMs), and (iii) active learning/ experimental design. The requirements of these problems can be quite different and can lead to very restricted choices for the sparse prior and final approximation adopted. In general, we find that the best approximate model often does not correspond with the most plausible full model. Finally, we consider several extensions of the sparse linear model, including classification, covariance component estimation, and the incorporation of non-negativity constraints. While closed-form expressions for the moments needed for dealing with these problems may be intractable, we show an alternative implementation that involves transforming to a dual space using simple auxiliary functions. Preliminary results show that substantial improvement is possible over existing methods.
3881 en Message-Passing Algorithms for GMRFs and Non-Linear Optimization We review a variety of iterative methods for inference and estimation in Gaussian graphical models, also known as Gauss-Markov random fields (GMRFs), and consider how to adapt these methods to non-linear optimization problems involving graph-structured objective functions. Specifically, we review the ''walk-sum'' interpretation of Gaussian belief propagation (GaBP) and consider a novel stable form of GaBP which always converges to the optimal estimate. In another direction, we discuss an iterative Lagrangian relaxation method applicable for GMRFs which decompose as a sum of convex quadratic potential functions defined on cliques of the graph. We then consider how such methods can be adapted to solve more general classes of MRFs with smooth potential functions. For instance, if the potential functions are convex, it is straight-forward to use Gaussian inference to efficiently implement Newton's method leading to the optimal solution. In non-convex MRFs, it is still possible to obtain approximate solutions using the method of Levenberg-Marquardt. As time permits, we may also consider the half-quadratic optimization approach for MRFs having non-smooth potential functions. In all of these approaches, the problem is approximated by a sequence of convex quadratic problems each of which can be solved in a distributed fashion using Gaussian inference techniques.
3882 en Bounds on the Bethe Free Energy for Gaussian Networks We consider approximate inference in Gaussian probabilistic models with approximate free energy methods. We define the (fractional) Bethe free energy and directly minimize it. A lower bound for the free energies is derived and we give necessary conditions for the fractional Bethe free energy to be bounded. Our results are in line with the earlier work on the analysis of standard message passing done by Malioutov et al.2006 and Weiss and Freeman 2001, and improve on them by showing that if pairwise normalizability does not hold standard message passing is guaranteed to converge to a global minimum only in special cases. Joint work with Tom Heskes.
3883 en Learning with Millions of Examples and Dimensions - Competition proposal Over the years many different classification methods have been proposed in machine learning. However it is currently very difficult to judge which method is the most efficient with respect to training time and memory requirements and classification performance, which are the practically relevant criteria. A possible explanation for this difficulty is that methods are (often) evaluated under different conditions: For instance different datasets, evaluation criteria, model parameters and stopping conditions are used. We would therefore like to organize a competition, that is designed to be fair and enables a direct comparison of current large scale classifiers. To this end we plan to provide a generic evaluation framework tailored to the specifics of the competing methods, for example for Support Vector Machine classifiers, one would in addition to test-error record the objective value of the primal problem. Providing a wide range of datasets, each of which having specific properties, like extremely sparse, dense, high or low dimensional, we propose to evaluate the methods based on the following figures: training time vs. test error, dataset size vs. test error and dataset size vs. training time. We seek help from the community to gather relevant large-scale real-world data sets and to critically review and discuss fair evaluation criteria and finally invite researchers to co-organize and to participate in this challenge.
3884 en Interview with Yann LeCun His lab has projects in computer vision, object detection, object recognition, mobile robotics, bio-informatics, biological image analysis, medical signal processing, signal processing, and financial prediction,...The Videolectures.Net team talked to him at NIPS 2007, we asked him stuff like: *What is your current topic of research? *How can you comment on your humoristic approach in giving lectures? *Humor and content? *What happend in your research between ML Summer school 2006 in Chicago and today? *How can you explain your work to a 4 year old child? *Machine Learning dream come true.... *What is your philosophy?
3885 en Introduction to the Workshop Until recently, the issue of musical representation had focused primarily on symbolic notation of musical information and structure, and on the representation of musical performance. Research on how we represent musical experience in the brain is emerging as a rich area of investigation thanks to ongoing advances in brain-scanning technology such as EEG and fMRI. This day of the workshop addresses the problem of representation of musical experience in the brain from a computational modelling approach chiefly based on machine learning and signal processing. The overarching question addressed by this workshop is whether we can devise efficient methods to study and model the representation of musical experience in the brain by combining traditional forms of musical representation (musical notation, audio, performance, etc.) with brain scanning technology. This problem is of particular relevance for the successful modelling of cognitive music behaviour, design of interactive music systems, informing techniques in contemporary composition, providing methods to enhance music performance, and even helping music analysis in suggesting ways of listening to music.
3886 en Psychoacoustic Influences on the Neural Correlates of music Syntactic Processing Music consists of perceptually discrete elements that are organized according to syntactic regularities. Violations of these regularities typically elicit two ERP components: the ERAN and the N5. In several studies, we tried to disentangle music-syntactic and psychoacoustic influences on the underlying cognitive processes. In a first study, we compared the electrophysiological response to chord sequences that differed in the music-syntactic regularity, but were similar with regard to sensory factors (such as pitch commonality, pitch repetition, and roughness). We showed that ERAN and N5 were elicited, irrespective of these similarities, indicating that these ERP components are an index of music-syntactic processing. A second study evaluated effects of long-term exposure on the processing of music-syntactic irregularities. The ERAN amplitude declined over the course of an experimental session (about 120 min), suggesting that cognitive representations of musical regularities can change implicitly, in response to the repeated presentation of unexpected, irregular harmonies. A third study explored, whether the ERAN is actually elicited by irregular chords (containing several voices) or by a deviance in the (most prominent) upper voice. We demonstrated two different patterns of neurophysiological responses to these two irregularities, strengthening the assumption that two different cognitive mechanisms are involved in the processing of irregularities in chord sequences, and melodies, respectively. Finally, we tried to follow up the development of the neural correlates of music-syntactic processing. We found an increase between the second and the fifth followed by a decline between the fifth and the eleventh year of age for the ERAN. Almost the same course of development was observed for the N5 (which was however not present in the two year olds). However, the observed differences between the age groups were not significant, indicating a quite high stability in the brain responses to music-syntactic violations.
3888 en Learning novel concepts: beyond one-class OLINDDA (OnLIne Novelty and Drift Detection Algorithm) addresses the problem of novelty detection in an online continuous learning scenario as an extension to a single-class classification problem. This paper presents its current version, that evolved toward the discovery of new concepts initially as emerging clusters and further as cohesive sets of clusters. New strategies for validation and merging of clusters as well as for dynamically adapting the number of clusters are discussed and experimentally evaluated.
3889 en An architecture for context-aware adaptive data stream mining In resource-constrained devices, adaptation of data stream processing to variations of data rates, availability of resources and environment changes is crucial for consistency and continuity of running applications. Context-aware adaptation, as a new dimension of research in data stream mining, enhances and optimizes distributed data stream processing tasks. Context-awareness is one of the key aspects of ubiquitous computing as applicationsC¸ successful operations rely on detecting changes and adjusting accordingly. This paper presents a general architecture for context-aware adaptive mining of data streams that aims to dynamically and autonomously adjust data stream mining parameters according to changes in context and resource availability in distributed and heterogeneous computing environments.
3890 en Learning an Outlier-Robust Kalman Filter In this talk, we introduce a modified Kalman filter that performs robust, real-time outlier detection, without the need for manual parameter tuning by the user. Systems that rely on high quality sensory data (for instance, robotic systems) can be sensitive to data containing outliers. The standard Kalman filter is not robust to outliers, and other variations of the Kalman filter have been proposed to overcome this issue. However, these methods may require manual parameter tuning, use of heuristics or complicated parameter estimation procedures. Our Kalman filter uses a weighted least squares-like approach by introducing weights for each data sample. A data sample with a smaller weight has a weaker contribution when estimating the current time step’s state. Using an incremental variational Expectation-Maximization framework, we learn the weights and system dynamics. We evaluate our Kalman filter algorithm on data from a robotic dog.
3891 en A Model for Quality Guaranteed Resource-Aware Stream Mining Data streams are produced continuously at a high speed. Most data stream mining techniques address this challenge by using adaptation and approximation techniques. Adapting to available resources has been addressed recently. Although these techniques ensure the continuity of the data mining process under resource limitation, the quality of the output is still an open issue. In this paper, we propose a generic model that guarantees the quality of the output while maintaining efficient resource consumption. The model works on estimating the quality of the output given the available resources. Only a subset of these resources will be used that guarantees the minimum quality loss. The model is generalized for any data stream mining technique.
3892 en Efficient Secure Query Processing in XML Data Stream As various users and applications require the distribution and sharing of information in XML documents, the need for an efficient secure access of XML data in a ubiquitous data stream environment has become very important. In this paper, we propose an efficient secure XML query processing method to solve the two problems by using role-based prime number labeling and XML fragmentation. A medical records XML document has the characteristic of an infinite addition in width rather than in depth because of the increment of patients. But a role-based prime number labeling method can fully manage the size of documents that increases to infinity and can minimize the maintenance cost caused by dynamic changes. Experimental evaluation clearly demonstrates that our approach is efficient and secure.
3893 en Enhanced Anytime Algorithm for Induction of Oblivious Decision Trees Real-time data mining of high-speed and non-stationary data streams has a large potential in such fields as efficient operation of machinery and vehicles, wireless sensor networks, urban traffic control, stock data analysis etc.. These domains are characterized by a great volume of noisy, uncertain data, and restricted amount of resources (mainly computational time). Anytime algorithmsoffer a tradeoff between solution quality and computation time, which has proveduseful in applying artificial intelligence techniques to time-critical problems. Inthis paper we are presenting a new, enhanced version of an anytime algorithm forconstructing a classification model called Information Network (IN). The algorithmimprovement is aimed at reducing its computational cost while preservingthe same level of model quality. The quality of the induced model is evaluatedby its classification accuracy using the standard 10-fold cross validation. Theimprovement in the algorithm anytime performance is demonstrated on severalbenchmark data streams.
3894 en Quasi-Incremental Bayesian Classifier This talk describes and empirically evaluates a Quasi-Incremental Bayesian Classifier (QBC) designed to be used when a classification task must be performed in dynamic systems such as sensor networks, which are continuously receiving new piece of information to be stored in huge databases. Therefore, the knowledge that needs to be extracted from these databases is continuously evolving and the learning process may need to go on almost indefinitely. The induction proposed by QBC is performed in two steps; in the first one a traditional Bayesian Network (BN) induction algorithm is performed using an initial amount of data. As far as new data is available, only the numerical parameters of the classifier are updated. The conducted experiments showed that QBC tends to maintain the average correct classification rates obtained with non-incremental classifiers while decreasing the time needed to induce the classifier.
3895 en State of the Art in Data Stream Mining 
3896 en Resource -aware distributed online data mining for wireless sensor networks Online data mining in wireless sensor networks is concerned with the problem of extracting knowledge from a large continuous amount of data streams with an in-network processing mode. Unlike other types of networks, the limited computational resources require the mining algorithms to be highly efficient and compact.We propose a distributed resource-aware online data mining framework for wireless sensor networks which can be used to enable existing mining techniques to be applied to sensor network environments. We have applied the framework to develop and implement a distributed resource adaptive online clustering algorithm on the novel Sun MicrosystemTM Small Programmable Object Technology Sun SPOT platform. We have evaluated the performance of the algorithm on the actual sensor nodes. Experimental results show that the clustering algorithm can improve significantly in resource utilization while maintaining acceptable accuracy level.
3897 en A Semi-fuzzy approach for online divisive-agglomerative clustering The Online Divisive-Agglomerative Clustering (ODAC) is an incremental approach for clustering streaming time series using a hierarchical procedure over time. It constructs a tree-like hierarchy of clusters of streams, using a top-down strategy based on the correlation between streams. The system also possesses an agglomerative phase to enhance a dynamic behavior capable of structural change detection. However, the split decision used in the algorithm focus on the crisp boundary between two groups, which implies a high risk since it has to decide based on only a small subset of the entire data. In this work we propose a semi-fuzzy approach to the assignment of variables to newly created clusters, for a better trade-off between validity and performance. Experimental work supports the benefits of our approach.
3898 en State of the Art in Data Stream Mining 
3899 en S-means: similarity driven clustering and its application in gravitational-wave astronomy data mining Clustering is to classify unlabeled data into groups. It has been wellresearched for decades in many disciplines. Clustering in massive amount of astronomical data generated by multi-sensor networks has become an emerging new challenge; assumptions in many existing clustering algorithms are often violated in these domains. For example, K means implicitly assumes that underlying distribution of data is Gaussian. Such an assumption is not necessarily observed in astronomical data. Another problem is the determination of K, which is hard to decide when prior knowledge is lacking. While there has been work done on discovering the proper value for K given only the data, most existing works, such as X-means, G-means and PG-means, assume that the model is a mixture of Gaussians in one way or another. In this paper, we present a similarity-driven clustering approach for tackling large scale clustering problem. A similarity threshold T is used to constrain the search space of possible clustering models such that only those satisfying the threshold are accepted. This forces the search to: 1) explicitly avoid getting stuck in local minima, and hence the quality of models learned has a meaningful lower bound, and 2) discover a proper value for K as new clusters have to be formed if merging them into existing ones will violate the constraint given by the threshold. Experimental results on the UCI KDD archive and realistic simulated data generated for the Laser Interferometer Gravitational Wave Observatory (LIGO) suggest that such an approach is promising.
3900 en PQStream: a data stream architecture for electrical power quality In this talk, a data stream architecture is presented for electrical power quality (PQ) which is called PQStream. PQStream is developed to process and manage time-evolving data coming from the country-wide mobile measurements of electrical PQ parameters of the Turkish Electricity Transmission System. It is a full-fledged system with a data measurement module which carries out processing of continuous PQ data, a stream database which stores the output of the measurement module, and finally a Graphical User Interface for retrospective analysis of the PQ data stored in the stream database. The presented model is deployed and is available to PQ experts, academicians and researchers of the area. As further studies, data mining methods such as classification and clustering algorithms will be applied in order to deduce useful PQ information from this database of PQ data.
3901 en Relational Transformation-based Tagging for Human Activity Recognition The ability to recognize human activities from sensory information is essential for developing the next generation of smart devices. Many human activity recognition tasks are from a machine learning perspective quite similar to tagging tasks in natural language processing. Motivated by this similarity, we develop a relational transformation-based tagging system based on inductive logic programming principles, which is able to cope with expressive relational representations as well as a background theory. The approach is experimentally evaluated on two activity recognition tasks and compared to Hidden Markov Models, one of the most popular and successful approaches for tagging.
3902 en Random k-Labelsets: An Ensemble Method for Multilabel Classification 
3903 en Seeing the Forest through the Trees: Learning a Comprehensible Model from an Ensmble 
3904 en Constraint Selection by Committee: An Ensemble Approach to Identifying Informative Constraints for Semi-Supervised Clustering 
3905 en Efficient Weight Learning for Markov Logic Networks 
3906 en Separating Precision and Mean in Dirichlet-enhanced High-order Markov Models 
3907 en Diskriminative Sequence Labeling by Z-score Optimization 
3908 en Learning Partially Observable Markov Model from First Passage Times 
3909 en Context-specific Independence Mixture Modelling for Protein Families 
3910 en Clustering Trees with Instance Level Constraints 
3911 en A Prediction-based Visual Approach for Cluster Exploration and Cluster Valadation by HOV3 
3912 en Spectral Clustering and Embedding with Hidden Markov Models 
3913 en Plenary session-ECML poster highlights 
3914 en Approximating Gaussian Processes with H2-matrices 
3915 en Learning to Detect Adverse… 
3916 en Privacy Preserving Market Basket Data Analysis 
3917 en Towards data mining without Information on Knowledge Structure 
3918 en Generating Social Network Features for Link-based Classification 
3919 en An Algorithm to find OverlappingCommunity Structure in Networks 
3920 en Bayesian Substructure Learning - Approximate Learning of Very Large Network Structures 
3921 en Shrinkage Estimator for Bayesian Network Parametrs 
3922 en Efficient AUC Optimization for classification 
3923 en Fast optimization for L1 Regularization: Evaluation and Two New Approaches 
3924 en A Gaphical Model for Content Based Image Suggestion and Feature Selection 
3925 en Stability based Sparse LSI/PCA: Incorporating Feature Selection in LSI and PCA 
3926 en Classification in Very High Dimensional Problems with Handfuls of Examples 
3927 en Speeding up Feature Subset Selection through Mutual Information Relevance Filtering 
3928 en Efficient Continuos-Time Reinforcement Learning with Adaptive State Graphs 
3929 en Adventures in Personalized Information Access 
3930 en An Introduction to Statistical Relational Learning 
3931 en What Semantic Web researchers need to know about Machine Learning? The tutorial will cover basic topics from the field of Machine Learningn explained in an intuitive way relevant for Semantic Web researchers andn practitioners. In the first part the topics will cover brief top leveln overview of the Machine Learning field, its algorithms, and data typesn being analyzed. In the second part we will cover relation to Semanticn Web and Web2.0. In the last part we will perform hands-on exercise withn some of the tools for modeling text semantics and social networks inn analytical way.
3932 en What Semantic Web researchers need to know about Machine Learning? The tutorial will cover basic topics from the field of Machine Learning explained in an intuitive way relevant for Semantic Web researchers and practitioners. In the first part the topics will cover brief top level overview of the Machine Learning field, its algorithms, and data types being analyzed. In the second part we will cover relation to Semantic Web and Web2.0. In the last part we will perform hands-on exercise with some of the tools for modeling text semantics and social networks in analytical way.
3933 en What Semantic Web researchers need to know about Machine Learning? The tutorial will cover basic topics from the field of Machine Learning explained in an intuitive way relevant for Semantic Web researchers and practitioners. In the first part the topics will cover brief top level overview of the Machine Learning field, its algorithms, and data types being analyzed. In the second part we will cover relation to Semantic Web and Web2.0. In the last part we will perform hands-on exercise with some of the tools for modeling text semantics and social networks in analytical way.
3934 en Welcome and Introduction 
3935 en Introduction and Overview to the Semantic Web 
3936 en Semantic Interoperability In the same way that the Web is composed of heterogeneous resources the Semantic Web is composed of heterogeneous ontologies. In this session Jerome and Natasha will discuss what interoperability means at the semantic level. Additionally, they will outline different techniques which can be used to address this problem. At the end of this session attendees will understand the notions and issues underlying semantic interoperability.
3937 en Ontology-based Data Access In this tutorial we provide a comprehensive understanding of the problem of ontology-based data access, from both the theoretical and the practical points of view. We address several problems that are crucial in this context, such as expressiveness/efficiency tradeoff, query processing, impedance mismatch between ontology and data levels, and integration of multiple data sources. We present solutions to these problems based on recent research results in the area of tractable Description Logics, and we provide also a ``hands-on'' experience with QuOnto, a state-of-the-art system for ontology-based data access.
3938 en Ontology-based Data Access In this tutorial we provide a comprehensive understanding of the problem of ontology-based data access, from both the theoretical and the practical points of view. We address several problems that are crucial in this context, such as expressiveness/efficiency tradeoff, query processing, impedance mismatch between ontology and data levels, and integration of multiple data sources. We present solutions to these problems based on recent research results in the area of tractable Description Logics, and we provide also a ``hands-on'' experience with QuOnto, a state-of-the-art system for ontology-based data access.
3939 en Using Social Network Analysis, Geotemporal Reasoning and RDFS++ Reasoning for Business Intelligence Most of the attention in the Semantic Web world is currently focused on using ontologies, rdfs and owl reasoning to get more value out of enterprise data. Many enterprise databases are full of information about people, companies, relationships between people and companies, places and events. The Semantic Web literature also carries the promise of analyzing networks of people, networks of companies and events in time and space. This talk will show how Business Intelligence problems can be solved with a combination of basic semantic web reasoning and complementary techniques such as social network analysis and geotemporal reasoning. We will be using AllegroGraph in this talk, but the concepts learned will transfer to other Semantic Web solutions.
3940 en Opening Ceremony 
3942 en A Generative Model for Rhythms Modeling music involves capturing long-term dependencies in time series, which has proved very difficult to achieve with traditional statistical methods. The same problem occurs when only considering rhythms. In this paper, we introduce a generative model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.
3943 en A Maximum Likelihood Approach to Multiple Fundamental Frequency Estimation From the Amplitude Spectrum Peaks This paper presents a Maximum Likelihood approach to multiple fundamental frequency (F0) estimation in each frame of music signals in the frequency domain. The frequencies and amplitudes of the spectral peaks are viewed as observations, and the F0s are viewed as parameters to be estimated. The proposed method considers the potential errors in the peak detection algorithm and treats each peak as “true” and “false” separately. The likelihood models of the “true” and “false” peaks are learned from the monophonic training data, with the assumption that the statistics of the peaks in monophonic and polyphonic signals are similar. The proposed method also incorporates a rectified Bayesian Information Criteria (BIC) to estimate the number of the parameters, i.e. the polyphony. Evaluation is held on randomly mixed chords, which are generated from the previously unseen monophonic tones. Experimental results show the feasibility of this method.
3944 en Finding Musically Meaningful Words by Sparse CCA A musically meaningful vocabulary is one of the keystones in building a computer audition system that can model the semantics of audio content. If a word in the vocabulary is not clearly represented by the underlying acoustic representation, the word can be considered noisy and should be removed from the vocabulary. This paper proposes an approach to construct a vocabulary of predictive semantic concepts based on sparse canonical component analysis (sparse CCA). The goal is to find words that are highly correlated with the underlying audio feature representation with the expectation that these words can me modeled more accurately. Experimental results illustrate that, by identifying these musically meaningful words, we can improve the performance of a previously proposed computer audition system for music annotation and retrieval.
3945 en Can Style be Learned? A Machine Learning Approach Towards ‘Performing’ as Famous Pianists In this paper a novel method for performing music in the style of famous pianists is presented. We use Kernel Canonical Correlation Analysis (KCCA), a method which looks for a common semantic representation between two views, to learn the correlation between a representation of a musical score and a representation of an artist’s performance of that score. We use the performance representation based on the variations of beat level global loudness and tempo through time, as suggested by [3]. Therefore, the crux of the matter is the representation of the musical scores and by implication a similarity measure between relevant features that capture our prior knowledge of music. We therefore proceed to propose a novel kernel for musical scores, which is a Gaussian kernel adaptation to the distances between rhythm patterns, melodic contours and chords progressions.
3946 en Discovering Music Structure via Similarity Fusion Automatic methods for music navigation and music recommendation exploit the structure in the music to carry out a meaningful exploration of the “song space”. To get a satisfactory performance from such systems, one should incorporate as much information about songs similarity as possible; however, how to do so is not obvious. In this paper, we build on the ideas of the Probabilistic Latent Semantic Analysis (PLSA) that have been successfully used in the document retrieval community. Under this probabilistic framework, any song will be projected into a relatively low dimensional space of “latent semantics”, in such a way that all observed similarities can be satisfactorily explained using the latent semantics. Therefore, one can think of these semantics as the real structure in music, in the sense that they can explain the observed similarities among songs. The suitability of the PLSA model for representing music structure is studied in a simplified scenario consisting of 4412 songs and two similarity measures among them. The results suggest that the PLSA model is a useful framework to combine different sources of information, and provides a reasonable space for song representation.
3947 en Modeling Natural Sounds with Modulation Cascade Processes Auditory scene analysis is extremely challenging. One approach, perhaps that adopted by the brain, is to shape useful representations of sounds on prior knowledge about their statistical structure. For example, sounds with harmonic sections are common and so time-frequency representations are efficient. Most current representations concentrate on the shorter components. Here, we propose representations for structures on longer time-scales, like the phonemes and sentences of speech. We decompose a sound into a product of processes, each with its own characteristic time-scale. This demodulation cascade relates to classical amplitude demodulation, but traditional algorithms fail to realise the representation fully. A new approach, probabilistic amplitude demodulation, is shown to out-perform the established methods, and to easily extend to representation of a full demodulation cascade.
3948 en What/When Causal Expectation Modelling in Monophonic Pitched and Percussive Audio A causal system for representing a musical stream and generating further expected events is presented. Starting from an auditory front-end which extracts low-level (e.g. spectral shape, MFCC, pitch) and mid-level features such as onsets and beats, an unsupervised clustering process builds and maintains a set of symbols aimed at representing musical stream events using both timbre and time descriptions. The time events are represented using inter-onset intervals relative to the beats. These symbols are then processed by an expectation module based on Predictive Partial Match, a multiscale technique based on N-grams. To characterise the system capacity to generate an expectation that matches its transcription, we use a weighted average F-measure, that takes into account the uncertainty associated with the unsupervised encoding of the musical sequence. The potential of the system is demonstrated in the case of processing audio streams which contain drum loops or monophonic singing voice. In preliminary experiments, we show that the induced representation is useful for generating expectation patterns in a causal way. During exposure, we observe a globally decreasing prediction entropy combined with structure-specific variations.
3949 en Modeling and Visualizing Tonality in North Indian Classical Music North Indian classical music (NICM) is based on raag, a melodic structure within which musicians improvise. Raags define hierarchical pitch relationships that can be described as tonal. The study of tonality in raag music may help to elucidate whether a cross-culturally valid cognitive model of tonal perception exists. We describe a basic model of tonality in raag music based on pitch-class distributions. We derive visualizations of tonal raag spaces based on self-organizing maps trained on pitch-class distributions calculated on actual performances, as well theoretically derived maps. We discuss implications of the theory and visualizations, as well as anticipated empirical verifications of the model.
3950 en Book-Adaptive and Book-Dependent Models to Accelerate Digitization of Early Music Optical music recognition (OMR) enables early music collections to be digitized on a large scale. The workflow for such digitisation projects also includes scanning and preprocessing, but the cost of expert human labour to correct automatic recognition errors dominates the cost of these other two steps. To reduce the number of recognition errors in the OMR process, we present an innovative application of maximum a posteriori (MAP) adaptation for hidden Markov models (HMMs) to build book-adaptive models, taking advantage of the new learning data generated from human editing work, which is part of any music digitization project. We also experimented with using the generated data to build book-dependent models from scratch, which sometimes outperform the book-adaptive models after enough corrected data is available. Our experiments show that these approaches can reduce human editing costs by more than half and that they are especially well suited to highly variable sources like early or degraded documents.
3951 en Interview with David Hardoon 
3952 en Recipes for Semantic Web Dog Food - The ESWC and ISWC Metadata Projects Semantic Web conferences such as ESWC and ISWC offer prime opportunities to test and showcase semantic technologies. Conference metadata about people, papers and talks is diverse in nature and neither too small to be uninteresting or too big to be unmanageable. Many metadata-related challenges that may arise in the Semantic Web at large are also present here. Metadata must be generated from sources which are often unstructured and hard to process, and may originate from many different players, therefore suitable workflows must be established. Moreover, the generated metadata must use appropriate formats and vocabularies, and be served in a way that is consistent with the principles of linked data. This paper reports on the metadata efforts from ESWC and ISWC, identifies specific issues and barriers encountered during the projects, and discusses how these were approached. Recommendations are made as to how these may be addressed in the future, and we discuss how these solutions may generalize to metadata production for the Semantic Web at large.
3953 en EIAW: Towards a Business-friendly Data Warehouse Using Semantic Web Technologies Data warehouse is now widely used in business analysis and decision making processes. To adapt the rapidly changing business environment, we develop a tool to make data warehouses more business-friendly by using Semantic Web technologies. The main idea is to make business semantics explicit by uniformly representing the business metadata (i.e. conceptual enterprise data model and multidimensional model) with an extended OWL language. Then a mapping from the business metadata to the schema of the data warehouse is built. When an analysis request is raised, a customized data mart with data populated from the data warehouse can be automatically generated with the help of this built-in knowledge. This tool, called Enterprise Information Asset Workbench (EIAW), is deployed at the Taikang Life Insurance Company, one of the top five insurance companies of China. User feedback shows that OWL provides an excellent basis for the representation of business semantics in data warehouse, but many necessary extensions are also needed in the real application. The user also deemed this tool very helpful because of its flexibility and speeding up data mart deployment in face of business changes.
3954 en DBpedia: A Nucleus for a Web of Open Data 
3955 en PORE: Positive-Only Relation Extraction from Wikipedia Text Extracting semantic relations is of great importance for the creation of the Semantic Web content. It is of great benefit to semi-automatically extract relations from the free text of Wikipedia using the structured content readily available in it. Pattern matching methods that employ information redundancy cannot work well since there is not much redundancy information in Wikipedia, compared to the Web. Multi-class classification methods are not reasonable since no classification of relation types is available in Wikipedia. In this paper, we propose PORE (Positive-Only Relation Extraction), for relation extraction from Wikipedia text. The core algorithm B-POL extends a state-of-the-art positive-only learning algorithm using bootstrapping, strong negative identification, and transductive inference to work with fewer positive training examples. We conducted experiments on several relations with different amount of training data. The experimental results show that B-POL can work effectively given only a small amount of positive training examples and it significantly outperforms the original positive learning approaches and a multi-class SVM. Furthermore, although PORE is applied in the context of Wikipedia, the core algorithm B-POL is a general approach for Ontology Population and can be adapted to other domains.
3956 en SALT: Weaving the claim web In this paper we present a solution for “weaving the claim web”, i.e. the creation of knowledge networks via so-called claims stated in scientific publications created with the SALT (Semantically Annotated LATEX) framework. To attain this objective, we provide support for claim identification, evolved the appropriate ontologies and defined a claim citation and reference mechanism. We also describe a prototypical claim search engine, which allows to reference to existing claims and hence, weave the web. Finally, we performed a small-scale evaluation of the authoring framework with a quite promising outcome.
3957 en Making More Wikipedians: Facilitating Semantics Reuse for Wikipedia Authoring Wikipedia, a killer application in Web 2.0, has embraced the power of collaborative editing to harness collective intelligence. It can also serve as an ideal Semantic Web data source due to its abundance, influence, high quality and well-structuring. However, the heavy burden of up-building and maintain-ing such an enormous and ever-growing online encyclopedic knowledge base still rests on a very small group of people. Many casual users may still feel dif-ficulties in writing high quality Wikipedia articles. In this paper, we use RDF graphs to model the key elements in Wikipedia authoring, and propose an inte-grated solution to make Wikipedia authoring easier based on RDF graph match-ing, expecting making more Wikipedians. Our solution facilitates semantics reuse and provides users with: 1) a link suggestion module that suggests and au-to-completes internal links between Wikipedia articles for the user; 2) a catego-ry suggestion module that helps the user place her articles in correct categories. A prototype system is implemented and experimental results show significant improvements over existing solutions to link and category suggestion tasks. The proposed enhancements can be applied to attract more contributors and relieve the burden of professional editors, thus enhancing the current Wikipedia to make it an even better Semantic Web data source.
3958 en Ontology-based Interpretation of Keywords for Semantic Search Current information retrieval (IR) approaches do not formally capture the explicit meaning of a keyword query but provide a comfortable way for the user to specify information needs on the basis of keywords. Ontology-based approaches allow for sophisticated semantic search but impose a query syntax more difficult to handle. In this paper, we present an approach for translating keyword queries to DL conjunctive queries using background knowledge available in ontologies. We present an implementation which shows that this interpretation of keywords can then be used for both exploration of asserted knowledge and for a semantics-based declarative query answering process.We also present an evaluation of our system and a discussion of the limitations of the approach with respect to our underlying assumptions which directly points to issues for future work.
3959 en HealthFinland - Finnish Health Information on the Semantic Web This talk shows how semantic web techniques can be applied to solving problems of distributed content creation, discovery, linking, aggregation, and reuse in health information portals, both from end-users’ and content publishers’ viewpoints. As a case study, the national semantic health portal HEALTHFINLAND is presented. It provides citizens with intelligent searching and browsing services to reliable and up-to-date health information created by various health organizations in Finland. The system is based on a shared semantic metadata schema, ontologies, and ontology services. The content includes metadata about thousands of web documents such as web pages, articles, reports, campaign information, news, services, and other information related to health.
3960 en Unlocking the Potential of Public Sector Information with Semantic Web Technology Governments often hold very rich data and whilst much of this information is published and available for re-use by others, it is often trapped by poor data structures, locked up in legacy data formats or in fragmented databases. One of the great benefits that Semantic Web (SW) technology offers is facilitating the large scale integration and sharing of distributed data sources. At the heart of information policy in the UK, the Office of Public Sector Information (OPSI) is the part of the UK government charged with enabling the greater re-use of public sector information. This paper describes the actions, findings, and lessons learnt from a pilot study, involving several parts of government and the public sector. The aim was to show to government how they can adopt SW technology for the dissemination, sharing and use of its data.
3961 en Matching Patient Records to Clinical Trials Using Ontologies This talk describes a large case study that explores the applicability of ontology reasoning to problems in the medical domain. We investigate whether it is possible to use such reasoning to automate com- mon clinical tasks that are currently labor intensive and error prone, and focus our case study on improving cohort selection for clinical trials. An obstacle to automating such clinical tasks is the need to bridge the semantic gulf between raw patient data, such as laboratory tests or specific medications, and the way a clinician interprets this data. Our key insight is that matching patients to clinical trials can be formulated as a problem of semantic retrieval. We describe the technical challenges to building a realistic case study, which include problems related to scalability, the integration of large ontologies, and dealing with noisy, inconsistent data. Our solution is based on the SNOMED CT R&#160; ontology, and scales to one year of patient records (approx. 240,000 patients).
3962 en A cognitive support framework for ontology mapping Ontology mapping is the key to data interoperability in the semantic web. This problem has received a lot of research attention, however, the research emphasis has been mostly devoted to automating the mapping process, even though the creation of mappings often involve the user. As industry interest in semantic web technologies grows and the number of widely adopted semantic web applications increases, we must begin to support the user. In this paper, we combine data gathered from background literature, theories of cognitive support and decision making, and an observational case study to propose a theoretical framework for cognitive support in ontology mapping tools. We also describe a tool called COGZ that is based on this framework.
3963 en Potluck: Data Mash-Up Tool for Casual Users As more and more reusable structured data appears on the Web, casual users will want to take into their own hands the task of mashing up data rather than wait for mash-up sites to be built that address exactly their individually unique needs. In this paper, we present Potluck, a Web user interface that lets casual users— those without programming skills and data modeling expertise—mash up data themselves. Potluck is novel in its use of drag and drop for merging fields, its integration and extension of the faceted browsing paradigm for focusing on subsets of data to align, and its application of simultaneous editing for cleaning up data syntactically. Potluck also lets the user construct rich visualizations of data in-place as the user aligns and cleans up the data. This iterative process of integrating the data while constructing useful visualizations is desirable when the user is unfamiliar with the data at the beginning—a common case—and wishes to get immediate value out of the data without having to spend the overhead of completely and perfectly integrating the data first. A user study on Potluck indicated that it was usable and learnable, and elicited excitement from programmers who, even with their programming skills, previously had great difficulties performing data integration.
3964 en The Semantic Web and Human Inference: A Lesson from Cognitive Science For the development of Semantic Web technology, researchers and developers in the Semantic Web community need to focus on the areas in which human reasoning is particularly difficult. Two studies in this paper demonstrate that people are predisposed to use class-inclusion labels for inductive judgments. This tendency appears to stem from a general characteristic of human reasoning – using heuristics to solve problems. The inference engines and interface designs that incorporate human reasoning need to integrate this general characteristic underlying human induction.
3965 en SPARK: Adapting Keyword Query to Semantic Search Semantic search promises to provide more accurate result than present-day keyword search. However, progress with semantic search has been delayed due to the complexity of its query languages. In this paper, we explore a novel approach of adapting keywords to querying the semantic web: the approach automatically translates keyword queries into formal logic queries so that end users can use familiar keywords to perform semantic search. A prototype system named ‘SPARK’ has been implemented in light of this approach. Given a keyword query, SPARK outputs a ranked list of SPARQL queries as the translation result. The translation in SPARK consists of three major steps: term mapping, query graph construction and query ranking. Specifically, a probabilistic query ranking model is proposed to select the most likely SPARQL query. In the experiment, SPARK achieved an encouraging translation result.
3966 en Web Search Personalization via Social Bookmarking and Tagging In this talk, we present a new approach to web search personalization based on user collaboration and sharing of information about web documents. The proposed personalization technique separates data collection and user profiling from the information system whose contents and indexed documents are being searched for, i.e. the search engines, and uses social bookmarking and tagging to re-rank web search results. It is independent of the search engine being used, so users are free to choose the one they prefer, even if their favorite search engine does not natively support personalization. We show how to design and implement such a system in practice and investigate its feasibility and usefulness with large sets of real-word data and a user study.
3967 en Purpose-Aware Reasoning about Interoperability of Heterogeneous Training Systems We describe a novel approach by which software can assess the ability of a confederation of heterogeneous systems to interoperate to achieve a given purpose. The approach uses ontologies and knowledge bases (KBs) to capture the salient characteristics of systems, on the one hand, and of tasks for which these systems will be employed, on the other. Rules are used to represent the conditions under which the capabilities provided by systems can fulfill the capabilities needed to support the roles and interactions that make up each task. An Analyzer component employs these KBs and rules to determine if a given confederation will be adequate, to generate suitable confederations from a collection of available systems, to pre-diagnose potential interoperability problems that might arise, and to suggest system configuration options that will help to make interoperability possible. We have demonstrated the feasibility of this approach using a prototype Analyzer and KBs.
3968 en Ontology-based Information Extraction for Business Intelligence Business Intelligence (BI) requires the acquisition and aggregation of key pieces of knowledge from multiple sources in order to provide valuable information to customers or feed statistical BI models and tools. The massive amount of information available to business analysts makes information extraction and other natural language processing tools key enablers for the acquisition and use of that semantic information. We describe the application of ontology-based extraction and merging in the context of a practical e-business application for the EU MUSING Project where the goal is to gather international company intelligence and country/region information. The results of our experiments so far are very promising and we are now in the process of building a complete end-to-end solution.
3969 en A Collaborative Semantic Web Layer to Enhance Legacy Systems This talk introduces a framework to add a semantic web layer to legacy organizational information, and describes its application to the use case provided by the Italian National Research Council (CNR) intraweb. Building on a traditional web-based view of information from different legacy databases, we have performed a semantic porting of data into a knowledge base, dependent on an OWL domain ontology. We have enriched the knowledge base by means of text mining techniques, in order to discover on-topic relations. Several reasoning techniques have been applied, in order to infer relevant implicit relationships. Finally, the ontology and the knowledge base have been deployed on a semantic wiki by means of theWikiFactory tool, which allows users to browse the ontology and the knowledge base, to introduce new relations, to revise wrong assertions in a collaborative way, and to perform semantic queries. In our experiments, we have been able to easily implement several functionalities, such as expert finding, by simply formulating ad-hoc queries from either an ontology editor or the semantic wiki interface. The result is an intelligent and collaborative front end, which allow users to add information, fill gaps, or revise existing information on a semantic basis, while keeping the knowledge base automatically updated.
3970 en YARS2: A Federated Repository for Querying Graph Structured Data from the Web We present the architecture of an end-to-end semantic search engine that uses a graph data model to enable interactive query answering over structured and interlinked data collected from many disparate sources on the Web. In particular, we study distributed indexing methods for graph-structured data and parallel query evaluation methods on a cluster of computers. We evaluate the system on a dataset with 430 million statements collected from the Web, and provide scale-up experiments on 7 billion synthetically generated statements.
3971 en Sindice.com: Weaving the Open Linked Data Developers of SemanticWeb applications face a challenge with respect to the decentralised publication model: where to nd statements about encountered resources. The \linked data" approach, which man- dates that resource URIs should be de-referenced and yield metadata about the resource, helps but is only a partial solution.We present Sindice, a lookup index over resources crawled on the Semantic Web. Our index al- lows applications to automatically retrieve sources with information about a given resource. In addition we allow resource retrieval through inverse- functional properties, oer full-text search and index SPARQL endpoints.
3972 en Enabling Advanced and Context-Dependent Access Control in RDF Stores Semantic Web databases allow efficient storage and access to RDF statements. Applications are able to use expressive query languages in order to retrieve relevant metadata to perform different tasks. However, access to metadata may not be public to just any application or service. Instead, powerful and flexible mechanisms for protecting sets of RDF statements are required for many Semantic Web applications. Unfortunately, current RDF stores do not provide fine-grained protection. This paper fills this gap and presents a mechanism by which complex and expressive policies can be specified in order to protect access to metadata in multi-service environments.
3973 en Ontology-based Controlled Natural Language Editor Using CFG with Lexical Dependency In recent years, CNL (Controlled Natural Language) has received much attention with regard to ontology-based knowledge acquisition systems. CNLs, as subsets of natural languages, can be useful for both humans and computers by eliminating ambiguity of natural languages. Our previous work, OntoPath [10], proposed to edit natural language-like narratives that are structured in RDF (Resource Description Framework) triples, using a domain-specific ontology as their language constituents. However, our previous work and other systems employing CFG for grammar definition have difficulties in enlarging the expression capacity. A newly developed editor, which we propose in this paper, permits grammar definitions through CFG-LD (Context-Free Grammar with Lexical Dependency) that includes sequential and semantic structures of the grammars. With CFG describing the sequential structure of grammar, lexical dependencies between sentence elements can be designated in the definition system. Through the defined grammars, the implemented editor guides users’ narratives in more familiar expressions with a domain-specific ontology and translates the content into RDF triples.
3974 en How Useful are Natural Language Interfaces to the Semantic Web for Casual End-users? Natural language interfaces offer end-users a familiar and convenient option for querying ontology-based knowledge bases. Several studies have shown that they can achieve high retrieval performance as well as domain independence. This paper focuses on usability and investigates if NLIs are useful from an end-user’s point of view. To that end, we introduce four interfaces each allowing a different query language and present a usability study benchmarking these interfaces. The results of the study reveal a clear preference for full sentences as query language and confirm that NLIs are useful for querying Semantic Web data.
3975 en Evaluating the Semantic Web: A Task-based Approach The increased availability of online knowledge has led to the design of several algorithms that solve a variety of tasks by harvesting the Semantic Web, i.e., by dynamically selecting and exploring a multitude of online ontologies. Our hypothesis is that the performance of such novel algorithms implicitly provides an insight into the quality of the used ontologies and thus opens the way to a task-based evaluation of the Semantic Web. We have investigated this hypothesis by studying the lessons learnt about online ontologies when used to solve three tasks: ontology matching, folksonomy enrichment, and word sense disambiguation. Our analysis leads to a suit of conclusions about the status of the Semantic Web, which highlight a number of strengths and weaknesses of the semantic information available online and complement the findings of other analysis of the Semantic Web landscape.
3976 en An Ontology Design Pattern for Representing Relevance in OWL Design patterns are widely-used software engineering abstractions which define guidelines for modeling common application scenarios. Ontology design patterns are the extension of software patterns for knowledge acquisition in the Semantic Web. In this work we present a design pattern for representing relevance depending on context in OWL ontologies, i.e. to assert which knowledge from the domain ought to be considered in a given scenario. Besides the formal semantics and the features of the pattern, we describe a reasoning procedure to extract relevant knowledge in the resulting ontology and a plug-in for Prot´eg´e which assists pattern use.
3977 en Lifecycle-Support in Architectures for Ontology-Based Information Systems Ontology-based applications play an increasingly important role in the public and corporate Semantic Web. While today there exist a range of tools and technologies to support specific ontology engineering and management activities, architectural design guidelines for building ontology-based applications are missing. In this paper, we present an architecture for ontology-based applications—covering the complete ontology-lifecycle—that is intended to support software engineers in designing and developing ontology-based applications. We illustrate the use of the architecture in a concrete case study using the NeOn toolkit as one implementation of the architecture.
3978 en COMM: Designing a Well-Founded Multimedia Ontology for the Web Semantic descriptions of non-textual media available on the web can be used to facilitate retrieval and presentation of media assets and documents containing them. While technologies for multimedia semantic descriptions already exist, there is as yet no formal description of a high quality multimedia ontology that is compatible with existing (semantic) web technologies. We explain the complexity of the problem using an annotation scenario. We then derive a number of requirements for specifying a formal multimedia ontology before we present the developed ontology, COMM, and evaluate it with respect to our requirements. We provide an API for generating multimedia annotations that conform to COMM.
3979 en How Service Choreography Statistics Reduce the Ontology Mapping Problem 
3980 en A method for recommending ontology alignment strategies In different areas ontologies have been developed and many of these ontologies contain overlapping information. Often we would therefore want to be able to use multiple ontologies. To obtain good results, we need to find the relationships between terms in the different ontologies, i.e. we need to align them. Currently, there already exist a number of different alignment strategies. However, it is usually difficult for a user that needs to align two ontologies to decide which of the different available strategies are the most suitable. In this paper we propose a method that provides recommendations on alignment strategies for a given alignment problem. The method is based on the evaluation of the different available alignment strategies on several small selected pieces from the ontologies, and uses the evaluation results to provide recommendations. In the paper we give the basic steps of the method, and then illustrate and discuss the method in the setting of an alignment problem with two well-known biomedical ontologies. We also experiment with different implementations of the steps in the method.
3981 en An empirical study of instance-based ontology matching Instance-based ontology mapping is a promising family of solutions to a class of ontology alignment problems. It crucially depends on measuring the similarity between sets of annotated instances. In this paper we study how the choice of co-occurrence measures affects the performance of instance-based mapping. To this end, we have implemented a number of different statistical cooccurrence measures. We have prepared an extensive test case using vocabularies of thousands of terms, millions of instances, and hundreds of thousands of co-annotated items. We have obtained a human Gold Standard judgement for part of the mapping-space. We then study how the different co-occurrence measures and a number of algorithmic variations perform on our benchmark dataset as compared against the Gold Standard. Our systematic study shows excellent results of instance-based match- ing in general, where the more simple measures often outperform more sophisticated statistical co-occurrence measures.
3982 en Discovering Simple Mappings Between Relational Database Schemas and Ontologies Ontologies proliferate with the growth of the Semantic Web. However, most of data on theWeb are still stored in relational databases. Therefore, it is important to establish interoperability between relational databases and ontologies for creating a Web of data. An e®ective way to achieve interoperability is ¯nding mappings between relational database schemas and ontologies. In this paper, we propose a new approach to discovering simple mappings between a relational database schema and an ontology. It exploits simple mappings based on virtual documents, and eliminates incorrect mappings via validating mapping consistency. Additionally, it also constructs a special type of semantic mappings, called contextual mappings, which is useful for practical applications. Experimental results demonstrate that our approach performs well on several data sets from real world domains.
3983 en combiSQORE: An Ontology Combination Algorithm Automatic knowledge reuse for Semantic Web applications imposes several challenges on ontology search. Existing ontology retrieval systems merely return a lengthy list of relevant single ontologies, which may not completely cover the specified user requirements. Therefore, there arises an increasing demand for a tool or algorithm with a mechanism to check concept adequacy of existing ontologies with respect to a user query, and then recommend a single or combination of ontologies which can entirely fulfill the requirements. Thus, this paper develops an algorithm, namely combiSQORE to determine whether the available collection of ontologies is able to completely satisfy a submitted query and return a single or combinative ontology that guarantees query coverage. In addition, it ranks the returned answers based on their conceptual closeness and query coverage. The experimental results show that the proposed algorithm is simple, efficient and effective.
3984 en Continuous RDF Query Processing over DHTs We study the continuous evaluation of conjunctive triple pattern queries over RDF data stored in distributed hash tables. In a continuous query scenario network nodes subscribe with long-standing queries and receive answers whenever RDF triples satisfying their queries are published. We present two novel query processing algorithms for this scenario and analyze their properties formally. Our performance goal is to have algorithms that scale to large amounts of RDF data, distribute the storage and query processing load evenly and incur as little network traffic as possible. We discuss the various performance tradeoffs that occur through a detailed experimental evaluation of the proposed algorithms.
3985 en Kernel Methods for Mining Instance Data in Ontologies The amount of ontologies and meta data available on the Web is constantly growing. The successful application of machine learning techniques for learning of ontologies from textual data, i.e. mining for the Semantic Web, contributes to this trend. However, no principal approaches exist so far for mining from the Semantic Web. We investigate how machine learning algorithms can be made amenable for directly taking advantage of the rich knowledge expressed in ontologies and associated instance data. Kernel methods have been successfully employed in various learning tasks and provide a clean framework for interfacing between non-vectorial data and machine learning algorithms. In this spirit, we express the problem of mining instances in ontologies as the problem of defining valid corresponding kernels. We present a principled framework for designing such kernels by means of decomposing the kernel computation into specialized kernels for selected characteristics of an ontology which can be flexibly assembled and tuned. Initial experiments on real world Semantic Web data enjoy promising results and show the usefulness of our approach.
3986 en Semplore: An IR Approach to Scalable Hybrid Query of Semantic Web Data As an extension to the current Web, Semantic Web will not only contain structured data with machine understandable semantics but also textual information. While structured queries can be used to find information more precisely on the Semantic Web, keyword searches are still needed to help exploit textual information. It thus becomes very important that we can combine precise structured queries with imprecise keyword searches to have a hybrid query capability. In addition, due to the huge volume of information on the Semantic Web, the hybrid query must be processed in a very scalable way. In this paper, we define such a hybrid query capability that combines unary tree-shaped structured queries with keyword searches. We show how existing information retrieval (IR) index structures and functions can be reused to index semantic web data and its textual information, and how the hybrid query is evaluated on the index structure using IR engines in an efficient and scalable manner. We implemented this IR approach in an engine called Semplore. Comprehensive experiments on its performance show that it is a promising approach. It leads us to believe that it may be possible to evolve current web search engines to query and search the Semantic Web. Finally, we breifly describe how Semplore is used for searching Wikipedia and an IBM customer’s product information.
3987 en Prodigy or Sociopath: The Adolescent Semantic Web 
3988 en CLOnE: Controlled Language for Ontology Editing This paper presents a controlled language for ontology editing and a software implementation, based partly on standard NLP tools, for processing that language and manipulating an ontology. The input sentences are analysed deterministically and compositionally with respect to a given ontology, which the software consults in order to interpret the input’s semantics; this allows the user to learn fewer syntactic structures since some of them can be used to refer to either classes or instances, for example. A repeated-measures, task-based evaluation has been carried out in comparison with a well-known ontology editor; our software received favourable results for basic tasks. The paper also discusses work in progress and future plans for developing this language and tool.
3989 en Interview with Barney Pell 
3990 en Invited Tutorial: An Introduction to the Semantic Web This invited tutorial will give an overview of the Semantic Web enabling conference attendees to better understand the technical presentations in the main conference and associated workshops. Building upon a series of week long Semantic Web summer schools which have been running successfully since 2003 (see http://knowledgeweb.semanticweb.org/sssw07) this tutorial brings together some of the key researchers in the area of the Semantic Web.
3992 en Making Value Out of Semantic Web Data 
3994 en Testing Distributions for Goodness of fit, Homogeneity, and Independence In this talk, I will describe algorithms for several fundamental statistical inference tasks. The main focus of this research is the sample complexity of each task as a function of the domain size for the underlying discrete probability distributions. The algorithms are given access only to i.i.d. samples from the distributions and make inferences based on these samples. The inference tasks studied here are: (i) similarity to a fixed distribution (i.e., goodness-of-fit); (ii) similarity between two distributions (i.e., homogeneity); (iii) independence of joint distributions; and (iv) entropy estimation. For each of these tasks, an algorithm with sublinear sample complexity is presented (e.g., a goodness-of-fit test on a discrete domain of size $n$ is shown to require $O(\sqrt{n}polylog(n))$ samples). Accompanying lower bound arguments show that all but one of these algorithms achieve a near-optimal performance. Given some extra information on the distributions (such as the distribution is monotone or unimodal with respect to a fixed total order on the domain), the sample complexity of these tasks become polylogarithmic in the domain size.
3997 en The Styrian activities in nano Surface Engineering: Development of nanostructured coatings for the design of multifunctional surfaces The surface of technical products and components of daily life determines properties like visual appearance, friction and wear behaviour, corrosion and oxidation properties and biocompatibility. Well-known examples can be found in the tool industry, where the lifetime of tools is increased by hard wear-resistant coatings and surfaces, in the optical industry, where reflectance of mirrors or absorbance of lenses is controlled via thin films, in microelectronics, where integrated circuits are built from individual layers of different electrical properties, or in data storage devices, where thin layers with tailored magnetic behaviour are utilized. Surface engineering, i.e. the design of surfaces with tailored properties, plays a dominating role in modern engineering, emerging from structural components in mechanical engineering, where significant increase in performance and lifetime have been realized, to functional devices in electronics, optoelectronics or data storage, where surface engineering is an enabling technology. For this reason, surface engineering is being classified in all industrial countries as a key technology. The methods available range from well developed techniques, which have been in industrial use for several decades, to highly sophisticated processes emerging from their former position as experimental laboratory techniques. In particular, modern plasma- and laser-assisted vapour deposition methods have been developed rapidly, where coatings and surfaces are designed atom-by-atom, thus enabling their design on the nano-scale. Austria and in particular Styria have strong research activities in the field of Surface Science &amp; Technology at universities and non-university research organisations as well as in industry. The design of functional thin coatings in the nanometre range is one primary objective. Within the frame of the Austrian nano initiative a research project cluster (RPC) consisting of 10 fundamental and applied research projects has been established in Styria. Long-term scientific goal of the RPC entitled “NANOCOAT” (Development of nanostructured coatings for the design of multifunctional surfaces) is to develop the knowledge and the methods which are necessary for a load oriented design of coatings and surfaces with regard to their chemical composition, nanostructure, morphology of phases, topography, and architecture. The activities focus on the interrelationship between coating materials, the deposition process and the resulting multiphase micro- / nanostructure and coating architecture, the coating properties, and on the response of coatings onto specific loading conditions. In order to strengthen the activities in thin film technology in the Austrian province Styria a Center for Nanostructured Multi-functional Coatings and Deposition Technologies has been founded and established in Leoben. The so-called nanoSurface Engineering Center (nSEC) is a joint venture of the University of Leoben and JOANNEUM RESEARCH. The R&amp;D activities of the nSEC focus on the following three thematic areas: • coatings for tools • coatings for components • coatings for functional devices The presentation will give an introduction of the research project cluster NANOCOAT and the nanoSurface Engineering Center Leoben. The scientific part of the presentation will show project results and will give a benchmarking of different plasma- and laser-assisted vapour deposition techniques. In addition potential areas of application will be shown.
3998 en Concurrent Innovation – Vision 2020 
3999 en Filas Inovation Agency 
4000 en French Pole of Competitiveness - Living Labs - What's News 
4001 en Helsinki Living Lab 
4002 en Amsterdam Living Lab 
4003 en European Network of Livin Labs 
4004 en Why Living Lab 
4005 en Living Lab in the European Innovation System 
4006 en Large Enterprise Open Innovation Strategy - NOKIA Open Innvation 
4007 en LAZIO Connect Aerospace 
4008 en Multisector VEN 
4009 en Automotive Cluster Portugal 
4010 en Wireless Communication Laboratori 
4011 en Workshop introduction and objectives 
4012 en Concepts for CNOs 
4013 en ECOLEAD results and Benefit for CNOs 
4014 en Perfomance, Measurement and Improvements of CNOs 
4015 en Legal Best Practices for Collaborative Innovative Clusters 
4016 en Current Solutions an Future Trends 
4017 en Discussion and Closure 
4019 en Gold Multipliers: what ECOLEAD 
4020 en Gold Multipliers:what ECOLEAD 
4021 en PVC Motivation and Principles 
4022 en PVC Problem Solving 
4023 en PVC Legal Entity and Agreements Templates 
4024 en Business Models for sustainability 
4025 en ICT developers PVC – VT Formation - Part I 
4026 en ICT developers PVC – VT Formation - Part II 
4027 en Human Resources PVC - Social Interaction 
4028 en ACP Hot Functionalities 
4030 en Realistic Synthetic Data for Testing Association Rule Mining Algorithms for Market Basket Databases 
4031 en Learning Multi-Dimensional Functions: Gas Turbine Engine Modeling 
4032 en Constructing High Dimensional Feature Space for Time Series Classification 
4033 en A Method for Multi-relational Classification Using Single and Multi-feature Aggregation Functions 
4034 en MINI: Mining Informative Non-redundant Itemsets 
4035 en Automatic Hidden Web Database Classification 
4036 en The Most Reliable Subgraph Problem 
4037 en Matching Partitions over Time to Reliably Capture Local Clusters in Noisy Domains 
4038 en Tag Recommendations in Folksonomies 
4039 en Multilevel Conditional Fuzzy C-Means Clustering of XML Documents 
4040 en Uncovering Fraud in Direct Marketing Data with a Fraud Auditing Case Builder 
4041 en Real Time GPU-Based Fuzzy ART Skin Recognition 
4042 en Dynamic Bayesian Networks for Real-Time Classification of Seismic Signals 
4043 en Robust Visual Mining of Data with Error Information 
4044 en Automatic Categorization of Human-Coded and Evolved CoreWar Warriors 
4045 en Utility-Based Regression 
4046 en Multi-label Lazy Associative Classification 
4048 en Association Mining in Large Databases: A Re-examination of Its Measures 
4096 en Pre-processing Large Spatial Data Sets with Bayesian Methods 
4097 en Privacy Preserved 
4099 en Relational Latent Class Models 
4100 en Machine learning open source software 
4101 en In the Eye of the Beholder? Another look at Cognitive Systems 
4102 en Probabilistic inference methods in robotics-filling the gap between high-level reasoning and low-level motion control 
4103 en Learning to Reason Knowledge Acquisition in Cyc 
4104 en Curriculum Development Programme 
4105 en Report on the 2007 PASCAL Bootcamp in Machine Learning 
4106 en Machine Learning Summer Schools 
4107 en Curriculum development 
4108 en Curriculum development 
4109 en Panel 
4110 en A tour of the Pascal Challenge programme 
4111 en Recognizing Textual Entailment 
4112 en Visual object recognition 
4113 en Challenges with XML 
4114 en Challenge in Astrophysics The GRavitational lEnsing Accuracy Testing 2008 (GREAT08) Challenge focuses on a problem that is of crucial importance for future observations in cosmology. The shapes of distant galaxies can be used to determine the properties of dark energy and the nature of gravity, because light from those galaxies is bent by gravity from the intervening dark matter. The observed galaxy images appear distorted, although only slightly, and their shapes must be precisely disentangled from the effects of pixelisation, convolution and noise. The worldwide gravitational lensing community has made significant progress in techniques to measure these distortions via the Shear TEsting Program (STEP). Via STEP, we have run challenges within our own community, and come to recognise that this particular image analysis problem is ideally matched to experts in statistical inference, inverse problems and computational learning. Thus, in order to continue the progress seen in recent years, we are seeking an infusion of new ideas from these communities. This document details the GREAT08 Challenge for potential participants. Please visit [[http://www.great08challenge.info/|www.great08challenge.info]] for the latest information.
4115 en Challenge with Large Scale Problems 
4116 en Competitions and Challenges 
4117 en PASCAL within Cognitive Science Research and objectives of FP7 2.1 call 
4118 en Operational overview and historical background, scientific programme 
4119 en Harvest Programme 
4120 en TP1 Leveraging Complex Prior Knowledge for Learning 
4121 en TP2 Multi-Component learning 
4122 en TP3 Partial or Delayed Feedback 
4123 en Textual Entailment 
4124 en Languages as Hyperplanes: Grammatical Inference with String Kernels 
4125 en Semantic Information Extraction 
4126 en Learning Stochastic Edit Distances from Structured Data: Application in Music Retrieval 
4127 en Human Motion Modelling through Dimensional Reduction with Gaussian Processes 
4128 en Classifying Visual Scenes with Affine Invariant Regions and Text Retrieval Methods 
4129 en Dynamics and Interaction in BCI and Music Spaces 
4130 en Methods for Fusing Eye Movements and Text Content for Information Retrieval 
4131 en Online Reinforcement Learning and Sequential Forecasting and Partial Feedback 
4132 en Techniques for Learning Multiple Related Tasks 
4133 en Large Scale Multilingual and Multimodal Integration 
4134 en Similarity Analysis by Data Compression 
4135 en Outreach demos: targeting the general public with web applications 
4136 en Why and how is this a related document? 
4137 en Popularising the science and researchers of PASCAL 
4138 en PASCAL virtual learning space 
4139 en Text mining for semantically enabled social browsing 
4140 en Dynamic Network and Content Visualization of Log Files 
4142 en The Evolution of Aluminum Hydroxides During the AlN Powder Hydrolysis The reaction of aluminum nitride (AlN) powder with water has been known for a long time. In the presence of water AlN will decompose, forming aluminum hydroxide and ammonia: AlN + 3H2O ? Al(OH)3 + NH3  Bowen et al.1 proposed more detailed reaction scheme for the reaction of AlN powder with water at room temperature (RT): AlN + 2H2O ? AlOOH(amorph) + NH3 NH3 + H2O ? NH4+ + OH- AlOOH(amorph) + H2O ? Al(OH)3(xstal) According to Bowen et al.1, AlN powder first reacts with water to form amorphous aluminum hydroxide (pseudoboehmite, AlOOH) recryctallizing to bayerite (Al(OH)3) with time. The kinetics of AlN hydrolysis was described using an unreacted-core model and the chemical reaction at the product-layer/unreacted-core interface was proposed to be the rate-controlling step. It is anticipated that the dissolution-recrystallization process during AlN hydrolysis is very similar to that of the crystallization of aluminum hydroxide gels, where pseudoboehmite forms from fresh, highly hydrated, amorphous hydroxide. In the present work the influence of hydration temperature, from RT up to 90 °C, and ageing time, from 10 minutes to 24 hours, on the formation of crystalline products found after the AlN powder hydrolysis was investigated. The AlN hydrolysis behaviour was observed by measuring the pH of the suspension, whereas for the characterization of the reaction products XRD, SEM and TEM analyses were employed. After a short incubation time (&lt; 0.27 to 17 minutes), which was found to be temperature dependant, the hydrolysis reaction started accompanied with the increase in pH and temperature. Higher starting temperatures also increase the reaction rate. The starting temperature and especially the ageing time (time of hydrolysis) strongly influence the reaction products and their morphology. It was confirmed that at RT the main crystalline reaction product is bayerite (Al(OH)3), regardless on ageing time in the mother liquor. It appears in the form of large somatoids. At elevated temperatures the first crystalline product is nanostructured boehmite (AlOOH), also exhibiting a high specific surface area. With prolonged ageing the bayerite conversion takes place with dissolution of pseudoboehmite and recrystallization of bayerite. After 24 hours of ageing in the temperature range from 40 °C to 70 °C bayerite became the predominant phase. At higher temperatures, e.g., at 80 °C and at 90 °C, both phases are present, but after 24 hours of ageing boehmite remains the predominant phase (Fig 1). Based on these results, an extension of Bowen et al’s1 model, for the AlN powder hydrolysis at RT to elevated temperatures is proposed. It is anticipated that at any temperature the very first solid reaction product formed on the surface of the AlN particles is nanostructured pseudoboehmite. Once formed, part of the pseudoboehmite will be transformed to crystalline boehmite, while the other part will further react with water to form bayerite, according to the reaction scheme: AlOOH(amorph) ? AlOOH(xstal) AlOOH(amorph) + H2O ? Al(OH)3(xstal)
4143 en Dynamics of nanoscopic magnetic clusters in (Pr,Ca)MnO3 thin films observed by ultrafast magnetooptics Phase separated state of thin films of (Pr0.6Ca0.4)MnO3 on LaAlO3 and SrTiO3 substrates was investigated by means of the ultrafast time-resolved magnetooptics in the magnetic field up to 1.1 T. The photoinduced Kerr rotation and ellipticity show remarkably different magnetic-field dependence. From comparison with the static Kerr rotation and ellipticity we conclude that two different magnetic phases are present in the samples at low temperatures. According to small angle neutron scattering results [1] one of the phases originates from nanoscopic ferromagnetic metallic clusters. Temporal dependence of the photoinduced Kerr signals indicates that upon photoexcitation changes of the volume fraction of these phases take place on a timescale of a few tens of picoseconds.
4144 en Coverage Dependence of DNA Hybridization in Nanostructured Monolayers: a Nanografting AFM Study One of the main challenges in the development of new analytical methods for life sciences is to dramatically reduce the minimum amount of DNA and RNA that can be directly and precisely characterized. Micro-arrays can not operate in this limit, and generally need the use of enzymatic amplification processes, that introduce statistical uncertainties, crucially affecting the performance of the device. Towards this end, new techniques at the nano-scale for amplification-free and label-free detection of DNA hybridization need to be explored. We use nanografting (an atomic force microscopy (AFM) based nanolithography technique) to fabricate nanopatches of self-assembled monolayers of single stranded DNA (ss-DNA) within a "matrix" of other thiols on gold surfaces. By opportunely varying the nanografting parameters, we establish a relative scale for the surface coverage of the ss-DNA spots. We find that the height of the patches grows with growing coverage reaching a "saturation" regime at very high ss-DNA coverage, and that the height of each patch increases upon hybridization. Not surprisingly, maximum sensitivity for hybridization has been obtained before the height of the grafted patch reaches saturation, and therefore high packing densities. Surprisingly, however, in the height/packing saturation regime the compressibility of hybridized ss-DNA grafted patches is much smaller than the one of ss-DNA patches, but the same as that of ds-DNA patches grafted as such. We conclude that, in contrast with several statements present in the current literature, in our nanopatches DNA has little trouble in hybridizing even at high surface densities. The level of molecular order in the nanopatches, with respect to that in spontaneously assembled ss-DNA monolayers, is responsible for the different hybridization efficiencies. Our findings provide new insights on the recombination of short DNA fragments on surfaces, with important consequences for the field of solid surface supported DNA hybridization detection.
4145 en Carbon nanotubes wrapped by DNA molecules Complexes of carbon nanotubes (CNTs) and nucleic acids allow fully exploit the potential of the CNTs in nanoelectronic devices, both by a size-specific matching of the two components and by the possibility to anchor also non-polar CNTs on the polar substrates such as oxides. The wrapping CNTs by the nucleic acid molecules allow also a transfer of CNTs into water solutions and a performance for their radii and lengths separation using chromatographical methods. In the present work for the first time the stability and electronic properties of the associates of the single-walled carbon nanotubes wrapped by homopolymeric single-stranded DNA molecules (CNT@DNA) are studied using a dispersion corrected modification of quantum mechanical density-functional tight-binding method (DFTB). A phenomenological model of the CNT@DNA formation energy depending on the nanotube radii is developed, which shows that the decoration of a CNT by a few (not single) DNA chains leads to a high water solubility of CNT@DNA. Pyrimidine-based DNAs are found to be more effective to wrap the CNTs, whereas purine-based DNAs are in wrapping more sensitive to the change of radii. The densities-of-states of the CNT@DNA complexes are close to the superposition of those of the “free” components with some additional states below Fermi level. The band gap in a hybrid CNT@DNA system is determined by the competition between the Fermi levels of the “free” DNA and CNT. In a few specific cases (complexes of polycytosine-DNA and a chiral metallic CNT) a considerable charge transfer from the DNA to the CNT was observed, combined with an additional gain in the CNT@DNA formation energy.
4146 en Verification of Biochemical Activity for Protein Monolayers Nanostructured on Gold Surfaces We demonstrate that an Atomic Force Microscope (AFM) can be used to immobilize a di-cysteine-terminated protein (Maltose Binding Protein, MBP-cys-cys for short) at well-defined locations directly on gold substrates via nanografting, and characterize the in situ bioactivity of these proteins within the fabricated nanopatterns. This method exploits the high spatial and orientational control of protein monolayer assembly allowed by nanografting, combined with the high sensitivity of the AFM for detecting ligand-binding events. The maltose-mediated conformational changes within the MBP have been found to change the AFM-tip-protein interaction, therefore causing the frictional signal to change. Our data show that the protein ligand-binding function is maintained upon the immobilization.
4147 en Analytical electron microscopy of nanoparticles Analytical Electron Microscopy (AEM) is an essential tool for microstructural investigations of nanostructured materials. Dedicated FEG instruments enable study of nanosized volumes using various methods, such as high resolution transmission electron microscopy (HRTEM), hig-resolution Z-contrast (STEM/HAADF) imaging, different techniques of electron diffraction (SAED, CBED, nanodiffraction), X-ray energy dispersive spectroscopy (XEDS) and electron energy loss spectroscopy (EELS). In the work the main stress will be on AEM study of particles, determination of their size, morphology, chemical and structural composition, orientation, etc. Study of the nucleation and crystallization of nanosized particles from amorphous phase, analysis of light elements in small volumes with complicated geometries, determination of atomic clusters of secondary element in monocrystals, investigation of self-assembly of quantum dots in an amorphous matrix are examples that will be presented and commented in the work. Determination of the crystallinity is in many cases quite complicated and ambiguous. The boundary between amorphous and crystalline phase in certain materials is very broad and smeared. Amount, crystallite size and shape and defectiveness of the crystal structure of nanoparticles (clusters, embryos, nuclei, precipitates, nanowires, nanorods, etc.) could be in usual cases determined using classical approaches (bright-field, dark-field experiments, selected area electron diffraction, high-resolution TEM). In specific cases some novel approaches should be used to extract the useful information from the sample. Such approaches are the use of chemical composition fluctuations of nano-regions in the determination of the presence of nanoclusters, the comparison of calculated and experimental electron diffraction pattern in the particle size and the degree of crystallinity determination and the use of non-standard geometries for absorption correction procedures during the chemical composition determination of particle using X-ray energy dispersive spectroscopy.
4148 en Hydroxylapatite coatings on ZrO2 and Al2O3 ceramics by bio-mimetic method The materials that can be used for the weight-carrying bone implants are: titanium, Al2O3 or ZrO2 ceramics. All those materials, with appropriate mechanical properties are bio-inert. When bio-inert material is implanted into the living body, a fibrous capsule is developed to isolate the implant from the surrounding tissue. When a bio-inert implant is loaded such that interfacial movement can occur (implantation of a hip or a knee), the fibrous tissue can become very thick and the implant loosens quickly. The problem can be solved by use of bio-active materials (bio-active glasses, glass-ceramics composites, hydroxylapatite, etc.). These materials stimulate the growth of the bone or the soft tissue directly on their surface and are strongly bonded to the surrounding tissue. However, the low mechanical strength of the known bio-active materials does not permit these materials to be used as weight-carrying bone implants. An alternative for weight-carrying bone implants is the use of bio-active coatings on bio-inert material with a high mechanical strength. The plasma coating of the hydroxylapatite is mostly used. Still, there are some problems connected to this method: it is difficult to control the thickness of the coating and the compounds formed during the process; due to the high temperature during the synthesis the biological active molecules cannot be incorporated into the coating. With the use of bio-mimetic methods, which imitate the crystallization process of the bone, the monophase hydroxylapatite coating with a uniform thickness can be prepared. Using this method we can also incorporate the biologically active molecules, e.g., proteins that stimulate the bone formation. Compared to plasma spraying the bio-mimetic methods are also much cheaper. When the coated implants are used the adhesion between the implant and its coating becomes very important, which is the main problem with the use of the bio-mimetic method. The bio-mimetic methods are based on the soaking of the implant in a supersaturated solution of calcium and phosphate ions. Besides other solutions the simulated body fluid (SBF) and its higher concentrations are most commonly used. In our work the coatings of bio-active calcium phosphate were prepared on the surface of ZrO2 and Al2O3 ceramics using, from the literature already known, Ca/P supersaturated solution with the ion concentrations: Na+ 25.5, Ca2+ 2.5, Cl- 5.0 HCO3- 18.0 and PO43- 2.5 mM. The hydroxylapatite coating on the surface of Al2O3 ceramic, which is identical on ZrO2 ceramic. The adhesion between the coating and the ceramic substrate was improved by a heat treatment at 1050 °C for 1 hour. Before and after the heat treatment the coating was analysed by XRD and a TEM equipped with an EDS detector and a parallel EELS spectrometer. In order to minimise any possible microstructural changes due to the electron beam irradiation in the TEM the cooling holder was used. All the EDS and EELS measurements were carried out when the temperature in the cold stage was stabilized (approximately 100 K). After precipitation the coating was composed polycrystal plate-like particles, orientated perpendicularly to the substrate surface. Each individual plate-like particle was composed of poorly crystalline elongated nano crystals. The heat treatment increased the coating’s crystallinity and particles grow isotropic up to 200 nm. Using XRD, EDS and EELS, the crystal structure was determined to be apatite. The simple in-vitro bio-activity test using SBF solution proved the bio-activity of the “as-prepared” coatings and after a heat treatment as well.\\ After soaking the coated substrates in the SBF solution for 7 days the layer of hydroxylapatite was formed on the coated surface of Al2O3 and on ZrO2 ceramics. The bio-mimetic method of soaking in the Ca/P solution is simple, fast and with the use of it we produced the uniform bio-active coating on the bio-inert material, which can be used as weight-carrying bone implants.
4149 en "XAFS study of Zr local environment in amorphous precursors of La2Zr2O7 ceramics prepared by a nitrate-modified alkoxide synthesis route We prepared La2Zr2O7 by the nitrate-modified alkoxide-based sol-gel synthesis route. The dehydrated lanthanum nitrate was mixed with zirconium n-butoxide in 2-methoxyethanol and refluxed for 3 h. The obtained liquid was slowly dried. At 150oC, the sample self-ignited. According to XRD, the powder was amorphous upon heating up to 700oC and crystallization took place at 800oC. The powder was composed of friable agglomerates of about 70 nm sized nano-particles. By EXAFS analysis we investigated the Zr and La local environment upon transition from the sol to the amorphous powder in order to learn about the distribution of the constituent metal atoms at the sub-nanometer level. We observed that the Zr EXAFS spectra of the sol and the precursor powder dried at 150 oC and heated at 500 oC were similar, indicating that the local environment of Zr atoms was not affected during the liquid-amorphous solid transition, Zr – O – Zr links were formed already in the sol and preserved even in the powder heated at 500 oC. No La – O – Zr links in the sol or in the amorphous powder were observed. The La EXAFS spectrum of the LZ sol is similar to the spectrum of dehydrated lanthanum nitrate (LN sol), while in the spectra of dried and heated powder, vanishing of the second peak was noticed. No La-La or La-Zr corelation could be established. Since no link, between La and Zr species, was confirmed in the early stages of the synthesis, the reaction between the species proceeded as a solid-state reaction when long range diffusion became available leading to the pure pyrochlore phase. The proposed synthesis route, allowed a very good mixing of the species at the nanometre level.
4150 en Detection of irreversible fusion of iron oxide magnetic nanoparticle into single-domain clusters by magnetic measurements Ferrofluids are colloidally stable suspensions of magnetic nanoparticles in a suitable carrier liquid. Because of their unique properties, such as magnetoviscous effect and a wide spectrum of possible medical and technical applications, they are a subject of intense research. The extent of aggregation of a ferrofluid is one of its key properties determining the suitability of such a ferrofluid for applications. In order to synthesize ferrofluids with satisfactory and controllable aggregation properties it is crucial to understand the mechanism of aggregate formation and to have a method for determining the extent of aggregation. We prepared a ferrofluid composed of maghemite nanoparticles of the average size of 11 nm dispersed in n-decane. The nanoparticles were prepared by coprecipitation of Fe2+ and Fe3+ ions in basic medium. Dispersion of nanoparticles in decane was achieved by oleic acid coating. Ferrofluids with two different concentration (concentrated and diluted) were prepared. TEM images of zero-field dried and field dried ferrofluids suggest that fusion of single nanoparticles into clusters of few nanoparticles takes place if the ferrofluid is exposed to an external magnetic field. Because of the bigger size of the clusters in comparison to single particles there is a stronger interaction between the clusters so they easily combine into aggregates. The fusion of particles into clusters could be thus an important mechanism in the process of aggregate formation. In our work we tried to detect the irreversible fusion of single particles into clusters by magnetic measurements. As the clusters are single-domain particles behaving as super-spins the problem of cluster detection reduces to the known problem of determining the average magnetic particle size. Zero-field-cooled/field-cooled magnetization (ZFC/FC), AC susceptibility and M(H) measurements were carried out on a commercial SQUID-based magnetometer (Quantum Design MPMS XL-5). We show that the most evident method for detecting single-domain clusters in diluted as well as in concentrated ferrofluids is determining the particle size from M(H) magnetic measurement.
4151 en Electron Transport Through M-m-M Junctions: A Scanning Differential CP-AFM Investigation Conductive Tip AFM (CT-AFM) is commonly used for electrical characterization of organic and inorganic molecular surface systems. Understanding charge transport at the molecular level is of crucial importance for developing molecular assemblies with uncommon properties for novel applications, such as molecular electronic devices and sensors. Measurements of the charge transport at the contacts and through molecules will provide crucial insight into the electronic coupling within and between molecules and at the interface. From a more general point of view such studies aim at expanding our fundamental understanding of electron-transport processes, that is a central issue of biophysics and chemistry. In this work we follow an approach to the study of Metal-molecule-Metal surface junctions that uses a combination of different AFM-based techniques. We first use Nanografting to build nanopatches of the molecules of interest into a hosting reference self assembled monolayer (SAM) typically made of alkanethiols. After the tip is changed to a conductive one, CT-AFM is used to characterize electrically the whole system recording, at the same time, the system topography. Some of the advantages of this approach are the possibility to build and study a wide range of different monolayers side-by-side in a relative way, overcoming all the problems of an absolute measure, and the in-situ control of the quality both of the hosting monolayer and that of the grafted patches. Preliminary results demonstrating the reliability of the technique will be presented for alkanethiols spontaneously self-assembled and nanografted on Au(111) surfaces. Moreover, we will show that in the case of alkanethiol molecules of one specific length, i.e. C10, nanografted into a SAM carpet of the same molecules, a contrast in current images appears, that can be correlated to the higher quality of the molecular packing inside the nanopatches with respect to the surrounding SAM.
4152 en Recent achievement in characterization of micro- and nano-materials by scanning photoemission imaging and spectromicroscopy With respect to the other photoelectron microscopy techniques a Scanning PhotoEmission Microscope (SPEM) uses the most direct approach to photoelectron spectromicroscopy which is the use of a small focused photon probe to illuminate the surface. The SPEM at the Elettra synchrotron light source can operate in two modes: imaging and spectroscopy. In the first mode the sample surface is mapped by synchronized-scanning the sample with respect to the focused photon beam and collecting photoelectrons with a selected kinetic energy. The second mode is photoelectron spectroscopy from a microspot. The SPEM on the ESCAmicroscopy beamline at Elettra has a lateral resolution of 150 nm; and an overall energy resolution which is now better than 200 meV. Samples can be heated, cooled (liquid N2) and biased during the measurements. The beamline is open to the public and private research community; two call for proposals of experiment are available per year together with the possibility of dedicated collaborations on specific projects. Some recent achievements in the chemical, physical and electronic characterization of nano- and micro-structured materials will be presented providing an overview of the capabilities of this powerful technique. Metallic adsorbate interaction, oxidation and supporting properties of multiwall carbon, semiconducting and metal-based nanotubes will be presented, showing how even dynamic phenomena such as mass transport along the nanotube surface can be monitored by the SPEM. A special design of the samples allows for the investigation of single nanotubes with diameter down to 60nm. The study of compositional and electronic properties of morphological complex 3D semiconducting structures will be presented as well. Important industrial collaborations with international companies have been established in last years. A first example will report on the study of the degradation processes occurring on Organic Light Emitting Devices (OLEDs). Results on both OLEDs operated in ambient atmosphere and grown and operated in ultra-high-vacuum will be compared. Another example will illustrate the chemical characterization of the cathode surfaces of Solid Oxide Fuel Cells under operating conditions; the elemental distribution and its change under biasing and the observation and explanation of the cathode electrochemical activation have been addressed. Finally an overview of the limits in the applications of the x-ray photoelectron microscopes imposed by the operation principles will be given together with the future developments allowing the investigation of materials at mbar and even ambient pressure.
4153 en Studies of Free Clusters and of Low Energy Cluster Beam Deposition at TASC The understanding of the interplay between size and properties is a necessary step towards the production of tailored cluster-assembled materials. With the aim of investigating functional properties of clusters, their size-dependent geometry and electronic structure, and the correlations with nanoassembled materials obtained by cluster deposition, TASC recently acquired a new UHV-Compatible Roaming Supersonic Cluster Source. The procesess of cluster production is based on the pulsed microsputtering, localised on the surface of a rotating rod (PMCS). The material must be a conductor one or a semiconductor heated to the conductive temperature. This apparatus is conceived as a facility able to be coupled to a variety of UHV compatible systems located at TASC-INFM (both in-house apparatus and syncrotron radiation beam lines at ELETTRA). Here we presents some of our first results we obtained within a collaboration with the prof. P. Milani’s group. In the first class of experiment we concentrated our efforts on the study of inner shell absorption spectroscopy both on free clusters. The experiments have been carried out at the Gas Phase beamline, and for the first time it was possible to obtain a complete set of Near Edge X-ray Absorption Fine Structure spectra on free transition metal clusters (Ti L-edge for Ti clusters with size comprised between 15 and 1000 atoms/cluster). Within the field of material science, our first measurements were dedicated to the study of oxidation dynamics of nanostructured titanium films performed at the Analytical Division Laboratory at TASC
4154 en Infrared nanoscopy of structured SAMs A challenging scientific task is the label-free and non-invasive investigation of molecules with a nanometer scale resolution. Due to the significant absorption lines in the infrared region - the so-called fingerprint region - a combination of apertureless near-field scanning optical microscopy with unique infrared laser spectroscopy provides a powerful method for spectroscopic characterization of self-assembled monolayers (SAMs) at the nanoscale. First experiments were performed on organic microstructured monolayers of octadecanethiol and biotinylated alkylthiol using a tunable CO-Laser as radiation source in the characteristic amide I region around 1700 cm-1. With our scanning near-field infrared microscopy (SNIM) we were able to record the frequency dependence of a single monolayer with a lateral resolution of 90 x 90 nm2 corresponding to ?/60 which is well below the Abbe limit. The detection limit for biotinylated alkylthiol was estimated to be 5x10-20 mol corresponding to 27 attogram.
4155 en The twin cantilever approach to the single molecule detection Detection and manipulation of single molecules by means of mechanical systems has recently received constantly growing attention both for fundamental and applied reasons. In particular, great interest has been aroused by sensors based on the principle of frequency shift detection of mechanical resonators, the application of which to medicine and biology has lead to very sensitive diagnostic methods. However, in the effort to push the sensitivity to the single molecule level, extreme conditions of frequency in the GHz range, size with oscillating structures sized below 1?m in all directions, vacuum better than 10-10 mbar and temperatures as low as a few Kelvin, have been exploited. Here we present an alternative strategy capable single molecule sensitivity, that uses twin cantilever structures of several micron in size, operated at room temperature and under ordinary vacuum conditions. Two aspects are treated in detail. The formation of a tunable gap with controllable size in the nanometer range has been obtained by a two step process: first a fracture along a Si 100 crystallographic plane has been induced on a suitable designed silicon suspended structure ensuring the formation of two atomically flat edges. The controlled opening of the gap is then induced by bending mechanically the wafer in analogy with what is normally done in mechanical break junction. The detection of one or more molecules placed across the gap can be then obtained either by measuring the perturbation to the mechanical eigenfrequency of the system or by detecting the mechanical cross talk induced by the molecular link between a short driver cantilever and a longer follower cantilever that face each other. In presence of molecules bridging the gap, if the driver is moved electrostatically at the eigenfrequency of the follower, the latter experience large oscillations than can be easily detected by optical or piezoresistive methods, also for weak binding forces. We fabricated an asymmetric twin cantilever system with a link formed by multi walled carbon nanotubes across the gap and we demonstrated that, working at room temperature and under normal vacuum conditions, in a driver-follower scheme, it is possible to detect the presence of a few molecules. Moreover we investigated the evolution of the mechanical properties of the system as the molecules bridging the gap change in number or strength. We believe that with a proper control of the bonding chemistry, the sensitivity of our system could be improved to reach the single molecule level.
4156 en Novel scanning microscope for visualization of individual emission sites on flat field emission cathodes We present first results obtained by a novel scanning projection field emission microscope (SPFEM) designed to study flat broad-area field emission cathodes. The instrument merges capabilities of measuring the electron field emission current from an individual emitting site and genuine projection of electrons onto a luminescent screen. This is achieved by an optimized shape of the anode probe having a 0.04 mm aperture which generates an uniform macroscopic electric field across the investigated area of the cathode. This fact also enables presentation of the relation between the current density and the applied electric field. The magnification of the electron-optical system alone was calculated by computational modeling for some cathode – probe distances and for some voltages. The unique SPFEM performance is demonstrated on smooth sulfur doped nanodiamond films synthesized on molybdenum substrates.
4157 en PicoNewton Force Spectroscopy of Live Neuronal Cells, using Optical Tweezers It is known that migration of the axons of neuronal cells is driven by guidance cues sensed by receptors located on the growth cone.1 Filopodia and lamellipodia, the highly motile structures extruding from the tip of the growth cone, explore the environment. Their motion has been analysed, but little is known about the force these neuronal structures exert on the structures they might find during their navigation. In fact, the analysis of this force has been limited to theoretical considerations and experimental analysis have been restricted to samples of isolated filaments2 or to migrating cells.3 In this study, we used optical tweezers4 to measure the forces exerted by filopodia and lamellipodia with a millisecond temporal resolution. We found that a single filopodium exerts a force not exceeding 3 pN. In contrast, the force exerted by lamellipodia ranged up to 20 pN or more with a duration varying from less than 1 second to more than 30 seconds. These measurements suggest that in the absence of actin polymerisation no force can be produced and that microtubule polymerisation is required in order to develop forces larger than 3 pN. These results show that neurons not only process information but also they act on their environment exerting forces varying by 1 to 2 orders of magnitude. Silica beads of 1 µm in diameter, functionalised with amino groups to reduce sticking, were trapped with an 1064 nm infrared (IR, mW power at the sample) optical tweezers close to the growth cone of a migrating axon (Fig.1a).5 The growth cone displaced the bead both laterally and axially from its equilibrium position by even 2 or 3 microns (Fig.1b). At the end the bead did not remain attached to the growth cone and could return to its original position in the trap (Fig.1c). We measured the lateral force exerted by the growth cone Fneu = (Fx, Fy) by following the bead position both by using back focal interferometry with quadrant photo diode (QPD)4 and by video tracking. When the bead was far from the growth cone the QPD recordings of Fx and Fy were quiet with a standard deviation ? of approximately 0.18 pN (Fig. 1d upper trace), but collisions producing a force larger than 5? were observed when the bead was moved near the growth cone (Fig.1d lower trace). In several occasions, Fx and Fy increased within 1-10 seconds, reaching values of the order of 20 pN (Fig.1e) and, when the growth cone stopped pushing, the bead returned to its equilibrium position, often in less than 1 ms. As the presence of floating debris and wandering filopodia near the bead could affect the light pattern impinging on the QPD, a collision was considered reliable when the bead displacement obtained with the QPD and videotracking were in agreement (black and yellow traces respectively, in Fig. 1f) and the presence of a colliding filopodium or lamellipodium was verified by visual inspection of the movie. We have analysed collisions between growth cones and trapped beads in more than 200 experiments. Each experiment lasted for 3 minutes and in many experiments several collisions significant for statistical analysis were observed. These collisions produced maximal forces ranging from less than 1 pN to at least 20 pN with a maximal rate of increase of 10 pN/second. These collisions lasted from less than 1 sec to about 40-60 seconds. Larger forces were usually observed during long lasting collisions. As these forces extend over a wide range of intensities and durations, we measured separately the forces developed by filopodia and lamellipodia for hundreds experiments in order to have a good statistics.
4158 en Single atom manipulation and spectroscopy using low-temperature STM STM, operated under extremely clean and stable conditions became in the last decade a very powerful nanotechnological tool. It is not only used to probe individual atoms and molecules but also to displace, decompose and assemble individual species into new artificial nanostructures1. These are formed in an atom-by-atom process by precisely controlling the quantum-mechanical interactions between the STM tip and the sample. Such experiments require cryogenic temperatures (below 20 K), atomically clean (pressure below 10-11 hPa) and properly ordered surfaces (controlled by LEED and AES) as well as extreme mechanical and electrical stability of the tip-sample tunneling junction. The experiments presented were performed with a home-built Besocke type UHV STM, cooled with liquid helium. Cu (111) and (112) single crystal surfaces, cleaned by repeated cycles of ion sputtering and annealing, were used as substrates, while STM tips were prepared by electronically controlled etching of polycrystalline W wires in 2M NaOH. Single adatoms of Cu were extracted from the substrate surface by a controlled dipping of the tip into the Cu surface under relatively high tunneling bias voltages. Individual adatoms were manipulated in a controlled manner into desired nanostructures. Submonolayer amounts of Co were deposited onto clean Cu substrates from a Knudsen source. Co atoms, deposited at room-temperature, form agglomerates during deposition already, but can be separated at lower temperatures into individual adatoms by means of the tip-sample interaction. The electronic structures of both, Cu and Co were studied by STS with the lock-in technique2. Differential conductance (dI/dV) spectra, proportional to LDOS, are fully reproducible and can be used to differentiate between Cu and Co species.
4159 en In situ comparison of hybridization properties of nanografted and self-assembled DNA monolayers Using a conventional AFM and nano-patterned low and high density single stranded DNA SAMs we have investigated the elastic response of surface tethered DNA molecules after various hybridization times and under different loading forces. Upon hybridization, as expected, only for the low density single stranded DNA SAM a transition to the “standing up” phase is seen. We find that high density DNA SAMs are lacking the capability to hybridize not because of density but because of inherent disorder which is reflected by their low DNA SAM height. Nanografting is known to increase both packing density and molecular order. Therefore we have performed patch-in-a-patch experiments to nano-pattern a conventional low density single stranded DNA SAM with 2 subsequent nanografting processes. First a reference area of HS-C6-OH molecules was created into which in a second process single stranded DNA was nanografted. Thereby an accurate in situ comparison under equivalent conditions between conventional DNA SAMs and nanografted DNA patches becomes accessible. Side by side height and compression measurements can provide valuable biophysical insights on the organization of DNA molecules close to a solid interface e.g. discriminating between molecular order and density effects.
4160 en Crowding effects in Enzymatic Restriction Reactions within DNA Nanostructures We have studied the effect of restriction enzyme digestion reactions (DPN II) within DNA nanostructures on flat gold substrates by Atomic Force Microscopy (AFM). Typically we work with a few patches of Self Assembled Monolayers (SAMs) DNA that are hundred nm in size and are fabricated within alkylthiol SAMs on gold films by means of an AFM based lithographical technique known as Nanografting.1,2 We start by nanografting a few patches of a single stranded DNA (ssDNA) molecule of 44 base pairs (bps) with a recognition sequence of 4 bps (specific for the DPN II enzyme) in the middle. Afterwards, to obtain reaction ready DNA, the nanopatches are hybridized with a complementary ssDNA sequence of the same length. The enzymatic reactions were carried out over nanopatches with different molecular density and different geometries. Using nanopatch height measurements carried out with an AFM we are able to show that the capability of the DPN II enzyme to reach and react at the recognition site significantly depends on the geometry and the molecular density of the nanopatches. In general it was found that the digestion of the DNA by the enzyme it is strongly inhibited at a relatively low dsDNA density. Reference experiments were similarly carried out with nanopatches of a DNA sequence without the recognition site and it was found that in that case the enzymatic reaction didn’t lead to digestion of the nanopatches. These findings suggest that, due to the enzyme size, it is possible to tune the efficiency of an enzymatic reaction on a surface by changing the crowding conditions of the DNA in the nanopatches.
4161 en New approach for the determination of the hybridation efficiency of ssDNA nanopatches Films of single-stranded DNA (ssDNA) immobilized on surfaces form the basis of a number of important biotechnology applications, including DNA micro- and nano-arrays. However, the question about how the film structure affects the hybridization process is not properly addressed in literature. While it is commonly accepted that the hybridization efficiency in extensive self assembled monolayers of ssDNA is basically inversely proportional to the molecular density of the probes on the surface, the same agreement has not been reached when the monolayers are confined in nanometric structures. Using nanografting (a nanolithographic technique performed in liquid environment with an AFM tip) we are able to fabricate ssDNA micro- and nanostructures with well defined height, lateral size. The hybridization causes an increase of height that can be monitored by AFM microscopy. Surprisingly, we observed the highest efficiency of ssDNA hybridization at densities halfway to saturation; moreover, in this regime the mechanical properties of the ssDNA hybridized film and of a nanografted double-stranded DNA film are equivalent. To quantify the hybridization efficiency on our nanostructures, and, more in general, the efficiency of chemical processes which involve only few molecules, the analytical chemistry techniques are no longer effective. On the contrary, a physical approach offers many suitable solution. In particular micro- and nano-mechanical resonating sensors experienced a significant progress in the last 5 years, allowing the measurement/enumeration of single molecules and a mass sensitivity of 10-21 g. We fabricate several devices consisting of silicon cantilevers 5x2x15?m3 and 5x2x25?m3 in size with mass of 350 and 580 pg and resonance frequency at 10.3 and 3.7 MHz respectively. The top side was covered with an ultraflat Au layer (0.8nm RMS) on which an organic self assembled monolayer was formed. ssDNA nanostructures on the free end of the cantilevers are fabricated by nanografting. When the ssDNA hybridizes the mass of the cantilever increases and the resonance frequency shifts toward lower values. Operating in vacuum conditions we obtained a quality factor better than 104 that allowed us to attain a mass sensitivity of 5x10-16 g, corresponding to the hybridization of only 1% of a 3x3?m2 ssDNA nanostructure.
4162 en EU framework programmes and Slovenia Slovenia successfully participated in 5th and 6th Framework programmes. The FP6 ended at the end of last year. Participants from Slovenia participate in all priorities and quite a lot have been from business sector. FP7, proposed by Commissioner Poto?nik, lasting for the first time for seven years, starting at the end of last year, is a continuation of the FP6 but it introduces several new items and features. The basic research as a new type of EC research funding with new rules is directed by the last year established European Research Council (ERC). Usual research activities are grouped under Collaborative research umbrella which incorporates 10 thematic areas including Nanosciences, Nanotechnologies, Materials and new Production Technologies (NMP) as Thematic priority 4 and Information and Communication Technologies (ICT) as Thematic priority 3. The first FP7 calls were the last Christmas presents. Work programmes, defining detailed work plan for each thematic priority and defining calls details, are under preparation and adoption for the 2008 calls which are planned to be announced before the end of this year. EU adopted in summer 2005 a document Nanosciences and nanotechnologies: An action plan for Europe 2005-2009; the first intermediate report was prepared in September 2007 by EC.
4163 en MoSIx nanowires funtionalization for molecula-scale connectivity We report on a new highly reproducible route to recognitive self-assembly of molecular-scale circuits using sulfur-terminated subnanometer diameter Mo6S9-xIx (MoSIx) molecular nanowires. We demonstrate solution-processed attachment of MoSIx connecting leads to gold nanoparticles (GNPs). We also show that naked nanowires have the potential to bind thiolated proteins such as green fluorescent protein directly, thus providing a universal construct to which almost any protein could be attached. We further demonstrate three-terminal branched circuits with GNPs, opening a self-assembly route to multiscale complex molecular-scale architectures at the single-molecule level.
4164 en Director and polarization fluctuations in suspensions of ferroelectric nanoparticles in nematic liquid crystals Phase separated state of thin films of (Pr0.6Ca0.4)MnO3 on LaAlO3 and SrTiO3 substrates was investigated by means of the ultrafast time-resolved magnetooptics in the magnetic field up to 1.1 T. The photoinduced Kerr rotation and ellipticity show remarkably different magnetic-field dependence. From comparison with the static Kerr rotation and ellipticity we conclude that two different magnetic phases are present in the samples at low temperatures. According to small angle neutron scattering results one of the phases originates from nanoscopic ferromagnetic metallic clusters. Temporal dependence of the photoinduced Kerr signals indicates that upon photoexcitation changes of the volume fraction of these phases take place on a timescale of a few tens of picoseconds.
4165 en Self-assembled Mo6S9-xIx networks Mo6S9-xIx (MoSIx) nanowires have been known to easily bond to golden nanoparticles (GNPs) due to their sulfur terminated ends. Solution-processed attachment was used for connecting MoSIx nanowires as leads to GNPs. We have recently observed irregular self-assembled networks consisting of small nanoparticles and thin MoSIx nanowires (&lt;2.5 nm in diameter) that span on a scale of several tens of micrometers. One such network has been analyzed by measuring heights of particles and number, diameter and orientation of nanowires connected to them. Results confirm that the bonding between nanowires happens more easily via nanoparticle than without it. We have also monitored this network by taking its topographic picture every few weeks for the last six months and degradation of the network with time has been observed – the nanoparticles have been reduced in size and nanowire diameter has substantially decreased.
4166 en Non-equilibrium optical properties of MoSI nanowires MoSI nanowires are the one-dimensional systems with the weakest known interaction with their neighbours. Therefore they are expected to show most clearly the effects of one-dimensionality. We studied equilibrium and non-equilibrium optical properties via optical reflectivity and absorbance as well as femtosecond pump-probe spectra of oriented Mo6S3I6 nanowire thin films. Absorption of light polarised parallel to the axis of orientation shows a series of resonances that are absent for perpendicular polarisation. The sharp Van Hove features expected from the highly one-dimensional character of the material are not observed, partly because of the large density of electron sub-bands and partly because of electron energy damping. The electronic relaxation from a non-equilibrium situation was studied via femtosecond pump-probe spectroscopy. By exciting into the second optical resonance we found a complex relaxation behaviour involving three distinct excited states and determined the lifetimes of the involved states.
4167 en Synthesis of metallic Ag and semiconducting ZnS nanoparticles in self -assembled polyelectrolyte templates Metal and semiconductor nanoparticles have attracted much interest lately due to their unique size dependant properties, stemming from their quantum confinement effects and large surface areas. The main problem in nanoparticle synthesis is their aggregation which often prohibits tailoring of particle size. One of the convenient methods to manipulate and process these nanoparticles in technologically useful formulations is an “in-situ formation” of nanoparticles in polymer matrices which prevents the aggregation of the nanoparticles and enables their uniform distribution. The polyelectrolyte multilayers (PEMs) of polyallylamine (PAH) and polyacrylic acid (PAA) were assembled on hydrophilic polystyrene tissue-cultured and surface modified quartz substrates by alternately dipping the substrates into polyanion and polycation aqueous solutions with various pH values until the desired thickness of the PEMs was obtained. The linear charge density or concentration of free carboxylic acid repeating units of PAA was controlled by adjusting the pH of the dipping PAA solutions. Thus formed PEMs were exposed to the metal ion solution at nominally neutral pH. By subsequent reduction of Ag ions or sulfidication of the Zn ions, metallic (Ag) or semiconductor nanoparticles (ZnS) are formed within the PEMs. Since upon nanoparticle formation carboxylic groups of PAA are regenerated, the synthesis methodology can be repeatedly cycled to incorporate more metal ions, which enables the formation of nanoparticulate films where the nanoparticle size and concentration can be manipulated. The aim of this work is to control the size and concentration of the in-situ formed inorganic nanoparticles by varying the synthesis conditions.
4168 en The influence of space restriction on the formation and stability of polymorphic and amorphous forms It is known from the classical nucleation theory that crystallization can only occur if critical nuclei are formed. Thus, there has to be enough space available for the formation of such nuclei. In the present work, we determined theoretical criteria that have to be satisfied for the occurence of a crystalline phase from a supersaturated solution in confinement conditions (e. g. resulting from entrapment of a drug solution into a porous matrix). Similar criteria were determined for the occurence of crystallization from amorphous or metastable polymorphic phases in confinement conditions. It was shown theoretically that spatial constraint can cause vitrification from solution or polymorph selection1. Metastable phases, which are formed in this manner, can be effectively protected against crystallization into thermodynamically more stable products through space restriction. The theoretical criteria were tested experimentally with nifedipine entrapped into a nanoporous silica xerogel matrix with an average pore diameter of 2,5 nm. The results of thermal analysis and X-ray powder diffraction have shown that the amorphous nifedipine, formed inside the xerogel pores, is protected against crystallization in the temperature interval in which it is chemically stable. With a combination of differential scanning calorymetry and BET analysis we estimated the total amount of nifedipine that can be stabilized in the xerogel. Also, it was shown through calculations, that entrapment into a hypotetical nanoporous matrix with a certain pore size allows selective formation of sulfathiazole polymorphs. If the pore radius is equal to 1,7 nm, then form IV is formed selectively from a toluene solution. If the pore radius is less than 1,4 nm, an amorphous phase is formed.
4170 en Introduction to the Conference 
4171 en Automated Character Annotation in Multimedia We describe progress in automatically identifying characters in films and TV series using their detected faces together with readily available annotation in the form of subtitles and transcripts. We describe how the subtitles and transcript can be aligned to give weak supervision on the characters present in a shot (as well as on the actions, emotions, locations etc). The supervision is weak because of correspondence problems and the character may not be visible. The visual problem of face recognition is challenging because faces appear in images at various sizes and pose, and also vary considerably in expression. Fortunately, videos contain multiple face examples of each person in a form that can easily be associated automatically using straightforward visual tracking. These multiple examples reduce the ambiguity of recognition. We show that the text supervision can be strengthened by speaker detection. Although the labelling is still incomplete and noisy, it is then sufficient to learn visual models for recognition, and achieve successful character identification. This is joint work with Mark Everingham and Josef Sivic.
4172 en Shape modelling via higher-order active contours and phase fields For the most part, shape modelling has focused on modelling families of regions consisting of deviations around a given reference shape with a simple topology. There are applications; however, where the family of regions involved does not show such constrained behaviour. Cases where the number of objects is unknown a priori, or where the topology of the region may be otherwise complex (for example network shapes), require new techniques. 'Higher-order active contours' (HOACs) represent one approach to modelling such families of regions. By introducing explicit long-range interactions between region boundary points, HOACs can model families of regions sharing geometric properties without overly constraining region topology. Representing regions by their boundaries is often inconvenient, however, both analytically and numerically, especially for complex topologies. An alternative is the approach known as 'phase field' modelling. The phase field representation and modelling framework offers a number of advantages, both for the simplest region models and for HOACs. By way of illustration, the use of HOAC and HOAC phase field models to estimate the regions corresponding to road networks and tree crowns in satellite and aerial images will be described.
4173 en A contrario matching of local features between images 
4174 en Recognising Animals 
4175 en Person detection and recognition, tracking and analysis The presentation describe the work carried out in E-team 3, focusing on the multi-camera scenarios. First, the context and applications of these scenarios are reviewed. Afterwards, a technique developed during the last year for multi-level foreground detection is presented. 3D tracking of multiple people and human body motion capture are presented next. Finally, some videos showing results of other works developed within the e-team are presented.
4176 en Action class detection and recognition 
4177 en On Sequence Kernels for SVM classification of sets of vector 
4178 en Crossing textual and visual content in different application scenarios In this presentation, we present a method based on Trans-media Pseudo-Relevance Feedback that allows crossing visual and textual content through multimodal knowledge base. The main idea is to use one of the modalities to retrieve multimodal documents from the knowledge base and then to switch (or to fuse with) the other modality in the next step. The different potentials of the method (retrieval, image annotation, text illustration ...) are illustrated within a Travel Blog Assistance System example scenario.
4179 en Fully Bayesian Source Separation with Application to the CMB Blind source separation refers to the inferring of the values of variables (known as sources) from observations that are linear combinations of them. The observations and sources are usually vectors. Both the sources and the matrix of linear coefficients may be unknown. Here we describe an approach where the sources are assumed to be Gaussian mixtures. An MCMC procedure has been developed that computes the posterior distribution of sources and the matrix of linear coefficients from observations. It is applied to source separation in multi-channel extra-terrestrial microwave data, with the goal of separating out the cosmic microwave background signal.
4180 en Audio Content Search Dr. Joachim Köhler showed several techniques and applications in the area of audio content search. Robust segmentation algorithms were applied to detect speech and non/speech events and to perform a speaker segmentation and clustering task. Further speech and music alignment techniques were demonstrated to generate a time code to existing textual and score descriptions. The current state-of-the-art technology for spoken document retrieval was given and alternative approaches and innovative applications were presented to avoid the Out-Of-Vocabulary problem in speech indexing tasks.
4181 en Audio-Visual Speech Analysis & Recognition Human speech production and perception mechanisms are essentially bimodal. Interesting evidence for this audiovisual nature of speech is provided by the so-called Mc Gurk effect. To properly account for the complementary visual aspect we propose a unified framework to analyse speech and present our related findings in applications such as audiovisual speech inversion and recognition. Speaker's face is analysed by means of Active Appearance Modelling and the extracted visual features are integrated with simultaneously extracted acoustic features to recover the underlying articulator properties, e.g., the movement of the speaker's tongue tip, or recognize the recorded utterance, e.g. the sequence of the numbers uttered. Possible asynchrony between the audio and visual stream is also taken into account. For the case of recognition we also exploit feature uncertainty as given by the corresponding front-ends, to achieve adaptive fusion. Experimental results are presented in QSMT, MOCHA and CUAVE audiovisual databases.
4182 en Multimodal Interfaces Two important issues in multimodal system design, is the selection and mix of input and output modalities and the exploitation of the synergies between the modalities in order to maximize the usability of the system. In this talk, we propose two new objective metrics, relative modality efficiency and multimodal synergy, that can provide valuable information and identify usability problems during the evaluation of multimodal systems. Relative modality efficiency (when compared with modality usage) can identify suboptimal use of modalities due to poor interface design or information asymmetries. Multimodal synergy measures the added value from efficiently combining multiple input modalities, and can be used as a single measure of the quality of modality fusion and fission in a multimodal system. The proposed metrics are used to evaluate a multimodal system that combines pen and speech input, and are compared with traditional evaluation metrics. The results provide much insight into multimodal interface usability issues, and demonstrate how multimodal systems should adapt to maximize modalities synergy resulting in efficient, natural, and intelligent multimodal interfaces.
4183 en Multimodal Processing and Multimedia Understanding: Image Retrieval Using Eye Movements His presentation describes experiments that explored eye behaviour when carrying out purely visual tasks on a Corel database of 1000 images. Results are reported that indicate that image identification can be carried out significantly faster with an eye tracker than with a mouse. Participants performing image search tasks were able to reach target images in significantly fewer steps with an eye tracker than by random selection. The effects of the intrinsic difficulty of finding images and the time allowed for successive selections were also investigated. Finally the results yielded evidence of the use of rapid pre-attentive vision during visual search.
4184 en Feature extraction from audio and their application in music organization and transient enhancement in recorded music 
4185 en Interactive Visualization tool with Graphic Table of Video Contents In this paper, we present an interactive visualization, called Table of Video Contents (TOVC), for browsing structured TV programs such as news, magazines or sports. In these telecasts, getting a good segmentation can be very time-consuming, especially in an annotating context. Our visualization, connected with a classical media player, offers a very handy video browser. This system allows a global overview by showing the temporal structure and by giving some semantic information. The drawn structure enables a non linear video access by suggesting relevant key frames. The TOVC is created from a graphic framework designed for computing similarities on visual contents, and displaying the associated proximities in a 2D map with graph representation. TOVC is one of its first applications and shows interesting capabilities.
4186 en Relational Learning as Collective Matrix Factorization We present a unified view of matrix factorization models, including singular value decompositions, non-negative matrix factorization, probabilistic latent semantic indexing, and generalizations of these models to exponential families and non-regular Bregman divergences. One can model relational data as a set of matrices, where each matrix represents the value of a relation between two entity-types. Instead of a single matrix, relational data is represented as a set of matrices with shared dimensions and tied low-rank representation. Our example domain is augmented collaborative filtering, where both user ratings and side information about items are available. To predict the value of a relation, we extend Bregman matrix factorization to a set of related matrices. Using an alternating minimization scheme, we show the existence of a practical Newton step. The use of stochastic second-order methods for large matrices is also covered.
4187 en BilVideo: MPEG-7 Compliant Video Database Management System BilVideo is a video database management system that provides integrated support for spatiotemporal, semantic, low-level feature queries. These queries were processed by a rule-based system based on a knowledge-base, a feature and object-relational databases in the earlier version of BilVideo. BilVideo is now MPEG-7 compatible, which is a standard for describing multimedia content. An XML-native feature database for efficient indexing, and an MPEG-7 feature extractor tool are used for this purpose. New version of our video DBMS is also based on client-server architecture, where users may specify queries by using the visual query interface. With the help of MPEG-7 support, it will be possible to process low-level feature queries (color, shape, texture, motion) and integrate with other multimedia platforms such as Multimedia Metadata Management (4M) of CNR ISTI, and Web Content platform of CEA LIST. MPEG-7 compatibility issues and the new system design are briefly discussed in the presentation.
4188 en Open Vocabulary Speech Analysis in VITALAS Automatic indexing of TV and radio speech data requires robust components for both speech recognition and spoken document retrieval. Due to the high topic variability and the resulting large vocabularies, classic word-based approaches have to cope with a high number of out-of-vocabulary words. This talk presents a phonetic approach to open vocabulary indexing based on syllable decoding and retrieval. Current experimental results are presented, followed by a demonstration of the Fraunhofer IAIS AudioMining system for spoken term detection.
4189 en Implicit feedback learning in semantic and collaborative information retrieval systems Information retrieval is a very wide domain which can involve various types of activities and tasks. Many complex factors are participating in a search for information and many systems have been experimented. Nowadays a general consensus has been established around a keyword/document matching process which appears to be efficient on large scale and have enough reliability to satisfy a significant part of the users. Btu this claim has to be limited and for some subjects, search is still a difficult task. Many reasons can be proposed to explain these phenomena, but the most salient ones are the difficulty for users to express their needs while searching for information and the limitation of shared knowledge between users and information retrieval systems, meaning that both users and machines don't really understand the information and knowledge space used as references by the other. This presentation try to provide an overview of one way to resolve those gaps: using feedback learning. The aim is to make the system learning on user behaviour in order to better define its current needs. Machine learning algorithms applied on signal coming from user while performing a search can lead to the understanding of what is really relevant to the users and then can be exploited to help him during its tasks. The work, engaged through the VITALAS1 project, is presented: study of users search logs and definition of a feedback learning framework. Then research on implicit relevance feedback and query optimisation is presented as a first attempt to exploit the feedback learning framework. Finally an overview of the next steps within those studies is presented and especially their impact on the VITALAS project.
4192 en Introduction 
4193 en Women in science: From exclusion to (complete) inclusion Women's mass entry into the academic and scientific fields (particularly in the second part of the 20th Century) is happening in social circumstances where sexist patterns are slowly dissolving. Therefore, the issue of women in science is multifaceted and can be separated into two kinds: a) how the »second sex« is perceived and considered as a subject in scientific analysis and estimations, which is an especially urgent subject in social sciences and humanities, and b) how are women situated as direct participants in scientific activities, which is important in all the domains of science. In the lecture the author will be focused on the second aspect, particularly on the question of the politics of equal opportunities in science from the perspective of it's placement inside historical and socio-cultural milieus. According to this starting-point she will represent the key features of the EU political orientation regarding the abolition of discrimination against women in science and focus on the situation in Slovenia.&#160;
4194 en Promoting Women in Science Central European Centre for Women and Youth in Science aims at promoting gender equality in research and development and young scientists in the Central European region www.cec-wys.org. Funded by the European Commission, the Centre is the first regional centre to advance the position of women scientists and of young scientists in Central Europe. Building on report produced by a group of EC experts on the position of women in science in Central and Eastern European countries (Enlarge Women in Science East, Enwise), the project focused on promoting equal opportunities in research and development in four Central European countries and examining and improving the position of young people in science. Besides creating a user-friendly information-laden website, the Centre established a database of women experts from the Central European region with a view to increasing the participation of women scientists in European research and the European Commission’s expert database.
4195 en Physics is my Profession 
4196 en Women and Science in the EU: Perceptions from the East The issue of Women and Science and more specifically, of women and scientific careers has been attracting considerable attention in recent years at the EU level. Concern about the considerable waste of human resources and the persistence of institutional discrimination impeding women’s scientific careers has prompted a number of significant documents and concerted action at the trans-national level designed to tackle these entrenched problems. Although female participation in science has increased in recent decades, women are still rarely seen in top scientific positions, such as professorships or other high-level research positions. Career opportunities in science are determined by a number of complex factors, which cannot easily be described using simple statistical indicators. ‘Internal’ factors (which depend on the organization, operation, and structuring of the scientific community itself) play an important role. The internal factors interact with ‘external’ factors, which are determined and shaped by society at large – such as existing gender roles inside and outside the family, the changing status of women with regard to education and the labor market, and the political framework. The existing differences, based on the historical and political background from different countries, must not stand in the way of women of Europe coming together as a unified force to develop a common strategy and agenda for realizing gender equality in science in an enlarged Europe. Women in the East and the West still know too little and display sometime limited interest in what what’s happening on the ‘other side’. Women in the East express high hopes that the legal rights and directives of the EU and higher standards of the older members of the EU will have a positive impact on gender equality policies in the Eastern European countries. Using gender sensitive indicators and statistics, this lecture will present an overview of the presence and participation of women in science, and will show gender-specific patterns of career opportunities in Western and Eastern European countries. A short presentation of the existing efforts at European level in promoting gender equality in science and increasing the proportion of women scientists will be also shown. It will be outlined the initiatives, additional measures that need to be taken in order to strengthen the role of female scientists at regional and European level. References
4197 en Career Chances of Women in Nanosciences in European Universities – Conditions for International Diversity The lecture will place two different questions in the centre: * What are the reasons for gender inequality in relation to the chances to become a University professor in Nanosciences? * How can cross-national differences with respect to the share of women in University career positions in this field be explained? The paper combines a cultural and an institutional perspective. It is argued that institutions like the University are based on, and reproduce, expectations towards the behaviour of individuals. In many European Universities, particularly also in Western Europe, the traditional expectations in relation to the main qualifications and the behaviour of an University professor were traditionally mainly also based on two cultural constructions: * on the image of the male breadwinner who is comprehensively available for his profession, while another person takes over the organisation of his everyday life, as well as housework and childcare; * on a ‘male ‘habitus (according to Bourdieu 1987): this means, that specific qualities that the professor is expected to have are in society defined as ‘male’ qualities. However, together with a fundamental change of European universities in the last fifteen years that can be called an ‘economisation’ of Universities, also the cultural ideal of the University professor, and the expectations which are connected with this position, have in part fundamentally changed. Such change has in part already started earlier in East European countries than in the West. It is argued here that this development has opened new options for women for a University career. However, there are considerable cross-national differences with respect to the chances for women to make a University career in Nanosciences. It is argued that particularly also some cultural factors contribute to explaining such differences: the respective cultural model of the University professor in a society , the societal esteem that is connected with this position, and the way in which “care” is constructed in the dominant cultural model of the family.
4198 en Round table Discussion 
4201 en Transitioning Applications to Ontologies 
4202 en QuestIO: a Question - based Interface to Ontologies 
4203 en WP8: Exploitation and Dissemination 
4204 en WP2: Learning Web-service Domain Ontologies 
4205 en Opening 
4207 en Opening 
4208 en FP7 “ICT for Transport” Strategic Objectives 
4209 en "Mastering logistics complexity - physical challenges and IT opportunities". 
4210 en Increasing efficiency with global visibility 
4212 en Interview with Andrea Furlanetto 
4213 en Interview with Henri Barthel 
4214 en Interview with Wolfgang Höfs 
4215 en EURIDICE Project Overview 
4216 en P1 Intelligent Cargo Integration Framework Workplan Review 
4218 en P2 Pilot Applications Workplan Review 
4220 en "P3 Impact Creation Workplan Review 
4222 en Project Infrastructure 
4223 en Project to Improve the Logistic Flow In the Eyewear Supply Chain 
4224 en Lessons learned from innovation diffusion in the US air traffic control 
4225 en Logistics system and IT state-of-the-art, critical issues in the fishing logistics system 
4226 en Relevant EU transportation projects 
4227 en Road transport transformation scenarios 
4228 en M2M communication systems 
4229 en Knowledge Discovery in extensive data sets 
4230 en "Management of collaboration in networks 
4231 en Performances evaluation approaches and standards 
4232 en Value innovation for the logistic sector 
4233 en WP33 Dissemination and Outreach 
4234 en WP11 Framework Architecture 
4235 en WP12 Domain Knowledge Formalization 
4236 en WP13 Cargo Intelligence 
4237 en WP14 Services Authoring and Orchestration 
4238 en Joint discussion on cooperation between Project 1 and Project 2 
4239 en Joint discussion on cooperation between Project 1 and Project 2 
4240 en Joint discussion on cooperation between Project 1 and Project 2 
4241 en Meeting conclusions 
4242 en Overview of New Developments in Boosting I will give an overview of recent developments in boosting, focusing on three papers which take very different approaches towards making boosting more efficient and effective. Boosters iteratively choose base classifiers via a weak learner and then update a distribution over training examples. Roughly, the three papers show progress on the three issues implicit in this one-sentence description of boosting: the number of iterations required, the computational cost of choosing good base classifiers, and the time and space complexity from maintaining a distribution over training examples. Warmuth, Liao, and Ratsch (2006) propose TotalBoost, which is a "totally corrective" boosting algorithm. Intuitively, on each round, totally corrective boosters choose base classifiers which give more information not present in previously chosen base classifiers. This leads to fewer iterations and smaller final hypotheses. Barutcuoglu, Long, and Servedio (2007) describe an alternative model for boosting where assumptions about the diversity of base classifiers allow the booster to learn in a single pass over the set of base classifiers. This eliminates the need to optimize over all base classifiers on each round. Bradley and Schapire (2007) propose an algorithm called FilterBoost which trains on examples drawn from an oracle rather than a fixed set of examples. This alternative learning framework can model learning via a single pass over the set of training examples and allows the booster to train efficiently on very large datasets.
4244 en Content Analysis and MPEG-7 Description Tools 
4245 en Video Browsing Tool 
4246 en From Representing Knowledge to Deploying Metadata in the Multimedia Domain 
4248 en Truth in Fan Fiction The lecture is an introduction to "fan fiction" or "amateur fiction". The first part comprises philosophy and moves to fan fiction itself with examples. At the end we will discuss philosophical puzzels that fan fiction poses wihtin theory of fiction. At the end we will solve the problem or we will atleast try to.
4249 en Virtual Worlds, Contextualism, and the Myth of Fiction In this lecture I will tell you a story about my exploits in Second Life, at the end of that I will extract two philosophical conclusions. At the beginning of the lecture we will have fun and discuss about various philosophical aspects and at the end we will get serious and try to establish a conclusion. We will start off by discussing about two virtual worlds...
4255 en The Concept of Time and Space in European folklore 
4257 en The Concept of Time and Space in European folklore 
4260 en The Concept of Time and Space in European folklore 
4261 en The Concept of Time and Space in European folklore 
4263 en Economy and values after a century of empirical research 
4264 en The economist as therapist: Behavioural economics and "light" paternalism We review methodological issues that arise in designing, implementing and evaluating the efficacy of 'light' paternalistic policies. In contrast to traditional 'heavy-handed' approaches to paternalism, light paternalistic policies aim to enhance individual choice without restricting it. Although light paternalism is a 'growth industry' in economics, a number of methodological issues that it raises have not been adequately addressed. The first issue is how a particular pattern of behavior should be judged as a mistake, and, relatedly, how the success of paternalistic policies designed to rectify such mistakes should be evaluated – i.e.,the welfare criterion that should be used to judge light paternalistic policies. Second,paternalism, and especially light paternalism, introduces new motives for attempting to understand the psychological processes underlying economic behavior. An enhanced understanding of process can help to explain why people make mistakes in the first place,and, more importantly, provide insights into what types of policies are likely to be effective in correcting the mistakes. Third, there is an acute need for testing different possible policies before implementing them on a large scale, which we argue is best done in the field rather than the lab. Fourth, in addition to methodological issues, there are pragmatic issues concerning who will implement light paternalistic policies, especially when they involve positive expenditures. We discuss how economic interests can be rechanneled to supportendeavors consistent with light paternalism.
4266 en Why European Translators Should Want to De-Westernize Translation Studies 
4267 en On a L1-Test Statistic of Homogeneity My presentation will be divided in two parts. First, I will present two simple and explicit procedures for testing homogeneity of two independent multivariate samples of size $n$. The nonparametric tests are based on the statistic $T_n$, which is the $L_1$ distance between the two empirical distributions restricted to a finite partition. Both tests reject the null hypothesis of homogeneity if $T_n$ becomes large, i.e., if $T_n$ exceeds a threshold. I will first discuss Chernoff-type large deviation properties of $T_n$. This results in a distribution-free strong consistent test of homogeneity. Then the asymptotic null distribution of the test statistic is obtained, leading to an asymptotically $\alpha$-level test procedure. In the second part, I will consider the problem of selecting an unknown multivariate density $f$ belonging to a set of densities ${\cal F}_{k^*}$ of finite associated Vapnik-Chervonenkis dimension, where the complexity $k^*$ is unknown, and $\mathcal F_k \subset \mathcal F_{k+1}$ for all $k$. Given an i.i.d. sample of size $n$ drawn from $f$, I will show how the statistic $T_n$ can be used to build an estimate $\hat f_{K_n}$ yielding almost sure convergence of the estimated complexity $K_n$ to the true but unknown $k^*$, and with the property $\mathbf E \{\int|\hat f_{K_n}-f|\}=\mbox{O}(1/\sqrt{n})$. The methodology includes a wide range of density models, such as mixture models and exponential families. This talk is a summary of two papers written with B. Cadre (ENS Cachan, France), L. Devroye (McGill University, Montreal) and L. Gyorfi (Technical University, Budapest)
4268 en That's close! On translators, interpreters, researchers, texts - and their interrelations 
4269 en Adaptive Representations for Efficient Inference for Distributions on Permutations Permutations are ubiquitous in many real world problems, such as voting, rankings and data association. Representing uncertainty over permutations is challenging, since there are $n!$ possibilities, and typical compact representations, such as graphical models, cannot efficiently capture the mutual exclusivity constraints associated with permutations. In this talk, we use the ''low-frequency'' terms of a Fourier decomposition to represent such distributions compactly. We first describe how the two standard probabilistic inference operations, conditioning and marginalization, can be performed entirely in the Fourier domain in terms of these low frequency components, without ever enumeration $n!$ terms. We also describe a novel approach for adaptively picking the complexity of this representation in order control the resulting approximation error. We demonstrate the effectiveness of our approach on a real camera-based multi-people tracking setting. This presentation is joint work with Jon Huang and Leo Guibas.
4270 en Introduction to the Workshop 
4272 en A Framework for Probability Density Estimation The talk introduces a new framework for learning probability density functions by assessing their performance against a set of 'test functions'. A theoretical analysis suggests that we can tailor a distribution for a class of tasks by training it to fit a small subsample. There is a trade-off between the complexity of the class of distributions used for the distribution and the complexity of the set of tasks. Experimental evidence is given to support the theoretical analysis.
4273 en The opening of the symposium 
4274 en Kullback-Leibler Divergence Estimation of Continuous Distributions We present a universal method for estimating the KL divergence between continuous densities and we prove it converges almost surely. Divergence estimation is typically solved estimating the densities first. Our main result shows this intermediate step is unnecessary and that the divergence can be either estimated using the empirical cdf or k-nearest-neighbour density estimation, which does not converge to the true measure for finite k. The convergence proof is based on describing the statistics of our estimator using waiting-times distributions, as the exponential or Erlang. We illustrate the proposed estimators and show how they compare to existing methods based on density estimation, and we also outline how our divergence estimators can be used for solving the two-sample problem.
4275 en The Changing Image of the Turk Western European-Ottoman relations had a long and many-facet history. As long as the Ottoman Empire was expanding, its civil and military institutions were idealised as far superior to those of their contemporaries in Western Europe. In the 15th century, Pope Pius II even summoned the Sultan, Mehmet II, to let himself be baptised and become the greatest of Christian princes and a papal protégé. However, due to Ottoman defeats in the late 17th century, the overall prestige of the Ottoman Empire declined. When, in the 19th century, the tables were turned, the once formidable empire became ‘the Sick Man of Europe.’ Its idealised image had faded away to dwindle into obscurity. In the 1856 Treaty of Paris, the Ottoman Empire was nevertheless officially recognised as a permanent part of the European power balance. As such, it was the first non-European political entity to gain that status, which was codified at the Hague Conference in 1899 where the Ottoman Empire appeared as one of the participants, and confirmed by the 1923 Treaty of Lausanne. Regardless of these changes, Turks have been the Western European ‘other’ par excellence ever since the first encounter of the two cultures.
4276 en The Influence of the Ottoman Regional Policy on the Cultural and Economic Life in Slovenia The Ottoman State applied various economic and social policies in every province even in certain districts in the boundaries of those provinces according to their local social and economic structures. It would be proper to name this policy as ‘regional policy.’ This is also the main characteristic of the general administrative policy of the Ottoman State. This paper studies the regional policy of the Ottoman State in Slovenia and its influences on the international trade in ports and trade centres, and cultural and economic life of the local people in the 16th and 17th centuries. This study, focuses particularly on the protection of legal and economic rights of merchants and the local people providing the security and justice by the Ottoman State in certain districts. In addition to the main subject of the paper, some information are given on the commercial activities of merchants from Bursa, Belgrad, Dubrovnik and Venice in important ports and trade centres in Slovenia. On the other hand, Ottoman political and military activities in some districts under the Venetian administration in Dalmatia in relation to the main political objective of the Ottoman State preventing a Venetian domination in this region is also considered. The main sources studied for this paper are the accounts of eye-witnesses and the Ottoman archives.
4277 en Testing with Kernel-based Test Statistics Power Against Sequences of Local Alternatives Testing for homogeneity between two samples offers a natural quantitative framework to investigate how to compare two distributions from empirical data. We shall discuss some recently proposed nonparametric test statistics, whose common feature is to share a kernel-based formulation. In particular, we discuss their respective power against different classes of alternatives.
4278 en From Indifference and Contempt to Love and Hate. The Perception of ‘Franks’ in Ottoman Culture From the very start of their rise to statehood, the Ottomans have been confronted and exposed to a wide variety of western peoples and cultures: Venetians and Genoese in the 15th, French and Austrian in the 16th, British and Dutch in the 17th centuries … Most of these early contacts were characterised by a general feeling of indifference, mixed with a considerable amount of contempt deriving from an imperial sense of superiority and a marked bias against infidels falling outside of Islamic jurisdiction. This general feeling of mistrust did not preclude interaction between Ottomans and Westerners, be it at a diplomatic, institutional, communal, or individual level; yet they remained superficial and sporadic, all the more so if one considers that contacts were almost exclusively one-sided, the Ottomans generally comfortably remaining at the receiving end of such relations. Things began to change in the 18th century as a result of a rapidly growing web of communication between the West and the Empire, and, most of all, due to a gradual change in the rapport de force between the two worlds. Confronted with the first concrete signs of western predominance, the Ottomans—especially members of the ruling elite—felt a greater urge to intensify their contacts with, and understanding of, western peoples and culture. Though not yet westernisation per se, this process gradually paved the way to the extraordinary intensity that relations with the West would acquire during the 19th century. Under these circumstances, it was inevitable that perceptions of the Franks, now redefined as Europeans and/or Westerners—with all the civilisational connotations that came with the terms—would change radically: indifference was no longer possible; contempt had lost its justification. The Ottomans moved toward a love/hate relationship with the West, which can still be felt underlying the complex feelings of Turks toward Europe today.
4279 en Subjective Measure for Distribution Similarity We propose a 'subjective' way of defining similarity between probability distributions.Our measure is parameterized by a collection H of subsets of the domain over which the probability distributions are defined. Intuitively speaking, H is the collection of 'subsets of interest' with respect to theproperties of the distributions that one wishes to analyze. The motivation behind the introduction of that measure comes from real life scenarios in which one cares only about certain distribution changes. In contrast with more traditional notions of distribution similarity (such as the L1 Norm) our measure can be reliably estimated from a pair of finite samples drawn from the two distributions. We have demonstrated the usefulness of the new measure in several areas of applications, including change detection in streaming data, the analysis of sensor network data and domain adaptation learning.
4280 en Sketching and Streaming for Distributions In this talk we look at the problem of sketching distributions in the data-stream model. This is a model that has become increasingly popular over the last ten years as practitioners in a variety of areas have sought to design systems that handle massive amounts of data in a time and space efficient manner. Problems such as estimating the distance between two streams, testing independence or identifying correlations, and determining if a distribution is compressible play an important role. We start by reviewing results on using $p$-stable distributions to compute small-space sketches that can be used to estimate the $L_p$ distance between two distributions. We then present recent results on extending this work to estimate the strength of correlations between two distributions. We finish with an overview of work that seeks to characterize the limits of these techniques with a particular emphasis on what is possible in regards to sketching information divergences such as the Kullback-Leibler, Jensen-Shannon, and Hellinger divergences.
4311 en Discovering Cyclic Causal Models by Independent Components Analysis This talk will start by presenting Shimizu et al's (2006) ICA-based approach (LiNGAM) for discovering acyclic (DAG) linear Structural Equation Models (SEMs) from causally sufficient, continuous-valued observational data. This is remarkable because it determines the direction of every causal arrow when no experimental data is available. Our work generalizes the above. By relaxing the acyclicity constraint, our approach, LiNG-DG, enables the discovery of arbitrary directed graph (DG) linear SEMs. We present various algorithm sketches for causal discovery with LiNG-DG, and show results of simulation for one such algorithm. When the error terms are non-Gaussian, LiNG-DG discovery algorithms output a smaller set of candidate SEMs than Richardson's Cyclic Causal Discovery (CCD) algorithm. We prove that all the models output by LiNG-DG entail the same observational distribution and are equally simple (i.e. same number of edges). This implies that without further assumptions, no algorithm can reliably narrow the set of candidate SEMs output by LiNG-DG using just observational data. However, we show that under the additional assumption of "stability", the set of candidate models output by LiNG-DG can be further narrowed down (under some conditions, to a single model).
4313 en Networked organizations - EU projects results and lessons learned 
4314 en E4: Project presentation and results 
4315 en ToolEast: Open source enterprise planning and order management system for eastern european tool and die making workshops 
4316 en Managing production networks by key performance indicators (KPIs) 
4317 en Kovinastroj Gastronom, Factroy of catering equipment d.d. 
4318 en E4 Project 
4319 en ToolEast: Open source enterprise planning and order management system for eastern european tool and die making workshops 
4320 en TCS - Toolmakers Cluster of Slovenia 
4321 en ToolEast: Open source enterprise planning and order management system for eastern european tool and die making workshops 
4322 en ToolEast: Portal and its Other Functionalities 
4323 en Introduction to SSGL 
4324 en Knowledge technologies for network organisations 
4325 en Creating outside of the box 
4327 en Introduction to the MLSS 2008 
4328 en Introduction to Statistical Machine Learning The first part of his tutorial provides a brief overview of the fundamental methods and applications of statistical machine learning. The other speakers will detail or built upon this introduction. Statistical machine learning is concerned with the development of algorithms and techniques that learn from observed data by constructing stochastic models that can be used for making predictions and decisions. Topics covered include Bayesian inference and maximum likelihood modeling; regression, classification, density estimation, clustering, principal component analysis; parametric, semi-parametric, and non-parametric models; basis functions, neural networks, kernel methods, and graphical models; deterministic and stochastic optimization; overfitting, regularization, and validation.
4329 en Kernel methods and Support Vector Machines The tutorial will introduce the main ideas of statistical learning theory, support vector machines, and kernel feature spaces. This includes a derivation of the support vector optimization problem for classification and regression, the v-trick, various kernels and an overview over applications of kernel methods.
4330 en Monte Carlo Simulation for Statistical Inference, Model Selection and Decision Making **The first part** of his course will consist of two presentations. In the first presentation, he will introduce fundamentals of Monte Carlo simulation for statistical inference, with emphasis on algorithms such as importance sampling, particle filtering and smoothing for dynamic models, Markov chain Monte Carlo, Gibbs and Metropolis-Hastings, blocking and mixtures of MCMC kernels, Monte Carlo EM, sequential Monte Carlo for static models, auxiliary variable methods (Swedsen-Wang, hybrid Monte Carlo and slice sampling), and adaptive MCMC. The algorithms will be illustrated with several examples: image tracking, robotics, image annotation, probabilistic graphical models, and music analysis. \\n**The second presentation** will target model selection and decision making problems. He will describe the reversible-jump MCMC algorithm and illustrate it with application to simple mixture models and nonlinear regression with an unknown number of basis functions. He will show how to apply this algorithm to general Markov decision processes (MDPs). The course will also cover other Monte Carlo simulation methods for partially observed Markov decision processes (POMDPs) using policy gradients, common random number generation, and active exploration with Gaussian processes. An outline to some applications of these methods to robotics and the design of computer game architectures will be given. The presentation will end with the problem of Monte Carlo simulation for Bayesian nonlinear experimental design, with application to financial modeling, robot exploration, drug treatments, dynamic sensor networks, optimal measurement and active vision.
4331 en Latent Variable Models for Document Analysis Wray Buntine will consider various problems in document analysis (named entity recognition, natural language parsing, information retrieval), and look at various probabilistic graphical models and algorithms for addressing the problem. This will not be an extensive coverage of information extraction or natural language processing, but rather a look at some of the theory, methods and practice of particular cases, including the use of software environments.
4332 en Introduction to Reinforcement Learning The tutorial will introduce Reinforcement Learning, that is, learning what actions to take, and when to take them, so as to optimize long-term performance. This may involve sacrificing immediate reward to obtain greater reward in the long-term or just to obtain more information about the environment. The first part of the tutorial will cover the basics, such as Markov decision processes, dynamic programming, temporal-difference learning, Monte Carlo methods, eligibility traces, the role of function approximation. In the second part we cover some recent developments, namely policy gradient and second order methods, such as LSPI and the modified Bellman residual minimization algorithm.
4333 en Foundations of Machine Learning Machine learning is usually taught as a bunch of methods that can solve a bunch of problems (see above). The second part of the tutorial takes a step back and asks about the foundations of machine learning, in particular the (philosophical) problem of inductive inference, (Bayesian) statistics, and artificial intelligence. It concentrates on principled, unified, and exact methods.
4334 en Inference in Graphical Models This short course will cover the basics of inference in graphical models. It will start by explaining the theory of probabilistic graphical models, including concepts of conditional independence and factorisation and how they arise in both Markov random fields and Bayesian Networks. He will then present the fundamental methods for performing exact probabilistic inference in such models, which include algorithms like variable elimination, belief propagation and Junction Trees. He will also briefly discuss some of the current methods for performing approximate inference when exact inference is not feasible. Finally, he will illustrate a range of real problems whose solutions can be formulated as inference in graphical models.
4335 en Contrast Data Mining: Methods and Applications The ability to distinguish, differentiate and contrast between different datasets is a key objective in data mining. Such an ability can assist domain experts to understand their data, and can help in building classification models. His presentation will introduce the principal techniques for contrasting different types of data, covering the main dataset varieties such as relational, sequence, and graph forms of data, clusters, as well as data cubes. It will also focus on some important real world application areas that illustrate how mining contrasts is advantageous.
4336 en Learning in Computer Vision This tutorial he will cover some of the core fundamentals in vision and demonstrate how they can be interpreted in terms of machine learning fundamentals. Unbeknownst to most researchers in the field of machine learning, the fundamentals of object registration and tracking such as optical flow, interest descriptors (e.g., SIFT), segmentation and correlation filters are inherently related to the learning topics of regression, regularization, graphical models, generative models and discriminative models. As a result many aspects of vision can be interpreted as applied forms of learning. From this discussion on fundamentals we shall also explore advanced topics in object registration and tracking such as non-rigid object alignment/ tracking and non-rigid structure from motion and how the application of machine learning is continuing to improve these technologies.
4338 en Group Theory and Machine Learning **Machine Learning Tutorial Lecture** The use of algebraic methods—specifically group theory, representation theory, and even some concepts from algebraic geometry—is an emerging new direction in machine learning. The purpose of this tutorial is to give an entertaining but informative introduction to the background to these developments and sketch some of the many possible applications, including multi-object tracking, learning rankings, and constructing translation and rotation invariant features for image recognition. The tutorial is intended to be palatable by a non-specialist audience with no prior background in abstract algebra.
4339 en Spectral Clustering **Machine Learning Tutorial Lecture** Spectral clustering is a technique for finding group structure in data. It is based on viewing the data points as nodes of a connected graph and clusters are found by partitioning this graph, based on its spectral decomposition, into subgraphs that posses some desirable properties. My plan for this talk is to give a review of the main spectral clustering algorithms, demonstrate their abilities and limitations and offer some insight into when the method can be expected to be successful. No previous knowledge is assumed, and anyone who is interested in clustering (or fun applications of linear algebra) might find this talk interesting.
4374 en Treating drop-foot in hemiplegics: the role of matrix electrode 
4375 en FES treatment of lower extremities of patients with upper / lower motor neuron lesion: A comparison of rehabilitation strategies and stimulation equipment 
4376 en Stochastic Rank Correlation - A novel merit function for dual energy 2D/3D registration in image-modulated radiation therapy 
4378 en Improving Patient Safety Through Clinical Alarms Management 
4379 en System for Tracing of blood transfusions and RFID 
4380 en Optimal Control of Walking with Functional Electrical Stimulation: Inclusion of Physiological Constraints 
4381 en Acceleration driven adaptive filter to remove motion artifact 
4382 en Using Heuristics for the Lung Fields Segmentation in Chest Radiographs 
4383 en Markov chain based edge detection algorithm for evaluation of capillary microscopic images 
4384 en The Influence of Reduced Breathing During Incremental Bicycle Exercise on Some Ventilatory and Gas Exchange Parameters 
4385 en Snoring and CT Imaging 
4386 en The E-HECE e-Learning Experience in BME Education 
4387 en Troubleshooting for DBS patients by a non-invasive method with subsequent ex-amination of the implantable device 
4390 en How New and Evolving Biomedical Engineering Programs Benefit from EVICAB project 
4392 en European Virtual Campus for Biomedical Engineering - EVICAB 
4394 en Internet Examination - A New Tool in e-Learning 
4401 en Brief presentation of editors and respective Journals 
4403 en The effect of afferent training on long-term neuroplastic changes in the human cerebral cortex 
4405 en BIOMEDEA 
4406 en A Clinical Engineering Initiative within the Irish Healthcare System toward a Safer Patient Environment 
4407 en Web-based Visualization Interface for Knee Cartilage 
4408 en Control for Therapeutic Functional Electrical Stimulation 
4409 en The Cavitational Potential of a Single-leaflet Virtual MHV: A Multi-Physics and Multiscale Modelling Approach 
4410 en Learning Managements System as a Basis for Virtual Campus Project 
4411 en Measuring Red Blood Cell Velocity with a Keyhole Tracking Algorithm 
4412 en Biomedical Engineering Education, Virtual Campuses and the Bologna Process 
4413 en A Novel Testing Tool for Balance in Sports and Rehabilitation 
4414 en Evaluation of Tomographic Reconstruction for Small Animals using micro Digital Tomosynthesis (microDTS) 
4415 en What are most common reasons for rejecting a paper? 
4416 en Change of mean frequency of EMG signal during 100 meter maximal free style swimming 
4417 en MIDS-project - a National Approach to Increase Patient Safety through Improved Use of Medical Information Data Systems 
4418 en A Pervasive Computing Approach in Medical Emergency Environments 
4419 en A preliminary setup model and protocol for checking electromagnetic interference between pacemakers and RFID (Radio Frequency IDentification) 
4420 en An Experimental Test of Fuzzy Controller Based on Cycle-to-Cycle Control for FES-induced Gait: Knee Joint Control with Neurologically Intact Subjects 
4421 en Methods for Automatic Honeycombing Detection in HRCT images of the Lung 
4422 en Closing Ceremony 
4423 en Purchasing Process for Medical Devices - a Swedish Model 
4424 en Main ideas of EVICABs logistic structure 
4425 en Deconstructing The Myth Of AIDS In 1984 we were told that HIV was the cause of AIDS. In his provocative documentary film, “Deconstructing the Myth of AIDS,” Gary Null, Ph.D., challenges virtually every statement ever made by the American medical industrial complex on the virus - including those of the Centers for Disease Control and Prevention (CDC), the National Institute for Health (NIH) and the Food and Drug Administration (FDA). While presenting the findings of Nobel Prize-winning scientists and leading virologists, the film exposes the political maneuvering, conspiracies and cover-ups that have provided obstacles to the study of this human catastrophe from the start. While presenting the findings of Nobel Prize-winning scientists and leading virologists, the film exposes the political maneuvering, conspiracies and cover-ups that have provided obstacles to the study of this human catastrophe from the start. For example, there are experts who believe that AIDS is the result of multiple factors, including drug use, stress and nutritional deficiency, but that government agencies made a politically strategic decision to de-emphasize these hypotheses and thus discourage certain researchers and their funding. Meanwhile, AZT, an infamously failed treatment for cancer, and now the primary FDA-approved approach to treating AIDS, is highly toxic and can produce the very symptoms of the illness it is prescribed to treat. “Deconstructing the Myth of AIDS” goes beyond medicine and science to question the very foundation of our reliance on government bureaucracies where it concerns matters of life and death.\\
4428 en Digital Technology and Legal Challenges to Copyright 
4429 en Rights Management and Educational Repositories 
4430 en Creative Commons License 
4431 en Digital Rights Management in the educational sector 
4432 en Project manager TSS 
4433 en Welcome 
4459 en Formal and Informal knowledge representation 
4460 en Introduction to Network Analysis 
4461 en Costs, benefits and incentives (of semantic techologies) 
4466 en FP7 Space Work Programme and the 2008 Call for Proposals 
4467 en Novel Aspects in Plant Microtubule Parmacology: Molecular Effects of Antimitotic Drugs seen with Tubulin Eyes 
4468 en Strengths and Weaknesses of EU-Russia/CIS cooperation in S&T: Two Visions Deriving from ISTCExperience 
4469 en Safety/Risk Assessment of GMOs, using GM Crops as an Example 
4470 en Supporting science in Slovenia - The SRA perspective 
4471 en Overview of Bio NCP of Russia 
4472 en Russian Technology Platforms in Biotechnology 
4473 en STCU Science Research Fields Overview 
4474 en Overview of Science Research in Azerbaijan 
4475 en Slovenia Science Research Fields Overview 
4476 en STCU Science Research Fields Overview 
4477 en Reusable Multi-layer Thermal Protection System: Design, Manufacture and Test of the Mock-up 
4478 en Systems Biology Approaches in Research at Department of Biotechnology and Systems biology, National Institute of Biology 
4479 en Opening Words 
4480 en Opening Words 
4481 en Development of Effective Antiviral Agents of a New Type 
4482 en ISTC Contribution to ESA "EXPERT" Project : Aero-thermodynamics altitude of capsule EXPERT 
4483 en Structural Properties of Antimicrobial Peptides acting on Bacterial 
4484 en Review of Scientific Research in Georgia 
4485 en Testing Aerodynamics of Reentry Space Vehicles (EXPERT) with Simulation of Real-Flight Viscous Effects 
4486 en Opening Words 
4487 en Opportunities for Technical Innovation 
4488 en Measurements of Earth Gravitational Field by Satellite Navigation Systems 
4489 en Opening Words 
4490 en Electroporation based Technologies and Treatments 
4491 en ESTEC experience as ISTC collaborator in Russian contribution to EXPERT programme 
4492 en Strengths and Weaknesses of EU-Russia/CIS cooperation in S&T: Two Visions Deriving from ISTC Experience 
4493 en Space Transportation Assets Valorisation in Europe 
4494 en Novel HIV Inhibitors 
4495 en Opening Words 
4496 en Examples of Successful STCU Projects in Future Launcher Program 
4497 en New Tools to Assure Food Safety? 
4498 en Research in Slovenia 
4499 en Presentation of CNES-DLR-NKAU TacisTwinning Project 
4500 en DLR in Space, Aeronautics, Transport andEnergy, Experiences in cooperation with ISTC 
4501 en Overview of Science Research in Moldova 
4502 en Yuzhnoye Advanced Space Technologies 
4503 en Overview of Activities of the National Academy of Sciences 
4504 en Opening Words 
4505 en Experiences with Performed ISTC Projects and Expectations on Future ISTC Projects 
4506 en Opening Words 
4507 en STCU Science Research Fields Overview 
4508 en Silicon-based Minerals for Ecological Agriculture 
4509 en Creation of high-effective solar power facilities on the base of rigidizable structures - step in the space solar power development 
4510 en Demo 
4511 en Conclusion of the Session 
4512 en Opening Words 
4515 en Review of Russian research 
4516 en Presentation of the Institute of Radio Engineering and Electronics 
4521 en The Future of the Internet, Perspectives emerging from R&D in Europe The Internet world as we know it today has undergone far-reaching changes since its early days while becoming a critical communications infrastructure underpinning our economic performance and social welfare. With more than 1 billion fixed users world-wide today the Internet is poised to become a fully pervasive infrastructure providing anywhere, anytime connectivity. nWith the further deployment of wireless technologies, the number of users of the Internet is expected to jump to some 4 billion in a matter of few years. As the Internet extends its reach and serves an ever growing population of users, sensors and actuators and intelligent devices, new innovative services will be introduced, that contribute in turn to further developing an environment supporting innovation, creativity and economic growth. nnSuch development has been positively acknowledged by the European Union in several documents that are emphasizing the European effort and strategy regarding the development of the "Future Internet". nnThe lecture will address the EU approach and the R&D agenda in the technological and associated policy domains that have a bearing on the network and service infrastructure elements of the Internet of tomorrow.
4523 en Interview In this interview for the Videolectures.Net team Joao Schwarz da Silva speaks about the problems of terrorism, internet security, wifi and gives an overlook about the future of the Internet. He later continues with the Internet's relation to Science, Education and its integration into today's society. He also suggested some ideas about the 3rd world situation and the Internet age. We also asked him some more fun questions; Whether he thinks the Internet will die and how aqurate science fiction writers were regarding the future of cyber space and the Internet. At the end we also discussed future plans.  **"We need to ensure that the network of tomorrow must be safe, reliable and secure to withstands not only accidents but also threats."**
4550 en Perspectives emerging from R&D in Europe 
4551 en Perspectives emerging from R&D in Europe 
4552 en Perspectives Emerging from R&D in Europe 
4553 en The Next Steps to the Future of the Internet 
4554 en Session chair 
4555 en Emerging Issues from the Public Policy Aspect 
4556 en Exploring Europe’s Asset: Critical Concepts for the Future Internet 
4557 en Search Technologies in the Critical Path of the Next Generation Internet 
4558 en Participative Web: what economic value, what challenges? 
4559 en Emerging Issues from the Public Policy Aspect 
4560 en Panel Discussion 
4561 en GENI - Global Environment for Network Innovations 
4562 en Design for the New Generation Network - The Japanese approach 
4563 en Panel Discussion 
4564 en Towards the Future of the Internet 
4565 en The Future of Internet: Architectures 
4566 en Challenges of Future Internet Mobile Perspectives 
4567 en Future of the Internet 
4568 en Open Innovation in Services 
4569 en Beyond Images - THOMSON’s strategy for the Future Internet 
4570 en Towards a Future Internet Assembly in Europe 
4571 en Towards a Future Internet Assembly in Europe 
4572 en Ideas and Questions from the Audience 
4574 en Evolved Internet Future for European Leadership: Some Thoughts on the Future 
4575 en FP7 Project Statements 
4576 en FP7 Project Statements 
4577 en FP7 Project Statements 
4578 en FP7 Project Statements 
4579 en FP7 Project Statements 
4580 en FP7 Project Statements 
4581 en Next Step – European Commission 
4582 en Panel Discussion 
4590 en Probability Distributions on Permutations: Compact Representations and Inference Permutations arise in a variety of real-world problems, such as voting, ranking, and data association. Representing uncertainty over permutations, however, is difficult since there are n! permutations, and unlike many problems, they cannot be represented effectively by graphical models due to the mutual exclusivity constraints typically associated with permutations. I will present a more effective representation which uses the "low-frequency'' terms of a Fourier decomposition to represent distributions over permutations compactly. For such representations to be truly useful, we need to be able to efficiently do inference, and to this end, I will discuss a set of useful inference operations, including marginalization and conditioning, which work completely in the Fourier domain. Performing inference on low frequency Fourier-based approximations, can often lead to functions which do not correspond to any valid distribution. To combat such errors, I will talk about a method for projecting approximations to a relaxed marginal polytope and demonstrate the effectiveness of the approach on a real camera-based multi-person tracking scenario. Finally, I will talk about approaches for splitting a distribution into independent factors and the inverse problem of merging independent factors to form a joint in the Fourier domain. I will discuss how the splitting and joining operations can be applied to our ongoing work on "adaptive inference", where we exploit independence in order to gain speedups for inference. This is joint work with Carlos Guestrin and Leonidas Guibas.
4591 en Introduction 
4592 en Gaussian process modelling of transcription factor networks using Markov Chain Monte-Carlo Ordinary differential equations (ODEs) can provide an useful framework for modelling the dynamics of biological networks. In this study, we focus on a small biological sub-system where a set of target genes are regulated by one transcription factor protein. The concentration of the protein and the gene specific kinetic parameters such as basal rates, decay rates and sensitivities are typically unknown. The objective of modelling is to estimate these quantities by making use of a set of observed gene expression levels. We consider a Bayesian framework for modelling the system of ODEs that is based on Gaussian processes. The Gaussian process is used as the prior for the transcription factor protein and allows us to infer the concentration of the protein in a time continuous manner. We present a Markov chain Monte Carlo algorithm for a full Bayesian statistical inference. The essential property of our MCMC algorithm is that we efficiently infer the protein concentration by applying a novel sampling algorithm for Gaussian process models. We apply our technique to linear and non-linear models.
4593 en Data variability could be your friend Deterministic modeling, in the form of ordinary differential equations (ODE), is the dominant paradigm in systems biology. This stems partially from the type of data that is available. Input data (e.g. gene expression data, protein concentrations) for these models is normally derived from whole cell populations. Consequently, what is modeled is the behaviour of one average cell rather than a multitude of individual cells. Variability within the data originates mainly from the measurement apparatus (technical error) or from difficult-to-control environmental conditions that precede the measurement (biological variability) and can constitute an impediment to clear cut conclusions. For example, kinetic parameters cannot be known with absolute precision and have to be accompanied with confidence intervals that are generally commensurate with the rather high variability attached to biological data. Data variability can also put obstacles in the way of decisive model selection. Measurement techniques are, however, increasingly being applied to individual cells. It is possible to average the individual cell observations, estimate the dispersion of this synthetic measurement, and use these data along with the modeling paradigms outlined above. However, inter-cell variability can be the result of intrinsic system noise. In particular, this is the case if molecular species involved exist in very low concentrations, such as in signaling networks. We argue that because this variability is in part intrinsic, it can be harnessed rather than tolerated, so that it provides novel insights into the mechanisms governing the system under study. This requires a paradigm shift –from deterministic to stochastic modeling- even though ODEs are still central in the latter. To illustrate this, the example system we use is DNA Double Strand Break repair dynamics in irradiated human cells. Recent assaying techniques allow the quantification of DNA Double Strand Break (DSB) at the individual cell level. Repeated measurements in time form a dynamic image of the DSB decay process of cells after they have been exposed to a pulse of ionising irradiation. Crucially, individual cell measurements allow the monitoring of distributional features of the DSB count in a population. Existing deterministic models correctly mimic global features in this system. In particular, they can fit very well different decay regimes that are being observed when one focuses on the average DSB count in the population. We show however that these models, when translated into the stochastic realm, provide a poor data fit when one considers distributional features, such as the variance of the DSB count. Furthermore, using simple stochastic models that are partly amenable to analytical manipulation, we show that enriching the existing models with extra feedback loops produces an outcome more in tune with observations. Three independent data sets are used. Possible biological consequences are briefly discussed.
4594 en Time delay analysis Bayesian Inference and Markov Chain Monte Carlo methods have been ad- vocated for the estimation of model parameters from ODEs by Rogers et al. (Bayesian model-based inference of transcription factor activity, BMC Bioin- formatics, 8(2), 2006). We look at some of the issues involved in extend- ing Bayesian inference methods to systems containing time delays. Verdugo and Rand (Hopf bifurcation in a DDE model of gene expression, Commu- nications in Nonlinear Science and Numerical Simulation, 13:235-242, 2008) apply Lindstedt's method to the nonlinear system of delay di®erential equa- tions proposed as a model by Monk (Oscillatory Expression of Hes1, p53 and NF ¡·B Driven by Transcriptional Time Delays, Current Biology, 13:1409- 1413, 2003) for the Hes1 feedback loop, resulting in closed form approximate expressions for the amplitude and frequency of oscillation. Analysis shows that oscillatory solutions can arise through Hopf bifurcation in the delay pa- rameter. We extend the work of Verdugo and Rand to the more realistic case where the decay parameters of hes1 mRNA and Hes1 protein, key com- ponents of the feedback, are not equal, focusing on oscillatory behaviours. We aim for results that explain how the model parameters a®ect the system dynamics and hence could be used to inform a parameter estimation from expression data. We illustrate our results by applying Bayesian inference to some real biological data. It has been observed that mRNAs for Notch signalling molecules such as the bHLH factor Hes1 oscillate with 2-hour cycles during somite segmentation. Hirata et al. (Oscillatory Expression of the bHLH Factor Hes1 Regulated by a Negative Feedback Loop, Science 298, 840-843, 2002) investigated the molec- ular mechanism behind observed oscillations of mRNAs for Notch signalling molecules. They examined the time course of hes1 mRNA in detail. Hirata et al. measured the half lives of hes1 mRNA and Hes1 protein and identi¯ed the proteases for Hes1 protein degradation. Their experiments show that the degradation of Hes1 protein is required for Hes1 mRNA increase and that de novo production of the protein is required for reduction of hes1 mRNA. These facts together support their theory that Hes1 is an essential compo- nent of a two hour cycle clock and not just an output of a primary clock. The Hirata data comprises scaled hes1 mRNA expression level every 30 min- utes over a 12 hour period. Monk's model was able to explain, via numerical simulations, the oscillation of hes1 mRNA and Hes1 protein in cultured cells observed by Hirata et al. We use a Bayesian approach to the parameter ¯t- ting problem which takes into account the inherent uncertainity in the data and uses our a priori bifurcation analysis to inform the choice of priors.
4595 en Gaussian process modelling of latent chemical species: Applications to inferring transcription factor activity 
4596 en Learning Bayesian networks from postgenomic data with an improved structure MCMC sampling scheme Our paper contributes to recent research on sampling Bayesian network structures from the poste- rior distribution with MCMC. Two principled paradigms have been applied in the past. Structure MCMC, first proposed by Madigan and York, defines a Markov chain in the space of graph struc- tures by applying basic operations to individual edges of the graph, like the creation, deletion or reversal of an edge. Alternatively, order MCMC, proposed by Friedman and Koller, defines a Markov chain in the space of node orders. While the second approach has been found to sub- stantially improve the mixing and convergence of the Markov chain, it does not allow an explicit specification of the prior distribution over graph structures or, to phrase this difierently, it incurs a distortion of the specified prior distribution as a consequence of the marginalization over node or- ders. This distortion can lead to problems for applications in systems biology, where owing to the limited number of experimental conditions the integration of biological prior knowledge into the inference scheme becomes desirable. Diferent approaches and modifications have been developed in the literature to address this shortcoming (e.g. by Ellis, Eaton and Murphy). Unfortunately, these methods incur extra computational costs and are not practically viable for inferring large networks with more than 20 to 30 nodes. There have been suggestions of how to improve the classical structure MCMC approach by using the concept of the inclusion boundary, as proposed by Castelo and Kocka, but these methods only partially address the convergence and mixing problems. In the present paper we propose a novel structure MCMC scheme, which augments the classical structure MCMC method of Madigan and York with a novel edge reversal move. The idea of the new move is to resample the parent sets of the two nodes involved in such a way that the selected edge is reversed subject to the acyclicity constraint. The proposal of the new parent sets is done efectively by adopting ideas from importance sampling; in this way faster convergence is efected. For methodological consistency, and in contrast to inclusion-driven MCMC, we have properly derived the Hastings factor, which is a function of various partition functions that are straightforward to compute. The resulting Markov chain is reversible, satisfies the condition of detailed balance, and is hence guaranteed to theoretically converge to the desired posterior dis- tribution. For our empirical evaluation, we have tested our method on various data sets from the UCI repository, such as Vote, Flare, Boston Housing, and Alarm, which have previously been used by Friedman and Koller to demonstrate that order MCMC outperforms structure MCMC. Our experimental results show that integrating the novel edge reversal move yields a substantial improvement of the resulting MCMC sampler over classical structure MCMC, with convergence and mixing properties that are similar to those of order MCMC. To demonstrate the avoidance of the distortional effect incurred with order MCMC, we have extended our empirical evaluation by analysing ow cytometry protein concentrations from the Raf-Mek-Erk signalling pathway. The experimental results show that the novel MCMC scheme can lead to a slight yet significant performance improvement over order MCMC when explicit prior knowledge is integrated into the learning scheme. This suggests that the avoidance of any systematic distortion of the prior probability distribution on network structures renders our improved structure MCMC sampler preferable to order MCMC, especially for those contemporary systems biology applications where the number of experimental conditions relative to the complexity of the investigated system, and hence the weight of the likelihood, is relatively low, and explicit prior knowledge about network structures from publicly accessible data bases is included.
4597 en Statistical learning of biological networks: a brief overview Identification of biological networks such as signalling pathways, gene regulatory networks, protein-protein interaction networks and metabolic networks is considered as a key challenge in computational biology. Using machine learning framework, this problem can be addressed using different points of view, depending of course on the nature of the biological interactions to be inferred but also on the level of abstraction of the chosen modeling and the amount of prior knowledge available. Since 2000, research in statistical learning of biological networks have given rise to a rich panel of approaches whose interest overcomes the field of computational biology. Network identification has been tackled using large scale data-mining approaches, supervised predictive approaches and reverse-modeling approaches. In this sole last family, it is very instructive to focus on the numerous graphical models that have been proposed so far such as Graphical Gaussian Models, Bayesian networks, Dynamical Bayesian networks and state-space models. I will present a short review of these methods discussing among other issues model complexity, relevance to biology, ability to deal with hidden variables and scalability. I will also plead for the construction of a benchmark repository devoted to examples of relevant test problems even if the true relevant test has always to be made in vivo or in vitro.
4598 en Validating inferred gene networks using ODE models of regulation dynamics Inferring gene regulatory networks from expression data remains one of the most important and challenging problems in bioinformatics and systems biology. Traditionally, validation of inferred networks is performed by comparison with experimentally identifiedtrue networks: if the inferred network (or, more generally, one of its subnets) accurately describes known biological behaviour, then we will have a greater degree of belief in its validity. However, inferred networks typically predict many new interactions that have not previously been observed. Verifying each of these predictions experimentally would be a difficult, time-consuming, expensive, and ultimately tedious, task. We here present a data-driven method for validating inferred gene regulatory networks. In the first stage of our work, we infer a regulatory network from time-course mRNA expression data. Assuming the inferred network to be correct, we propose a parametric ODE model to link the observed mRNA expression levels with the hidden transcription factor activity. In the second stage, we infer the parameters of our ODE system and then assess how well the resulting model describes the dynamic behaviour of the observed expression data. A good description of the data would lend support to the validity of both the inferred network and the ODE model, while a poor fit would suggest a reformulation of the model at some level. The value of this approach is illustrated by applying it to yeast gene expression data.
4599 en Relationship between structure and dynamics of gene regulatory networks Gene regulatory networks in living cells are comprised of recurring network motifs, which perform key functions to control cellular responses. Mathematical models of network motifs are already developed and experimentally validated. We analyze the stability of previously validated mathematical models of network motifs against the fluctuations in their associated biochemical reaction rates and find that all of these elementary functional modules are stable against any perturbation in their rates of reaction. In gene regulatory networks the motifs are connected to each other in a directed acyclic manner. Any large assembly of stable and robust motifs connected with each other in a directed acyclic manner exhibits stable and robust dynamics. All the motifs except those having feedback loops in their structure preserve certain dynamic properties when embedded within large operational networks. Our study also suggests that evolutionary mechanisms selected stable and robust functional modules with which to build regulatory networks in order to ensure stability and reliability on a larger scale instead of finding the simplest canonical representation with which to serve the same purpose.
4600 en Parameter estimation using moment-closure methods This poster will give tackle one of the key problems in the new science of systems biology: inference for the rate parameters underlying complex stochastic kinetic biochemical network models, using partial, discrete, and noisy time-course measurements of the system state. Although inference for exact stochastic models is possible, it is computionally intensive for relatively small networks. We explore Bayesian estimation of stochastic kinetic rate parameters using approximate models, based on moment closure analysis of the underlying stochastic process. By assuming a Gaussian distribution and using moment-closure estimates of the first two-moments, we can greatly increase the speed of parameter inference. The parameter space can be efficiently explored by embedding this approximation into an MCMC procedure.
4601 en Parameter estimation in biochemical reaction networks: An observer-based approach An important bottleneck in the modelling of biological systems is the scarcity of experimental data on kinetic parameters. Recent advances in measurement technologies increase the feasibility of infer ring these parameters from time series data (Anguelova et al., 2007; Voit and Almeida, 2004). We present a methodology for estimating kinetic parameters from time series data, in a way that is particularly tailored to biological models consisting of nonlinear ordinary differential equations, in particular for systems in which the nonlinearities are polynomial, such as in mass action or generalised mass action kinetics, or rational functions of the states, as in Michaelis-Menten or Hill kinetics. The proposed approach consists of three steps. First, the system is transformed into an ex- tended system in observer normal form. The extended system does only depend on structural information, not on the value of the parameters (Xia and Zeitz, 1997; Fey et al., 2008). This allows to design a high-gain observer estimating the states of the extended system (Vargas and Moreno, 2005). As the extended system is not observable everywhere, but only trajectory ob- servable, the observer can only be an approximate observer. However, the observer error can be chosen to be arbitrarily small. In a final step, the parameters are determined based on the observer states, as the unique solutions of simple nonlinear functions of these states. Thus, the proposed parameter scheme estimates is a global estimation algorithm. The parameter estimation methodology is illustrated on a simple model of the circadian rhythm in neurospora (Leloup et al., 1999). The model contains three species, six reactions and exhibits autonomous oscillations corresponding to the day-night cycle The proposed observer-based pa- rameter estimation method is able to recover all parameters, even if the trajectory comes close to singularities of the observability.
4603 en BioBayes: Bayesian inference for Systems Biology There are several levels of uncertainty involved in modelling biochemical systems. For example, the experimental data usually contains considerable amount of observation errors, and there may be alternative hypotheses about the processes involved in studied phenomena. The methods of Bayesian inference provide a consistent framework for modelling and predicting in uncertain conditions. We present a software package for applying Bayesian inferential methodology to problems in Systems Biology. This software package is named BioBayes, and it provides a framework for parameter estimation and evidential hypotheses testing over models of biochemical systems defined using ordinary differential equations. The package is available from http://www.dcs.gla.ac.uk/BioBayes/. The software is based on modular architecture and allows plugging-in third party methods and extensions.
4604 en Factor models for QTL studies The recent availability of large scale data sets profiling single nucleotide polymorphisms (SNPs) and gene expression across different human populations, has directed much attention towards discovering patterns of genetic variation and their association with gene regulation. Two aspects of the nature of expression profiles make the identification and interpretation of such associations difficult. Firstly, we expect that a variety of environmental, developmental and other factors influence gene expression which can obscure such associations. Secondly, the regulatory network linking genes makes it difficult to pinpoint causal relationships between SNPS and regulatory elements. We address the first issue by proposing FA-eQTL, a factor-model that explicitly takes non-genetic variability into account, and thereby can significantly improve the power of an expression Quantitative Trait Loci (eQTL) study. We discuss a variational Bayesian implementation of this model, and point out rapid approximations that are applicable in certain situations. Applying our model to simulated and real world data we can demonstrate a significant improvement in performance. On data from the HapMap project, we find more than three times as many significant associations than a standard eQTL method. To address co-expression of genes, we further extended FA-eQTL by jointly reducing the dimensionality of the ex-pression profile and modelling non-genetic factors. We discuss results applying this enhanced QTL-model to biological data, including human as well as datasets from yeast.
4605 en Probabilistic multi-class multi-kernel learning: On protein fold recognition and remote homology detection The problems of protein fold recognition and remote homology detection have recently attracted a great deal of interest as they represent challenging multi-feature multi-class problems for which modern pattern recognition methods achieve only modest levels of performance. As with many pattern recognition problems, there are multiple feature spaces or groups of attributes available, such as global characteristics like the amino-acid composition (C), predicted secondary structure (S), hydrophobicity (H), van der Waals volume (V), polarity (P), polarizability (Z), as well as attributes derived from local sequence alignment such as the Smith-Waterman scores. This raises the need for a classification method that is able to assess the contribution of these potentially heterogeneous object descriptors while utilizing such information to improve predictive performance. To that end, we offer a single multi-class kernel machine that informatively combines the available feature groups and, as is demonstrated in this paper, is able to provide the state-of-the-art in performance accuracy on the fold recognition problem. Furthermore, the proposed approach provides some insight by assessing the significance of recently introduced protein features and string kernels. The proposed method is well-founded within a Bayesian hierarchical framework and a variational Bayes approximation is derived which allows for efficient CPU processing times. Results: The best performance which we report on the SCOP PDB-40D benchmark data-set is a 70% accuracy by combining all the available feature groups from global protein characteristics but also including sequence-alignment features. We offer an 8% improvement on the best reported performance that combines binary SVM classifiers while at the same time reducing computational costs and assessing the predictive power of the various available features. Furthermore, we examine the performance of our methodology on the SCOP 1.53 benchmark data-set that simulates remote homology detection and examine the combination of various state-of-the-art string kernels that have recently been proposed.
4606 en Predicting anti-cancer molecule activity using machine learning algorithms In this paper we study the anti-cancer activity of - 4.000 unique compounds against a set of 60 cell lines (e.g. Leukemia, Prostate, Breast). Small molecules play an important role in biology as they can be used as building blocks for more complex molecules and also interact with proteins inhibiting or promoting their action. In this case the consequence of adding such a compound to a cell can be far reaching as the protein may be involved in a very complex chain reaction. As such it is possible to design small molecules which can be useful drugs. Here we concentrate only in predicting a property of a given molecule: whether it will show anti-cancer activity (measured as causing at least 50% cell growing inhibition) against a given cancerous cell line. This computational prediction is important as there are a growing number of small molecules in databases worldwide and the capacity for proper lab testing is limited. For instance, the In Vitro Cell Line Screening Project at the National Cancer Institute (NCI) can currently evaluate (only) up to 3000 compounds per year for potential anti-cancer activity. From a machine learning perspective, biological problems are a good application because datasets are abundant, the data is real, the type of algorithms most suitable for a particular problem may vary substantial and it is not unusual for a problem to highlight research needs in machine learning. Finally, helping to solve biological problems may have a big impact in the wider scientific community. The molecule dataset we used is publicly available at the NCI site. We applied a range of data mining classification algorithms to this problem: Decision Trees, Inductive Logic Programming and Support Vector Machines (SVMs). As molecular features used for the learning we have used molecular weight, octanol water partition coefficient (logp) and fragment counts. A fragment is a set of connected atoms where each atom in a fragment is simply identified by its type. (e.g. carbon). If we look at the molecule as a graph, the fragment list consists of all connected components with diameter two. The experiments demonstrate that our results using support vector machines (with RBF kernel) are identical to previous published state of the art work yielding an average 73% predictive accuracy (having 54% as the baseline). We noticed however, to our surprise, that if instead of using fragment counts we use only atom counts the results are nearly identical (about 1% less accuracy, although the diference is statistical significant). An important point that must be made is that, although numerical black box algorithms like SVMs tend to be slightly more accurate than logic models (Decision Trees and ILPs in this dataset have an accuracy 3% to 4% below SVMs), it is arguable the relevance of this predictive accuracy for important practical applications like drug design. In a drug design setting what is useful is to have a set of rules that describe what a "good" compound should look like. That goal is much easily achieved with a human readable logic model like the ones we also describe in the paper.
4611 en TRA 2008 Keynote Address 
4612 en TRA 2008 Keynote Address 
4613 en TRA 2008 Keynote Address 
4614 en TRA 2008 Keynote Address 
4617 en EURIDICE - European Inter-Disciplinary Research on Intelligent Cargo for Efficient, Safe and Environment-friendly Logistics 
4644 en Introduction to the MIT 8.01 course //The goal is to introduce the students for the first time to physics that's to say calculus based physics. Many students have had that in high school and many have not, and 8.01 the first course of physics covers not only Newtonian mechanics that is at the heart of the course.//
4645 en Lecture 1: Powers of Ten - Units - Dimensions - Measurements - Uncertainties - Dimensional Analysis - Scaling Arguments **1. Fundamental Units:**nnThe fundamental units are length, time and mass.nn**2. Powers of Ten:**nn"The Powers of Ten" (© Charles &amp; Ray Eames and Pyramid Media) movie, covering 40 orders of magnitude, has been removed from the video for reasons of copyright.nn**3. Dimensions:**nnDimensions are denoted with brackets; some examples are given.nn**4. The Art of Making Measurements:**nnA measurement is meaningless without knowledge of its uncertainty. The lengths of an aluminum rod and the length of a student are both measured standing straight up and lying down horizontally to test whether the student's length is larger when he is lying down than when he is standing straight up. Within the uncertainty of the measurements, the difference between standing and lying is substantial for the student (NOT for the aluminum rod).nn**5. Was Galileo Galilei's Reasoning Correct?**nnWhy are mammals as large as they are, and not much larger? The argument suggests that if they become too heavy, the bones will shatter. Galileo Galilei suggested that material properties of our bones impose a natural limit on the size of things. Professor Lewin brings this to a test by presenting Galilei's scaling arguments, and he compares them with actual measurements.nn**6. Dimensional Analysis:**nnThe dimensions of both sides of the equation must be the same; this is non-negotiable in physics. Using this idea, Professor Lewin reasons that the time for an object to fall from a certain height is independent of its mass and proportional to the square root of the height from which it is dropped. He confirms this conclusion by dropping an apple from 3.000 m and 1.500 m with an uncertainty in each of 3 mm. He then shows why his "prediction" was a cheat.
4646 en Lecture 2: 1D Kinematics - Speed - Velocity - Acceleration **1. Introduction to 1-Dimensional Motion:**nnProfessor Lewin describes 1D motion of a particle. He talks about average velocity, the importance of "+" and "-" signs, and our free choice of origin.nn**2. Average Speed vs. Average Velocity:**nnThe two are VERY different. The average velocity can be ZERO, while the average speed is LARGE.nn**3. Instantaneous Velocity:**nnConsidering the incremental change in position x with time t, we arrive at v=dx/dt. The instantaneous velocity is the derivative of the position with respect to time. Professor Lewin reviews when the velocity is zero, positive and negative; he distinguishes speed from velocity.nn**4. Measuring the Average Speed of a Bullet:**nnProfessor Lewin shoots a bullet through two wires. The average speed can be calculated from the distance between the wires and the elapsed time. All uncertainties in the measurements are discussed; they have to be taken into account in the final answer.nn**5. Introducing Average Acceleration:**nnThe average acceleration between time t1 and t2 is the vectorial change in velocity divided by (t2-t1).nn**6. Instantaneous Acceleration:**nnThe acceleration, dv/dt, is the derivative of the velocity with time. It is the second derivative of the position x with time. Professor Lewin shows how to find the sign of the acceleration from the slope in an x-t plot.nn**7. Quadratic Equation of Position in Time:**nnWhen the position is proportional to the square of the time, the velocity depends linearly on time, and the acceleration is constant.nn**8. 1D Motion with Constant Acceleration:**nnProfessor Lewin writes down a general quadratic equation for the position as a function of time, and he relates the constants in this equation to the initial conditions at time t=0. The gravitational acceleration is a constant (9.80 m/s^2 in Boston), and it is independent of the mass and shape of a free-falling object, if air drag can be ignored (see Lecture #12). You can use this result to measure g using the free fall time measurements from the falling apples in lecture 1. 9. Strobing an Object in Free Fall: Professor Lewin drops an apple from 3.20 m and takes a polaroid picture of the falling apple which is illuminated by a strobe light. First two light flashes per second, and then ten flashes per second.
4647 en Lecture 3: Vectors - Dot Products - Cross Products - 3D Kinematics **1. Vectors - Direction Distinguishes Vectors from Scalars**nn**2. Decomposition of a Vector:**nnA vector can be projected onto three coordinate axes x,y,z, along which lie unit vectors (denoted with roofs). Professor Lewin works an example.nn**3. Scalar Product:**nnThe "dot" product of two vectors is a scalar. A scalar can be positive, negative or zero and we'll use it later in the course to calculate work and energy. Professor Lewin calculates "A dot B" in a couple of examples.nn**4. Vector Product:**nnThe cross product (also called vector product) of two vectors results in a vector. Professor Lewin presents two methods for calculating it. A cross product of the vectors A and B is always perpendicular to both A and B. The direction is easily found using the right-hand corkscrew rule. We'll use cross products to calculate torques and angular momentum later in the course. Always use Right Handed coordinate systems, x-hat cross y-hat gives z-hat. If you don't, you'll get into trouble for which you will have to pay dearly.nn**5. Decomposition of 3D Vectors r, v and a:**nnProfessor Lewin writes the equations for position (r), velocity (v) and acceleration (a) showing their projection onto the x,y,z axes, and he introduces a shorthand notation for time derivatives. 3D motion can be reduced to three 1D motions which can greatly simplify matters.nn**6. Projectile Motion in the Vertical Plane:**nnProfessor Lewin throws an object up, and decomposes its initial velocity into a horizontal and a vertical direction. If air drag can be ignored, the horizontal velocity remains constant. Gravitational acceleration is only in the vertical direction and is not affected by the horizontal motion. This acceleration is constant in the lecture hall if air drag can be ignored (see Lecture 12).
4648 en Lecture 4: 3D Kinematics - Free Falling Reference Frames **1. Shape of the Projectile Trajectory:**nnProfessor Lewin reviews the equations for projectile motion, showing that the trajectory is a parabola. He derives formulas for the highest point (maximum height), the time to reach the highest point, the time of flight (until impact), and the horizontal distance traveled. For given initial speed (speed is a scalar, velocity is a vector), an object thrown at 45 degrees from the vertical will go the farthest.nn**2. How to Measure the Initial Speed?**nnAn object is shot upwards from a gun-like device. By measuring the height that it reaches, we can find the initial speed. Uncertainties in the results are discussed and are taken into account in the demonstrations that follow.nn**3. Shoot a Ball for Maximum Horizontal Distance:**nnThe ball is shot at an angle of 45 degrees from the vertical (the uncertainty in the angle is estimated to be about 1 degree). Professor Lewin predicts where the ball will hit the long desk in the lecture hall. He takes into account the uncertainty in the initial speed of the ball and the 1 degree uncertainty in the angle. He marks the locations between which the ball should hit. He then shoots the ball, and indeed it lands as predicted.nn**4. Shoot a Ball at 30 and 60 Degrees:**nnFor given initial speed, the horizontal range is the same for angles of 30 and 60 degrees from the vertical (but the ball travels higher for 60 degrees -- which of these trajectories takes the longest?). Professor Lewin sets the angle at 30 degrees, and predicts where the ball will hit. He takes the uncertainties into account. The ball lands as predicted.nn**5. Shoot a Ball at a Monkey Doll:**nnSomeone shoots a ball and aims straight at a monkey who is hanging in a tree. Gravitational acceleration curves the ball's trajectory substantially, and there is no danger that the monkey will get hit. However, tragically the monkey sees the light flash of the gun and he lets go. He falls to the ground, and ... the ball hits the monkey independent of the initial speed of the ball (provided the speed is high enough to reach the tree).nn**6. Reference Frame of the Falling Monkey:**nnBoth the monkey and the ball are falling with the same gravitational acceleration. From the monkey's point of view (its reference frame) the ball is coming straight at it (no curved trajectory).nn**7. Professor Lewin (Dressed in Safari Outfit) Fires the Gun**
4649 en Lecture 5: Circular Motion - Centrifuges Moving - Reference Frames - Perceived Gravity **1. Uniform Circular Motion and Centripetal Acceleration:**nnA particle travels in a circle of radius r with constant speed. The period of one rotation is T (sec); the frequency is f (the number of rotations/sec or Hz), omega is the angular velocity (radians/sec), omega=2pi/T, the speed v= omega*r. The velocity vector is constantly changing direction because of the centripetal acceleration (=v^2/r= omega^2*r). The centripetal acceleration for the rotor of a vacuum cleaner is estimated to be about 400 m/sec^2 which is 40 times larger than g. Note that the centripetal acceleration depends linearly on the radius.nn**2. There Must be a Pull or a Push:**nnSitting on a chair bolted to a fast-rotating turntable, you'll feel a push in your back. Alternatively if you stand on the turntable and you hold onto a post mounted on the table, you will experience a pull in your arms. This pull or push is responsible for the change in velocity (centripetal acceleration).nn**3. What Happens if there is no Pull or Push?**nnYour velocity will not change. Thus you move along a straight line with constant speed.nn**4. Motion of Planets around the Sun:**nnThe gravitational pull provides the centripetal acceleration which is inversely proportional to the distance squared.nn**5. Swirling Objects Around:**nnThe idea behind a centrifuge and salad spinners.nn**6. Creating Artificial Gravity via Rotation:**nnProfessor Lewin gives several examples of "perceived" gravity. A space station could rotate such that an astronaut perceives an Earth-like acceleration of 10 m/s^2. However, the direction will be changing all the time!nn**7. A Centrifuge in Action:**nnA glass tube filled with a liquid solution with fine particles is spun around in a standard laboratory centrifuge. The acceleration is about 20,000 m/s^2. Thus the particles perceive a "gravitational" force about 2000 times larger than normal, and they "fall" in the direction of this huge "gravitational field". Professor Lewin demonstrates this, mixing NaCl+AgNO_3 =&gt; NaNO_3+AgCl; this produces a milky solution. After spinning for a few minutes, the AgCl has precipitated at the end of the glass tube, and the remaining solution has become clear.nn**8. Swinging a Bucket of Water on a String:**nnIn order to swirl a bucket around in a vertical plane, a centripetal acceleration is required. If you spin fast enough the water will stay in the bucket as the bucket is upside down. To be on the safe side ... bring an umbrella to class!
4650 en Lecture 6: Newton's Laws **1. Newton's First Law and Inertial Reference Frames:**nnGalilei's law of inertia and Newton's First Law are valid only in inertial reference frames, those are reference frames which are not accelerating.nn**2. Newton's Second Law:**nnThe pull, e.g. from an extended spring, acting on an object of a certain mass, attached to the spring, can be quantitatively expressed by the vector relationship F=ma. F is the force, "m" the mass, and "a" the acceleration. This law, like Newton's First, is ONLY valid in an inertial reference frame.nn**3. Superposition of Forces and Net Force:**nnThe gravitational force acting on a mass m on Earth is mg. A mass m held in your hand doesn't accelerate, therefore the net force on it must be zero, and thus your hand is exerting an upward force equal to mg.nn**4. Newton's Third Law, Action = —Reaction:**nnThe contact force between two objects can be described as a pair of forces, equal in magnitude but opposite in direction. Every-day examples are described such as snaking of a garden hose, releasing air from a balloon (jet action) and the recoil of a gun. Professor Lewin shows a demonstration with Hero's engine.nn**5. Consequences of Newton's Third Law:**nnWhile an apple is falling towards the Earth, the Earth is falling (moving) towards the apple!nn**6. Decomposing Forces in x and y Directions:**nnHang an object of mass "m" from two strings each with negligible mass. The net force on the object is zero. The sum of the tension vectors in the strings must equal mg pointing vertically up.nn**7. Bizarre Demo with a 2 kg Block and 2 Strings:**nnTwo identical strings, one suspending a mass, the other string is suspending from the mass. Which one breaks when you pull on the lower string, the upper string or the lower string? Professor Lewin pulls a fast one!
4651 en Lecture 7: Weight - Perceived Gravity - Weightlessness Free Fall - Zero Gravity in Orbit (misnomer) **1. What is Weight?**nnWeight is the force exerted on you by a bathroom scale. Your weight increases when you are in an elevator which is accelerated upward; you weigh less than your normal weight when the elevator is accelerating downward, and your weight is zero when the elevator is in free fall. When you hang from a rope, your weight is indicated by the tension in the rope.nn**2. Tension in Massless String:**nnConsider a string of negligible mass suspending two objects with different mass on either side of a frictionless pin. The tension in this string is everywhere the same because the string is "massless" and the pin frictionless. The masses of the two objects are NOT equal but the weight of the two objects IS THE SAME!nn**3. Weight, when Swinging around on a String:**nnAn object is swirled around on a string in a vertical plane. The tension in the string is evaluated when the object is at the top and when it is at the bottom of its circular trajectory. The tension at the bottom is always higher than the normal weight of the object, but the tension can be zero when the object is at the top in which case the object is weightless.nn**4. Objects in Free Fall are Weightless:**nnExploring the weight of a tennis ball being tossed in the air, and of a bottle of water in Professor Lewin's hands when he jumps off a table. The bottle and Lewin are in free fall, thus both are weightless.nn**5. Weight Measurements of a Free Falling Object:**nnBathroom scales have too slow a response time to indicate your zero weight when you weigh yourself while jumping off a table. Professor Dave Truemper has built a scale with a response time of 10 ms (it uses pressure gauges instead of springs). Professor Lewin places a 10 lb object on the scale, and we see that the scale indicates this weight. He then tapes the object firmly to the scale and drops the scale with the object from about 2 meters. The weight of the object is zero during free fall!nn**6. Professor Young's "Zero Gravity" Experiments:**nnNASA has sponsored "zero gravity" experiments to study motion sickness. These are zero weight environments (not zero gravity) created in air planes which free fall for 30 seconds. The air plane's (a KC135) trajectory is discussed in detail, and one of Professor Young's video clips is shown.
4652 en Lecture 8: Friction **1. Normal and Frictional Forces:**nnStarting with a block at rest on a horizontal surface, Professor Lewin describes the normal force, the maximum frictional force that must be overcome to budge the block, and the coefficient of static friction. Once the block is budged and begins to slide, you encounter a smaller coefficient of kinetic friction.nn**2. Measurements of the Coefficient of Static Friction:**nnConsider a block at rest on an inclined plane, and increase the tilt until the block just starts to slide. By measuring this tilt angle you can calculate the coefficient of static friction. The friction coefficient only depends on the materials in contact; it is independent of the mass of a sliding object and independent of its surface area (this is rather non-intuitive).nn**3. Another Way to Measure Friction:**nnThe system consists of a block on an inclined plane. The block is counterbalanced by a second mass connected to the block with a massless string and a near massless, frictionless, pulley. The direction of the frictional force depends on whether the block on the incline wants to move uphill or downhill or stay at rest. By increasing the second mass until the block on the incline budges uphill, you can measure the coefficient of static friction.nn**4. Ways to Reduce Friction - Fleas are Good for Something!**nnScenarios for reducing friction are presented including hydroplanes and air tracks. Even a flea can move a very heavy book if the friction is near zero (as demonstrated).
4653 en Lecture 9: Exam Review 1 **1. Scaling Arguments:**nnThe cross-sectional area of femurs should scale with mass if mother nature were protecting the femurs of large animals from crushing. But that is not the case. The diameter of a femur scales with its length. That protects the femurs against buckling (sideways deformation).nn**2. Dot Products:**nnTwo methods are reviewed for obtaining the scalar product, by decomposition and by projection.nn**3. Cross Products:**nnThe magnitude of the cross product equals the product of the magnitude of the two vectors and the sine of the angle between them. The direction of the vector product is determined using the right-hand corkscrew rule.nn**4. 1D Kinematics:**nnA graphic example of the position x(t) is given, and the velocity and acceleration are derived at various points in time. The average velocity and average speed are calculated. A plot of velocity vs. time is constructed.nn**5. Trajectories:**nnTrajectories lie in a plane; they therefore reduce to 2 dimensional problems. A detailed example is worked using the trajectory of the "zero gravity" experiments in the KC135 (see Lecture 7).nn**6. Uniform Circular Motion:**nnThe parameters for uniform (constant speed) circular motion are reviewed, including the equations for angular velocity and centripetal acceleration. The numerical example worked out is NASA's centrifuge to test astronauts; the centripetal acceleration is about 10g!nn**7. Brain Teaser with a Yardstick:**nnProfessor Lewin slides his fingers underneath a yardstick, towards the center. Something strange happens, the fingers seem to take turns moving, they alternate sliding and stopping. Can you explain this?
4654 en Lecture 10: Hooke's Law - Springs - Simple Harmonic Motion - Pendulum - Small Angle Approximation **1. Restoring Force of a Spring:**nnThe restoring force of a spring, described by Hooke's Law (F=-kx) is introduced. Professor Lewin discusses how to measure the spring constant, k, and he gives a brief demonstration.nn**2. Dynamic Equations of a Displaced Spring:**nnA differential equation is derived for a spring in the absence of damping forces. Using springs, spray paint and a moving target, a sketch of x(t) is created, suggesting a sine or cosine dependence of x on time. The angular frequency (and therefore the period) is shown to depend only on k and m (so you can measure k dynamically). The amplitude and phase depend on initial conditions (the displacement and velocity at t=0). An example is worked out to demonstrate this.nn**3. Measuring the Period of a Spring System:**nnThe period of oscillation is measured for a mass on a spring system on an air track (to minimize friction). A measurement is made of 10 periods to reduce the relative error. Professor Lewin demonstrates that the period is independent of the amplitude. The mass is doubled, the new period is predicted and then empirically confirmed.nn**4. Dynamic Equations of a Pendulum:**nnA pair of differential equations is derived for a mass, m, suspended on a near massless string of length L. The small angle approximation is quantitatively justified and applied to arrive at a simple differential equation analogous to that for a spring. The period of oscillation is shown to be proportional to the square root of L/g; it is independent of m.nn**5. Comparing the Spring and Pendulum Periods:**nnIntuitive insights are presented as to why the period of an oscillating spring depends on the mass (attached to the spring) and the spring constant. Yet the period of a pendulum is independent of the mass hanging from the string. These insights are reinforced with several experiments with a very long pendulum. The uncertainties in the measurements are taken into account. To demonstrate that the period is independent of the mass of the bob, Professor Lewin places himself at the end of the 5 meter long cable and measures the period.
4655 en Lecture 11: Work - Kinetic Energy - Potential Energy - Conservative Forces - Conservation of Mechanical Energy - Newton's Law of Universal Gravitation **1. 1D Work and Kinetic Energy:**nnThe equation for work, and its units are introduced. The work-energy theorem is derived showing that the change in kinetic energy equals the work done on a particle by the sum of all forces (thus the net force). Gravity does negative work on an object thrown upwards until it reaches its maximum height of its trajectory.nn**2. Work Calculated in 3-Dimensions:**nnWork in 3D is shown to decompose into the sum of each 1D component.nn**3. Gravity is a Conservative Force:**nnWork done by gravity while a particle moves upwards a vertical distance h is -mgh, regardless of the path taken. When the work done by a force is independent of the path, that force is called a conservative force.nn**4. When Gravity is the only Force:**nnThe equation for gravitational potential energy is introduced by rearranging the work-energy theorem. Potential energy and kinetic energy can be converted back and forth but their sum, the mechanical energy, is conserved if only conservative forces are involved. Friction is not a conservative force. When friction is at stake, the work-energy theorem can be applied, but mechanical energy is not conserved.nn**5. What Matters is the Difference in Potential Energy:**nnGravitational potential energy can be positive, negative or zero depending on your choice of origin. It really doesn't matter where you choose the origin.nn**6. A Roller Coaster, Upside-down:**nnThe conservation of mechanical energy is used to analyze the velocity of an object on a roller coaster. The centripetal acceleration, when the roller coaster is upside down, must be greater than g; the mechanical energy must therefore exceed a threshold value.nn**7. Newton's Law of Universal Gravitation:**nnNewton's law of universal gravitation is introduced. The gravitational force falls off as one over the distance squared. If large distances are involved, the gravitational potential due to an object of mass M is taken to be zero at infinity. The gravitational potential is proportional to M and inversely proportional to the distance from M. This formalism is consistent with the small distance approximation used near the Earth's surface.nn**8. Conservation of Mechanical Energy and a Wrecking Ball:**nnA wrecking ball is converting gravitational potential energy into kinetic energy and back and forth. If released with zero speed, the wrecking ball should NOT swing higher than its height when it was released. Professor Lewin puts his life on the line by demonstrating this.
4656 en Lecture 12: Non-Conservative Forces - Resistive Forces - Air Drag - Terminal Velocity **1. Resistive and Drag Forces:**nnResistive forces have a viscous term that is linear in velocity; it is temperature sensitive and reflects the stickiness of the medium. In addition they have a pressure term that is proportional to the speed squared and the fluid's density. Resistive forces are always in the direction opposite the velocity. The resistive force grows as the speed increases. Therefore, a falling object in air (or in a liquid) will reach a terminal velocity.nn**2. Two Regimes and the Critical Velocity:**nnThe drag, or resistive force has two terms; the viscous and pressure terms. They are equal in magnitude at a "critical" speed. At speeds much smaller than this, the terminal speed for spherical objects (all with the same density) increases as the radius squared of the objects. At speeds much larger than the critical speed, the pressure term dominates and the terminal velocity increases as the square root of the radius.nn**3. Measurements with Steel Balls in Syrup:**nnSmall ball bearings are dropped in Karo Corn Syrup. Professor Lewin explains why the terminal velocity of the ball bearings will vary with the radius squared. He conducts measurements to validate this.nn**4. Reaching Terminal Velocity in the Blink of an Eye:**nnWhen the ball bearings are dropped in the syrup, their speeds at first increase. It is shown that they reach their terminal velocity very fast.nn**5. Air Drag and the Pressure Term:**nnThe air drag on almost all objects that fall in air from a considerable height (raindrops or sky divers) is dominated by the pressure term. Thus the terminal speed increases with the square root of the radius of spheres with given density. If you want to calculate the time it takes to reach this terminal speed, you have to include both the v and v-squared terms. Lewin's graduate student, Dave Pooley, solved the equation numerically, and a measurement is made with a balloon (filled with air) that is dropped from a height of about 3 meter.nn**6. Numerical Calculations of Air Drag Examples:**nnA pebble, dropped from a height of 475 meters (the Empire State Building), reaches a terminal speed of about 75 miles per hour in 5-6 seconds. Professor Lewin also discusses the contribution of air drag to the quantitative experiments done earlier in the course with falling apples.nn**7. Resistive Forces and Trajectories:**nnAir drag will result in an asymmetric trajectory for an object thrown up in the air. The resistive force is about the same for a tennis ball as for a styrofoam ball of the same radius, but the resistive force has a much more dramatic effect on the lighter ball's trajectory.
4657 en Lecture 13: Potential Energy - Energy Considerations to Derive Simple Harmonic Motion **1. Gravitational Potential Energy:**nnA review is given of the spatial dependence of the gravitational potential energy both close to the Earth's surface and at large distances from Earth. The gravitational force pulls objects in the direction of decreasing potential energy.nn**2. Calculating U(x) from F(x) and Vice Versa:**nnThe potential energy, U(x), of a spring system is derived and sketched as a function of displacement x. The force can be derived if the function U(x) is known: F(x)=-dU(x)/dx.nn**3. Equilibrium Points:**nnThe minima and maxima of potential energy are positions where the net force is zero. At the stable equilibrium points the 2nd derivative of U(x) is positive, at the unstable equilibrium points the 2nd derivative is negative.nn**4. Parabolic Potential Energy Well ==&gt; SHO:**nnUsing the parabolic shape of the potential energy for a spring, and the conservation of mechanical energy, it is shown that the mass on the spring oscillates as a simple harmonic oscillator (SHO).nn**5. Circular Potential Energy Well ==&gt; SHO:**nnUsing a circular potential energy well and the conservation of mechanical energy, it is shown that for SMALL ANGLES, the oscillations are simple harmonic. A circular track with very large radius is used to demonstrate this.nn**6. Sliding on a Circular Track ==&gt; SHO:**nnThe known radius of a circular air track is used to predict the period of oscillation of a sliding object (small angles!), and a measurement is made to confirm this. The process is repeated for a ball bearing rolling in another circular track. The period of oscillation can now not be predicted in a similar way as was possible in the case of the air track. Why? ==&gt; No, it has nothing to do with friction!
4658 en Lecture 14: Escape Velocities - Bound and Unbound Orbits - Circular Orbits - Various Forms of Energy - Power **1. Escape Velocity:**nnThe escape velocity is the minimum speed required to escape the gravitational pull. You can calculate it using the conservation of mechanical energy.nn**2. Circular Orbits:**nnThe gravitational force provides the centripetal acceleration required for orbiting satellites. If you know the radius of a circular orbit of a satellite around the Earth, you can calculate the orbital speed and the orbital period. Examples are worked for both the shuttle and the moon around the Earth, and for the Earth around the sun. The orbital period is independent of the mass of the orbiting object.nn**3. Power:**nnPower is the rate at which a force does work on an object. Instantaneous power is the time derivative of work. Power is the dot product of the force acting on an object and the velocity of that object. Power is a scalar; it can be positive, negative, or zero. A force diagram for a bicycle rider is discussed. The required power to be delivered by the rider scales with the third power of the speed.nn**4. Heat and Various Forms of Energy:**nnHeat energy, calories and specific heat are discussed. Joule's experimental apparatus for converting mechanical energy into heat energy is described. Several sources of heat are mentioned (including body heat). The energy to heat up bath water is calculated. Conversion of energy from one form to another is discussed, and a student is asked to convert mechanical power to electrical power. How long will he last?nn**5. Energy Conversion:**nnBatteries convert chemical energy to electricity. A make-shift battery is constructed of sulfuric acid with copper and zinc electrodes to briefly light a lamp.nn**6. Global Energy Consumption and Sources:**nnThe US has about 1/30 of the world's population but accounts for 1/5 of the global energy consumption. Harvesting solar radiation is numerically considered. Nuclear power is discussed and the public sentiment. We have an Energy Crisis! Fossil fuels could be depleted within 100 years. Nuclear fusion is being explored. The presence of radioactive uranium in Fiestaware glaze is demonstrated. (Professor Lewin has a large Fiestaware collection).
4659 en Lecture 15: Momentum - Conservation of Momentum - Center of Mass **1. Conservation of Momentum:**nnThe momentum vector, internal forces, external forces and the conservation of momentum are discussed.nn**2. Kinetic Energy and Momentum for a 1D Collision:**nnConservation of momentum is used to calculate the final velocity of a pair of masses that collide and stick together (this is called a completely inelastic collision). It is shown that kinetic energy is then always lost, but momentum is conserved.nn**3. Energy and Momentum for a 2D Car Collision:**nnThe impact time is so short that the work done by the frictional force from the road exerted on the cars during the impact can be ignored. Internal frictional forces between the cars will merge the wrecks into one mass. A momentum diagram is sketched. This is a completely inelastic collision. If we compare the moment just before and just after the collision, kinetic energy is lost, but momentum is conserved.nn**4. Scenarios that Increase the Kinetic Energy:**nnWhen there is a bomb explosion, the momentum and kinetic energy are zero before the explosion. Thus the total momentum must remain zero, but the kinetic energy clearly increases after the explosion. Professor Lewin does some air track experiments where the released energy is from a compressed spring; kinetic energy increases but momentum is conserved.nn**5. Center of Mass of a System:**nnThe definition of the center of mass is described. The center of mass behaves as if all the matter were together at that point. The center of mass of a system of objects moves with constant speed along a straight line in the absence of external forces on the system (internal forces between the objects are allowed - e.g. the objects can collide). An example is worked calculating the position vector for the center of mass for a system of three masses. An air track demonstration shows the center of mass of an oscillating system (2 objects) is moving at constant velocity. The center of mass of a tennis racket follows a parabolic trajectory while it tumbles through the air.
4660 en Lecture 16: Collisions - Elastic and Inelastic - Center of Mass Frame of Reference **1. 1D Elastic Collisions:**nnA mass with given speed collides with a second mass (initially at rest) in a one dimensional collision. Momentum is conserved. If kinetic energy is also conserved, the velocities of both objects after the collision can be calculated. Three limiting cases are explored analytically, and then demonstrated. The equations are used to predict the outcome of some air track experiments.nn**2. Brain Teaser - Elastic Collision with a Wall:**nnA tennis ball bounces off a wall elastically. The momentum of the wall changes, but the kinetic energy of the wall remains zero. How is that possible? Something to think about!nn**3. Center of Mass (CM) Frame of Reference:**nnA 1D elastic collision is considered as seen from the CM frame of reference (where the total momentum is zero). Using the velocity of the CM in the Lab frame, you can transfer between the two frames.nn**4. 1D Inelastic Collision and Internal Energy: **nnA 1D inelastic collision is considered from the laboratory and the CM frame. The kinetic energy is calculated in both frames and it is shown that the initial KE in the CM frame is the maximum KE that can be converted to heat (this is called the internal energy of a system). The equations are used to predict the results of an air track experiment.nn**5. Newton's Cradle Demonstration:**nnProfessor Lewin solicits an analytical proof of his demo showing a lineup of colliding balls.
4661 en Lecture 17: Impulse - Rockets **1. Ballistic Pendulum:**nnA massive pendulum absorbs a bullet and the bullet's momentum. The kinetic energy which is left over after this completely inelastic collision is converted to potential energy of the pendulum. The relationship between horizontal displacement of the pendulum and bullet velocity is derived and empirically observed. The initial kinetic energy in the bullet is almost totally converted into heat.nn**2. Impulse and Impact Time:**nnImpulse is the product of a force (acting on an object) and the brief time that it acts. This results in an abrupt change of momentum. For a ball bouncing off the floor, the impact time is typically milliseconds. A movie is shown to demonstrate this. Courtesy of Dr. Peter Dourmashkin, MIT.nn**3. Surprising Bounce Demo:**nnA tennis ball on top of a much heavier basketball is dropped from a height of about 3 m. The tennis ball bounces way higher than 3 m. Try calculate how high it bounced by assuming the basketball bounces off the floor elastically and then collides elastically with the tennis ball.nn**4. Thrust of a Rocket:**nnAn analogy is drawn between the force felt by the target of a tomato thrower, the reaction force felt by the thrower, and the propulsion (thrust) of a rocket. The Saturn rockets spewed out about 15 tons/sec at a speed of 2.5 km/sec relative to the rocket to provide a thrust of about 34 million Newton. The mass of the rocket decreases substantially with time as it burns its fuel, so the rocket's acceleration increases.nn**5. Fuel Consumption and Rocket Velocity:**nnConsuming a given amount of fuel translates into a fixed change of the rocket's momentum, not into a fixed change of the rocket's kinetic energy.
4662 en Lecture 18: Exam Review 2 **1. Work-Energy Theorem:**nnThe conservation of mechanical energy applies when there are only conservative forces. The work-energy theorem always applies (also when non-conservative forces, such as friction, operate). A block on an inclined plane with friction is analyzed to solve for the coefficient of static friction. The work-energy theorem is shown to provide an elegant solution to solving for the block's velocity.nn**2. Bizarre Spinning Top - Part I:**nnA top is spun on the desk in the lecture hall to show that friction dissipates the top's kinetic energy into heat (the friction does negative work), and the top quickly falls over. Professor Lewin then spins the same top on a small magic black box. The top does not fall over, how bizarre!nn**3. Pendulum, Work and Energy:**nnA pendulum problem is discussed with a set of initial conditions. The maximum angular swing of the pendulum is calculated using the initial conditions and the conservation of mechanical energy. The work-energy theorem gives the same result. Numerical results are obtained for both the maximum angular swing and the phase angle.nn**4. Bizarre Spinning Top - Part II:**nnFifteen minutes later the top is still spinning. How on Earth is this possible?nn**5. Spring, SHO and Initial Conditions:**nnConservation of mechanical energy is used to calculate the maximum displacement of an object attached to a spring for given initial conditions.nn**6. Newton's Law of Universal Gravitation:**nnThe orbital speed of the Earth around the sun is calculated. The kinetic and potential energies of the Earth are reviewed. The escape velocity, to leave the solar system is discussed.nn**7. Resistive Forces, Viscous Term:**nnThis segment reviews concepts and measurements from lecture 12 such as the viscous and pressure drag terms, the terminal velocity, and the critical velocity.nn**8. Collisions and Conservation of Momentum:**nnIn the absence of a net external force on a system, momentum of the system as a whole is conserved even when objects in the system collide and when kinetic energy is destroyed (a rather non-intuitive concept).nn**9. Bizarre Spinning Top - Part III:**nnThe top has been spinning now for over 30 minutes; what's going on?
4663 en Lecture 19: Rotating Rigid Bodies - Moment of Inertia - Parallel Axis and Perpendicular Axis Theorem - Rotational Kinetic Energy - Fly Wheels - Neutron Stars - Pulsars **1. Angular Acceleration in Circular Motion:**nnAn object in circular motion can experience a tangential acceleration (resulting in a change of its speed). Similarities between equations for linear motion and rotational motion are drawn.nn**2. Kinetic Energy of Rotation - Moments of Inertia:**nnThe kinetic energy of rotation of a disk is derived and related to its moment of inertia and angular velocity. The moment of inertia depends upon the shape and mass of an object; it differs for different axes of rotation.nn**3. Parallel Axis and Perpendicular Axis Theorems:**nnThe Parallel Axis theorem is very useful for calculating the moment of inertia about an axis offset from the center of mass. The Perpendicular Axis theorem is useful for thin objects.nn**4. Energy Management with Flywheels:**nnA scenario is explored where the potential energy of a car coming down a mountain is stored in a flywheel (while stepping on the brakes) rather than dissipated into heat. The rotational kinetic energy of the flywheel can, in principle, at a later time be used to increase the car's speed when needed. A demonstration shows how rotational energy in a flywheel gets converted into linear motion of a toy car.nn**5. Flywheels at MIT's Magnet Lab:**nnA pair of very massive flywheels rotating at 6 Hz are used to convert rotational kinetic energy into magnetic energy and vice versa for energy storage.nn**6. Rotational KE in Planets and Stars:**nnThe rotational kinetic energy of the sun and Earth due to their spin are presented, along with data from the Crab pulsar. The Crab pulsar, with a spin period of 33 ms, is spinning down; its spin period is increasing by 36.4 nanoseconds/day. The radiated power (in the form of radio waves, light, X-rays and gamma-rays) results in a loss of rotational kinetic energy. In other words, rotational KE is converted to electromagnetic radiation.nn**7. Slides of MIT's Flywheel and the Crab Nebula:**nnSlides are shown of the Flywheels in MIT's Magnet Lab, and of the Crab Nebula (home of the Crab Pulsar). Stroboscopic pictures show that the Crab Pulsar blinks on and off in optical light as it rotates about its spin axis. An X-ray image of the Crab Nebula made with the Chandra Observatory is also shown.
4664 en Lecture 20: Angular Momentum - Torques - Conservation of Angular Momentum - Spinning Neutron Stars - Stellar Collapse **1. Angular Momentum of a Particle:**nnAngular momentum is defined relative to an origin whose position can be freely chosen. Angular momentum is therefore not an intrinsic property of a moving object, it depends on the position of the origin. The angular momentum of a projectile's motion is explored. The angular momentum changes along its trajectory. The angular momentum of the Earth's orbit if measured relative to the sun's position, is constant (it is NOT constant if it is measured relative to any other origin).nn**2. Rate of Change of Angular Momentum:**nnTorque equals the time derivative of the angular momentum. The torque acting on the Earth is zero if we choose the sun as our origin. It is NOT zero relative to any other origin. Thus the orbital angular momentum of the Earth (sun as origin) is constant.nn**3. Angular Momentum of Rigid Bodies:**nnThe angular momentum of a disk rotating about its center of mass is proportional to its moment of inertia. The angular momentum associated with rotational motion of a rigid body about a stationary axis through the center of mass is called spin angular momentum. Spin angular momentum is an intrinsic property of a spinning object. It is independent of the point of origin chosen. The Earth spins about an axis through its center of mass. The total angular momentum of the Earth with the sun as the origin is the vectorial sum of the spin angular momentum and the orbital angular momentum.nn**4. Ice Skater's Delight:**nnA person stands on a turntable with weights in each hand; this person can change her moment of inertia by moving the weights near to and away from her body. A numerical example is worked out showing that the moment of inertia can easily change by a factor of three, and a live demo is given. Angular momentum is conserved, so the angular velocity increases when the weights are pulled in.nn**5. Angular Momentum of a System and Stellar Spin-up:**nnThe angular momentum of a system of objects can only be changed by external torques. In stellar collapse (resulting in neutron stars and black holes) there is conversion of gravitational potential energy into kinetic energy (heat). As the star collapses, its moment of inertia decreases dramatically but its spin angular momentum is conserved. If a solar-size star with a 100 day spin period collapses into a neutron star, its spin period will become about 1 ms.nn**6. Supernova Explosions and Stellar Spin-up:**nnSlides and commentary cover: Jocelyn Bell's discovery of radio pulsars (at first called LGMs for "Little Green Men"), the mis-alignment of the magnetic dipole axis and the spin axis of a neutron star, the Crab Nebula supernova remnant, and other supernova observations.
4665 en Lecture 21: Torques - Oscillating Bodies - Hoops **1. Key Equations for Angular Momentum and Torque:**nnA review is given of equations for angular momentum and torque, and the importance of choosing the point of origin. These equations are exercised using an example of a circular orbit. If the point of origin is placed at the center of the circle, angular momentum is conserved; for any other point, angular momentum is not conserved.nn**2. Dynamics of a Spinning Rod:**nnTwo examples are analytically described: (1) a rod spinning around an off-center pin and (2) a rod spinning around its center of mass. A third example is discussed in great detail: we hit a rod, initially at rest, on a frictionless horizontal table. The resulting motion can be viewed as a linear motion (with constant speed) of the center of mass and a rotational motion around the center of mass. The discussion concludes with a demo of a ruler being struck (at various places) on a horizontal surface (though not frictionless).nn**3. Physical Pendulum:**nnA ruler is suspended from a pin offset from the ruler's center of mass to form a pendulum. The equation of motion is derived by analyzing the torque and moment of inertia using the pin as the point of origin. For small angles, the motion is simple harmonic. This analysis is also conducted for a hoop suspended from a pin. When gravity is the only restoring force, the period of such a pendulum is determined solely by geometry and is independent of mass. The predicted periods of these pendulums are quantitatively confirmed in demos.nn**4. Friction, Kinetic Energy and a Spinning Top:**nnRemember the bizarre top that kept spinning during lecture 18? Maybe there was something hidden in that black box. A completely new challenge is now to explain the bizarre behavior of a spinning blue plastic object which seems to defy the laws of physics. Another brain teaser!
4667 en Lecture 1: The Geometrical View of y'=f(x,y): Direction Fields, Integral Curves 
4668 en Lecture 2: Euler's Numerical Method for y'=f(x,y) and its Generalizations 
4669 en Lecture 3: Solving First-order Linear ODE's; Steady-state and Transient Solutions 
4670 en Lecture 4: First-order Substitution Methods: Bernouilli and Homogeneous ODE's 
4671 en Lecture 5: First-order Autonomous ODE's: Qualitative Methods, Applications 
4672 en Lecture 6: Complex Numbers and Complex Exponentials 
4673 en Lecture 7: First-order Linear with Constant Coefficients: Behavior of Solutions, Use of Complex Methods 
4674 en Lecture 8: Continuation; Applications to Temperature, Mixing, RC-circuit, Decay, and Growth Models 
4675 en Lecture 9: Solving Second-order Linear ODE's with Constant Coefficients: The Three Cases 
4676 en Lecture 10: Continuation: Complex Characteristic Roots; Undamped and Damped Oscillations 
4677 en Lecture 11: Theory of General Second-order Linear Homogeneous ODE's: Superposition, Uniqueness, Wronskians 
4678 en Lecture 12: Continuation: General Theory for Inhomogeneous ODE's. Stability Criteria for the Constant-coefficient ODE's 
4679 en Lecture 13: Finding Particular Sto Inhomogeneous ODE's: Operator and Solution Formulas Involving Ixponentials 
4680 en Lecture 14: Interpretation of the Exceptional Case: Resonance 
4681 en Lecture 15: Introduction to Fourier Series; Basic Formulas for Period 2(pi) 
4682 en Lecture 16: Continuation: More General Periods; Even and Odd Functions; Periodic Extension 
4683 en Lecture 17: Finding Particular Solutions via Fourier Series; Resonant Terms; Hearing Musical Sounds 
4684 en Lecture 19: Introduction to the Laplace Transform; Basic Formulas 
4685 en Lecture 20: Derivative Formulas; Using the Laplace Transform to Solve Linear ODE's 
4686 en Lecture 21: Convolution Formula: Proof, Connection with Laplace Transform, Application to Physical Problems 
4687 en Lecture 22: Using Laplace Transform to Solve ODE's with Discontinuous Inputs 
4688 en Lecture 23: Use with Impulse Inputs; Dirac Delta Function, Weight and Transfer Functions 
4689 en Lecture 24: Introduction to First-order Systems of ODE's; Solution by Elimination, Geometric Interpretation of a System 
4690 en Lecture 25: Homogeneous Linear Systems with Constant Coefficients: Solution via Matrix Eigenvalues (Real and Distinct Case) 
4691 en Lecture 26: Continuation: Repeated Real Eigenvalues, Complex Eigenvalues 
4692 en Lecture 27: Sketching Solutions of 2x2 Homogeneous Linear System with Constant Coefficients 
4693 en Lecture 28: Matrix Methods for Inhomogeneous Systems: Theory, Fundamental Matrix, Variation of Parameters 
4694 en Lecture 29: Matrix Exponentials; Application to Solving Systems 
4695 en Lecture 30: Decoupling Linear Systems with Constant Coefficients 
4696 en Lecture 31: Non-linear Autonomous Systems: Finding the Critical Points and Sketching Trajectories; the Non-linear Pendulum 
4697 en Lecture 32: Limit Cycles: Existence and Non-existence Criteria 
4698 en Lecture 33: Relation Between Non-linear Systems and First-order ODE's; Structural Stability of a System, Borderline Sketching Cases; Illustrations Using Volterra's Equation and Principle 
4699 en Lecture 22: Kepler's Laws - Elliptical Orbits - Satellites - Change of Orbits - Ham Sandwich **1. Kepler's Laws and Elliptical Orbits:**nnA review of equations for the period, velocity and mechanical energy of a circular orbit is given. Kepler's 3 Laws, and the planetary data that led him to his third law are introduced. The equations for elliptical orbits are discussed and compared with the equations for circular orbits.nn**2. Elliptical Orbit from Initial Conditions:**nnThe elliptical orbit follows from the initial conditions. Using the conservation of mechanical energy, you can find the semimajor axis, and this, in combination with Kepler's 3rd law, enables you to calculate the orbital period. A numerical example for a high eccentricity orbit of an Earth orbiting satellite, is worked out. Using the conservation of angular momentum, one can determine the distances to apogee and perigee, and the satellite's velocity at apogee and perigee.nn**3. Changing from Circular to Elliptical Orbits:**nnBy firing a rocket on board a spacecraft that is in a circular orbit, the spacecraft's velocity (vector) will change, and this leads to an elliptical orbit and a change in orbital period. The new elliptical orbits are sketched along with the original circular orbit, setting the stage for how astronauts in different spacecrafts can pass a sandwich.nn**4. Astronauts Pass a Ham Sandwich:**nnPeter and Mary are astronauts in different spacecrafts but in the same circular orbit. Peter wants to throw a ham sandwich to Mary. The question is: how to do that? There is a large family of solutions which are discussed.nn**5. Simulations of the Passing of the Sandwich:**nnA computer model for finding solutions to how astronauts Peter and Mary can pass a sandwich is introduced and exercised by its author, Dave Pooley. GREAT FUN!
4700 en Lecture 23: Doppler Effect - Binary Stars - Neutron Stars and Black Holes **1. Doppler Shift with Sound Waves - Circular Orbits:**nnThe received frequency changes if the source of sound moves towards or away from an observer. This is the Doppler Effect, demonstrated by Professor Lewin with a tuning fork. The fractional change in frequency reveals the velocity component along your line of sight to the moving sound source. If the source of sound is in circular motion, and if the observer is somewhere in the orbital plane, you can determine the orbital radius and the speed of the source in its orbit. This is demonstrated with a rotating wind organ.nn**2. Doppler Shift of Electromagnetic Radiation:**nnElectromagnetic radiation travels at the speed of light, c, in vacuum. If a source of light has a velocity component towards you, the frequencies that you will observe will be higher than those of the emitted radiation, and the received wavelengths will be shorter (blue-shift) than the emitted wavelengths. If the source is receding from you the received wavelength is longer (red-shift). The spectroscopic Doppler shift is used by astronomers to measure the radial velocity of emitters and absorbers of light.nn**3. Star Mass Determinations from Doppler Shift:**nnA binary star system consists of a pair of stars orbiting about their center of mass. By measuring the Doppler shifts of both stars as a function of time, you can determine the orbital period, the radial velocity of each star and, if the observer is located in the orbital plane, the orbital radii can be found for both stars. The orbital radii and Kepler's third Law determine the total mass of the system, enabling the determination of each star's mass separately.nn**4. X-ray Binary Systems:**nnIn an X-ray binary system, there is a neutron star (or black hole) pulling matter off its donor companion. Matter spirals toward the neutron star, and potential energy is converted to kinetic energy. This, coupled with the high mass transfer rate between the pair, generates tremendous power and astronomical temperatures (it radiates mainly X-rays). The accreting ionized matter gets funnelled onto hot spots by the neutron star's magnetic field, which spins with the neutron star (making it an X-ray pulsar). Doppler shifts in the pulsar period and X-ray eclipses can provide orbital parameters (and masses) for the stellar system.nn**5. Black Holes:**nnA black hole is a massive object with no size, but with a characteristic surface called the event horizon from within which nothing can escape the black hole. In black-hole binary systems the accreting matter radiates X-rays as it approaches the event horizon of the black hole, but the black hole has no surface, therefore does not exhibit a pulsar-like behavior. You can measure the optical Doppler shift of the donor star, and from its spectrum estimate the donor's mass. This then leads to the mass of the accretor. If this mass is in excess of about 3 solar masses, it is believed to be a black hole. Cygnus X-1 was the first such discovery (in 1972). Its black hole is about 10 solar masses.
4701 en Lecture 24: Rolling Motion - Gyroscopes - VERY NON-INTUITIVE **1. Pure Roll of Hollow and Solid Cylinders:**nnIn pure roll, the object is not skidding or slipping, and the speed of the center of mass equals the circumferential speed. Professor Lewin derived an equation for the acceleration of an object rolling down a ramp (under pure roll conditions). For solid cylinders with uniform mass density, this acceleration is independent of the mass and radius of these cylinders (this is rather non-intuitive). However, the acceleration is higher for a solid cylinder than it is for a hollow cylinder. This is demonstrated.nn**2. Applying Torque to a Spinning Wheel:**nnWhen you apply a torque to a fast spinning wheel, it moves the spin angular momentum in the direction of the torque (torque is a vector). This is called precession. This very non-intuitive concept is demonstrated with a bicycle wheel.nn**3. Precession of a Flywheel:**nnThe bizarre behavior of a spinning flywheel that experiences a torque due to gravity is explored. Professor Lewin demonstrates this by suspending the axle of a fast rotating bicycle wheel from a rope. By increasing the torque, the precession frequency increases. The direction of precession can be reversed if the direction of rotation of the bicycle wheel is reversed. You can also observe this very non-intuitive behavior with a toy gyroscope.nn**4. Mysterious Suitcase:**nnA suitcase is brought in that requires special handling. There is a fast rotating flywheel inside! A student volunteers to carry the suitcase around. The suitcase behaves in a weird manner as the student turns around.nn**5. Gyroscope in Gimbals:**nnA spinning object, e.g. a coin on edge, is more stable against falling over than when it isn't spinning. This concept is used in mechanical inertial guidance systems, where a spinning wheel is mounted in gimbals to prevent torques to the axis of the wheel. Professor Lewin walks through the lecture hall with such a 3-axis gimballed gyroscope. The direction of the axis of rotation of the spinning flywheel does not change as he moves around.
4702 en Lecture 25: Static Equilibrium - Stability - Rope Walker **1. Rotation vs. Translation:**nnThe analogy between rotational and translational equations is summarized.nn**2. Stability of a Ladder:**nnA ladder leaning against the wall is analyzed to determine the minimum angle it can make with the floor without sliding. The ladder is then set at this critical angle, and the stability is investigated as someone climbs the ladder.nn**3. Rope Tension Reduced by Friction:**nnSailors often wind a rope around a cylinder to reduce the tension that needs to be applied to hold an object in place. The tension is reduced by friction along the wrapped portion of the rope. You can use this device to balance a strong force. Professor Lewin has constructed a device that demonstrates this in a very dramatic way.nn**4. Locating the Center of Mass of a Rigid Body:**nnThe center of mass always lines up below the point of suspension such that the net torque (relative to the suspension point) is zero. When the center of mass is positioned below the point of suspension, the system is stable.nn**5. Stability of a Rope Walker:**nnThe stability of a rope walker is improved by lowering her center of mass below the rope. If the walker also wears special shoes so that she can't slide sideways off the rope, she cannot fall.
4703 en Lecture 26: Elasticity - Young's Modulus **1. Elasticity of Materials:**nnA relationship between stress, strain and Young's Modulus is introduced by analogy with springs. The properties of various metals are compared, and the stress vs. strain curve is described.nn**2. Measuring Stress vs. Strain:**nnAn apparatus for measuring very small elongations of a wire under stress is described. A set of measurements are made to construct the stress vs. strain curve for a copper wire. From these, both Young's modulus and the ultimate tensile strength can be calculated.nn**3. Spring Constant of a Wire:**nnAt low stress values, where the stress vs. strain curve is linear, one can generate simple harmonic oscillations in the vertical direction by hanging an object from a wire.nn**4. Speed of Sound in Materials:**nnThe speed of sound in a material depends on the stiffness (Young's modulus) and density of the material. The speed of sound and length of a rod determine the roundtrip time of a pressure disturbance introduced at one end. From this follows the fundamental frequency at which a rod resonates.nn**5. Demo with a 2 kg Block and Two Strings:**nnA 2 kg block is suspended from one string, and an identical string is suspended from the block. Professor Lewin pulls on the lower string. Which string will break first, the upper one or the lower one? The lower string will break first if the force is impulsive (a quick jerk) because it will elongate faster than the upper string. If we pull slowly the upper string will break first as its tension will then always exceed that of the lower string.
4704 en Lecture 27: Fluid Mechanics - Pascal's Principle - Hydrostatics - Atmospheric Pressure - Over Pressure in Lungs and Tires **1. Pressure and Pascal's Principle:**nnPressure is a scalar. Pascal's Principle is explained. In hydraulic jacks, a small force is applied to move a large mass a small distance.nn**2. Gravity and Hydrostatic Pressure:**nnBecause of gravity, pressure increases with depth in a fluid. This is called hydrostatic pressure.nn**3. Compressibility of Gases vs. Liquids:**nnUnlike liquids, gases are compressible, so they cushion impacts. Liquids do not act like cushions. This is demonstrated in a very dramatic way by firing a bullet in a sealed can filled with air and one filled to the brim with liquid.nn**4. Pressure Difference and a Column of Liquid:**nnThe pressure difference between the bottom and top of a vessel of liquid depends only on the height and density of the liquid, not on the area or weight of the column. This is rather non-intuitive.nn**5. Atmospheric or Barometric Pressure:**nnThe pressure at sea level due to the air above determines the atmospheric or barometric pressure. It can be measured by raising (sucking up) a column of liquid from an open reservoir with a tube sealed at the end where we pump the air out. For every 10 meters depth in water, the hydrostatic pressure increases by about one atmosphere.nn**6. Submarines and Overpressure:**nnCornelis Drebbel is credited with inventing the first submarine operating at a depth up to 5 meters. At this depth, the hydrostatic pressure is about half an atmosphere. A sealed paint can was evacuated to demonstrate the enormous forces acting upon it with an over pressure of about one atmosphere. The can imploded.nn**7. Overpressure in our Lungs:**nnThe lung capacity, our ability to overcome hydrostatic pressure, is measured with a manometer. This is related to how deep snorkeling works, and why scuba-divers use pressurized air tanks. Professor Lewin demonstrates that by blowing on a manometer, or by sucking on it, we can raise or lower a column of water by about 1 meter (0.1 atmosphere). So why then was he able to suck fluid up a straw of several meters long?
4705 en Lecture 28: Hydrostatics - Archimedes' Principle - Fluid Dynamics - What Makes Your Boat Float? - Bernoulli's Equation **1. Archimedes' Principle:**nnThe buoyant force on an immersed body is shown to equal the weight of the displaced fluid (Archimedes' Principle). Such weight measurements can be used to determine the average density of irregular objects, and also to estimate body fat in people.nn**2. Floating Objects: **nnMost of an iceberg is submerged because its density is only a little less than the density of water. Boats float, rocks sink. Professor Lewin poses another brain teaser.nn**3. Stability of Floating Objects:**nnThe center of mass of a floating object should be below the center of mass of the displaced fluid for stability; this is an important ship design concept.nn**4. Balloons:**nnThe buoyant force of air on a balloon is discussed. It is demonstrated how a balloon and a pendulum behave in accelerated, closed containers.nn**5. Fluid Dynamics, Bernoulli's Equation:**nnBernoulli's equation is presented as the conservation of energy for an incompressible fluid flow. For flow through a pipe of varying cross section, Bernoulli's equation shows that the pressure is the lowest at the smallest cross section where the velocity of the fluid is the highest. Bernoulli's equation is also applied to syphons.nn**6. Fluid Mechanic Magic: **nnSome non-intuitive demos show how ping pong balls behave in air streams.
4706 en Lecture 29: Exam Review 3 **1. Review of 1D Collisions:**nnTwo masses moving in one dimension collide in a completely inelastic manner. The equations for an elastic collision are also discussed.nn**2. Atwood's Machine:**nnA "massless" rope runs (without slipping) over a pulley (the moment of inertia of the pulley is not negligible). The rope has a pair of masses hanging from it. The equations of rotational (pulley) and translational (the 2 masses) motion are related, and solved for the acceleration of the masses.nn**3. SHO of a Suspended Rod:**nnA pendulum consists of a ruler/rod suspended from one end. The equation of motion is derived. The moment of inertia is determined using the parallel axis theorem. Rotational kinetic energy is also discussed. For small angles, the motion is that of a simple harmonic oscillator.nn**4. Conservation Laws for a Satellite's Orbit:**nnA satellite is launched from a planet. Using the conservation of angular momentum and mechanical energy, and the maximum distance between the planet and satellite, one can calculate the launch velocity and other orbital parameters.nn**5. Doppler Shift Review:**nnIf stars are receding from us, the starlight that we observe is red-shifted; the observed wavelengths are longer than those emitted by the star. The Doppler shift depends on the component of the velocity along the line-of-sight (so-called radial velocity). A somewhat similar equation applies to sound received from moving sound sources. However, in the case of sound, it DOES matter whether the sound source moves or whether the observer moves. For light, only the relative motion between source and observer matters.nn**6. Pure Roll:**nnThe acceleration of a cylinder and sphere is derived under the condition of pure roll. A sliding object accelerates faster down an incline plane than a rolling object. The static friction coefficient must be sufficient to support pure roll. Kinetic energy for a rolling object has both a translational and rotational term.
4707 en Lecture 30: Simple Harmonic Oscillations - Energy Considerations - Torsional Pendulum **1. SHO of a Physical Pendulum:**nnThe SHO equation of motion (small angle approximation) for a rigid body pendulum is derived from the torque equation about the point of suspension. The periods of oscillation are worked out for four different shapes (rod, hoop, disk and for a billiard ball hanging from a massless string).nn**2. SHO of a Liquid in a U-Tube:**nnThe SHO equation of motion for a liquid sloshing in a U-shaped tube is derived by taking the time derivative of the conservation of mechanical energy. The angular frequency and period are calculated and compared to empirical data.nn**3. Torsional Pendulum:**nnThe SHO equation of motion for a torsional pendulum is derived from the torque equation about the center of mass. The displacement angle is in the horizontal plane. The restoring torque is due to the twisted wire; no small angle approximations are required. The demo uses piano wire.
4708 en Lecture 31 - Forced Oscillations - Normal Modes - Resonance - Natural Frequencies - Musical Instruments **1. Forced Oscillations:**nnWhen applying an external sinusoidal force (a driving force), to a spring system, there will be a transient response in addition to a steady state response. When the transient response has died (due to friction), the steady state remains. Resonance responses are described.nn**2. Resonances and Normal Modes:**nnSystems of coupled oscillators with multiple resonant frequencies are described, including a violin string. Normal modes of an oscillating string occur at integral multiples (harmonics) of the fundamental; these natural frequencies are determined by the length, the tension, and the mass per unit length of the string.nn**3. Resonance in Wind Instruments:**nnAir pressure waves in boxes, cavities, and hollow tubes exhibit a series of resonances determined by the velocity of sound in air and the dimensions of the systems. The frequency of these resonances can be altered by holes or pistons to change their effective size. Professor Lewin demonstrates this with a wooden flute and he plays Jingle Bells on a wooden trombone.nn**4. Nonlinear Response at Resonance:**nnExcitation with a spectrum of frequencies often reveals some resonant frequencies as the object responds most strongly to those frequencies. Resonance can be destructive if the driving force is strong enough, as dramatically shown with a wine glass driven by strong sound waves, and the Tacoma Narrows bridge driven by the wind.nn**5. Speed of Sound in a Resonant Cavity:**nnEach person's distinctive voice is produced in a manner similar to the way sounds are produced by wind instruments. If these instruments were filled with helium instead of air, the frequencies of the sound would be very different as the speed of sound in helium is substantially larger than that in air. Professor Lewin inhales helium to demonstrate this. His familiar voice cannot be recognized anymore.
4709 en Lecture 32: Heat - Thermal Expansion **1. Heat and Temperature:**nnVarious temperature scales are discussed: Celsius, Fahrenheit, Kelvin.nn**2. Linear Thermal Expansion:**nnThe linear thermal expansion coefficient is introduced. Expansion leads to a need for expansion joints in railroad rails to avoid bulging on hot days. Thermal expansion is demonstrated by heating and cooling a brass rod. An important application of thermal expansion is bi-metals which are used in thermostats, safety devices and thermometers, as demonstrated.nn**3. Cubical Thermal Expansion:**nnThe fractional change in volume with temperature is given by the coefficient of cubical expansion. A mercury thermometer is discussed.nn**4. Shrink Fitting:**nnShrink fitting is a technique that makes use of the thermal expansion by heating one object (of two). After the two objects are assembled, they cool, and the fit is perfect and "for ever".nn**5. Cubical Thermal Expansion of Water:**nnWater has a maximum density at 4 degrees Celsius; between 0 and 4 degrees Celsius the cubic thermal expansion coefficient is negative so the water expands as it cools below 4 degrees Celsius. The density of ice is about 8% lower than water, so ice cubes and icebergs float in water.
4710 en Lecture 33: Kinetic Gas Theory - Ideal Gas Law - Isothermal Atmosphere - Phase Diagrams - Phase Transitions **1. Intro to Ideal-Gas Law and Avogadro's Number:**nnLiquids are near incompressible, but gases are not; the density of gases can be increased with relative ease by increasing the pressure, but that is not the case for liquids. The ideal-gas law is introduced and explained; it is a good approximation for the compressibility of most gases. Avogadro's number is the number of molecules/mole, and defined as the number of atoms in 12 grams of carbon12.nn**2. Ideal-Gas Law Insights:**nnThe ideal-gas law predicts that the volume of a mole of gas for a given temperature and pressure is independent of the molecular mass of the gas. The momentum transfer per second from the gas molecules to the vessel walls is proportional to mv^2 (which is reminiscent of the kinetic energy of the molecules). This is proportional to the gas pressure. If two different gases have the same temperature, the molecules must have the same average translational kinetic energy. Thus the molecules with the lowest mass must have, on average, the highest speed.nn**3. Ideal-Gas Law Experimentally Applied:**nnA demonstration of the ideal-gas law uses a pressure gauge that measures overpressure (the pressure in excess of the atmospheric pressure). The temperature of a fixed number of air molecules in a fixed volume is increased from melting ice temperature (273 K, pressure=1atm) to boiling water temperature (373 K). The resulting pressure increase is measured.nn**4. Phase Diagrams and Phase Transitions:**nnIntroduction to phase diagrams and phase transitions. Taking a gas at a constant temperature, and using a piston to increase its pressure, the gas volume decreases as the pressure increases until you approach the gas-&gt;liquid phase transition. At constant pressure of one atmosphere, but with increasing temperature, you start with ice at low temperature, which becomes liquid water at 273 K, and the water will boil at 373 K, and it will become water vapor (gas) above this temperature.nn**5. A Fire Extinguisher:**nnA fire extinguisher is filled with CO2. Given the dimensions of the tank (i.e. its volume), room temperature (293 K), the mass of CO2 in the extinguisher (from the label), and the ideal-gas law (this law is only valid if there is ONLY gas inside and NO liquid), the pressure is calculated inside the cylinder. It is concluded that it can't be just gas, there must also be liquid CO2 in the fire extinguisher. The phase diagram for C02 shows a phase transition at 60 atm at 293 K; the CO2 gas and liquid would co-exist in thermal equilibrium at room temperature and 60 atm. If you tried to further compress at room temperature, more gas would turn into liquid but the pressure would remain at 60 atm until all the gas had turned into liquid (after which the pressure can increase).nn**6. Boiling Water - Part 1:**nnIn Lecture 27 there was discussion of hydrostatic pressure, the overpressure submarines must survive as they go deeper in the water. Conversely the atmospheric (barometric) pressure should decrease with increasing altitude, but with a different height dependence (because air is compressible, its density changes with pressure). A differential equation is solved to determine that in an isothermal atmosphere the pressure decreases exponentially with altitude. Consequently the boiling point of water decreases with altitude. A demo of water boiling at room temperature but low pressure is shown.nn**7. Boiling Water - Part 2: **nnWhile waiting for the pressure in the bell jar to decrease, Professor Lewin starts a second demo, boiling the water (373 K and 1 atm) in a can so the air in the can gets displaced by water vapor (he then seals the can, and lets it cool). Inside the can is liquid water and water vapor in thermal equilibrium, as the can cools the vapor condenses into liquid, and the pressure in the can decreases; the pressure in the can should drop to about 17 mm Hg (about 0.02 atm) as the can cools to room temperature. The can implodes due to the external force of the atmospheric pressure. Meanwhile, the water in the bell jar boils at room temperature! A third demo involves air-filled balloons that shrink much more than naively predicted using the ideal-gas law. What's going on?
4711 en Lecture 34: The Wonderful Quantum World - Breakdown of Classical Mechanics **1. Discrete Energy Levels:**nnElectrons orbit their atomic nucleus in well defined orbits corresponding to discrete energy levels. The electrons can jump from one energy level to a vacant energy level, but they cannot exist in between. Transitions between these energy levels gives rise to absorption and emission of light in discrete spectral lines (wavelengths). The students are encouraged to look through their diffraction gratings at helium and neon light sources to see evidence of these discrete wavelengths of emitted light.nn**2. Particles and Waves:**nnQuantum mechanics introduces some very non-intuitive concepts, e.g. light behaves as both a particle (a photon) and a wave, and a particle behaves like a wave with a wavelength inversely proportional to its momentum. Interference is a wave phenomenon, and indeed particles can interfere with each other. Both the position and momentum of a particle cannot be accurately specified at the same time (Heisenberg's uncertainty principle).nn**3. Diffraction by a Slit:**nnDiffraction of light by a narrow vertical slit is a well understood classical wave phenomenon consistent with Heisenberg's uncertainty principle. The narrower the slit, the smaller is the uncertainty in the horizontal position of the photons which have to sneak through the narrow opening, so the greater is the horizontal spread of the transmitted protons (uncertainty in their momentum). Quantum mechanics only allows you to predict positions of particles with certain probabilities. In the classical, Newtonian, world you can predict the position and movement of a particle to any degree of accuracy - NOT in the microscopic quantum world. The Newtonian picture is perfect for describing the behaviour of basketballs and planets in the macroscopic world.
4712 en Lecture 35: Farewell Special - High-energy Astrophysics **1. X-ray Astronomy from Balloon Flights:**nnProfessor Lewin takes us back to 1966 when Professor George Clark and he pioneered X-ray observations from balloon-borne telescopes at altitudes of 145,000 ft.nn**2. Slides of High Altitude Ballooning Expeditions:**nnA series of slides are shown of the construction of an X-ray telescope, the manufacturing of the balloons, and balloon launches in both Alice Springs, Australia and Palestine, Texas. The risks that arise during launch and during flight are shown, as are some interesting encounters during payload recovery.nn**3. X-ray Observations:**nnThe science gained from the balloon-borne telescopes is described, such as the first ever flaring event and the 2.3 minute periodicity observed from a previously unknown source (GX 1+4). We now know of hundreds of binary star systems where gas from a "donor" swirls onto a neutron star (the accretor). The gas reaches the neutron star's surface with about 1/3 the speed of light. It heats up its surface to a few million degrees Kelvin which is why the neutron star emits large amounts of X-rays.nn**4. Binary Stars and X-ray Bursts: **nnProfessor Lewin reviews the Doppler shift of the neutron star's pulsar period and of the donor star's spectral lines in X-ray binaries. He then talks about X-ray bursts. These are thermonuclear flashes (nuclear bomb explosions) on the surface of neutron stars. The X-rays from these flashes temporarily excite the matter in the accretion disk, resulting in delayed optical flashes. This delay provided the first measurement of the size of an accretion disk.
4713 en TansSlo – Sustainable Transport Research 
4718 en Introduction to the Session 
4719 en Practical experience with the application of alternative tender procedures in connection with maintenance of regional and local road networks 
4720 en A scientific approach to research and innovation governance 
4721 en Introduction to the Strategic Session 
4722 en Sustainable Pavements for New Member States 
4723 en ARCHES - a gaze on Central European highway structures 
4724 en Central European Research in TRAnsport INfrastructure 
4725 en Arena – a public-private approach for development of a road user charging 
4726 en Introduction to the Session 
4727 en Establishment of road transport research needs in Hungary 
4728 en Evaluation of materials for road upgrading 
4729 en The Nordic Road and Transport Research Program enters its 5th year 
4730 en Living lab part of competitiveness model of the automotive sector in Slovenia 
4731 en Conclusion remarks 
4732 en Introduction to the Session 
4733 en International cooperation on Road Transport leading to improved mobility, safety, security and prosperity 
4734 en Sustainable viability of transport systems in Sub-Saharan Africa: which reforms? 
4735 en Public transport service design requirements for the changing face of the South African customer 
4736 en Introduction to the Session 
4737 en Experimental Evaluation of Various Biofuel - Diesel Blends as Diesel Engine Fuels 
4738 en Measures to abate green house gas emissions in logistics companies 
4739 en Air management for lowest emission passenger car diesel engines 
4740 en Hydrogen technology activities in Slovenia 
4741 en Introduction to the Session 
4742 en Environmental consequences of better roads 
4743 en Dynamic speed limits to improve local air quality 
4744 en ARTEMIS: the new European tools for estimating at different scales the pollutant emissions from road transport 
4745 en Strategies for mitigating road pollution of water bodies 
4746 en Conclusion remarks 
4747 en Debate 
4748 en Sustainable mobility for everybody - complementary solutions for passenger and commercial transportation 
4750 en Welcome MIT OpenCourseWare is an idea - and an ideal - developed by the MIT faculty who share the Institute's mission to advance knowledge and educate students in science, technology, and other areas of scholarship to best serve the world. In 1999, the Faculty considered how to use the Internet in pursuit of this goal, and in 2000 proposed OCW. MIT published the first proof-of-concept site in 2002, containing 50 courses. By November 2007, MIT completed the initial publication of virtually the entire curriculum, over 1,800 courses in 33 academic disciplines. Going forward, the OCW team is updating existing courses and adding new content and services to the site.
4751 en Unlocking Knowledge, Empowering Minds In December we reported on the great news that MIT’s Open CourseWare initiative had released their 1,800th course, thereby publishing the entire MIT curriculum on OCW. MIT has now released comprehensive video recordings of the celebratory event, all published under a Creative Commons Attribution-Noncommercial-Share Alike license. The keynote address was given by New York Times columnist Thomas Friedman.
4752 en The World is Flat 3.0 Thomas Friedman, author and columnist for the New York Times, gave the keynote presentation at MIT’s recent event in celebration of reaching 1800 published OCW courses. The speech was insightful and timely, but also very well-delivered.nnThe World Is Flat 3.0 is Thomas L. Friedman's account of the great changes taking place in our time, as lightning-swift advances in technology and communications put people all over the globe in touch as never before-creating an explosion of wealth in India and China, andnchallenging the rest of us to run even faster just to stay in place.nnThe World Is Flat 3.0 is an essential update on globalization, its opportunities for individual empowerment, its achievements at lifting millions out of poverty, and its drawbacks--environmental, social, and political, powerfully illuminated by the Pulitzer Prize--winning author of The Lexus and the Olive Tree.nnn
4753 en The Future of OCW and Education MIT OpenCourseWare continues to attract a global audience of educators, students and self learners. Nearlynhalf of all educators visiting the site have incorporated OCW content into their own teaching materials. Students use the site both to supplement materials from courses they are taking and to study beyond the bounds of their formal course of study.
4754 en Highlights for High School Announcement MIT President Susan Hockfield announces the launch of a new web site, Highlights for High School, that will provide resources to improve science, technology, engineering and math (STEM) instruction at the high school level.nnThe web site builds on the success of MIT's revolutionary OpenCourseWare initiative and is designed to inspire the next generation of engineers and scientists and to be a valuable tool for high school teachers.
4755 en Highlights for High School Highlights for High School is your guide to MIT courses selected specifically to help you prepare for AP exams, learn more about the skills and concepts you learned in school, and get a glimpse of what you'll soon study in college.
4756 en Acknowledgements 
4757 en Interview with Walter H. Lewin Many of the Walter Lewin Lectures on Physics at MIT have been shown for over six years on UWTV in Seattle, reaching an audience of about four million people. Lewin personally responded to hundreds of e-mail requests that he received per year from UWTV viewers. For fifteen years he was on MIT Cable TV, with programs aired 24 hours per day helping freshmen with their weekly homework assignments. Lewin also teaches video courses on Newtonian Mechanics, Electricity and Magnetism, and Vibrations and Waves, which can be viewed from the MIT OpenCourseWare web site. His MIT lectures for science teachers and for middle school students can be viewed on MIT World. Various lecturers all around the world make use of these lectures to teach their students.
4758 en Promotional Video Professor Lewin is an international webstar. He is well-known at MIT and beyond for his dynamic, inspiring and engaging lecture style. His courses are also among the most downloaded at iTunes U. 8.01 Physics I: Classical Mechanics explains the basic concepts of Newtonian mechanics, fluid mechanics, and kinetic gas theory, and a variety of interesting topics such as binary stars, neutron stars, and black holes.nnLewin's demonstrations make difficult concepts easier to understand. Stanley, a doctor in the US, says, "I had this information presented to me in 1962 while pursuing my medical career. It was never so clearly presented as it was by the witty genius. Please thank [Professor Lewin] from an old student and tell him that he has truly presented the beauty of physics to me."nnWatch some of Walter Lewin's greatest moments. To see more, check out Professor Lewin's classes on [[http://ocw.mit.edu| MIT OpenCourseWare]].
4759 en Online Learning, Regret Minimization, and Game Theory The first part of tha tutorial will discuss adaptive algorithms for making decisions in uncertain environments (e.g., what route should I take to work if I have to decide before I know what traffic will like today?) and connections to central concepts in game theory (e.g., what can we say about how traffic will behave overall if everyone is adapting their behavior in such a way?). He will discuss the notions of external and internal regret, algorithms for "combining expert advice" and "sleeping experts" problems, algorithms for implicitly specified problems, and connections to game-theoretic notions of Nash and correlated equilibria. The second part of tha tutorial will be about some recent work on learning with similarity functions that are not necessarily legal kernels. The high-level question here is: if you have a measure of similarity between data points, how closely related does it have to be to your classification problem in order to be useful for learning?
4760 en Machine Learning Laboratory The first laboratory has not been recorded but has featured some hands on experiments with Elefant ([[http://elefant.developer.nicta.com.au/]]) mainly concentrating on installing, using, and developing machine learning algorithms within the Elefant framework. We will walk through examples of implementing a simple stochastic gradient descent algorithm as a part of this tutorial. This is the first part of the second session which is split with [[mlss08au_vishwanathan_mll|//S.V.N. Vishwanathan's// "%title"]] and will feature hands on experiments with BNRM (Bundle Methods for Regularized Risk Minimization) ([[http://users.rsise.anu.edu.au/~chteo/BMRM.html]]). The emphasis here will be on developing various loss function modules which can then be plugged into the BMRM solver.
4761 en Machine Learning Laboratory The first laboratory has not been recorded but has featured some hands on experiments with Elefant ([[http://elefant.developer.nicta.com.au]]) mainly concentrating on installing, using, and developing machine learning algorithms within the Elefant framework. We will walk through examples of implementing a simple stochastic gradient descent algorithm as a part of this tutorial. This is the second part of the second session which is split with [[mlss08au_webers_mll|//Christfried Webers's// "%title"]] and will feature hands on experiments with BNRM (Bundle Methods for Regularized Risk Minimization) ([[http://users.rsise.anu.edu.au/~chteo/BMRM.html]]). The emphasis here will be on developing various loss function modules which can then be plugged into the BMRM solver.
4764 en Magnetic quantum oscillations in 2D metals and metallic nanowires Quantum oscillations of magnetization and resistivity with the magnetic field are of a great experimental and theoretical value providing reliable and detailed Fermi-surfaces. Specifically interest in the oscillations in almost two-dimensional (2D) Fermi-liquids has recently gone through a vigorous revival due to experimental discoveries of magneto-oscillations in a few high-temperature cuprate superconductors. Another interesting application of the quantum oscillation is in the field of quasi one dimensional nanowires. In my talk I review experimental outcome of De Haas van Alphen and Shubnikov-De Haas effects and theoretical results obtained in our group.
4766 en Research Challenges in Enterprise Information Retrieval Information Retrieval is a major component of Knowledge Management systems in every business but most of the research that is being done in IR today focuses on the Web and not on the needs and challenges of businesses. This is primarily due to the availability of data on the Web for academic researchers as well as familiarity with the problems in Web IR since all of us are consumers and can relate to the domain. In contrast, for Enterprise Information Retrieval, the data is not available to most researchers and the challenges and needs are not obvious to people who are not everyday users of such systems. In this talk, I will point out some challenges in this domain, pose open research questions for the Information Retrieval, NLP, Machine Learning & Data Mining communities, and describe the experimental infrastructure that is being set up at Accenture Technology Labs to undertake those challenges. Our experimental test-bed for Enterprise Knowledge Management Research has access to potentially 150,000 users in Accenture and will allow us to collaborate with researchers and solve large-scale Enterprise Information Retrieval problems.
4767 en eBay - Searching, Finding, Buying, Selling: Story of a Long tail Online Marketplace In this talk we will present the personality of an Online Marketplace. This Online Marketplace has all the characteristics of an Offline Marketplace and more because of the simultaneous anonymity and familiarity of the buyers and sellers. The long tail nature of the transactions combined with differing capabilities, motives, and trust leads to a vibrant Social Commerce Network. We will present interesting technical challenges and opportunities in Finding, Classification, merchandizing and reputation systems.
4768 en Introduction to the Plenary Session 
4769 en Climate Change from the Scientific point of view 
4770 en Transport and Climate Change Roundtable 
4771 en XML based frameworks in managing and archiving (not only) textual data Text-technological application scenarios have become a fixed component in many fields of the Humanities, which produce increasing quantities of digital data in the research process. This trend induces the need for strategies for intelligent management and archiving of digital resources which are not covered by classical content management systems, such as flexible and web-based integration of archived data in retrieval, analysis and interpretation processes. Frameworks with XML based system architectures can offer appropriate support here. Concentrating on the approach of two frameworks, Cocoon (cocoon.apache.org) and FEDORA (www.fedora-commons.org) the lecture discusses the possibilities, limits and experiences of such approaches, based on our long-time research project (gams.uni-graz.at) at the Centre for Information Modelling at the Faculty of Humanities of the University Graz.
4772 en Policy introduction: the Green Paper on Urban Mobility and its Action Plan 
4773 en Monitoring mobility through innovation: the example of the city of Ljubljana in CIVITAS 
4774 en Achieving Sustainable Urban Mobility: The Challenges 
4775 en New solutions for collective transport: fuelling bus rapid transit (BRT) in Europe 
4776 en Debate 
4777 en Conclusions and outlook 
4778 en Round Table 
4779 en CO2 beyond tomorrow - a fundamental approach 
4780 en Strategies for future power-trains 
4781 en On board measurements results a tool to share future emission regulation 
4782 en An intelligent model for a cleaner environment and citizen's rights in road transport 
4783 en Debate - Future Directions for Cleaner Road Transport and Advanced Fuels 
4784 en Intelligent Cargo for Efficent, Safe and Environment-friendly Logistics 
4785 en The Road Transport Management System: A Self regulation initiative to promote load optimisation, vehicle maintenance and driver wellness in heavy vehicle transport in South Africa 
4786 en A Real-Time Decision Support System for the Safe and Efficient Routing of dangerous Goods Vehicles 
4787 en FIDEUS: Innovative logistics for European cities 
4788 en Mobility groups and their needs in the future 
4789 en New tools for linking transport and land use planning 
4790 en Public transport service design requirements for the changing face of the South African customer 
4791 en National Mobility Study in Poland - Needs and Requirements 
4792 en Traffic safety 
4793 en Intelligent car = safer roads 
4794 en Road safety in Europe 
4795 en How can inelligent transport systems contribute to road safety? 
4796 en IRF input for safer roads 
4797 en Road Safety Policy 
4798 en Debate - Traffic Safety 
4799 en Introduction 
4800 en Proactive role of road infastuctures in the overall transport system for safety 
4801 en Active Safety 
4802 en Cooperative Systems contributing to Safety 
4803 en Nomadic Systems 
4804 en Integrated safety in the Transpport system 
4805 en From black spots to grey road sections - what is the right way 
4806 en Implementation of rural road safety schemes: lessons from the Netherlands 
4807 en Best practices for road safety in Europe: A systematic approach 
4808 en HeavyRoute - Intelligent route guidance for heavy vehicles 
4809 en A vision of intelligent roads (INTRO project) 
4810 en Assessing the changes in operating traffic conditions due to weather conditions 
4811 en Co-operative driving and vehicle-based safety applications: the perspective of the European Integrated Project "SAFESPOT" 
4812 en Corporate social Responsibility and the road sector 
4813 en The results of public transport service modelling in Zilina county 
4814 en Debate - Towards a Sustainable Society 
4815 en Integrating mobility management into the spatial planning - State of the art analysis 
4816 en Transition to sustainable mobility; theory put into practise 
4817 en Comprehensive traffic management in Austria 
4818 en Concluding remarks 
4819 en Concluding remarks 
4820 en Concluding remarks 
4821 en Welcome to TRA 2010 in Belgium 
4822 en Closure of the TRA 2008 
4823 en Moderator 
4824 en Advances in Human Machine Interfaces (HMI) 
4825 en Real-Time Traffic and Travel Information (RTTI) 
4826 en Vehicle-to-Vehicle and Vehicle-to Infrastructure Linkages 
4827 en Integrated Safety in the Transport 
4828 en Supporting Transport Policy through Innovation 
4829 en Inland Waterway Transport via Donau 
4830 en Stimulate Telematic Applications (ITS) in the Transport System - From research to deployment 
4831 en Cross-Border Integration of Traffic Management: The PROMET Project 
4832 en Perspectives from a European Municipality 
4833 en Moderator 
4834 en Systematic Risk Analysis for Safety Assessments of Road Systems 
4835 en Comparative analysis of road safety parameters in the European motorways 
4836 en The Enhanced Crash Investigation Study (ECIS) 
4837 en Moderator 
4838 en Road safety and its modification 
4839 en ‘Crash violence’ within the traffic system. Risks and their reduction in road traffic on two-lane main roads in Finland 
4840 en The new Austrian Stopping Distance Model based on RoadSTAR skid resistance data 
4841 en Moderator 
4842 en Steve Asign Inuas 
4843 en International Transport Forum (ECMT) 
4844 en An Evolution of Road User Charging Systems in Europe with a Focus on Poland 
4845 en Royal Institute of Technology, Stockholm 
4846 en Panel Discussion 
4847 en Improved Mobility, Security and Safety of Pedestrians and Bicyclists on Roads through Small Towns and Villages 
4848 en An integrated approach to PTW road safety 
4849 en European R&D in the field of secondary (or passive) safety 
4850 en Using micro-stimulation modelling for driver assistance system assessment 
4851 en NICHES: Mainstreaming innovation and promoting innovative mobility concepts 
4852 en Advanced city cars, PRT and cyber cars, new froms of urban transportation 
4853 en Polluition reduction with new vehicle technologies in urban logistics - FIDEUS field tests in the City of Hanover 
4854 en Moderator 
4855 en Sevecom Project – Secure of future vehicle communication networks 
4856 en eValue project (+ ASTE) 
4857 en Security issues related to the use of cooperative systems - eSecurity WG of eSafety 
4858 en Good Route Project – Transportation of dangerous goods: routing, monitoring and enforcement 
4859 en Panel Discussion 
4860 en Moderator 
4861 en AC+DC - Achievements for Global Supply Chains 
4862 en Super Light Car - Sustainable Production Technologies of Emission reduced Light weight Car Concepts 
4863 en ECODISM - A New concept for an easy dismantling of structural bonded joints in auto elv and repair 
4864 en LITEBUS - Modular Lightweight Sandwich Bus Concept 
4865 en Panel Discussion 
4866 en ERA-Net Transport 
4867 en ERA-Net Road 
4868 en COST Transport 
4869 en Questions 
4870 en Future scenarios for freight transport through Alpine area 
4871 en Scenarios, policies and impacts for the linked transport and energy systems - results of the European TRIAS project 
4872 en Public Participation and Dialogue in the Road-Planning and Road-Design Process – Example Bypass Norrtälje in Sweden 
4873 en SatCom as part of the operational implementation of ITS applications – The SISTER Project 
4874 en Roads to the past: The management of cultural heritage constraints on Irish National Road schemes 
4875 en National plan for the protection of roads, bridges and associated cultural relics 
4876 en Modelling the spatial parameters for dynamic road pricing 
4877 en Traffic management at leisure destinations in the countryside: an integrated approach for transportation and tourism planning 
4878 en Subjectivity and Sentiment Analysis 
4879 en Graph Mining Techniques for Social Media Analysis 
4880 en Decentralization and Interop: past, present, future 
4881 en Link-PLSA-LDA: A new unsupervised model for topics and influence of blogs 
4882 en Competing to Share Expertise: the Taskcn Knowledge Sharing Community 
4883 en Finding Influencers and Consumer Insights in the Blogosphere 
4884 en What Elements of an Online Social Networking Profile Predict Target-Rater Agreement in Personality? 
4885 en Spontaneous Inference of Personality Traits from Online Profiles 
4886 en The Psychology of Word Use in Depression Forums in English and in Spanish 
4887 en Thin Slices of Online Profile Attributes 
4888 en Document Representation and Query Expansion Models for Blog Recommendation 
4889 en Polling the Blogosphere: a Rule-Based Approach to Belief Classification 
4890 en International Sentiment Analysis for News and Blogs 
4891 en Social dimension of social media 
4892 en Wikipedian Self-Governance in Action: Motivating the Policy Lens 
4893 en A Large-Scale Study of MySpace: Observations and Implications for Online Social Networks 
4894 en Space Planning for Online Community 
4895 en Scaling Innovation in a Network Effect World 
4896 en Recovering Implicit Thread Structure in Newsgroup Style Conversations 
4897 en A Social Network Based Approach to Personalized Recommendation of Participatory Media Content 
4898 en Wikipedia as an Ontology for Describing Documents 
4899 en On TREC Blog Track 
4900 en BLEWS: Using Blogs to Provide Context for News Articles 
4901 en Exploring Social Media Scenarios for the Television 
4902 en The Politics of Sourcing: A Study of Journalistic Practices in the Blogosphere 
4903 en Politics and Social Media 
4904 en Closing Remarks 
4907 en The i2010 Agenda – a European Success Story in the Making 
4908 en Position of the Industry in Slovenia 
4909 en The Future of Internet 
4910 en Challenge of NGN Networks and Services 
4911 en Universal Access to All Knowledge - Archive.org Advances in computing and communications mean that we can cost-effectively store every book, sound recording, movie, software package, and public web page ever created, and provide access to these collections via the Internet to students and adults all over the world. By mostly using existing institutions and funding sources, we can build this as well as compensate authors within what is the current worldwide library budget. The talk offers an update on the current state of progress towards that ideal, which would allow us to bequeath an accessible record of our cultural heritage to our descendants.
4912 en EICTA - Building digital Europe, Moving Towards a Very High Speed Europe 
4913 en Alternative Access Technologies 
4914 en Converged Broadband Systems 
4915 en Debate 
4916 en Deployment of Fibre 
4917 en Debate 
4918 en The Impact of important Single Market policies on the developement of Pan-European Services and Products 
4919 en Electronic Invoicing - 238 bilion reasons - to begin with... 
4920 en How can pan-European Public Services Benefit from CIP ICT PSP Pilot on eID 
4921 en E-health 
4923 en Connecting public service communities 
4924 en Innovation and Trust 
4925 en Citizen Centric Approach 
4926 en Building Consumer Confidence by Ensuring Consumer protection 
4927 en How much are Computers able to Understand text? 
4928 en Conclusions and outcomes 
4929 en Closing speech 
4930 en Opening Ceremony 
4931 en Opening Ceremony 
4932 en Opening Ceremony 
4933 en The picture of Europe 
4936 en Interview 
4939 en Interview 
4940 en Interview 
4941 en Interview 
4942 en Interview 
4943 en Interview The discussion with the person who helped to put AIESEC on the world map. AIESEC is an international, non-profit, non-political, organisation run by students and recent graduates of institutions of higher education. It describes itself as “The international platform for young people to discover and develop their potential so as to have a positive impact on society”. Its international office is currently in Rotterdam, Netherlands. The AIESEC network as of February 2008 includes 25,000 students in 105 countries at over 1100 universities across the globe, and realizes around 5000 exchanges yearly
4944 en I am an AIESEC alumnus and this is the story of our organization 
4945 en AIESEC Today 
4946 en AIESEC - a case of optimism in the world 
4947 en Interview 
4948 en Amazon Web Services Amazon has spent over a decade and $2 billion building the infrastructure, technical knowledge, and operational excellence to operate a world class web-scale computing platform. Amazon Web Services (AWS) has now released a variety of web services (programmatic access to its open APIs) that provide access to Amazon's robust infrastructure, easily and inexpensively. These fundamental services allow developers and their companies to build web applications in a reliable, scalable, and cost-effective manner. \\ [[http://aws-portal.amazon.com/|Amazon Web Services @ Amazon.com]]
4951 en Goals and Introduction This course is intended to give newcomers enough background in the field of **HCI **\\ to make their conference experience much more meaningful. It provides \\ a framework to understand how the various topics are related to research and practice. It is a tried-and true introduction and has become a CHI conference tradition.
4952 en Psychology in Human-Computer Interaction This course is intended to give newcomers enough background in the field of HCI to make their conference experience much more meaningful. It provides a framework to understand how the various topics are related to research and practice. It is a tried and true introduction and has become a CHI conference tradition.
4953 en Computer Science and Human-Computer Interaction This course is intended to give newcomers enough backgroundin the field of HCI to make their conference experience much more meaningful. It provides a framework to understand how the various topics are related to research and practice. It is a tried-andtrue introduction and has become a CHI conference tradition.
4954 en Communication Chains and Multitasking Observations revealed that information workers interact in“chains” of interactions, switching organizational contexts and communication mediums. We investigate how communication chains affect workplace stress and discuss chains as alignmentwork.
4955 en Effects of Intelligent Notification Management on Users and Their Tasks Reports the first empirical and behavioral results on how scheduling notifications using an automated system impacts interruption costs in practical settings. Results motivate new directions for research in interruption management.
4956 en Attention By Proxy? Issues in Audience Awareness for Webcasts to Distributed Groups Presents findings from a study of how classroom instructors pay attention to their students, and implications for displaying images of remote participants in classes with both local and remote students.
4958 en The Cost of Interrupted Work: More Speed and Stress Our study showed interrupted tasks are completed faster, with similar quality. We suggest that people compensate for interruptions by working faster, but at a price: higher stress, frustration, and pressure.
4959 en Measuring Trust in Wi-Fi Hotspots We describe a novel experimental methodology to measure trustin Wi-Fi hotspots. We found that decisions to access an unfamiliar hotspot may turn on locative images in its home page.
4960 en Undercover: Authentication Usable in Front of Prying Eyes We propose the first authentication scheme to rely on the humanability to combine different sensory inputs. We demonstrate thesystem is usable and resilient to eavesdropping and other attacks.
4961 en Access Control by Testing for Shared Knowledge We propose sharers of photos, blogs, etc. protect content withconcise questions like “What is Dad’s favorite phrase?” ratherthan explicit authenticated white/blacklists. Tested via attack byMechanical Turk workers.
4962 en Breaking the Disposable Technology Paradigm: Opportunities for Sustainable Interaction Design for Mobile Phones What prompts people to replace devices, and what do people do with them when they stop using them? We explore these issues to inform the design of sustainable mobile phones.
4963 en Sustainable Millennials: Attitudes towards Sustainability and the Material Effects of Interactive Technologies This paper describes the design and interprets the results of asurvey of 435 undergraduate students concerning the attitudes of this mainly millennial population towards sustainability apropos of the material.
4964 en Aligning Temporal Data by Sentinel Events: Discovering Patterns in Electronic Health Records Finding prevalent precursor, co-occurring, and after effect event sin patient records is difficult. We demonstrate how temporal alignment can effectively facilitate discovery of patterns incategorical data.
4965 en Celebratory Technology: New Directions for Food Research in HCI We describe the existing and potential HCI design space around human-food interaction and in particular motivate future researchon designing technology that celebrates the positive interactions people have around food.
4966 en MAHI: Investigation of Social Scaffolding for Reflective Thinking in Diabetes Management We describe the design and deployment of MAHI, a ubicompapplication for individuals with diabetes, and its effect on individuals’ ownership of their health and disease management.
4967 en Automatic Whiteout++: Correcting Mini-QWERTY Typing Errors Using Keypress Timing By analyzing features of users typing, Automatic Whiteout ++ detects and corrects up to 32.37% of the errors made by typists while using a mini-QWERTY (RIM Blackberry style) keyboard.
4968 en EdgeWrite with Integrated Corner Sequence Help A help system that informs users of the character shapes in the EdgeWrite text entry system was tested. It can replace printed character charts without hurting novice performances.
4969 en Interlaced QWERTY - Accommodating Ease of Visual Search and Input Flexibility in Shape Writing We present iQwerty, which offers excellent separation of wordshapes for shape writing text input, while maintaining low visual search time. Many findings also apply to traditional touch screen keyboards.
4970 en CareLog: A Selective Archiving Tool for Behavior Management in Schools Identifying the function of problem behavior can lead to the development of more effective interventions. One way to identifythe function is through functional behavior assessment (FBA).
4971 en Observing Presenters' Use of Visual Aids to Inform the Design of Classroom Presentation Software We conducted an observational study to examine current practice with both traditional blackboards and computer slides, with the ultimate goal of designing rich presentation tools for high resolution and multiple screens.
4972 en Readability of Scanned Books in Digital Libraries Readability of scanned picture books in the International Children’s Digital Library is improved by using computer vision and DHTML technologies to separate the text from the illustrations.
4973 en Collaborating to Remember: A Distributed Cognition Account of Families Coping with Memory Impairments We study ten families coping with the consequences of amnesia. By applying distributed cognition theory, we show that the families work together as “cognitive systems” to combat their memory issues.
4974 en Feasibility and Pragmatics of Classifying Working Memory Load with an Electroencephalograph We demonstrate high accuracies classifying working memory load using an electroencephalograph (EEG), even with little temporallag, not much training data, and a small number of EEG channels.
4975 en Human-Aided Computing: Utilizing Implicit Human Processing to Classify Images Human-Aided Computing uses an electroencephalograph (EEG)device to measure the outcomes of implicit cognitive processingto perform image classification, even when users are not explicitlyperforming the task.
4976 en Augmented Information Assimilation: Social and Algorithmic Web Aids for the Information Long Tail This study examines how users integrate new World Wide Webservices, such as social bookmarking, with everyday information assimilation practices.
4977 en What to Do When Search Fails: Finding Information by Association Feldspar lets people find personal information on their computer by specifying chains of associated information as queries, emulating the retrieval process of human associative memory.
4978 en Conversation Pivots and Double Pivots Conversation pivots and double pivots allow readers to navigate from pages about items to relevant conversations and to other items that are mentioned in the same conversations.
4979 en Query Suggestions for Mobile Search: Understanding Usage Patterns Entering search terms on mobile phones is a time consuming and cumbersome task. In this paper, we explore the usage patterns of query entry interfaces that display suggestions.
4980 en Topobo in the Wild: Longitudinal Evaluations of Educators Appropriating a Tangible Interface What issues arise when designing and deploying tangibles for learning in long term evaluations?
4981 en You Can Touch, but You Can't Look?: Interacting with In-Vehicle Systems Interacting with in-vehicle systems while driving a car can be dangerous. We examine driver attention and driving behavior for three different interaction techniques.
4982 en Touchers' and 'Mousers': Commonalities and Differences in Co-located Collaboration with Multiple Input Devices We present a qualitative analysis of multiple touch/mouse input device trajectories. We observe balanced selection of preferred device and reveal more commonalities than differences in interaction patterns on tabletops.
4983 en Information Distance and Orientation in Liquid Layout After accounting for vertical scrolling, horizontally separated portlets provided a 5-25% task completion time savings over vertically separated portlets in a study that provided guidelines for liquid page layout.
4984 en Impact of Screen Size on Performance, Awareness, and User Satisfaction With Adaptive Graphical User Interfaces We conducted a study to compare adaptive interfaces for small versus desktop-sized screens, showing that high accuracy adaptive menus have a relatively larger positive impact for small screen displays.
4985 en Improving the Performance of Motor-Impaired Users with Automatically-Generated, Ability-Based Interfaces Interacting with in-vehicle systems while driving a car can be dangerous. We examine driver attention and driving behavior for three different interaction techniques.
4986 en Evaluation of a Role-Based Approach for Customizing a Complex Development Environment In an interview study with software developers, we identify challenges of designing coarse-grained approaches to customization. The findings highlight potentially critical design choices and provide direction for future research.
4987 en Predictability and Accuracy in Adaptive User Interfaces We examine the relative effects of predictability and accuracy of the adaptive algorithm on the usability of user interfaces that automatically adapt to users’ tasks.
4988 en BlindSight: Eyes-Free Access to Mobile Phones BlindSight allows mobile phone users to access phone calendarand contacts in situations where they cannot look at the screen, such as while talking on the phone or while driving. Based on auditory feedback.
4989 en One-Handed Touchscreen Input for Legacy Applications We present two controlled studies aimed at understanding therelative tradeoffs that five different input methods (includingdirect and indirect) offer for operating dense mobile touchscreeninterfaces with one hand.
4990 en Target Acquisition with Camera Phones when used as Magic Lenses We examine target acquisition in magic lens pointing with cameraphones.We present a Fitts’ law extension that models the performance and show that dynamic peephole pointing follows the standard Fitts’ law.
4991 en Cross-channel Mobile Social Software: an empirical study We discuss the usage, design and effects of a cross-channel, mobile tool to support group socializing, and describe how it is different to other forms of group communication.
4993 en Use and Reuse of Shared Lists as a Social Content Type We describe the design, use, and reuse of shared lists in asocial networking system. Our findings suggest that users socialize more around lists than photos, and use lists for selfrepresentation.
4994 en How Accurate must an In-Car Information System be? Consequences of Accurate and Inaccurate Information in Cars Talking Cars should know their Drivers: Investigating how Inaccurate Information in Speech based In-Vehicle Information Systems affects Different Drivers’ Attitude and Driving Performance.
4995 en In-Car GPS Navigation: Engagement with and Disengagement from the Environment We explore how in-car GPS navigation leads to new forms of engagement and disengagement with the external environment. Fieldwork and theoretical framing lead to design suggestions for enriching engagement.
4996 en In-car Interaction Using Search-Based User Interfaces As in-vehicle information systems (IVIS) become more complexand offer extended functionalities, our research deals with searchbased user interfaces for IVIS. We explore their suitability for safeand efficient use.
4997 en Evaluating Motion Constraints for 3D Wayfinding in Immersive and Desktop Virtual Environments Studies the benefit of motion constraints for assisting 3D navigation in desktop and immersive environments. Presents a technique for desktop computers that outperforms immersive displays in terms of wayfinding efficiency.
4998 en Navigation Techniques for Dual-Display E-Book Readers We present the design and evaluation of a dual-display electronic reader, which supports a wide range of reading activities like lightweight navigation, global search and multi-documentinter actions.
4999 en Idea Navigation: Structured Browsing for Unstructured Text Don’t search for keywords! Search for ideas! Our system extracts subject-verb-object triples from unstructured text, groups them into hierarchies, and allows iterative refinement to findexactly what you want.
5000 en Rendering Navigation and Information Space with HoneyCombTM We introduce the HoneyComb, an information visualization and browsing paradigm. We present the design objectives, a prototype implementation, and the results of an evaluation in the context of search and browsing.
5060 en Chinese Rings and Hanoi Tower Graphs 
5061 en 7 Habits For Effective Text Editing 2.0 A large percentage of time behind the computer screen is spent on editing text. Investing a little time in learning more efficient ways to use a text editor pays itself back fairly quickly. This presentation will give an overview of the large number of ways of using Vim in a smart way to edit programs, structured text and documentation. Examples will be used to make clear how learning a limited number of habits will avoid wasting time and lower the number of mistakes. \\ [[bram_moolenaar]] is mostly known for being the benevolent dictator of the text editor Vim. His roots are in electrical engineering and for a long time he worked on inventing image processing algorithms and software for big photo copying machines. At some point his work on Open-Source software became more important, making the development of Vim his full time job. He also did the A-A-P project in between Vim version 6.0 and 7.0. Now he works for Google in Zurich, still improving Vim on the side. \\ His home page is [[http://www.moolenaar.net/]]. A [[http://en.wikipedia.org/wiki/Bram_Moolenaar|Wikipedia]] article is also available.
5064 en Document Freedom 
5067 en OpenMoko - A hackable, Open Source PDA phone 
5074 en Processes in Linux kernel: current and future/latest technology 
5075 en OpenPCD - Free software and hardware RFID reader 
5076 en Interview with Machtelt Garrels **Machtelt Garrels is a Linux veteran and freelance consultant/trainer**, she is the author of books such as Introduction to Linux - A hands-on Guide, Bash Guide for Beginners, Drupal Installation Guide, etc. All of them freely downlodable. \\nThe Videolectures.NET cheif software developer **[[http://videolectures.net/peter_kese/|Peter Kese]]** talked to her in Zagreb ta a Linux event where he asked her:n *What brought you to Zagreb?n *Which reading regarding Linux are the most popular in this moment?n *What is the most usefull ammunition that you use for the ODF propaganda?n *The next step is the Eu level?n *Family issues and Linux... nnCheck out another online interview by [[http://www.profoss.eu/events/june-2008-openoffice/speakers/machtelt-garrels/interview/|Machtelt]]\\n//The interview was terminated a bit too early by the conference organizers.//
5077 en Linux kernel development future, 2008 
5078 en OpenBSD 4.3 
5079 en PERL usage and PERL 6 
5080 en How to create your own language in 20 minutes 
5081 en Eduroam Service 
5082 en Telepathy 
5083 en Clusters with GlusterFS 
5085 en VPN bez muke: OpenVPN 
5097 en Transmission electron microscopy for nanomaterial characterization, how far can we go? In a first part, I will briefly survey how a magnetic field can be used in order to focus an electron beam and then how a transmission electron microscope works. Recent experimental developments such as the correction of the spherical aberration of electron lenses and the resulting atomic resolution will be showed. I will then present how this techniques are now widely used in the order to solve various problematics such as the imaging of nanostructures (from the industrial oriented characterisation of multilayers to the imaging of nanoparticles into organic matter) or the structural refinement of crystalline or defective systems (such as bonding at interfaces or the topological description of defects in functionalised nanotubes). Finally I will discuss how combining electron spectroscopy and microscopy, physical or chemical properties such as the oxidation state of transition metal oxide nanoparticles or the plasmonic properties of metallic clusters can be investigated.
5098 en Videolectures.Net - Past, Pizza, Future ;This is a recording of the lunch seminar at the JSIn ://We had a Past,n ://we are just having a Pizzan ://and we have a Future...n :This seminar is an overview of the problems we were facing a few months ago...
5099 en UI for Research Desktop/Analysis of Web Site Structures At the seminar there will presented two topics: (1) User interface design for Research Desktop (2) Social Network Analysis of Web Site Structures
5102 en Formalisation of Science: Ontology Based Projects in Aberystwyth University At Aberystwyth University, we have several projects for formalizing descriptions of scientific investigations: An ontology for a robot scientist, An ontology for drug design, An ontology for description of protocols, ... In my talk, I will describe the state-of-the-art for ontology development in these projects and prospects for ontology applications in science.
5105 en The Adobe RIA Platform Get first hand information on what the Adobe rich internet application platform has to offer. Enrique will inspire you with real-life examples running on Flex and AIR.
5108 en Towards A More Meaningful Web The Web of today is near the edge of its utility. Information sources and services are increasingly fragmented and plentiful, Google's metaphor of a page and a link is outdated. Web 2.0 is over. The talk will review some of the fresh concepts such as subjective relevance, user-centric data management, individual reputation and outline how those are applied to the upcoming Noovo platform.
5123 en Early-stage venture capital investing; shaping successful high tech startup companies towards accelerated exits Yoav Andrew Leitersdorf, an entrepreneur and venture capitalist who founded, grew and sold three successful startups discusses his experiences with his own companies and some issues that have been plaguing venture capital financing since the last bubble burst. He then discusses his firm's innovative approach to financing which involves calibrating investments to today's capital efficient startups and widening the exit window down to short-term, medium-size exits.
5125 en Start Your Startup, the European Way With thousands of new services coming out each and every year, competition is fierce. But opportunities are there, and probably there are more opportunities then ever. Ideas enough, but how do you know on what idea you should work on? This talk is about the web, its facilitators, passion and how to live your dream.
5126 en Can you parametrize happiness 
5127 en Your international customers are waiting for you Providing services and products for an international market is easier than you may think, while at the same time there are a lot of pitfalls you need to be aware of. But once you apply certain ways of thinking, making business with lucrative markets will be way easier. Using the German market as an example, this session will show you ways to think different and provide you with tips and ideas for your dream target market.
5132 en Panel discussion: Venture Capital in the Region 
5133 en Business Angel 
5134 en All Business Angels - Panel discussion 
5140 en Seedcamp 
5141 en Zemanta - Ljubljana, London, the World - Blog me up! What Zemanta learned from Seedcamp? Why moving to London was necessary? How to dip a toe into international web scene... cheaply? What Zemanta is doing and how we see the future of content authoring?
5143 en Exceptional Model Mining In most databases, it is possible to identify small partitions of the data where the observeddistribution is notably different from that of the database as a whole. In classical subgroup discovery, one considers the distribution of a single nominal attribute, and exceptional subgroups show a surprising increase in the occurrence of one of its values. In this talk, I'll introduce Exceptional Model Mining (EMM), a framework that allows for more complicated target concepts. Rather than finding subgroups based on the distribution of a single target attribute, EMM finds subgroups where a model fitted to that subgroup is somehow exceptional. I'll discuss regression as well as classification models, and define quality measures that determine how exceptional a given model on a subgroup is. Our framework is general enough to be applied to many types of models, even from other paradigms such as association analysis and graphical modeling.
5157 en NeOn Glowfest NeOn is a 14.7 million Euros project involving 14 European partners and co-funded by the European Commission’s Sixth Framework Programme under grant number IST-2005-027595. NeOn started in March 2006 and has a duration of 4 years. Our aim is to advance the state of the art in using ontologies for large-scale semantic applications in the distributed organizations. Particularly, we aim at improving the capability to handle multiple networked ontologies that exist in a particular context, are created collaboratively, and might be highly dynamic and constantly evolving. The first release of the NeOn Toolkit, one of the core outcomes of the NeOn project, is available for download and testing from the [[http://www.neon-toolkit.org/| NeOn Toolkit & Community site]].
5159 en Expected plugins for the new version of NeOn toolkit The NeOn toolkit is an extensible Ontology Engineering Environment. It is part of the reference implementation of the NeOn architecture. It contains plugins for ontology management and visualization. The core features include: * Basic Editing: Editing Schemasplash.jpg * Visualization/Browsing * Import/Export: F-Logic, (subsets of) RDF(S) and OWL A number of commercial plugins extend the toolkit by various functionalities, including: * Rule Support: Graphical/Textual editing, debugging * Mediation: Graphical Mapping Editor, life-interpretation of mappings * Database Integration: Database schema import, database-access * Queries: Query-Editor and persistent queries
5160 en The Watson plugin for the Neon toolkit Watson is a Semantic Web gateway. In a way similar to seacrh engines on the Web, Watson collects, analyses and indexes semantic documents (in RDF, OWL, DAML+OIL) in order to provide a variety of access mechanisms to this semantic data for intelligent applications. The idea is to support applications that requires to dynamically find, select and exploit the increasing amount of knowledge available online (that we call next generation semantic web applications). A more complete description of Watson is available on its website, and the latest development and news concerning Watson are generally announced on the Watson Blog. A number of applications have already been developed that dynamically exploit online knowledge thanks to Watson. In particular, I am (with the help of Fouad Zablith) in charge of the development of plugins for ontology editors that allow for a large scale reuse of existing knowledge on the Web. I also (quickly) developed an application called gowgle that suggests additional keywords to a query addressed to Google using the semantic relations linking these terms in online ontologies. I also contributed to the development of Scarlet, a relation discovery engine used in particular in ontology matching, and of PowerMagpie a Semantic browser based on Watson.
5162 en Optoacoustic imaging, a promising technique for non-invasive diagnosis of cancer Optoacoustics provides high optical contrast without the handicap of poor resolution in imaging of optically turbid tissues. In biomedical optoacoustics, tissue is illuminated with short laser pulses. The light is scattered inside the tissue and heats absorbing structures, such as blood vessels, hidden deeply inside the tissue. Image contrast is therefore provided by light absorbing chromophores, either endogenous (such as oxy- or deoxyhemoglobin) or exogenous (e.g. dyes, nanoparticles or quantum dots). By means of the thermoelastic effect, the inhomogeneous heating generates pressure transients exactly representing the absorbing structures. These acoustic transients propagate to the tissue surface and can be detected with an appropriate ultrasound transducer. In one-dimensional optoacoustic measurements, time delay between the laser pulse and detected pressure transient, its amplitude and temporal profile provide information about the location, strength and spatial dimension of the acoustic source. Three-dimensional images can be reconstructed by scanning the transducer. The image quality depends on a number of factors, including the irradiation geometry and image reconstruction algorithm. The talk will give an overview of the possibilities and limitations of optoacoustic imaging in turbid tissues, especially in terms of image contrast and depth resolution.
5163 en Knowledge Discovery in Life Sciences: overview, case studies, complexities, and lessons learned Knowledge discovery is the process of developing and applying strategies to discover useful and ideally all previously unknown knowledge from historical or real-time data. Applied to biological and life sciences data, knowledge discovery processes will help in various research and development activities, such as (i) studying data quality for possible anomalous or questionable expressions of certain genes or experiments, (ii) identifying relationships between genes and their functions based on time-series or other high throughput genomics profiles, (iii) investigating gene responses to treatments under various experimental conditions such as in-vitro or in-vivo studies, and (iv) discovering models for accurate diagnosis/classifications based on expression profiles among two or more classes. This presentation consists of three parts. In part one, we provide an overview of knowledge discovery focusing on bioinformatics domain and describe the BioMine project where we share our experiences on initiating and managing a data mining project involving several groups. In part two of this talk, we describe a few of our case studies using some existing or newly developed methods. These are all cases in which real genomics data sets (obtained from public or private sources) have been used for tasks such as gene function identification and gene response analysis. In the last part of this talk, we will describe complexities and challenges in dealing with real data, demonstrate important areas that need to be carefully understood in a typical data mining application, and share some of our experiences gained over the past 7 years.
5172 en Tailoring Density Estimation via Reproducing Kernel Moment Matching Moment matching is a popular means of parametric density estimation. We extend this technique to nonparametric estimation of mixture models. Our approach works by embedding distributions into a reproducing kernel Hilbert space, and performing moment matching in that space. This allows us to tailor density estimators to a function class of interest (i.e., for which we would like to compute expectations). We show our density estimation approach is useful in applications such as message compression in graphical models, and image classification and retrieval.
5173 en Nonextensive Entropic Kernels Positive definite kernels on probability measures have been recently applied in structured data classification problems. Some of these kernels are related to classic information theoretic quantities, such as mutual information and the Jensen-Shannon divergence. Meanwhile, driven by recent advances in Tsallis statistics, nonextensive generalizations of Shannon’s information theory have been proposed. This paper bridges these two trends. We introduce the Jensen-Tsallis q-difference, a generalization of the Jensen-Shannon divergence. We then define a new family of nonextensive mutual information kernels, which allow weights to be assigned to their arguments, and which includes the Boolean, Jensen-Shannon, and linear kernels as particular cases. We illustrate the performance of these kernels on text categorization tasks.
5174 en Nu-Support Vector Machine as Conditional Value-at-Risk Minimization The nu-support vector classification (nu-SVC) algorithm was shown to work well and provide intuitive interpretations, e.g., the parameter nu roughly specifies the fraction of support vectors. Although nu corresponds to a fraction, it cannot take the entire range between 0 and 1 in its original form. This problem was settled by a non-convex extension of nu-SVC and the extended method was experimentally shown to generalize better than original nu-SVC. However, its good generalization performance and convergence properties of the optimization algorithm have not been studied yet. In this paper, we provide new theoretical insights into these issues and propose a novel nu-SVC algorithm that has guaranteed generalization performance and convergence properties
5175 en A Generalization of Haussler's Convolution Kernel - Mapping Kernel Haussler's convolution kernel provides a successful framework for engineering new positive semidefinite kernels, and has been applied to a wide range of data types and applications. In the framework, each data object represents a finite set of finer grained components. Then, Haussler's convolution kernel takes a pair of data objects as input, and returns the sum of the return values of the predetermined primitive kernel calculated for all the possible pairs of the components of the input data objects. Due to the definition, Haussler's convolution kernel is also known as the cross product kernel, and is positive semidefinite, if so is the primitive kernel. On the other hand, the mapping kernel that we introduce in this paper is a natural generalization of Haussler's convolution kernel, in that the input to the primitive kernel moves over a predetermined subset rather than the entire cross product. Although we have plural instances of the mapping kernel in the literature, their positive semidefiniteness was investigated in case-by-case manners, and worse yet, was sometimes incorrectly concluded. In fact, there exists a simple and easily checkable necessary and sufficient condition, which is generic in the sense that it enables us to investigate the positive semidefiniteness of an arbitrary instance of the mapping kernel. This is the first paper that presents and proves the validity of the condition. In addition, we introduce two important instances of the mapping kernel, which we refer to as the size-of-index-structure-distribution kernel and the edit-cost-distribution kernel. Both of them are naturally derived from well known (dis)similarity measurements in the literature (e.g. the maximum agreement tree, the edit distance), and are reasonably expected to improve the performance of the existing measures by evaluating their distributional features rather than their peak (maximum/minimum) features.
5176 en Hierarchical sampling for active learning We present an active learning scheme that exploits cluster structure in data.
5177 en Active Kernel Learning Identifying the appropriate kernel function/matrix for a given dataset is essential to all kernel-based learning techniques. In the past, a number of kernel learning algorithms have been proposed to learn kernel functions or matrices from side information, in the form of labeled examples or pairwise constraints. However, most previous studies are limited to the "passive" kernel learning in which the side information is provided beforehand. In this paper we present a framework of "Active Kernel Learning" (AKL) that is able to actively identify the most informative pairwise constraints for kernel learning. The key challenge of active kernel learning is how to measure the informativeness of each example pair given its class label is unknown. To this end, we propose a min-max approach for active kernel learning that selects the example pairs that will lead to the largest classification margin even when the class assignments to the selected pairs are incorrect. We furthermore approximate the related optimization problem into a convex programming problem. We evaluate the effectiveness of the proposed active kernel learning algorithm by comparing it with two other implementations of active kernel learning. Empirical study with nine datasets on data clustering shows that the proposed algorithm is considerably more effective than its competitors.
5178 en Actively Learning Level-Sets of Composite Functions Scientists frequently have multiple types of experiments and data sets on which they can test the validity of their parametrized models and locate plausible regions for the model parameters. By examining multiple data sets, these scientists can obtain inferences for their problems which typically are much more informative than the deductions derived from each of the data sources independently. Several standard data combination techniques result in a target function which is a weighted sum of the observed data sources. Computing constraints on the plausible regions of the model parameter space can be formulated as that of finding a specified level set of the target function. We propose an active learning algorithm for this problem which at each step selects both a parameter setting (from the parameter space) and an experiment type upon which to compute the next sample. Empirical tests on synthetic functions and on real data for a eight parameter cosmological model show that our algorithm significantly reduces the number of samples required to identify desired regions.
5179 en Learning from Incomplete Data with Infinite Imputations We address the problem of learning decision functions from training data in which some attribute values are unobserved. This problem can arise for instance, when training data is aggregated from multiple sources, and some sources record only a subset of attributes. We derive a joint optimization problem for the final classifier in which the distribution governing the missing values is a free parameter. We show that the optimal solution concentrates the density mass on finitely many atoms, and provide a corresponding algorithm for learning from incomplete data. We report on empirical results on benchmark data, and on the email spam application that motivates the problem setting
5185 en Accurate Max-margin Training for Structured Output Spaces Tsochantaridis et al 2005 proposed two formulations for maximum margin training of structured spaces: margin scaling and slack scaling. While margin scaling has been extensively used since it requires the same kind of MAP inference as normal structured prediction, slack scaling is believed to be more accurate and better-behaved. We present an efficient variational approximation to the slack scaling method that solves its inference bottleneck while retaining its accuracy advantage over margin scaling. We further argue that existing scaling approaches do not separate the true labeling comprehensively while generating violating constraints. We propose a new max-margin trainer PosLearn that generates violators to ensure separation at each position of a decomposable loss function. Empirical results on real datasets illustrate that PosLearn can reduce test error by up to 25%. Further, PosLearn violators can be generated more efficiently than slack violators; for many structured tasks the time required is just twice that of MAP inference.
5186 en Training Structural SVMs when Exact Inference is Intractable While discriminative training (e.g., CRF, structural SVM) holds much promise for machine translation, image segmentation, and clustering, the complex inference these applications require make exact training intractable. This leads to a need for approximate training methods. Unfortunately, knowledge about how to perform efficient and effective approximate training is limited. Focusing on structural SVMs, we provide and explore algorithms for two different classes of approximate training algorithms, which we call undergenerating (e.g., greedy) and overgenerating (e.g., relaxations) algorithms. We provide a theoretical and empirical analysis of both types of approximate trained structural SVMs, focusing on fully connected pairwise Markov random fields. We find that models trained with overgenerating methods have theoretic advantages over undergenerating methods, are empirically robust relative to their undergenerating brethren, and relaxed trained models favor non-fractional predictions from relaxed predictor
5187 en Discriminative Structure and Parameter Learning for Markov Logic Networks Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing methods for learning the logical structure of an MLN are not discriminative; however, many relational learning problems involve specific target predicates that must be inferred from given background information. We found that existing MLN methods perform very poorly on several such ILP benchmark problems, and we present improved discriminative methods for learning MLN clauses and weights that outperform existing MLN and traditional ILP methods.
5188 en Laplace Maximum Margin Markov Networks Learning sparse Markov networks based on the maximum margin principle remains an open problem in structured prediction. In this paper, we proposed the Laplace max-margin Markov network (LapM3N), and a general class of Bayesian M3N (BM3N) of which the LapM3N is a special case and enjoys a sparse representation. The BM3N is built on a novel Structured Maximum Entropy Discrimination (SMED) formalism, which offers a general framework for combining Bayesian learning and max-margin learning of log-linear models for structured prediction, and it subsumes the unsparsified M3N as a special case. We present an efficient iterative learning algorithm based on variational approximation and existing convex optimization methods employed in M3N. We show that our method outperforms competing ones on both synthetic and real OCR data.
5189 en Fast Estimation of Relational Pattern Coverage through Randomization and Maximum Likelihood In inductive logic programming, theta-subsumption is a widely used coverage test. Unfortunately, testing theta-subsumption is NP-complete, which represents a crucial efficiency bottleneck for many relational learners. In this paper, we present a probabilistic estimator of clause coverage, based on a randomized restarted search strategy. Under a distribution assumption, our algorithm can estimate clause coverage without having to decide subsumption for all examples. We implement this algorithm in program ReCovEr. On generated graph data and real-world datasets, we show that ReCovEr provides reasonably accurate estimates while achieving dramatic runtimes improvements compared to a state-of-the-art algorithm
5190 en Fast Gaussian Process Methods for Point Process Intensity Estimation Point processes are difficult to analyze because they provide only a sparse and noisy observation of the intensity function driving the process. Gaussian Processes offer an attractive theoretical framework by which to infer optimal estimates of these underlying intensity functions. The result of this inference is a continuous function defined across time that is typically more amenable to analytical efforts. However, a naive implementation of this intensity estimation will become computationally infeasible in any problem of reasonable size, both in memory and run-time requirements. We demonstrate problem specific methods for a class of renewal processes that eliminate the memory burden and reduce the solve time by orders of magnitude.
5191 en Gaussian Process Product Models for Nonparametric Nonstationarity Stationarity is often an unrealistic prior assumption for Gaussian process regression. One solution is to predefine an explicit nonstationary covariance function, but such covariance functions can be difficult to specify and require detailed prior knowledge of the nonstationarity. We propose the Gaussian process product model (GPPM) which models data as the pointwise product of two latent Gaussian processes to nonparametrically infer nonstationary variations of amplitude. This approach differs from other nonparametric approaches to covariance function inference in that it operates on the outputs rather than the inputs, resulting in a significant reduction in computational cost and required data for inference, while improving scalability to high-dimensional input spaces. We present an approximate inference scheme using Expectation Propagation. This variational approximation yields convenient GP hyperparameter selection and compact approximate predictive distributions.
5192 en Sparse Multiscale Gaussian Process Regression Most existing sparse Gaussian process (g.p.) models seek computational advantages by basing their computations on a set of m basis functions that are the covariance function of the g.p. with one of its two inputs fixed. We generalise this for the case of Gaussian covariance function, by basing our computations on m Gaussian basis functions with arbitrary diagonal covariance matrices (or length scales). For a fixed number of basis functions and any given criteria, this additional flexibility permits approximations no worse and typically better than was previously possible. We perform gradient based optimisation of the marginal likelihood, which costs O(m2n) time where n is the number of data points, and compare the method to various other sparse g.p. methods. Although we focus on g.p. regression, the central idea is applicable to all kernel based algorithms, and we also provide some results for the support vector machine (s.v.m.) and kernel ridge regression (k.r.r.). Our approach outperforms the other methods, particularly for the case of very few basis functions, i.e. a very high sparsity ratio.
5193 en Topologically-Constrained Latent Variable Models In dimensionality reduction approaches, the data are typically embedded in a Euclidean latent space. However for some data sets this is inappropriate. For example, in human motion data we expect latent spaces that are cylindrical or a toroidal, that are poorly captured with a Euclidean space. In this paper, we present a range of approaches for embedding data in a non-Euclidean latent space. Our focus is the Gaussian Process latent variable model. In the context of human motion modeling this allows us to (a) learn models with interpretable latent directions enabling, for example, style/content separation, and (b) generalize beyond the data set enabling us to learn transitions between motion styles even though such transitions are not present in the data
5194 en Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression Modeling of conditional quantiles requires specification of the quantile being estimated and can thus be viewed as a parameterized predictive modeling problem. Quantile loss is typically used, and it is indeed parameterized by a quantile parameter. In this paper we show how to follow the path of cross validated solutions to regularized kernel quantile regression. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on data. This algorithm allows us to efficiently solve the whole family of bi-level problems.
5196 en Discriminative Parameter Learning for Bayesian Networks Bayesian network classifiers have been widely used for classification problems. Given a fixed Bayesian network structure, parameter learning can take two different approaches: generative and discriminative learning. While generative parameter learning is more efficient, discriminative parameter learning is more effective. In this paper, we propose a simple, efficient, and effective discriminative parameter learning method, called Discriminative Frequency Estimate (DFE), which learns parameters by discriminatively computing frequencies from data. Empirical studies show that the DFE algorithm integrates the advantages of both generative and discriminative learning: it performs as well as the state-of-the-art discriminative parameter learning method ELR in accuracy, but is significantly more efficient.
5197 en On the Quantitative Analysis of Deep Belief Networks Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. Annealed Importance Sampling (AIS), can be used to efficiently estimate the partition function of an RBM. We present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.
5198 en Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence algorithms on the tasks of modeling handwritten digits and classifying digit images by learning a model of the joint distribution of images and labels. The Persistent Contrastive Divergence algorithm outperforms other Contrastive Divergence algorithms, and is equally fast and simple.
5199 en A New Admixture Model for Inference of Population Structure in Light of Both Genetic Admixing and Allele Mutations Traditional methods for analyzing population structure, such as the Structure program, ignore the influence of mutational effects. We propose mStruct, an admixture of population-specific mixtures of inheritance models, that addresses the task of structure inference and mutation estimation jointly through a hierarchical Bayesian framework, and a variational algorithm for inference. We validated our method on synthetic data, and used it to analyze the HGDP-CEPH cell line panel of microsatellites used in (Rosenberg et al., 2002) and the HGDP SNP data used in (Conrad et al., 2006). A comparison of the structural maps of world populations estimated by mStruct and Structure is presented, and we also report potentially interesting mutation patterns in world populations estimated by mStruct, which is not possible by Structure.
5200 en Memory Bounded Inference in Topic Models What type of algorithms and statistical techniques support learning from very large datasets over long stretches of time? We address this question through a memory bounded version of a variational EM algorithm that approximates inference of a topic model. The algorithm alternates two phases: "model building" and "model compression" in order to always satisfy a given memory constraint. The model building phase grows its internal representation (the number of topics) as more data arrives through Bayesian model selection. Compression is achieved by merging data-items in clumps and only caching their sufficient statistics. Empirically, the resulting algorithm is able to handle datasets that are orders of magnitude larger than the standard batch version.
5201 en Nonnegative Matrix Factorization via Rank-One Downdate Nonnegative matrix factorization (NMF) was popularized as a tool for data mining by Lee and Seung in 1999. NMF attempts to approximate a matrix with nonnegative entries by a product of two low-rank matrices, also with nonnegative entries. We propose an algorithm called rank-one downdate (R1D) for computing a NMF that is partly motivated by singular value decomposition. This algorithm computes the dominant singular values and vectors of adaptively determined submatrices of a matrix. On each iteration, R1D extracts a rank-one submatrix from the dataset according to an objective function. We establish a theoretical result that maximizing this objective function corresponds to correctly classifying articles in a nearly separable corpus. We also provide computational experiments showing the success of this method in identifying features in realistic datasets. The method is much faster than either LSI or other NMF routines.
5202 en Dirichlet Component Analysis: Feature Extraction for Compositional Data We consider feature extraction (dimensionality reduction) for compositional data, where the data vectors are constrained to be positive and constant-sum. In real-world problems, the data components (variables) usually have complicated "correlations" while their total number is huge. Such scenario demands feature extraction. That is, we shall de-correlate the components and reduce their dimensionality. Traditional techniques such as the Principle Component Analysis (PCA) are not suitable for these problems due to unique statistical properties and the need to satisfy the constraints in compositional data. This paper presents a novel approach to feature extraction for compositional data. Our method first identifies a family of dimensionality reduction projections that preserve all relevant constraints, and then finds the optimal projection that maximizes the estimated Dirichlet precision on projected data. It reduces the compositional data to a given lower dimensionality while the components in the lower-dimensional space are de-correlated as much as possible. We develop theoretical foundation of our approach, and validate its effectiveness on some synthetic and real-world datasets.
5203 en Learning to Sportscast: A Test of Grounded Language Acquisition We present a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on. Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries.
5204 en A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning We describe a single convolutional neural network architecture that given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel way of performing semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in a learnt model with state-of-the-art performance.
5205 en Fully Distributed EM for Very Large Datasets In EM and related algorithms, E-step computations distribute easily, because data items are independent given parameters. For very large data sets, however, even storing all of the parameters in a single node for the M-step can be impractical. We present a framework which fully distributes the entire EM procedure. Each node interacts with only parameters relevant to its data, sending messages to other nodes along a junction-tree topology. We demonstrate improvements over a MapReduce approach, on two tasks: word alignment and topic modeling.
5206 en Structure Compilation: Trading Structure for Features Structured models often achieve excellent performance but can be slow at test time. We investigate structure compilation, where we replace structure with features, which are often computationally simpler but unfortunately statistically more complex. We analyze this tradeoff theoretically and empirically on three natural language processing tasks. We also introduce a simple method to transfer predictive power from structure to features via unlabeled data, while incurring a minimal statistical penalty.
5211 en Multi-modal interaction involving speech and language technologies 
5212 en Phrase-based and factored statistical machine translation 
5213 en Applying unsupervised learning in creating language models for information retrieval and machine translation 
5214 en Speech production models for automatic speech recognition 
5215 en Opening 
5217 en Change Management in the Product Development Process as a Challenge for OEMs, Partners, IT System Vendord and Research Organisations 
5218 en Challenges and Strategies for IT Services in Heterogeneous Enterprise Environments 
5219 en Enterprise Architecture: A Service Interoperability Analysis Framework 
5220 en Logical Foundations for the Infrastructure of the Information Market 
5221 en The Advantages of Hybrid Architectural Approaches for the Integrating Middleware 
5222 en MDA Technology to Support China Aviation Industry 
5223 en Ontology-driven Semantic Mapping 
5224 en Supporting Adaptive Enterprise Collaboration through Semantic Knowledge Services 
5231 en Musical Source Separation using Generalised Non-Negative Tensor Factorisation Models 
5233 en Learning Violinist's Expressive Trends 
5234 en The Potential of Reinforcement Learning for Live Musical Agents 
5235 en Modeling Celtic Violin Expressive Performance 
5236 en Identifying Cover Songs Using Normalized Compression Distance 
5237 en Towards Logic-based Representations of Musical Harmony for Classification, Retrieval and Knowledge Discovery 
5238 en Metropolis-Hastings Sampling in a FilterBoost Music Classifier 
5240 en Composer classification using grammatical inference 
5241 en Training Music Sequence Recognizers with Linear Dynamic Programming 
5242 en Genre Classification of Music by Tonal Harmony 
5243 en Using Mathematical Morphology for Geometric Music Retrieval 
5244 en Learning to analyse tonal music 
5245 en Chorale Harmonization in the Style of J.S. Bach, A Machine Learning Approach 
5246 en An information-dynamic model of melodic segmentation 
5247 en Melody Characterization by a Fuzzy Rule System 
5248 en Detecting Changes in Musical Texture 
5252 en Opening ceremony: Transition metal chalco/halide nanostructures 
5253 en Opening ceremony: The broader context of nanotechnology development: The integrated, safe and responsible policy of the European Union 
5254 en Opening ceremony 
5255 en Phantom ferromagnetism 
5256 en Bioactive nanoparticles and carbon nanotubes for new generation tissue engineering scaffolds 
5257 en Molybdenum chalcohalide nanowire mysteries: Structure, rigidity and quantum transport 
5258 en Towards small size ferroelectrics 
5259 en Nanostructures for few molecules detection 
5260 en Force-clamp spectroscopy of single proteins 
5261 en Probabilistic models for understanding images Getting a computer to understand an image is challenging due to the numerous sources of variability that influence the imaging process. The pixels of a typical photograph will depend on the scene type and geometry, the number, shape and appearance of objects present in the scene, their 3D positions and orientations, as well as effects such as occlusion, shading and shadows. The good news is that research into physics and computer graphics has given us a detailed understanding of how these variables affect the resulting image. This understanding can help us to build the right prior knowledge into our probabilistic models of images. In theory, building a model containing all of this knowledge would solve the image understanding problem. In practice, such a model would be intractable for current inference methods. The open challenge for machine learning and machine vision researchers is to create a model which captures the imaging process as accurately as possible, whilst remaining tractable for accurate inference. To illustrate this challenge, I will show how different aspects of the imaging process can be incorporated into models for object detection and segmentation, and discuss techniques for making inference tractable in such models. **//Disclaimer:// Videolectures.Net emphasises that the quality of this video can not be improved, because of low light quality conditions provided in the lecture auditorium.**
5262 en Structured Prediction Problems in Natural Language Processing Modeling language at the syntactic or semantic level is a key problem in natural language processing, and involves a challenging set of structured prediction problems. In this talk I'll describe work on machine learning approaches for syntax and semantics, with a particular focus on lexicalized grammar formalisms such as dependency grammars, tree adjoining grammars, and categorial grammars. n;I'll address key issues in the following areas: n:1) the design of learning algorithms for structured linguistic data;n:2) the design of representations that are used within these learning algorithms;n:3) the design of efficient approximate inference algorithms for lexicalized grammars, in cases where exact inference can be very expensive.nnIn addition, I'll describe applications to machine translation, and natural language interfaces.
5263 en STAIR: The STanford Artificial Intelligence Robot project This talk will describe the STAIR home assistant robot project, and several satellite projects that led to key STAIR components such as (i) robotic grasping of previously unknown objects, (ii) depth perception from a single still image, and (iii) apprenticeship learning for control. Since its birth in 1956, the AI dream has been to build systems that exhibit broad-spectrum competence and intelligence. STAIR revisits this dream, and seeks to integrate onto a single robot platform tools drawn from all areas of AI including learning, vision, navigation, manipulation, planning, and speech/NLP. This is in distinct contrast to, and also represents an attempt to reverse, the 30 year old trend of working on fragmented AI sub-fields. STAIR's goal is a useful home assistant robot, and over the long term, we envision a single robot that can perform tasks such as tidying up a room, using a dishwasher, fetching and delivering items, and preparing meals. STAIR is still a young project, and in this talk I'll report on our progress so far on having STAIR fetch items from around the office. Specifically, I'll describe: (i) learning to grasp previously unseen objects (including its application to unloading items from a dishwasher); (ii) probabilistic multi-resolution maps, which enable the robot to open/use doors; (iii) a robotic foveal+peripheral vision system for object recognition and tracking. I'll also outline some of the main technical ideas - such as learning 3-d reconstructions from a single still image, and reinforcement learning algorithms for robotic control - that played key roles in enabling these STAIR components.
5264 en Best Papers Awards 
5265 en An Asymptotic Analysis of Generative, Discriminative, and Pseudolikelihood Estimators Statistical and computational concerns have motivated parameter estimators based on various forms of likelihood, e.g., joint, conditional, and pseudolikelihood. In this paper, we present a unified framework for studying these estimators, which allows us to compare their relative (statistical) efficiencies. Our asymptotic analysis suggests that modeling more of the data tends to reduce variance, but at the cost of being more sensitive to model misspecification. We present experiments validating our analysis.
5266 en Knows What It Knows: A Framework For Self-Aware Learning We introduce a learning framework that combines elements of the well-known PAC and mistake-bound models. The KWIK (knows what it knows) framework was designed particularly for its utility in learning settings where active exploration can impact the training examples the learner is exposed to, as is true in reinforcement-learning and active-learning problems. We catalog several KWIK-learnable classes and list some open problems in this area.
5267 en SVM Optimization: Inverse Dependence on Training Set Size We discuss how the runtime of SVM optimization should decrease as the size of the training data increases. We present theoretical and empirical results demonstrating how a simple subgradient descent approach indeed displays such behavior, at least for linear kernels.
5268 en 10 Year Best Paper: Combining Labeled and Unlabeled Data with Co-Training We consider the problem of using a large unlabeled sample to boost performance of a learning algorithm when only a small set of labeled examples is available. In particular we consider a problem setting motivated by the task of learning to classify web pages in which the description of each example can be partitioned into two distinct views. For example the description of a web page can be partitioned into the words occurring on that page and the words occurring in hyperlinks that point to that page We assume that either view of the examplewould be sufficient for learning if we had enough labeled data but our goal is to use both views together to allow inexpensive unlabeled data to augment a much smaller set of labeled examples. Specically the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view and then each algorithms predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC style analysis for this setting and more broadly a PAC style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a stand-alone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.
5269 en Papers Introduction 
5274 en Ontology Engineering and Plug-in Development with the NeOn Toolkit - Introduction Our tutorial targets ontology modelers and engineers. The tutorial provides guidance for the development of ontologies and ontology-based applications with respect to the complete ontology lifecycle. We will start with an introduction to a variety of use cases for applications of ontologies, including information integration and knowledge management. Based on these use cases we will illustrate a typical ontology lifecycle and discuss specific ontology lifecycle activities, such as ontology development, selection and reuse, ontology mapping, and ontology evolution. After a short introduction to the NeOn toolkit and its functionalities, we will take a closer look at how the lifecycle activities are realized using the NeOn toolkit. As an example, we will demonstrate how to support ontology selection and reuse with a plug-in that integrates with the Watson Semantic Web gateway. In the hands-on exercises, the participants will work on practical activities from a real world use case. In the second half of the tutorial, we will demonstrate how to extend the functionalities of the ontology engineering environment by developing a NeOn plug-in to support an additional lifecycle activity from the initial use case. Therefore, we first provide further insights into the NeOn reference architecture, its plug-in concept and APIs. We will then demonstrate how to develop a plug-in in an easy-to-follow step-by-step way. After that, the participants will create their own working plug-in in a hands-on exercise.
5281 en Classification using Discriminative Restricted Boltzmann Machines Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a stand-alone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.
5285 en Ontology Engineering and Plug-in Development with the NeOn Toolkit - Ontology Lifecycle and Methodology Our tutorial targets ontology modelers and engineers. The tutorial provides guidance for the development of ontologies and ontology-based applications with respect to the complete ontology lifecycle. We will start with an introduction to a variety of use cases for applications of ontologies, including information integration and knowledge management. Based on these use cases we will illustrate a typical ontology lifecycle and discuss specific ontology lifecycle activities, such as ontology development, selection and reuse, ontology mapping, and ontology evolution. After a short introduction to the NeOn toolkit and its functionalities, we will take a closer look at how the lifecycle activities are realized using the NeOn toolkit. As an example, we will demonstrate how to support ontology selection and reuse with a plug-in that integrates with the Watson Semantic Web gateway. In the hands-on exercises, the participants will work on practical activities from a real world use case. In the second half of the tutorial, we will demonstrate how to extend the functionalities of the ontology engineering environment by developing a NeOn plug-in to support an additional lifecycle activity from the initial use case. Therefore, we first provide further insights into the NeOn reference architecture, its plug-in concept and APIs. We will then demonstrate how to develop a plug-in in an easy-to-follow step-by-step way. After that, the participants will create their own working plug-in in a hands-on exercise.
5286 en IMS and the Manufacturing Technology Platform Initiative IMS is an international program to support R&D in manufacturing among the advanced developed countries of Japan, USA, European Union, Korea and Switzerland. As the first government-supported program to offer a multi-lateral global approach to research in advanced manufacturing, IMS continues to innovate and reinvent itself in order to be relevant to researchers around the globe. In its latest response to researchers input from the IMS Vision Forum, IMS has launched the “Manufacturing Technology Platform Initiative”, or MTP. The MTP initiative is a unique program that threads research and researchers together in a simple way to solve manufacturing challenges of today and the future. The program not only simplifies the process for organizing research under the IMS banner, but it also promotes a spark of new ideas through wider networks that are created. MTPs are focused knowledge sharing platforms for researcher groups that are already engaged in a specific R&D domain. There is overlap in much research that is conducted. Rather than duplicate work, an MTP initiative seeks cooperation to conduct joint research in projects that are already running. This ultimately saves resources for the “golden nuggets” of their research, and finds common solutions to manufacturing challenges in the process. MTP’s also provide an opportunity for researchers to meet, exchange information, and generate new ideas for research.
5287 en Ontology Engineering and Plug-in Development with the NeOn Toolkit - Neon toolkit overview Our tutorial targets ontology modelers and engineers. The tutorial provides guidance for the development of ontologies and ontology-based applications with respect to the complete ontology lifecycle. We will start with an introduction to a variety of use cases for applications of ontologies, including information integration and knowledge management. Based on these use cases we will illustrate a typical ontology lifecycle and discuss specific ontology lifecycle activities, such as ontology development, selection and reuse, ontology mapping, and ontology evolution. After a short introduction to the NeOn toolkit and its functionalities, we will take a closer look at how the lifecycle activities are realized using the NeOn toolkit. As an example, we will demonstrate how to support ontology selection and reuse with a plug-in that integrates with the Watson Semantic Web gateway. In the hands-on exercises, the participants will work on practical activities from a real world use case. In the second half of the tutorial, we will demonstrate how to extend the functionalities of the ontology engineering environment by developing a NeOn plug-in to support an additional lifecycle activity from the initial use case. Therefore, we first provide further insights into the NeOn reference architecture, its plug-in concept and APIs. We will then demonstrate how to develop a plug-in in an easy-to-follow step-by-step way. After that, the participants will create their own working plug-in in a hands-on exercise.
5288 en Ontology Engineering and Plug-in Development with the NeOn Toolkit Our tutorial targets ontology modelers and engineers. The tutorial provides guidance for the development of ontologies and ontology-based applications with respect to the complete ontology lifecycle. We will start with an introduction to a variety of use cases for applications of ontologies, including information integration and knowledge management. Based on these use cases we will illustrate a typical ontology lifecycle and discuss specific ontology lifecycle activities, such as ontology development, selection and reuse, ontology mapping, and ontology evolution. After a short introduction to the NeOn toolkit and its functionalities, we will take a closer look at how the lifecycle activities are realized using the NeOn toolkit. As an example, we will demonstrate how to support ontology selection and reuse with a plug-in that integrates with the Watson Semantic Web gateway. In the hands-on exercises, the participants will work on practical activities from a real world use case. In the second half of the tutorial, we will demonstrate how to extend the functionalities of the ontology engineering environment by developing a NeOn plug-in to support an additional lifecycle activity from the initial use case. Therefore, we first provide further insights into the NeOn reference architecture, its plug-in concept and APIs. We will then demonstrate how to develop a plug-in in an easy-to-follow step-by-step way. After that, the participants will create their own working plug-in in a hands-on exercise.
5289 en Ontology Engineering and Plug-in Development with the NeOn Toolkit Our tutorial targets ontology modelers and engineers. The tutorial provides guidance for the development of ontologies and ontology-based applications with respect to the complete ontology lifecycle. We will start with an introduction to a variety of use cases for applications of ontologies, including information integration and knowledge management. Based on these use cases we will illustrate a typical ontology lifecycle and discuss specific ontology lifecycle activities, such as ontology development, selection and reuse, ontology mapping, and ontology evolution. After a short introduction to the NeOn toolkit and its functionalities, we will take a closer look at how the lifecycle activities are realized using the NeOn toolkit. As an example, we will demonstrate how to support ontology selection and reuse with a plug-in that integrates with the Watson Semantic Web gateway. In the hands-on exercises, the participants will work on practical activities from a real world use case. In the second half of the tutorial, we will demonstrate how to extend the functionalities of the ontology engineering environment by developing a NeOn plug-in to support an additional lifecycle activity from the initial use case. Therefore, we first provide further insights into the NeOn reference architecture, its plug-in concept and APIs. We will then demonstrate how to develop a plug-in in an easy-to-follow step-by-step way. After that, the participants will create their own working plug-in in a hands-on exercise.
5290 en Ontology Engineering and Plug-in Development with the NeOn Toolkit Our tutorial targets ontology modelers and engineers. The tutorial provides guidance for the development of ontologies and ontology-based applications with respect to the complete ontology lifecycle. We will start with an introduction to a variety of use cases for applications of ontologies, including information integration and knowledge management. Based on these use cases we will illustrate a typical ontology lifecycle and discuss specific ontology lifecycle activities, such as ontology development, selection and reuse, ontology mapping, and ontology evolution. After a short introduction to the NeOn toolkit and its functionalities, we will take a closer look at how the lifecycle activities are realized using the NeOn toolkit. As an example, we will demonstrate how to support ontology selection and reuse with a plug-in that integrates with the Watson Semantic Web gateway. In the hands-on exercises, the participants will work on practical activities from a real world use case. In the second half of the tutorial, we will demonstrate how to extend the functionalities of the ontology engineering environment by developing a NeOn plug-in to support an additional lifecycle activity from the initial use case. Therefore, we first provide further insights into the NeOn reference architecture, its plug-in concept and APIs. We will then demonstrate how to develop a plug-in in an easy-to-follow step-by-step way. After that, the participants will create their own working plug-in in a hands-on exercise.
5291 en Challenges in Developing Collaborative Workspaces for Solving Complex Problems This talk was mainly focused on industrial, human and technical challenges in creating collaborative workspaces for sectors such as aerospace, automotive, construction and urban planning. It initially took the complexity of designing aircraft as an example and illustrated the distributed nature of the team, need for maintaining large amount of information during the lifecycle and the need for assessing various design view points to ensure successful delivery. This part of the talk was mainly focused on creating an appreciation of the need for collaborative tools for design teams to work together more effectively. The talk then went on to explain the barriers and challenges in terms of deploying collaborative technology with in engineering organisations. Issues such as unwillingness to adapt to change, generation gap/change, lack of social cues, misunderstandings of language, unfamiliarity of collaborators with one another, cultural barriers, lack of training/guidelines and lack of accessibility for technology were considered as the current barriers to deploy collaborative technologies in industry. It was suggested that there are several drivers which might force people to overcome those barriers in order to survive in the future. Some of the examples of these barriers are global competition, migration of design and manufacturing facilities to low cost markets ( Eastern Europe and Far East) and new economical powers (China, India) and the need for implementing concurrent engineering to achieve low cost and high quality products on time. The talk then focused on the creating a collaborative workspace for distributed engineering organsiations. It first discussed the future scenarios developed by the project and discussed the need for innovative technologies for supporting collaboration among stakeholders. The scenario presented in the talk included a DMU scenario in aerospace, design of a toilet for disable people, design of car mirrors and a mobile maintenance for aircrafts. The following issues were identified as the key challenges in creating a collaborative workspaces for these scenarios : Information Sharing, Creation of Workspaces to Support Different Working Styles, Virtual Infrastructure, Enhanced Sense of Presence, Physical Environments, Communication of View Points.
5292 en Transitioning legacy applications to ontologies: Hands-on tutorial - TAO Bootstrapping Methodology 
5293 en Transitioning legacy applications to ontologies: Hands-on tutorial - Learning Domain Ontologies 
5294 en Transitioning legacy applications to ontologies: Hands-on tutorial - Semantic Annotation and search of Software Artefacts 
5295 en Transitioning legacy applications to ontologies: Hands-on tutorial - Transitioning Relational Databases to Ontologies 
5296 en Transitioning legacy applications to ontologies: Hands-on tutorial - Heterogeneous Knowledge repositories for storing legacy content: requirements, scalability, and applicability 
5297 en Transitioning legacy applications to ontologies: Hands-on tutorial 
5298 en Living Labs, ENoLL : "Research" Challenges What are the challenges in basic research still to be addressed by Living Labs and the European Network of Living Labs to further improve the concept ? How can the European Union assist in this? To this purpose I will present a roadmap of potential funding instruments in the period 2009 to 2010 proposed by Research (FP7), Innovation (CIP) and Regional Funds.
5299 en Semantic Wikis - Introduction to semantic wikis Semantic Wikis combine properties of wikis (ease of use, low technological barrier, collaboration, easy linking) with Semantic Web technologies (structuring of knowledge, linking with background knowledge models). Since 2005, when development on the first systems startet, Semantic Wikis have matured and are now in a state where they are increasingly deployed even in domains outside the Semantic Web community or even outside Computer Science. Reasons for this are that Semantic Wikis require no advanced knowledge and are thus usable also by laymen, and that Semantic Wikis provide immeadiate benefit over „Non-Semantic“ Wikis. In addition, Semantic Wikis are also an interesting testbed for the envisioned Semantic Web and associated technologies, as Wikis have structural similarities to the Web as a whole and hence share many of the potential chances and pitfalls. This tutorial introduces into the setup and usage of the two currently most popular Semantic Wiki systems, Semantic MediaWiki and IkeWiki. Both systems will have official releases by the time the tutorial takes place, and both systems are in the focus of two upcoming EU funded projects that will start in March 2008.
5300 en Semantic Wikis - Semantic MediaWiki Semantic Wikis combine properties of wikis (ease of use, low technological barrier, collaboration, easy linking) with Semantic Web technologies (structuring of knowledge, linking with background knowledge models). Since 2005, when development on the first systems startet, Semantic Wikis have matured and are now in a state where they are increasingly deployed even in domains outside the Semantic Web community or even outside Computer Science. Reasons for this are that Semantic Wikis require no advanced knowledge and are thus usable also by laymen, and that Semantic Wikis provide immeadiate benefit over „Non-Semantic“ Wikis. In addition, Semantic Wikis are also an interesting testbed for the envisioned Semantic Web and associated technologies, as Wikis have structural similarities to the Web as a whole and hence share many of the potential chances and pitfalls. This tutorial introduces into the setup and usage of the two currently most popular Semantic Wiki systems, Semantic MediaWiki and IkeWiki. Both systems will have official releases by the time the tutorial takes place, and both systems are in the focus of two upcoming EU funded projects that will start in March 2008.
5301 en Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management Semantic Wikis combine properties of wikis (ease of use, low technological barrier, collaboration, easy linking) with Semantic Web technologies (structuring of knowledge, linking with background knowledge models). Since 2005, when development on the first systems startet, Semantic Wikis have matured and are now in a state where they are increasingly deployed even in domains outside the Semantic Web community or even outside Computer Science. Reasons for this are that Semantic Wikis require no advanced knowledge and are thus usable also by laymen, and that Semantic Wikis provide immeadiate benefit over „Non-Semantic“ Wikis. In addition, Semantic Wikis are also an interesting testbed for the envisioned Semantic Web and associated technologies, as Wikis have structural similarities to the Web as a whole and hence share many of the potential chances and pitfalls. This tutorial introduces into the setup and usage of the two currently most popular Semantic Wiki systems, Semantic MediaWiki and IkeWiki. Both systems will have official releases by the time the tutorial takes place, and both systems are in the focus of two upcoming EU funded projects that will start in March 2008.
5302 en The Future Internet: a vision from European Research The infrastructure of the Internet has and will continually evolve to support and enable new services, trends and businesses. Europe is committed to take a leading role in exploring the emerging visions for the Future Internet that will drive the requirements for its underlying network and service infrastructure. Also, ICT is evolving from a facet of business operation and a collection of consumer gadgets to a critical infrastructure that underpins the economy and society. In parallel, the mechanisms for and even the nature of innovation are changing. The next EU Work Programme for Research in ICT will reflect this unavoidable move towards a larger share of the economy/social activities moving on line, with a need to make the Internet capable of supporting a larger number of usages whilst remedying to the current deficiencies in terms of presentation, security, trust, scalability, mobility, etc. All the "visions" presented would then become use cases in relation to an all-encompassing research objective federated under, and driving the requirements towards, a "Future Internet" Challenge.
5303 en Workshop on semantic search - Enhancing Semantic Search using N-Levels Document Representation In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies. On the other hand semantic repositories and reasoning engines have advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios. As such, semantic technologies are now in a state to provide significant contributions to IR problems. This workshop intends to investigate the potential and the challenges of Semantic Search systems. Main topics of interest of the workshop cluster around the areas (i) Tasks and Interaction Paradigms for Semantic Search, (ii) Query Construction and Resource Modelling for Semantic Search, (iii) Algorithms and Infrastructures for Semantic Search and (iv) Evaluation of Semantic Search.
5304 en Workshop on semantic search - The Interaction Between Automatic Annotation and Query Expansion: a retrieval experiment on a large cultural heritage archive In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies. On the other hand semantic repositories and reasoning engines have advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios. As such, semantic technologies are now in a state to provide significant contributions to IR problems. This workshop intends to investigate the potential and the challenges of Semantic Search systems. Main topics of interest of the workshop cluster around the areas (i) Tasks and Interaction Paradigms for Semantic Search, (ii) Query Construction and Resource Modelling for Semantic Search, (iii) Algorithms and Infrastructures for Semantic Search and (iv) Evaluation of Semantic Search.
5305 en Workshop on semantic search - Search, Natural Language Generation and Record Display Configuration: Research Directions Stemming From a Digital Library Application Development Experience In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies. On the other hand semantic repositories and reasoning engines have advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios. As such, semantic technologies are now in a state to provide significant contributions to IR problems. This workshop intends to investigate the potential and the challenges of Semantic Search systems. Main topics of interest of the workshop cluster around the areas (i) Tasks and Interaction Paradigms for Semantic Search, (ii) Query Construction and Resource Modelling for Semantic Search, (iii) Algorithms and Infrastructures for Semantic Search and (iv) Evaluation of Semantic Search.
5306 en Workshop on semantic search - Integration of semantic, metadata and image search engines with a engine for patent retrieval In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies. On the other hand semantic repositories and reasoning engines have advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios. As such, semantic technologies are now in a state to provide significant contributions to IR problems. This workshop intends to investigate the potential and the challenges of Semantic Search systems. Main topics of interest of the workshop cluster around the areas (i) Tasks and Interaction Paradigms for Semantic Search, (ii) Query Construction and Resource Modelling for Semantic Search, (iii) Algorithms and Infrastructures for Semantic Search and (iv) Evaluation of Semantic Search.
5307 en Workshop on semantic search - QuiKey In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies. On the other hand semantic repositories and reasoning engines have advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios. As such, semantic technologies are now in a state to provide significant contributions to IR problems. This workshop intends to investigate the potential and the challenges of Semantic Search systems. Main topics of interest of the workshop cluster around the areas (i) Tasks and Interaction Paradigms for Semantic Search, (ii) Query Construction and Resource Modelling for Semantic Search, (iii) Algorithms and Infrastructures for Semantic Search and (iv) Evaluation of Semantic Search.
5308 en Workshop on semantic search - Large Scale Search Improvement needs Large Scale Knowledge In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies. On the other hand semantic repositories and reasoning engines have advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios. As such, semantic technologies are now in a state to provide significant contributions to IR problems. This workshop intends to investigate the potential and the challenges of Semantic Search systems. Main topics of interest of the workshop cluster around the areas (i) Tasks and Interaction Paradigms for Semantic Search, (ii) Query Construction and Resource Modelling for Semantic Search, (iii) Algorithms and Infrastructures for Semantic Search and (iv) Evaluation of Semantic Search.
5310 en Workshop on semantic search - Concept Search: Semantics Enabled Syntactic Search In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies. On the other hand semantic repositories and reasoning engines have advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios. As such, semantic technologies are now in a state to provide significant contributions to IR problems. This workshop intends to investigate the potential and the challenges of Semantic Search systems. Main topics of interest of the workshop cluster around the areas (i) Tasks and Interaction Paradigms for Semantic Search, (ii) Query Construction and Resource Modelling for Semantic Search, (iii) Algorithms and Infrastructures for Semantic Search and (iv) Evaluation of Semantic Search.
5311 en Workshop on semantic search - Microsearch: An Interface for Semantic Search In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies. On the other hand semantic repositories and reasoning engines have advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios. As such, semantic technologies are now in a state to provide significant contributions to IR problems. This workshop intends to investigate the potential and the challenges of Semantic Search systems. Main topics of interest of the workshop cluster around the areas (i) Tasks and Interaction Paradigms for Semantic Search, (ii) Query Construction and Resource Modelling for Semantic Search, (iii) Algorithms and Infrastructures for Semantic Search and (iv) Evaluation of Semantic Search.
5312 en Workshop on semantic search - Exploring the Knowledge in Semi Structured Data Sets with Rich Queries In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies. On the other hand semantic repositories and reasoning engines have advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios. As such, semantic technologies are now in a state to provide significant contributions to IR problems. This workshop intends to investigate the potential and the challenges of Semantic Search systems. Main topics of interest of the workshop cluster around the areas (i) Tasks and Interaction Paradigms for Semantic Search, (ii) Query Construction and Resource Modelling for Semantic Search, (iii) Algorithms and Infrastructures for Semantic Search and (iv) Evaluation of Semantic Search.
5313 en Collective Semantics: Collective Intelligence & the Semantic Web - Information Retrieval vs. Knowledge Retrieval: A social network perspective Web 2.0 has introduced new style of information sharing featuring mass user participation, social networking, heterogeneity of data sources, and a huge scale of information and knowledge, posing difficulties in discovering relevant information. The Semantic Web may contribute by providing a language basis and ontologies to support structuring, or introducing new ways to explore the information space. This may be achieved by combining semantics from semantic web resources with structure of the sharing platforms (tags, social acquaintances etc.) and automatic content analysis tools. This workshop targets integration arising from the mining of Web 2.0 information, multimedia content and knowledge with help of the Semantic Web.
5314 en Collective Semantics: Collective Intelligence & the Semantic Web - Enriching Ontological User Profiles with Tagging History for Multi-Domain Recommendations Web 2.0 has introduced new style of information sharing featuring mass user participation, social networking, heterogeneity of data sources, and a huge scale of information and knowledge, posing difficulties in discovering relevant information. The Semantic Web may contribute by providing a language basis and ontologies to support structuring, or introducing new ways to explore the information space. This may be achieved by combining semantics from semantic web resources with structure of the sharing platforms (tags, social acquaintances etc.) and automatic content analysis tools. This workshop targets integration arising from the mining of Web 2.0 information, multimedia content and knowledge with help of the Semantic Web.
5315 en Collective Semantics: Collective Intelligence & the Semantic Web - From Web 2.0 to Semantic Web - A Semi-Automated Approach Web 2.0 has introduced new style of information sharing featuring mass user participation, social networking, heterogeneity of data sources, and a huge scale of information and knowledge, posing difficulties in discovering relevant information. The Semantic Web may contribute by providing a language basis and ontologies to support structuring, or introducing new ways to explore the information space. This may be achieved by combining semantics from semantic web resources with structure of the sharing platforms (tags, social acquaintances etc.) and automatic content analysis tools. This workshop targets integration arising from the mining of Web 2.0 information, multimedia content and knowledge with help of the Semantic Web.
5316 en Collective Semantics: Collective Intelligence & the Semantic Web - Use of multiple background ontologies in ontology matching Web 2.0 has introduced new style of information sharing featuring mass user participation, social networking, heterogeneity of data sources, and a huge scale of information and knowledge, posing difficulties in discovering relevant information. The Semantic Web may contribute by providing a language basis and ontologies to support structuring, or introducing new ways to explore the information space. This may be achieved by combining semantics from semantic web resources with structure of the sharing platforms (tags, social acquaintances etc.) and automatic content analysis tools. This workshop targets integration arising from the mining of Web 2.0 information, multimedia content and knowledge with help of the Semantic Web.
5317 en Collective Semantics: Collective Intelligence & the Semantic Web - Flickring Our World Web 2.0 has introduced new style of information sharing featuring mass user participation, social networking, heterogeneity of data sources, and a huge scale of information and knowledge, posing difficulties in discovering relevant information. The Semantic Web may contribute by providing a language basis and ontologies to support structuring, or introducing new ways to explore the information space. This may be achieved by combining semantics from semantic web resources with structure of the sharing platforms (tags, social acquaintances etc.) and automatic content analysis tools. This workshop targets integration arising from the mining of Web 2.0 information, multimedia content and knowledge with help of the Semantic Web.
5318 en Collective Semantics: Collective Intelligence & the Semantic Web - Semantically enriching folksonomies with flor Web 2.0 has introduced new style of information sharing featuring mass user participation, social networking, heterogeneity of data sources, and a huge scale of information and knowledge, posing difficulties in discovering relevant information. The Semantic Web may contribute by providing a language basis and ontologies to support structuring, or introducing new ways to explore the information space. This may be achieved by combining semantics from semantic web resources with structure of the sharing platforms (tags, social acquaintances etc.) and automatic content analysis tools. This workshop targets integration arising from the mining of Web 2.0 information, multimedia content and knowledge with help of the Semantic Web.
5319 en Workshop on Semantic Business Process Management - Using Semantics to Aid Scenario-Based Analysis The degree of automation in the management of the business process space of single enterprises and whole value chains is still unsatisfying. A key source of problems are representational heterogeneities between the various perspectives and the various stages in the life-cycles of business processes. Typical examples are incompatible representations of the managerial vs. the IT perspective, or the gap between normative modeling for compliance purposes and process execution log data. As early as in the 1990s, researchers have evaluated the potential of using ontologies for improving business process management in the context of the TOVE project; however, the impact of that work remained beyond initial expectations. Since 2005, there is now a renewed and growing interest in exploiting ontologies, of varying expressivity and focus, for advancing the state of the art in business process management, in particular in ERP-centric IT landscapes. The term “Semantic Business Process Management” has been suggested for the described branch of research in an early 2005 paper, which is now frequently cited as the first description of the overall vision. A flagship activity in the field is the European research project “SUPER”, with more than a dozen premier industrial and academic partners, among them SAP, IDS Scheer, and IBM. In the past two years, substantial advancement has been made in investigating the theoretical and practical branches of this vision. However, the interdisciplinary nature of the topic requires a tight collaboration of researcher from multiple fields of, namely the BPM, SOA, Semantic Web, Semantic Web services, and Economics communities. There is a clear need for an annual event at which those communities meet, debate, challenge each others approaches, and eventually align their research efforts. Due to the strong involvement of Semantic Web researchers in the field, ESWC is the ideal target venue for this event. In this workshop, we want to bring together experts from the relevant communities and help reach agreement on a roadmap for SBPM research. We aim at bundling experiences and prototypes from the successful application of Semantic Web technology to BPM in various industries, like automotive, engineering, chemical and pharmaceutical, and services domains. The particular focus is on deriving reusable best-practices from such experiences, and to yield convincing showcases of semantic technology.
5320 en Workshop on Semantic Business Process Management - Bussiness process excellence The degree of automation in the management of the business process space of single enterprises and whole value chains is still unsatisfying. A key source of problems are representational heterogeneities between the various perspectives and the various stages in the life-cycles of business processes. Typical examples are incompatible representations of the managerial vs. the IT perspective, or the gap between normative modeling for compliance purposes and process execution log data. As early as in the 1990s, researchers have evaluated the potential of using ontologies for improving business process management in the context of the TOVE project; however, the impact of that work remained beyond initial expectations. Since 2005, there is now a renewed and growing interest in exploiting ontologies, of varying expressivity and focus, for advancing the state of the art in business process management, in particular in ERP-centric IT landscapes. The term “Semantic Business Process Management” has been suggested for the described branch of research in an early 2005 paper, which is now frequently cited as the first description of the overall vision. A flagship activity in the field is the European research project “SUPER”, with more than a dozen premier industrial and academic partners, among them SAP, IDS Scheer, and IBM. In the past two years, substantial advancement has been made in investigating the theoretical and practical branches of this vision. However, the interdisciplinary nature of the topic requires a tight collaboration of researcher from multiple fields of, namely the BPM, SOA, Semantic Web, Semantic Web services, and Economics communities. There is a clear need for an annual event at which those communities meet, debate, challenge each others approaches, and eventually align their research efforts. Due to the strong involvement of Semantic Web researchers in the field, ESWC is the ideal target venue for this event. In this workshop, we want to bring together experts from the relevant communities and help reach agreement on a roadmap for SBPM research. We aim at bundling experiences and prototypes from the successful application of Semantic Web technology to BPM in various industries, like automotive, engineering, chemical and pharmaceutical, and services domains. The particular focus is on deriving reusable best-practices from such experiences, and to yield convincing showcases of semantic technology.
5321 en Workshop on Semantic Business Process Management - Semantic business process validation The degree of automation in the management of the business process space of single enterprises and whole value chains is still unsatisfying. A key source of problems are representational heterogeneities between the various perspectives and the various stages in the life-cycles of business processes. Typical examples are incompatible representations of the managerial vs. the IT perspective, or the gap between normative modeling for compliance purposes and process execution log data. As early as in the 1990s, researchers have evaluated the potential of using ontologies for improving business process management in the context of the TOVE project; however, the impact of that work remained beyond initial expectations. Since 2005, there is now a renewed and growing interest in exploiting ontologies, of varying expressivity and focus, for advancing the state of the art in business process management, in particular in ERP-centric IT landscapes. The term “Semantic Business Process Management” has been suggested for the described branch of research in an early 2005 paper, which is now frequently cited as the first description of the overall vision. A flagship activity in the field is the European research project “SUPER”, with more than a dozen premier industrial and academic partners, among them SAP, IDS Scheer, and IBM. In the past two years, substantial advancement has been made in investigating the theoretical and practical branches of this vision. However, the interdisciplinary nature of the topic requires a tight collaboration of researcher from multiple fields of, namely the BPM, SOA, Semantic Web, Semantic Web services, and Economics communities. There is a clear need for an annual event at which those communities meet, debate, challenge each others approaches, and eventually align their research efforts. Due to the strong involvement of Semantic Web researchers in the field, ESWC is the ideal target venue for this event. In this workshop, we want to bring together experts from the relevant communities and help reach agreement on a roadmap for SBPM research. We aim at bundling experiences and prototypes from the successful application of Semantic Web technology to BPM in various industries, like automotive, engineering, chemical and pharmaceutical, and services domains. The particular focus is on deriving reusable best-practices from such experiences, and to yield convincing showcases of semantic technology.
5322 en Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks The degree of automation in the management of the business process space of single enterprises and whole value chains is still unsatisfying. A key source of problems are representational heterogeneities between the various perspectives and the various stages in the life-cycles of business processes. Typical examples are incompatible representations of the managerial vs. the IT perspective, or the gap between normative modeling for compliance purposes and process execution log data. As early as in the 1990s, researchers have evaluated the potential of using ontologies for improving business process management in the context of the TOVE project; however, the impact of that work remained beyond initial expectations. Since 2005, there is now a renewed and growing interest in exploiting ontologies, of varying expressivity and focus, for advancing the state of the art in business process management, in particular in ERP-centric IT landscapes. The term “Semantic Business Process Management” has been suggested for the described branch of research in an early 2005 paper, which is now frequently cited as the first description of the overall vision. A flagship activity in the field is the European research project “SUPER”, with more than a dozen premier industrial and academic partners, among them SAP, IDS Scheer, and IBM. In the past two years, substantial advancement has been made in investigating the theoretical and practical branches of this vision. However, the interdisciplinary nature of the topic requires a tight collaboration of researcher from multiple fields of, namely the BPM, SOA, Semantic Web, Semantic Web services, and Economics communities. There is a clear need for an annual event at which those communities meet, debate, challenge each others approaches, and eventually align their research efforts. Due to the strong involvement of Semantic Web researchers in the field, ESWC is the ideal target venue for this event. In this workshop, we want to bring together experts from the relevant communities and help reach agreement on a roadmap for SBPM research. We aim at bundling experiences and prototypes from the successful application of Semantic Web technology to BPM in various industries, like automotive, engineering, chemical and pharmaceutical, and services domains. The particular focus is on deriving reusable best-practices from such experiences, and to yield convincing showcases of semantic technology.
5323 en Workshop on Semantic Business Process Management - Organization Structure Description for the Needs of Semantic Business Process Management The degree of automation in the management of the business process space of single enterprises and whole value chains is still unsatisfying. A key source of problems are representational heterogeneities between the various perspectives and the various stages in the life-cycles of business processes. Typical examples are incompatible representations of the managerial vs. the IT perspective, or the gap between normative modeling for compliance purposes and process execution log data. As early as in the 1990s, researchers have evaluated the potential of using ontologies for improving business process management in the context of the TOVE project; however, the impact of that work remained beyond initial expectations. Since 2005, there is now a renewed and growing interest in exploiting ontologies, of varying expressivity and focus, for advancing the state of the art in business process management, in particular in ERP-centric IT landscapes. The term “Semantic Business Process Management” has been suggested for the described branch of research in an early 2005 paper, which is now frequently cited as the first description of the overall vision. A flagship activity in the field is the European research project “SUPER”, with more than a dozen premier industrial and academic partners, among them SAP, IDS Scheer, and IBM. In the past two years, substantial advancement has been made in investigating the theoretical and practical branches of this vision. However, the interdisciplinary nature of the topic requires a tight collaboration of researcher from multiple fields of, namely the BPM, SOA, Semantic Web, Semantic Web services, and Economics communities. There is a clear need for an annual event at which those communities meet, debate, challenge each others approaches, and eventually align their research efforts. Due to the strong involvement of Semantic Web researchers in the field, ESWC is the ideal target venue for this event. In this workshop, we want to bring together experts from the relevant communities and help reach agreement on a roadmap for SBPM research. We aim at bundling experiences and prototypes from the successful application of Semantic Web technology to BPM in various industries, like automotive, engineering, chemical and pharmaceutical, and services domains. The particular focus is on deriving reusable best-practices from such experiences, and to yield convincing showcases of semantic technology.
5324 en Workshop on Semantic Business Process Management - Enterprise Attention Management System The degree of automation in the management of the business process space of single enterprises and whole value chains is still unsatisfying. A key source of problems are representational heterogeneities between the various perspectives and the various stages in the life-cycles of business processes. Typical examples are incompatible representations of the managerial vs. the IT perspective, or the gap between normative modeling for compliance purposes and process execution log data. As early as in the 1990s, researchers have evaluated the potential of using ontologies for improving business process management in the context of the TOVE project; however, the impact of that work remained beyond initial expectations. Since 2005, there is now a renewed and growing interest in exploiting ontologies, of varying expressivity and focus, for advancing the state of the art in business process management, in particular in ERP-centric IT landscapes. The term “Semantic Business Process Management” has been suggested for the described branch of research in an early 2005 paper, which is now frequently cited as the first description of the overall vision. A flagship activity in the field is the European research project “SUPER”, with more than a dozen premier industrial and academic partners, among them SAP, IDS Scheer, and IBM. In the past two years, substantial advancement has been made in investigating the theoretical and practical branches of this vision. However, the interdisciplinary nature of the topic requires a tight collaboration of researcher from multiple fields of, namely the BPM, SOA, Semantic Web, Semantic Web services, and Economics communities. There is a clear need for an annual event at which those communities meet, debate, challenge each others approaches, and eventually align their research efforts. Due to the strong involvement of Semantic Web researchers in the field, ESWC is the ideal target venue for this event. In this workshop, we want to bring together experts from the relevant communities and help reach agreement on a roadmap for SBPM research. We aim at bundling experiences and prototypes from the successful application of Semantic Web technology to BPM in various industries, like automotive, engineering, chemical and pharmaceutical, and services domains. The particular focus is on deriving reusable best-practices from such experiences, and to yield convincing showcases of semantic technology.
5325 en Workshop on Semantic Business Process Management - Event-driven Reactivity - A Survey and Requirements Analysis The degree of automation in the management of the business process space of single enterprises and whole value chains is still unsatisfying. A key source of problems are representational heterogeneities between the various perspectives and the various stages in the life-cycles of business processes. Typical examples are incompatible representations of the managerial vs. the IT perspective, or the gap between normative modeling for compliance purposes and process execution log data. As early as in the 1990s, researchers have evaluated the potential of using ontologies for improving business process management in the context of the TOVE project; however, the impact of that work remained beyond initial expectations. Since 2005, there is now a renewed and growing interest in exploiting ontologies, of varying expressivity and focus, for advancing the state of the art in business process management, in particular in ERP-centric IT landscapes. The term “Semantic Business Process Management” has been suggested for the described branch of research in an early 2005 paper, which is now frequently cited as the first description of the overall vision. A flagship activity in the field is the European research project “SUPER”, with more than a dozen premier industrial and academic partners, among them SAP, IDS Scheer, and IBM. In the past two years, substantial advancement has been made in investigating the theoretical and practical branches of this vision. However, the interdisciplinary nature of the topic requires a tight collaboration of researcher from multiple fields of, namely the BPM, SOA, Semantic Web, Semantic Web services, and Economics communities. There is a clear need for an annual event at which those communities meet, debate, challenge each others approaches, and eventually align their research efforts. Due to the strong involvement of Semantic Web researchers in the field, ESWC is the ideal target venue for this event. In this workshop, we want to bring together experts from the relevant communities and help reach agreement on a roadmap for SBPM research. We aim at bundling experiences and prototypes from the successful application of Semantic Web technology to BPM in various industries, like automotive, engineering, chemical and pharmaceutical, and services domains. The particular focus is on deriving reusable best-practices from such experiences, and to yield convincing showcases of semantic technology.
5326 en Workshop on Semantic Business Process Management - A Toolkit for Business Process Owners to Capture Early System Requirements The degree of automation in the management of the business process space of single enterprises and whole value chains is still unsatisfying. A key source of problems are representational heterogeneities between the various perspectives and the various stages in the life-cycles of business processes. Typical examples are incompatible representations of the managerial vs. the IT perspective, or the gap between normative modeling for compliance purposes and process execution log data. As early as in the 1990s, researchers have evaluated the potential of using ontologies for improving business process management in the context of the TOVE project; however, the impact of that work remained beyond initial expectations. Since 2005, there is now a renewed and growing interest in exploiting ontologies, of varying expressivity and focus, for advancing the state of the art in business process management, in particular in ERP-centric IT landscapes. The term “Semantic Business Process Management” has been suggested for the described branch of research in an early 2005 paper, which is now frequently cited as the first description of the overall vision. A flagship activity in the field is the European research project “SUPER”, with more than a dozen premier industrial and academic partners, among them SAP, IDS Scheer, and IBM. In the past two years, substantial advancement has been made in investigating the theoretical and practical branches of this vision. However, the interdisciplinary nature of the topic requires a tight collaboration of researcher from multiple fields of, namely the BPM, SOA, Semantic Web, Semantic Web services, and Economics communities. There is a clear need for an annual event at which those communities meet, debate, challenge each others approaches, and eventually align their research efforts. Due to the strong involvement of Semantic Web researchers in the field, ESWC is the ideal target venue for this event. In this workshop, we want to bring together experts from the relevant communities and help reach agreement on a roadmap for SBPM research. We aim at bundling experiences and prototypes from the successful application of Semantic Web technology to BPM in various industries, like automotive, engineering, chemical and pharmaceutical, and services domains. The particular focus is on deriving reusable best-practices from such experiences, and to yield convincing showcases of semantic technology.
5328 en Graph Kernels Between Point Clouds Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow to use kernel methods for such objects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.
5329 en Building a National Semantic Web Ontology and Ontology Service Infrastructure - The FinnONTO Approach 
5330 en KonneXSALT: First Steps towards a Semantic Claim Federation Infrastructure 
5331 en Semantic Email as a Communication Medium for the Social Semantic Desktop 
5332 en IVEA: An Information Visualization Tool for Personalized Exploratory Document Collection Analysis 
5333 en Message-passing for Graph-structured Linear Programs Linear programming relaxations are one promising approach to solving the MAP estimation problem in Markov random fields; in particular, a body of past work has focused on the first-order tree-based LP relaxation for the MAP problem. Although a variety of algorithms with interesting connections to this LP have been proposed, to date none is guaranteed to always solve the LP for any problem. In this paper, we develop a family of provably convergent LP solvers based on proximal minimization schemes using Bregman divergences that exploit the underlying graphical structure, and so scale well to large problems. All of our algorithms have a double-loop character, with the outer loop corresponding to the proximal sequence, and an inner loop of cyclic Bregman divergences used to compute each proximal update. The inner loop updates are distributed and respect the graph structure, and thus can be cast as message-passing algorithms. We establish various convergence guarantees for our algorithms, illustrate their performance on medium to large-scale problems, and also present a tree-based rounding scheme with provable optimality guarantees.
5334 en Combining SAWSDL, OWL-DL and UDDI for Semantically Enhanced Web Service Discovery 
5335 en Enhancing Workflow with a Semantic Description of Scientific Intent 
5336 en Conceptual Situation Spaces for Semantic Situation-Driven Processes 
5337 en Fast Incremental Proximity Search in Large Graphs In this paper we investigate two aspects of ranking problems on large graphs. First, we augment the deterministic pruning algorithm in Sarkar and Moore (2007) with sampling techniques to compute approximately correct rankings with high probability under random walk based proximity measures at query time. Second, we prove some surprising locality properties of these proximity measures by examining the short term behavior of random walks. The proposed algorithm can answer queries on the fly without caching any information about the entire graph. We present empirical results on a 600,000 node author-word-citation graph from the Citeseer domain on a single CPU machine where the average query processing time is around 4 seconds. We present quantifiable link prediction tasks. On most of them our techniques outperform Personalized Pagerank, a well-known diffusion based proximity measure.
5338 en Statistical Models for Partial Membership We present a principled Bayesian framework for modeling partial memberships of data points to clusters. Unlike a standard mixture model which assumes that each data point belongs to one and only one mixture component, or cluster, a partial membership model allows data points to have fractional membership in multiple clusters. Algorithms which assign data points partial memberships to clusters can be useful for tasks such as clustering genes based on microarray data and global positioning and orbit determination. Our Bayesian Partial Membership Model (BPM) uses exponential family distributions to model each cluster, and a product of these distibtutions, with weighted parameters, to model each datapoint. Here the weights correspond to the degree to which the datapoint belongs to each cluster. All parameters in the BPM are continuous, so we can use Hybrid Monte Carlo to perform inference and learning. We discuss relationships between the BPM and Latent Dirichlet Allocation, Mixed Membership models, Exponential Family PCA, and fuzzy clustering. Lastly, we show some experimental results and discuss nonparametric extensions to our model.
5340 en Hierarchical Kernel Stick-Breaking Process for Multi-Task Image Analysis The kernel stick-breaking process (KSBP) is employed to segment general imagery, imposing the condition that patches (small blocks of pixels) that are spatially proximate are more likely to be associated with the same cluster (segment). The number of clusters is not set a priori and is inferred from the hierarchical Bayesian model. Further, KSBP is integrated with a shared Dirichlet process prior to simultaneously model multiple images, inferring their inter-relationships. This latter application may be useful for sorting and learning relationships between multiple images. The Bayesian inference algorithm is based on a hybrid of variational Bayesian analysis and local sampling. In addition to providing details on the model and associated inference framework, example results are presented for several image-analysis problems.
5341 en Rabbit: Developing a Control Natural Language for Authoring Ontologies 
5342 en Data Spectroscopy: Learning Mixture Models using Eigenspaces of Convolution Operators In this paper we develop a spectral framework for estimating mixture distributions, specifically Gaussian mixture models. In physics, spectroscopy is often used for the identification of substances through their spectrum. Treating a kernel function K(x,y) as "light" and the sampled data as "substance", the spectrum of their interaction (eigenvalues and eigenvectors of the kernel matrix K) unveils certain aspects of the underlying parametric distribution p, such as the parameters of a Gaussian mixture. Our approach extends the intuitions and analyses underlying the existing spectral techniques, such as spectral clustering and Kernel Principal Components Analysis (KPCA). We construct algorithms to estimate parameters of Gaussian mixture models, including the number of mixture components, their means and covariance matrices, which are important in many practical applications. We provide a theoretical framework and show encouraging experimental results.
5343 en A Natural Language Query Interface to Structured Information 
5344 en Ontologies and Natural Language: Enriching an Ontology with Multilingual Information 
5345 en Distinguishing Between Instances and Classes in the Wikipedia Taxonomy 
5346 en Semi-supervised Learning of Compact Document Representations with Deep Networks Finding a good representation of text documents is crucial in document retrieval and classification systems. Nowadays, the most popular representation is simply based on a vector of counts storing the number of occurrences of each word in the document. This representation falls short in describing the dependence existing between similar words, and it cannot disambiguate phenomena like synonymy and polysemy of words. In this paper, we propose an algorithm to learn text document representations based on the recent advances in training deep networks. This technique can efficiently produce a very compact and informative representation of a document. Our experiments compare favorably this algorithm against similar algorithms but producing sparse and binary representations. Unlike other models, this method is trained by taking into account both an unsupervised and a supervised objective. We show that it is very advantageous to exploit even a few labeled samples during training, and that we can learn extremely compact representations by using deep and non-linear models.
5347 en Large Scale Manifold Transduction We show how the regularizer of Transductive Support Vector Machines (TSVM) can be trained by stochastic gradient descent for linear models and multi-layer architectures. The resulting methods can be trained online, have vastly superior training and testing speed to existing TSVM algorithms, can encode prior knowledge in the network architecture, and obtain competitive error rates. We then go on to propose a natural generalization of the TSVM loss function that takes into account neighborhood and manifold information directly, unifying the two-stage Low Density Separation method into a single criterion, and leading to state-of-the-art results.
5348 en Semantic Web Technology for Agent Communication Protocols 
5349 en Graph Transduction via Alternating Minimization Graph transduction methods label input data by learning a classification function that is regularized to exhibit smoothness along a graph over labeled and unlabeled samples. In practice, these algorithms are sensitive to the initial set of labels provided by the user. For instance, classification accuracy drops if the training set contains weak labels, if imbalances exist across label classes or if the labeled portion of the data is not chosen at random. This paper introduces a propagation algorithm that more reliably minimizes a cost function over both a function on the graph and a binary label matrix. The cost function generalizes prior work in graph transduction and also introduces node normalization terms for resilience to label imbalances. We demonstrate that global minimization of the function is intractable but instead provide an alternating minimization scheme that incrementally adjusts the function and the labels towards a reliable local minimum. Unlike prior methods, the resulting propagation of labels does not prematurely commit to an erroneous labeling and obtains more consistent labels. Experiments are shown for synthetic and real classification tasks including digit and text recognition. A substantial improvement in accuracy compared to state of the art semi-supervised methods is achieved. The advantage are even more dramatic when labeled instances are limited.
5350 en xOperator -- Interconnecting the Semantic Web and Instant Messaging Networks 
5351 en Stability of Transductive Regression Algorithms This paper uses the notion of algorithmic stability to derive novel generalization bounds for several families of transductive regression algorithms, both by using convexity and closed-form solutions. Our analysis helps compare the stability of these algorithms. It suggests that several existing algorithms might not be stable but prescribes a technique to make them stable. It also reports the results of experiments with local transductive regression demonstrating the benefit of our stability bounds for model selection, in particular for determining the radius of the local neighborhood used by the algorithm.
5352 en An Ontology for Software Models and its Practical Implications for Semantic Web Reasoning 
5353 en A Core Ontology for Business Process Analysis 
5354 en On Multi-View Active Learning and the Combination with Semi-Supervised Learning Multi-view learning has become a hot topic during the past few years. In this paper, we first characterize the sample complexity of multi-view active learning. Under the ?-expansion assumption, we get an exponential improvement in the sample complexity from usual Õ(1/?) to Õ(log 1/?), requiring neither strong assumption on data distribution such as the data is distributed uniformly over the unit sphere in ?d nor strong assumption on hypothesis class such as linear separators through the origin. We also give an upper bound of the error rate when the ?-expansion assumption does not hold. Then, we analyze the combination of multi-view active learning and semi-supervised learning and get a further improvement in the sample complexity. Finally, we study the empirical behavior of the two paradigms, which verifies that the combination of multi-view active learning and semi-supervised learning is efficient.
5355 en Combining Fact and Document Retrieval with Spreading Activation for Semantic Desktop Search 
5356 en Hybrid Search: Effectively Combining Keywords and Semantic Searches 
5357 en Q2Semantic: A Lightweight Keyword Interface to Semantic Search 
5358 en Estimating Labels from Label Proportions Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, also with known label proportions. This problem appears in areas like e-commerce, spam filtering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice.
5359 en Query Answering and Ontology Population: an Inductive Approach 
5360 en Instance Based Clustering of Semantic Web Resources 
5361 en Conceptual Clustering and its Application to Concept Drift and Novelty Detection 
5362 en A Functional Semantic Architecture 
5363 en An Entity Name System (ENS) for the Semantic Web 
5364 en Semantic Sitemaps: Efficient and Flexible Access to Datasets on the Semantic Web 
5365 en Self-taught Clustering This paper focuses on a new clustering task, called self-taught clustering. Self-taught clustering is an instance of unsupervised transfer learning, which aims at clustering a small collection of target unlabeled data with the help of a large amount of auxiliary unlabeled data. The target and auxiliary data can be different in topic distribution. We show that even when the target data are not sufficient to allow effective learning of a high quality feature representation, it is possible to learn the useful features with the help of the auxiliary data on which the target data can be clustered effectively. We propose a co-clustering based self-taught clustering algorithm to tackle this problem, by clustering the target and auxiliary data simultaneously to allow the feature representation from the auxiliary data to influence the target data through a common set of features. Under the new data representation, clustering on the target data can be improved. Our experiments on image clustering show that our algorithm can greatly outperform several state-of-the-art clustering methods when utilizing irrelevant unlabeled auxiliary data.
5366 en On Storage Policies for Semantic Web Repositories that Support Versioning 
5367 en Spectral Clustering with Inconsistent Advice Clustering with advice (often known as constrained clustering) has been a recent focus of the data mining community. Success has been achieved incorporating advice into the k-means framework, as well as spectral clustering. Although the theory community has explored inconsistent advice, it has not yet been incorporated into spectral clustering. Extending work of De Bie and Cristianini, we set out a framework for finding minimum normalized cuts, subject to inconsistent advice. Our results suggest that the framework will be successful in many situations.
5368 en Putting Ontology Alignment in Context: Usage Scenarios, Deployment and Evaluation in a Library Case 
5369 en Two Variations on Ontology Alignment: Methodological Issues 
5370 en CSR: Discovering Subsumption Relations for the Alignment of Ontologies 
5371 en Pairwise Constraint Propagation by Semidefinite Programming for Semi-Supervised Classification We consider the general problem of learning from pairwise constraints and unlabeled data. The pairwise constraints specify whether two objects belong to the same class or not, known as the must-link constraints and the cannot-link constraints. We propose to learn a mapping that is smooth over the data graph and maps the data onto a unit hypersphere, where two must-link objects are mapped to the same point while two cannot-link objects are mapped to be orthogonal. We show that such a mapping can be achieved by formulating a semidefinite programming problem, which is convex and can be solved globally. Our approach can effectively propagate pairwise constraints to the whole data set. It can be directly applied to multi-class classification and can handle data labels, pairwise constraints, or a mixture of them in a unified framework. Promising experimental results are presented for classification tasks on a variety of synthetic and real data sets.
5372 en Semantic Reasoning: A Path To New Possibilities of Personalization 
5373 en OntoGame: Weaving the Semantic Web by Online Games 
5374 en An User Interface Adaptation Architecture for Rich Internet Applications 
5375 en Listwise Approach to Learning to Rank - Theory and Algorithm This paper aims to conduct a comprehensive study on the listwise approach to learning to rank. The listwise approach learns a ranking function by taking individual lists as instances and minimizing a loss function defined on two lists (one is predicted result and the other ground truth). Existing work on the approach mainly focused on the development of new algorithms; methods such as RankCosine and ListNet have been proposed and better performances by them have also been observed. Unfortunately, the underlying theory was not sufficiently studied as far. To amend the problem, this paper proposes conducting theoretical analysis of learning to rank algorithms through investigation on the properties of the loss functions, including consistency, soundness, continuity, differentiability, convexity, and efficiency. A sufficient condition on consistency for ranking is given, which seems to be the first such result obtained in related research. The paper then conducts analysis on three loss functions: likelihood loss, cosine loss, and cross entropy loss. The latter two were used in RankCosine and ListNet respectively. The use of likelihood loss leads to the development of a new listwise method called ListMLE, whose loss function offers better properties. Experimental results have also verified the correctness of the theoretical results obtained in the paper.
5376 en Query-Level Stability and Generalization in Learning to Rank This paper is concerned with the generalization ability of learning to rank algorithms for information retrieval (IR). We point out that the key for addressing the learning problem is to look at it from the viewpoint of query, and we give a formulation of learning to rank for IR based on the consideration. We define a number of new concepts within the framework, including query-level loss, query-level risk, and query-level stability. We then analyze the generalization ability of learning to rank algorithms by giving query-level generalization bounds to them using query-level stability as a tool. Such an analysis is very helpful for us to derive more advanced algorithms for IR. We apply the proposed theory to the existing algorithms of Ranking SVM and IRSVM. Experimental results on the two algorithms verify the correctness of the theoretical analysis.
5377 en Predicting Diverse Subsets Using Structural SVMs In many retrieval tasks, one important goal involves retrieving a diverse set of results (e.g., documents covering a wide range of topics for a search query). First of all, this reduces redundancy, effectively presenting more information with the presented results. Secondly, search queries are often ambiguous at some level. For example, the query “Jaguar” can refer to many different topics (such as the car or the feline). A set of documents with high topic diversity ensures that fewer users abandon the query because none of the results are relevant to them. Unlike existing approaches to learning retrieval functions, we present a method that explicitly trains to diversify results. In particular, we formulate the learning problem of predicting a diverse subset and derive a training algorithm based on structural SVMs.
5378 en Learning Diverse Rankings with Multi-Armed Bandits Algorithms for learning to rank Web documents usually assume a document's relevance is independent of other documents. This leads to learned ranking functions that produce rankings with redundant results. In contrast, user studies have shown that diversity at high ranks is often preferred. We present two new learning algorithms that directly learn a diverse ranking of documents based on users' clicking behavior. We show that these algorithms minimize abandonment, or alternatively, maximize the probability that a relevant document is found in the top k positions of a ranking. We show that one of our algorithms asymptotically achieves the best possible payoff obtainable in polynomial time even as user's interests change. The other performs better empirically when user interests are static, and is still theoretically near-optimal in that case.
5379 en Confidence-Weighted Linear Classification We introduce confidence-weighted linear classifiers, a new class of algorithms that maintain confidence information about classifier parameters. Learning in this framework updates parameters by estimating weights and increasing model confidence. We investigate a new online algorithm that maintains a Gaussian distribution over weight vectors, updating the mean and variance of the model with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training.
5380 en The Projectron: a Bounded Kernel-Based Perceptron We present a discriminative online algorithm with a bounded memory growth, which is based on the kernel-based Perceptron. Generally, the required memory of the kernel-based Perceptron for storing the online hypothesis is not bounded. Previous work has been focused on discarding part of the instances in order to keep the memory bounded. In the proposed algorithm the instances are not discarded, but projected onto the space spanned by the previous online hypothesis. We derive a relative mistake bound and compare our algorithm both analytically and empirically to the state-of-the-art Forgetron algorithm (Dekel et al, 2007). The first variant of our algorithm, called Projectron, outperforms the Forgetron. The second variant, called Projectron++, outperforms even the Perceptron.
5381 en Efficient Bandit Algorithms for Online Multiclass Prediction This paper introduces the Banditron, a variant of the Perceptron, for the multiclass bandit setting. The multiclass bandit setting models a wide range of practical supervised learning applications where the learner only receives partial feedback (referred to as "bandit" feedback, in the spirit of multi-armed bandit models) with respect to the true label (e.g. in many web applications users often only provide positive "click" feedback which does not necessarily fully disclose a true label). The Banditron has the ability to learn in a multiclass classification setting with the "bandit" feedback which only reveals whether or not the prediction made by the algorithm was correct or not (but does not necessarily reveal the true label). We provide (relative) mistake bounds which show how the Banditron enjoys favorable performance, and our experiments demonstrate the practicality of the algorithm. Furthermore, this paper pays close attention to the important special case when the data is linearly separable --- a problem which has been exhaustively studied in the full information setting yet is novel in the bandit setting.
5382 en A Dual Coordinate Descent Method for Large-scale Linear SVM In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear SVM with L1- and L2-loss functions. The proposed method is simple and reaches an epsilon-accurate solution in O(log (1/epsilon)) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, Tron, svmperf, and a recent primal coordinate descent implementation.
5383 en Optimized Cutting Plane Algorithm for Support Vector Machines We have developed a new Linear Support Vector Machine (SVM) training algorithm called OCAS. Its computational effort scales linearly with the sample size. In an extensive empirical evaluation OCAS significantly outperforms current state of the art SVM solvers, like SVMLight, SVMPerf and BMRM, achieving speedups of over 1,000 on some datasets over SVMLight and 20 over SVMPerf, while obtaining the same precise Support Vector solution. OCAS even in the early optimization steps shows often faster convergence than the so far in this domain prevailing approximative methods SGD and Pegasos. Effectively parallelizing OCAS we were able to train on a dataset of size 15 million examples (itself about 32GB in size) in just 671 seconds --- a competing string kernel SVM required 97,484 seconds to train on 10 million examples sub-sampled from this dataset.
5384 en Fast Support Vector Machine Training and Classification on Graphics Processors Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training, using Platt's Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor. We also present a GPU-based system for SVM classification which achieves speedups of 81-138x over LibSVM (5-24x over our own CPU-based SVM classifier).
5385 en Improved Nystrom Low-Rank Approximation and Error Analysis Low-rank matrix approximation is an effective tool in alleviating the memory and computational burdens of kernel methods and sampling, as the mainstream of such algorithms, has drawn considerable attention in both theory and practice. This paper presents detailed studies on the Nystrom sampling scheme and in particular, an error analysis that directly relates the Nystrom approximation quality with the encoding powers of the landmark points in summarizing the data. The resultant error bound suggests a simple and efficient sampling scheme, the k-means clustering algorithm, for Nystrom low-rank approximation. We compare it with state-of-the-art approaches that range from greedy schemes to probabilistic sampling. Our algorithm achieves significant performance gains in a number of supervised/unsupervised learning tasks including kernel PCA and least squares SVM.
5386 en Polyhedral Classifier for Target Detection A Case Study In this study we introduce a novel algorithm for learning a polyhedron to describe the target class. The proposed approach takes advantage of the limited subclass information made available for the negative samples and jointly optimizes multiple hyperplane classifiers each of which is designed to classify positive samples from a subclass of the negative samples. The flat faces of the polyhedron provides robustness whereas multiple faces contributes to the flexibility required to deal with complex datasets. Apart from improving the prediction accuracy of the system, the proposed polyhedral classifier also provides run-time speedups as a by-product when executed in a cascaded framework in real-time. We introduce the Computer Aided Detection for Colon Cancer as a case study and evaluate the performance of the proposed technique on a real-world Colon dataset both in terms of prediction accuracy and online execution speed. We also compare the proposed technique against some benchmark classifiers.
5387 en Welcome and introduction 
5388 en Time Dependent Stick Breaking Processes The stick breaking construction of the Dirichlet process has been a popular starting point for many dependent nonparametric processes. This talk considers a temporal version of the Order-Based Dependent Dirichlet Process, which can be extended to more general stick-breaking marginal processes. Interestingly, the simplest constructions lead to marginal Dirichlet and Poisson-Dirichlet processes. Usefully, the first process also has a “Chinese restaurant”-type representation which will be described. Applications to time-dependent mixture modelling will be presented.
5389 en The Infinite Factorial Hidden Markov Model The innite factorial hidden Markov model is a non-parametric extension of the factorial hidden Markov model. Our model denes a probability distribution over an innite num- ber of independent binary hidden Markov chains which together produce an observable sequence of random variables. Central to our model is a new type of non-parametric prior distribution inspired by the Indian Buf- fet Process which we call the Markov Indian Buet Process.
5390 en Dynamic Non-Parametric Mixture Models and The Recurrent Chinese Restaurant Process Dirichlet process mixture models provide a °exible Bayesian framework for estimating a distribution as an in¯nite mixture of simpler distributions that could identify latent classes in the data [1]. However the full exchangeability assumption they employ makes them an unappealing choice for modeling longitudinal data such as text, audio and video streams that can arrive or accumulate as epochs, where data points inside the same epoch can be assumed to be fully exchangeable, whereas across the epochs both the structure (i.e., the number of mixture components) and the parameteriza- tions of the data distributions can evolve and therefore unexchangeable.
5391 en Nonparametric Functional Data Analysis 
5392 en Nonparametric Bayesian Density Modeling with Gaussian Processes We present the Gaussian Process Density Sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a fixed density function that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We describe two such MCMC methods. Both methods also allow inference of the hyperparameters of the Gaussian process.
5393 en Topic Models Conditioned on Arbitrary Features with Dirichlet-multinomial Regression Although fully generative models have been successfully used to model the contents of text documents, they are often awkward to apply to combinations of text data and document metadata. In this paper we propose a Dirichlet-multinomial regression (DMR) topic model that includes a log-linear prior on document-topic distributions that is a function of observed features of the document, such as author, publication venue, references, and dates. We show that by selecting appropriate features, DMR topic models can meet or exceed the performance of several previously published topic models designed for specific data.
5394 en Latent Topic Models for Hypertext Latent topic models have been successfully applied as an unsupervised topic discovery technique in large document collections. With the proliferation of hypertext document collection such as the Internet, there has also been great interest in extending these approaches to hypertext [6, 9]. These approaches typically model links in an analogous fashion to how they model words - the document-link co-occurrence matrix is modeled in the same way that the document-word co-occurrence matrix is modeled in standard topic models. In this paper we present a probabilistic generative model for hypertext document collections that explicitly models the generation of links. Specifically, links from a word w to a document d depend directly on how frequent the topic of w is in d, in addition to the in-degree of d. We show how to perform EM learning on this model efficiently. By not modeling links as analogous to words, we end up using far fewer free parameters and obtain better link prediction results.
5395 en Flexible Priors for Exemplar-based Clustering Exemplar-based clustering methods have been shown to produce state-of-the-art results on a number of synthetic and real-world clustering problems. They are appealing because they offer computational benefits over latent-mean models and can handle arbitrary pairwise similarity measures between data points. However, when trying to recover underlying structure in clustering problems, tailored similarity measures are often not enough; we also desire control over the distribution of cluster sizes. Priors such as Dirichlet process priors allow the number of clusters to be unspecified while expressing priors over data partitions. To our knowledge, they have not been applied to exemplar-based models. We show how to incorporate priors, including Dirichlet process priors, into the recently introduced affinity propagation algorithm. We develop an efficient max product belief propagation algorithm for our new model and demonstrate experimentally how the expanded range of clustering priors allows us to better recover true clusterings in situations where we have some information about the generating process.
5396 en Hierarchical Base Compiler 
5397 en The Catch-Up Phenomenon in Bayesian Inference Standard Bayesian model selection/averaging sometimes learn too slowly: there exist other learning methods that lead to better predictions based on less data. We give a novel analysis of this "catch-up" phenomenon. Based on this analysis, we propose the switching method, a modification of Bayesian model averaging that never learns slower, but sometimes learns much faster than Bayes. The method is related to expert-tracking algorithms developed in the COLT literature, and has time complexity comparable to Bayes. The switching method resolves a long-standing debate in statistics, known as the AIC-BIC dilemma: model selection/averaging methods like BIC, Bayes, and MDL are consistent (they eventually infer the correct model) but, when used for prediction, the rate at which predictions improve can be suboptimal. Methods like AIC and leave-one-out cross-validation are inconsistent but typically converge at the optimal rate. Our method is the first that provably achieves both. Experiments with nonparametric density estimation confirm that these large-sample theoretical results also hold in practice in small samples.
5398 en Convergent Message-Passing Algorithms for Inference over General Graphs with Convex Free Energies Inference problems in graphical models can be represented as a constrained optimization of a free energy function. It is known that when the Bethe free energy is used, the fixed points of the belief propagation (BP) algorithm correspond to the local minima of the free energy. However BP fails to converge in many cases of interest. Moreover, the Bethe free energy is non-convex for graphical models with cycles thus introducing great difficulty in deriving efficient algorithms for finding local minima of the free energy for general graphs. In this paper we introduce two efficient BP-like algorithms, one sequential and the other parallel, that are guaranteed to converge to the global minimum, for any graph, over the class of energies known as ”convex free energies”. In addition, we propose an efficient heuristic for setting the parameters of the convex free energy based on the structure of the graph.
5399 en Tightening LP Relaxations for MAP using Message Passing Linear Programming (LP) relaxations have become powerful tools for finding the most probable (MAP) configuration in graphical models. These relaxations can be solved efficiently using message-passing algorithms such as belief propagation and, when the relaxationis tight, provably find the MAP configuration. The standard LP relaxation is not tight enough in many real-world problems, however, and this has lead to the use of higher order cluster-based LP relaxations. The computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can use. We propose to solve the cluster selection problem monotonically in the dual LP, iteratively selecting clusters with guaranteed improvement, and quickly re-solving with the added clusters by reusing the existing solution. Our dual message-passing algorithm finds the MAP configuration in protein side chain placement, protein design, and stereo problems, in cases where the standard LP relaxation fails.
5400 en Learning Convex Inference of Marginals Graphical models trained using maximum likelihood are a common tool for probabilistic inference of marginal distributions. However, this approach suffers difficulties when either the inference process of the model is approximate. In this paper, the inference process is first defined to be the minimization of a convex function, inspired by free energy approximations. Learning is then done directly in terms of the performance of the inference process at univariate marginal prediction. The main novelty is that this is a direct minimization of empirical risk, where the risk measures the accuracy of predicted marginals.
5401 en Combinatorial Prediction Markets Several hundred organizations are now using prediction markets to forecast sales, project completion dates, and more. This number has been doubling annually for several years. Most, however, are simple prediction markets, with one market per number forecast, and several traders per market. In contrast, a single combinatorial prediction market lets a few traders manage an entire combinatorial space of forecasts. For millions of numbers or less, implementation is easy, and lab experiments have confirmed feasibility and accuracy. For larger spaces, however, many open computational problems remain.
5402 en Learning and Solving Many-Player Games through a Cluster-Based Representation In addressing the challenge of exponential scaling with the number of agents we adopt a cluster-based representation to approximately solve asymmetric games of very many players. A cluster groups together agents with a similar “strategic view” of the game. We learn the clustered approximation from data consisting of strategy profiles and payoffs, which may be obtained from observations of play or access to a simulator. Using our clustering we construct a reduced “twins” game in which each cluster is associated with two players of the reduced game. This allows our representation to be individually responsive because we align the interests of every individual agent with the strategy of its cluster. Our approach provides agents with higher payoffs and lower regret on average than model-free methods as well as previous cluster-based methods, and requires only few observations for learning to be successful. The “twins” approach is shown to be an important component of providing these low regret approximations.
5403 en A Polynomial-time Nash Equilibrium Algorithm for Repeated Stochastic Games We present a polynomial-time algorithm that always finds an (approximate) Nash equilibrium for repeated two-player stochastic games. The algorithm exploits the folk theorem to derive a strategy profile that forms an equilibrium by buttressing mutually beneficial behavior with threats, where possible. One component of our algorithm efficiently searches for an approximation of the egalitarian point, the fairest pareto-efficient solution. The paper concludes by applying the algorithm to a set of grid games to illustrate typical solutions the algorithm finds. These solutions compare very favorably to those found by competing algorithms, resulting in strategies with higher social welfare, as well as guaranteed computational efficiency.
5404 en Strategy Selection in Influence Diagrams using Imprecise Probabilities This paper describes a new algorithm to solve the decision making problem in Influence Diagrams based on algorithms for credal networks. Decision nodes are associated to imprecise probability distributions and a reformulation is introduced that finds the global maximum strategy with respect to the expected utility. We work with Limited Memory Influence Diagrams, which generalize most Influence Diagram proposals and handle simultaneous decisions. Besides the global optimum method, we explore an anytime approximate solution with a guaranteed maximum error and show that imprecise probabilities are handled in a straightforward way. Complexity issues and experiments with random diagrams and an effects-based military planning problem are discussed.
5405 en Multi-View Learning over Structured and Non-Identical Outputs In many machine learning problems, labeled training data is limited but unlabeled data is ample. Some of these problems have instances that can be factored into multiple views, each of which is nearly sufficient in determining the correct labels. In this paper we present a new algorithm for probabilistic multi-view learning which uses the idea of stochastic agreement between views as regularization. Our algorithm works on structured and unstructured problems and easily generalizes to partial agreement scenarios. For the full agreement case, our algorithm minimizes the Bhattacharyya distance between the models of each view, and performs better than CoBoosting and two-view Perceptron on several flat and structured classification problems.
5406 en Multi-View Learning in the Presence of View Disagreement Traditional multi-view learning approaches suffer in the presence of view disagreement, i.e., when samples in each view do not belong to the same class due to view corruption, occlusion or other noise processes. In this paper we present a multi-view learning approach that uses a conditional entropy criterion to detect view disagreement. Once detected, samples with view disagreement are filtered and standard multi-view learning methods can be successfully applied to the remaining samples. Experimental evaluation on synthetic and audio-visual databases demonstrates that the detection and filtering of view disagreement considerably increases the performance of traditional multi-view learning approaches.
5407 en Convex Point Estimation using Undirected Bayesian Transfer Hierarchies When related learning tasks are naturally arranged in a hierarchy, an appealing approach for coping with scarcity of instances is that of transfer learning using a hierarchical Bayes framework. As fully Bayesian computations can be difficult and computationally demanding, it is often desirable to use posterior point estimates that facilitate (relatively) efficient prediction. However, the hierarchical Bayes framework does not always lend itself naturally to this maximum a posteriori goal. In this work we propose an undirected reformulation of hierarchical Bayes that relies on priors in the form of similarity measures. We introduce the notion of “degree of transfer” weights on components of these similarity measures, and show how they can be automatically learned within a joint probabilistic framework. Importantly, our reformulation results in a convex objective for many learning problems, thus facilitating optimal posterior point estimation using standard optimization techniques. In addition, we no longer require proper priors, allowing for flexible and straightforward specification of joint distributions over transfer hierarchies. We show that our framework is effective for learning models that are part of transfer hierarchies for two real-life tasks: object shape modeling using Gaussian density estimation and document classification.
5408 en Concentration Inequalities In this talk by concentration inequalities we mean inequalities that bound the deviations of a function of independent random variables from its mean. Due to their generality and elegance, many of such results have served as standard tools in a variety of areas, including statistical learning theory, probabilistic combinatorics, and the geometry of Banach spaces. To illustrate some of the basic ideas, we start by showing simple ways of bounding the variance of a general function of several independent random variables. We show how to use these inequalities on a few key quantities in statistical learning theory. In the past two decades several techniques have been introduced to improve such variance inequalities to exponential tail inequalities. We focus on a particularly elegant and effective method, the so-called "entropy method", based on logarithmic Sobolev inequalities and their modifications. Similar ideas appear in a variety of areas of mathematics, including discrete and Gaussian isoperimetric problems, and estimation of mixing times of Markov chains. We intend to shed some light to some of these connections. In particular, we mention some closely related results on influences of variables of Boolean functions, phase transitions, and threshold phenomena.
5409 en CORL: A Continuous-state Offset-dynamics Reinforcement Learner Continuous state spaces and stochastic, switching dynamics characterize a number of rich, real world domains, such as robot navigation across varying terrain. We describe a reinforcement learning algorithm for learning in these domains and prove for certain environments the algorithm is probably approximately correct with a sample complexity that scales polynomially with the state-space dimension. Unfortunately, no optimal planning techniques exist in general for such problems; instead we use fitted value iteration to solve the learned MDP, and include the error due to approximate planning in our bounds. Finally, we report an experiment using a robotic car driving over varying terrain to demonstrate that these dynamics representations adequately capture real-world dynamics and that our algorithm can be used to efficiently solve such problems.
5410 en Partitioned Linear Programming Approximations for MDPs Approximate linear programming (ALP) is an efficient approach to solving large factored Markov decision processes (MDPs). The main idea of the method is to approximate the optimal value function by a set of basis functions and optimize their weights by linear programming (LP). This paper proposes a new ALP approximation. Comparing to the standard ALP formulation, we decompose the constraint space into a set of low-dimensional spaces. This structure allows for solving the new LP efficiently. In particular, the constraints of the LP can be satisfied in a compact form without an exponential dependence on the tree width of ALP constraints. We study both practical and theoretical aspects of the proposed approach. Moreover, we demonstrate its scale-up potential on an MDP with more than 2100 states.
5411 en Hierarchical POMDP Controller Optimization by Likelihood Maximization Planning can often be simplified by decomposing the task into smaller tasks arranged hierarchically. Charlin et al. recently showed that the hierarchy discovery problem can be framed as a non-convex optimization problem. However, the inherent computational difficulty of solving such an optimization problem makes it hard to scale to real world problems. In another line of research, Toussaint et al. developed a method to solve planning problems by maximum likelihood estimation. In this paper, we show how the hierarchy discovery problem in partially observable domains can be tackled using a similar maximum likelihood approach. Our technique first transforms the problem into a dynamic Bayesian network through which a hierarchical structure can naturally be discovered while optimizing the policy. Experimental results demonstrate that this approach scales better than previous techniques based on non-convex optimization.
5412 en Unsupervised Learning for Natural Language Processing Given the abundance of text data, unsupervised approaches are very appealing for natural language processing. We present three latent variable systems which achieve state-of-the-art results in domains previously dominated by fully supervised systems. For syntactic parsing, we describe a grammar induction technique which begins with coarse syntactic structures and iteratively refines them in an unsupervised fashion. The resulting coarse-to-fine grammars admit efficient coarse-to-fine inference schemes and have produced the best parsing results in a variety of languages. For co reference resolution, we describe a discourse model in which entities are shared across documents using a hierarchical Dirichlet process. In each document, entities are repeatedly rendered into mention strings by a sequential model of attentional state and anaphoric constraint. Despite being fully unsupervised, this approach is competitive with the best supervised approaches. Finally, for machine translation, we present a model which learns translation lexicons from non-parallel corpora. Alignments between word types are modeled by a prior over matchings. Given any fixed alignment, a joint density over word vectors derives from probabilistic canonical correlation analysis. This approach is capable of discovering high-precision translations, even when the underlying corpora and languages are divergent.
5413 en Explanation Trees for Causal Bayesian Networks Bayesian networks can be used to extract explanations about the observed state of a subset of variables. In this paper, we explicate the desiderata of an explanation and confront them with the concept of explanation proposed by existing methods. The necessity of taking into account causal approaches when a causal graph is available is discussed. We then introduce causal explanation trees, based on the construction of explanation trees using the measure of causal information flow (Ay and Polani, 2006). This approach is compared to several other methods on known networks.
5414 en A stochastic programming perspective on nonparametric Bayes We use Church, a Turing-universal language for stochastic generative processes and the probability distributions they induce, to study and extend several objects in nonparametric Bayesian statistics. We connect exchangeability and de Finetti measures with notions of purity and closures from functional programming. We exploit delayed evaluation to provide finite, machine-executable representations for various nonparametric Bayesian objects. We relate common uses of the Dirichlet process to a stochastic generalization of memoization, and use this abstraction to compactly describe and extend several nonparametric models. Finally, we briefly discuss issues of computability and inference.
5415 en Identifying Optimal Sequential Decisions We consider conditions that allow us to find an optimal strategy for sequential decisions from a given data situation. For the case where all interventions are unconditional (atomic), identifiability has been discussed by Pearl & Robins (1995). We argue here that an optimal strategy must be conditional, i.e. take the information available at each decision point into account. We show that the identification of an optimal sequential decision strategy is more restrictive, in the sense that conditional interventions might not always be identified when atomic interventions are. We further demonstrate that a simple graphical criterion for the identifiability of an optimal strategy can be given.
5416 en Identifying Dynamic Sequential Plans We address the problem of identifying dynamic sequential plans in the framework of causal Bayesian networks, and show that the problem is reduced to identifying causal effects, for which there are complete identification algorithms available in the literature.
5417 en Learning the Bayesian Network Structure: Dirichlet Prior versus Data In the Bayesian approach to structure learning of graphical models, the equivalent sample size (ESS) in the Dirichlet prior over the model parameters was recently shown to have an important effect on the maximum-a-posteriori estimate of the Bayesian network structure. In our first contribution, we theoretically analyze the case of large ESS-values, which complements previous work: among other results, we find that the presence of an edge in a Bayesian network is favored over its absence even if both the Dirichlet prior and the data imply independence, as long as the conditional empirical distribution is notably different from uniform. In our second contribution, we focus on realistic ESS-values, and provide an analytical approximation to the ‘optimal’ ESS-value in a predictive sense (its accuracy is also validated experimentally): this approximation provides an understanding as to which properties of the data have the main effect determining the ‘optimal’ ESS-value.
5418 en Discovering Cyclic Causal Models by Independent Components Analysis We generalize Shimizu et al's (2006) ICA-based approach for discovering linear non-Gaussian acyclic (LiNGAM) Structural Equation Models (SEMs) from causally sufficient, continuous-valued observational data. By relaxing the assumption that the generating SEM's graph is acyclic, we solve the more general problem of linear non-Gaussian (LiNG) SEM discovery. LiNG discovery algorithms output the distribution equivalence class of SEMs which, in the large sample limit, represents the population distribution. We apply a LiNG discovery algorithm to simulated data. Finally, we give sufficient conditions under which only one of the SEMs in the output class is \stable".
5419 en Constrained Approximate Maximum Entropy Learning of Markov Random Fields Parameter estimation in Markov random fields (MRFs) is a difficult task, in which inference over the network is run in the inner loop of a gradient descent procedure. Replacing exact inference with approximate methods such as loopy belief propagation (LBP) can suffer from poor convergence. In this paper, we provide a different approach for combining MRF learning and Bethe approximation. We consider the dual of maximum likelihood Markov network learning - maximizing entropy with moment matching constraints - and then approximate both the objective and the constraints in the resulting optimization problem. Unlike previous work along these lines (Teh & Welling, 2003), our formulation allows parameter sharing between features in a general log-linear model, parameter regularization and conditional training. We show that piecewise training (Sutton & McCallum, 2005) is a very restricted special case of this formulation. We study two optimization strategies: one based on a single convex approximation and one that uses repeated convex approximations. We show results on several real-world networks that demonstrate that these algorithms can significantly outperform learning with loopy and piecewise. Our results also provide a framework for analyzing the trade-offs of different relaxations of the entropy objective and of the constraints.
5420 en Ingredients for Developing NPB Software 
5421 en On a Sub-optimality Conjecture 
5422 en Covariate Dependent Random Partitions We propose a model for covariate-dependent clustering, i.e., we develop a probability model for random partitions that is indexed by covariates. The motivating application is inference for a clinical trial. As part of the desired inference we wish to define clusters of patients. Defining a prior probability model for cluster memberships should include a regression on patient baseline covariates. We build on product partition models (PPM). We define an extension of the PPM to include the desired regression. This is achieved by including in the cohesion function a new factor that increases the probability of experimental units with similar covariates to be included in the same cluster. We discuss implementations suitable for continuous, categorical, count and ordinal covariates.
5423 en The Mondrian Process We describe a novel stochastic process that can be used to construct a multidimensional generalization of the stick-breaking process and which is related to the classic stick breaking process described by Sethuraman [1994] in one dimension. We describe how the process can be applied to relational data modeling using the de Finetti representation for infinitely and partially exchangeable arrays.
5424 en An introduction to Levy processes with financial modelling in mind In this talk I will take some care to introduce the general class of Levy processes as well as the most relevant parametric families. I will explain how these can be used for modelling purposes, directly or as driving processes for more general stochastic processes. As an application, I'll discuss stochastic volatility modelling and some questions arising when doing inference in the presence of jumps, based on joint work with Ole Barndorff-Nielsen and Neil Shephard.
5425 en Variational filtering in generated coordinates of motion This presentation reviews a variational treatment of dynamic models that furnishes time-dependent conditional densities on the path or trajectory of a system's states and the time-independent densities of its parameters. These obtain by maximizing a variational action with respect to conditional densities, under a fixed-form assumption about their form. The action or path-integral of free-energy represents a lower-bound on the model’s log-evidence or marginal likelihood required for model selection and averaging. This approach rests on formulating the optimization in generalized co-ordinates of motion. The resulting scheme can be used for on-line Bayesian inversion of nonlinear dynamic causal models and is shown to outperform existing approaches, such as Kalman and particle filtering. Furthermore, it provides for dual and triple inference on a system’s states, parameters and hyperparameters using exactly the same principles. Free-form (Variational filtering) and fixed form (Dynamic Expectation Maximization) variants of the scheme will be demonstrated using simulated (bird-song) and real data (from hemodynamic systems studied in neuroimaging).
5426 en Density estimation of initial conditions for populations of dynamical systems A computational approach that estimates the probability density of the initial conditions for a population of dynamical systems is presented. Its scope extends to a family of problems which includes the described protein degradation example. It permits the formulation of hypotheses that can justify the discrepancy between single-cell and population dynamics. The approach is based on a preprocessing regression that permits the incorporation of domain knowledge. This knowledge is given under the form of prior information about the trajectory of a single cell and about the dynamical behavior of the noisy observations. In similar problems, additional knowledge can be available as a prior over functions. This advantage is not possible with purely data-driven approaches and, when existing, it must be exploited. In systems biology, the chemical reactions are often understood quite well, but complex systems or networks are still under investigation. However, integration of prior knowledge comes with an high cost and, in general, feasible approaches to compute inference must be approximated.
5427 en Sparse Multi-output Gaussian Processes In this work we propose a sparse approximation for the full covariance matrix involved in the multiple output convolution process. We exploit the fact that each of the outputs is conditional independent of all others given the input process. This leads to an approximation for the covariance matrix which keeps intact the covariances of each output and approximates the cross-covariances terms with a low rank matrix. It has a similar form to the Partially Independent Training Conditional (PITC) approximation for a single output GP.
5428 en Estimating the probability of rare climate events: inference from a large deterministic computer code Anthropogenic emission of greenhouse gases means that it is vital that we can predict future climates. One aspect of such possible future climates are so called low probability high impact events. These include things like the collapse of ice sheets that we hope are unlikely but if they did happen would have very major impacts on the climate. The only way we can address these problems is through computer models, we do not have any data that is applicable. Such models are very large and complex and require huge amounts of computer time. Thus simple Monte Carlo methods of inference cannot be used. Instead we use statistical methods to investigate the properties of the model. These are based around the concept of an emulator. An emulator is a statistical approximation to the model output given the model inputs, and includes a measure of its own uncertainty. We use Gaussian processes for our emulators but in principle other functions could be used. Having built an emulator we can use it to perform our inference rather than the computer model itself. We will illustrate these methods to estimate the risk of the collapse of the thermohaline circulation in the North Atlantic and discuss future improvements.
5429 en Approximate inference for continuous time Markov processes Continuous time Markov processes (such as jump processes and diffusions) play an important role in the modelling of dynamical systems in many scientific areas.n   In a variety of applications, the stochastic state of the system as a function of time is not directly observed. One has only access to a set of nolsy observations taken at a discrete set of times. The problem is then to infer the unknown state path as best as possible. In addition, model parameters (like diffusion constants or transition rates) may also be unknown and have to be estimated from the data. While it is fairly straightforward to present a theoretical solution to these estimation problems, a practical solution in terms of PDEs or by Monte Carlo sampling can be very time consuming and one is looking for efficient approximations. I will discuss approximate solutions to this problem such as variational approximations to the probability measure over paths and weak noise expansions.
5430 en Variational inference and learning for continuous-time nonlinear state-space models Inference in continuous-time stochastic dynamical models is a challenging problem. To complement existing sampling-based methods, variational methods have recently been developed for this problem. Our approach solves the variational continuous-time inference problem by discretisation that essentially reduces it to a discrete-time problem. Our framework makes learning the model in addition to inference easy. Other extensions such as heteroscedastic models are also relatively easy to consider within this framework.
5431 en An efficient Monte-Carlo algorithm for the ML-Type II parameter estimation of nonlinear diffusions The mathematical framework of non-linear diffusions has been playing an important role in modelling natural phenomena. Recently, much efforts have been made in developing inferential methods for such stochastic dynamical systems. Both state- and parameter estimation are of interests. The state-of-art Hybrid-Monte Carlo method has been applied to state estimation of non-linear diffusions. For parameter estimation, the data augmentation strategy is often adopted. Accordingly, state and parameters are sampled in a Gibbs-sampler setting. However, it has been reported that such a Monte-Carlo algorithm has very poor mixing property. This is due to strong correlations between state and parameter samples. In this paper, we propose a maximal likelihood (ML) type II approach to parameter estimation. Equipped with the Wang-Landau algorithm from statistical physics, the novel algorithm is shown to be both accurate and efficient.
5432 en MCMC schemes for partially observed diffusions - Some recent advances It is well known that likelihood inference for arbitrary nonlinear diffusion processes observed at discrete times is problematic since closed form transition densities are rarely tractable. One widely used solution involves the introduction of latent data points between every pair of observations to allow a sufficiently accurate Euler-Maruyama approximation of the true transition densities. In recent literature, Markov chain Monte Carlo (MCMC) methods have been used to sample the posterior distribution of latent data and model parameters; however, naive schemes suffer from a mixing problem that worsens with the degree of augmentation. We will consider some recently developed MCMC schemes that are not adversely affected by the amount of augmentation. In particular, by sampling parameters conditional on a skeleton of the driving Brownian motion rather than the sample path, the mixing problem can be overcome. The methodology will be illustrated by estimating parameters governing the diffusion approximations of some interesting systems biological models.
5433 en Normalized kernel-weighted random measures This talk discusses a wide class of probability measure-valued processes to be used as nonparametric priors for problems with time-varying, patially-varying or covariate-dependent distributions. They are constructed by normalizing correlated random measures, which are stationary and have a known marginal process. Dependence is modelled using kernels (a method that has become popular in spatial modelling). The ideas extend Griffin (2007), which used an exponential kernel in time series problems, to arbitrary kernel functions. Computational issues will be discussed and the ideas will be illustrated by examples in financial time series.
5434 en Solving the data association problem in multi-object tracking by Fourier analysis on the symmetric group In addition to modeling the position of individual targets, multi-object tracking must also address the combinatorial problem of matching objects to corresponding tracks. In general, maintaining a probability distribution over all n! possibilities is clearly infeasible, while just maintaining an n?n matrix of “first order marginals” is a very impoverished representation. In this work we explain how to harness the theory of harmonic analysis on the symmetric group to get a hierarchy of approximations of increasing fidelity to this problem. Importatantly, not only are such band-limited approximations theoretically well justifiable, but they also admit efficient observations updates based on some ideas from Clausen’s FFT for the symmetric group.
5435 en Approximate Bayesian computation: a simulation based approach to inference There is a large class of stochastic models for which we can simulate observations from the model, but for which the likelihood function is unknown. Without knowledge of the likelihood function standard inference techniques such as Markov Chain Monte Carlo are impossible, as the unnormalized likelihood function is explicitly required for the calculation of an acceptance rate. In this talk I shall introduce a group of Monte Carlo methods that can be used to perform inference for stochastic models from which we can cheaply simulate observations.
5436 en Exact simulation of jump diffusions In this talk I will present the Exact Algorithm for simulation of diffusions proposed by Beskos, Papaspiliopoulos and Roberts (2006) and its extension for simulation of jump diffusions proposed by Casella and Roberts (2008). The algorithm is exact in the sense that there is no discretisation error. I will show some simulation results where the Exact Algorithm is compared to the Euler Approximation in the simulation of jump diffusions.
5437 en An efficient approach to stochastic optimal control Stochastic optimal control theory is a principled approach to compute optimal actions with delayed rewards. The use of this approach in AI and machine learning has been limited due to the computational intractabilities. In this talk, I introduce a class of control problems where the intractabilities appear as the computation of a partition sum, as in a statistical mechanical system. This opens the possibility to study phase transitions and to apply exisiting approximation methods such as BP and the variational method to optimal control theory. The talk gives a gentle introduction into control theory and illustrates these new phenomena with a number of examples.
5438 en Information evolution of optimal learning It is widely accepted that learning is closely related to theories of optimisation and information. Indeed, there is no need to learn if there is nothing to optimise; if one possesses full information, then there is simply nothing new to learn. The paper considers learning as an optimisation problem with dynamical information constraints. Unlike the standard approach in the optimal control theory, where the solutions are given by the Hamilton–Jacobi–Bellman equation for Markov time evolution, the optimal solution is presented as the system of canonical Euler equations defining the optimal information–utility trajectory in the conjugate space. The optimal trajectory is parameterised by theinformation–utility constraints, which are illustrated on examples for finite and infinite–dimensional cases.
5439 en Approximate system identification: Misfit versus latency Two fundamentally different approaches in system identification, which are used for quantification of the model–data mismatch, are misfit and latency. The aim of this talk is to explain the rationale behind them and link them to statistical estimation methods—errors-in-variables regression and classical regression—respectively.
5440 en Sigma point and particle approximations of stochastic differential equations in optimal filtering The unscented transform (UT) is a relatively recent method for approximating non-linear transformations of random variables. Instead of the classical Taylor series approximations, it is based on forming a set of sigma points, which are propagated through the non-linearity. The unscented Kalman filter (UKF) is an alternative to the extended Kalman filter (EKF), which utilizes the unscented transform in the filter computations. However, in its original form, the UKF is a discrete-time algorithm and it cannot be directly applied to estimation problems, where the state dynamics are modeled in continuous-time as stochastic differential equations. In the talk I will review the Taylor series, sigma-point (unscented) and particle approximations of stochastic differential equations in optimal (Bayesian) filtering context and present some applications of the methods in navigation systems and in monitoring of chemical processes.
5441 en State estimation and prediction based on dynamic spike train decoding: noise, adaptation, and multisensory integration A key requirement facing organisms, or agents in general, acting in uncertain dynamic environments is the real-time estimation and prediction of environmental states, based upon which effective actions can be selected. In this work we show how an agent may use a simple real time neural network, receiving noisy multisensory input signals, to solve these tasks effectively.
5442 en Gaussian process toolkit for modelling the dynamics of transcriptional regulation The complex dynamics of cells and tissues are regulated in part by networks of interacting genes and proteins. The structure of the interaction network dictates which genes are regulated by which transcription factors (TFs). Particularly, a number of experimental and computational methods have been proposed to explore the mechanisms of transcriptional regulation. A key problem with the analysis of transcription network is that the concentration of the activated TF is difficult to measure directly whereas the target mRNA quantities are relatively easy to obtain with a microarray. Therefore, we focus on the problem of inferring the transcription factor activity given the mRNA expression level data.
5443 en On stratified path sampling of the Thermodynamic Integral: computing Bayes factors for nonlinear dynamical systems models Bayes factors provide a means of objectively ranking a number of plausible statistical models based on their evidential support. Computing Bayes factors is far from straightforward and methodology based on thermodynamic integration can provide stable estimates of the integrated likelihood. This talk will consider a stratified sampling strategy in estimating the thermodynamic integral and will consider issues such as optimal paths and the variance of the overall estimator. The main application considered will be the computation of Bayes factors for dynamical biochemical pathway models based on systems of nonlinear ordinary differential equations (ODE). A large scale study of the ExtraCellular Regulated Kinase (ERK) pathway will be discussed where recent Small Interfering RNA (siRNA) experimental validation of the predictions made using the computed Bayes factors is presented.
5456 en Garlik: Semantic Technology for the Consumer In under a decade the internet has changed our lives. Now we can shop, bank, date, research, learn and communicate online and every time we do we leave behind a trail of personal information. Organisations have a wealth of structured information about individuals on large numbers of databases. What does the intersection of this information mean for the individual? How much of your personal data is out there and more importantly, just who has access to it? As stories of identity theft and online fraud fill the media internet users are becoming increasingly nervous about their online data security. Also what opportunities arise for individuals to exploit this information for their own benefit? Garlik was formed to give individuals and their family's real power over the use of their personal information in the digital world. Garlik's technology base has exploited and extended results from research on the Semantic Web. It has built the world's largest, SPARQL compliant, native format, RDF triple store. The store is implemented on a low-cost network cluster with over 100 servers supporting a 24x7 operation. Garlik has built semantically informed search and harvesting; used industrial strength language engineering technologies across many millions of people-centric Web pages. Methods have been developed for extracting information from structured and semi structured databases. All of this information is organised against a people-centric ontology with facilities to integrate these various fragments. Garlik has received two substantial rounds of venture capital funding (as of March 2008), has established an active user base of tens of thousands of individuals, and is adding paying customers at an increasing rate. This talk reviews the consumer need, describes the technology and engineering, and discusses the lessons we can draw about the challenges of deploying Semantic Technologies.
5457 en From Capturing Semantics to Semantic Search: A Virtuous Cycle Semantic search seems to be an elusive and fuzzy target to IR, SW and NLP researchers. One reason is that this challenge lies in between all those fields, which implies a broad scope of issues and tech- nologies that must be mastered. In this extended abstract we survey the work of Yahoo! Research at Barcelona to approach this problem. Our research is intended to produce a virtuous feedback circuit by using ma- chine learning for capturing semantics, and, ultimately, for better search.
5458 en Foundations of RDF Databases 
5459 en Panel I: Does the Semantic Web Need Web Science? 
5460 en Panel II: Social Network Portability: Is the Semantic Web Ready? Panelists: Dan Brickley (ASemantics), Danny Ayers (Talis), Stefan Decker (DERI, Galway) and Kingsley Idehen (OpenLink) and Peter Mika (Yahoo! Research, Barcelona) and Alexandre Passant (LaLIC, University Paris-Sorbonne)
5461 en Demo Papers I 
5462 en Demo Papers II 
5463 en A Rate-Distortion One-Class Model and its Applications to Clustering We study the problem of one-class classification, in which we seek a rule to separate a coherent subset of instances similar to a few positive examples from a large pool of instances. We find that the problem can be formulated naturally in terms of a rate-distortion tradeoff, which can be analyzed precisely and leads to an efficient algorithm that competes well with two previous one-class methods. We also show that our model can be extended naturally to clustering problems in which it is important to remove background clutter to improve cluster purity.
5464 en Estimating Local Optimums in EM Algorithm over Gaussian Mixture Model EM algorithm is a very popular method to estimate the parameters of Gaussian Mixture Model from a large observation set. However, in most cases, EM algorithm is not guaranteed to converge to the global optimum. Instead, it stops at some local optimums, which can be much worse than the global optimum. Therefore, it is usually required to run multiple procedures of EM algorithm with different initial configurations and return the best solution. To improve the efficiency of this scheme, we propose a new method which can estimate an upper bound on the logarithm likelihood of the local optimum, based on the current configuration after the latest EM iteration. This is accomplished by first deriving some region bounding the possible locations of local optimum, followed by some upper bound estimation on the maximum likelihood. With this estimation, we can terminate an EM algorithm procedure if the estimated local optimum is definitely worse than the best solution seen so far. Extensive experiments show that our method can effectively and efficiently accelerate conventional EM algorithm.
5465 en A Decoupled Approach to Exemplar-based Unsupervised Learning A recent trend in exemplar based unsupervised learning is to formulate the learning problem as a convex optimization problem. Convexity is achieved by restricting the set of possible prototypes to training exemplars. In particular, this has been done for clustering, vector quantization and mixture model density estimation. In this paper we propose a novel algorithm that is theoretically and practically superior to these convex formulations. This is possible by posing the unsupervised learning problem as a single convex "master problem" with non-convex subproblems. We show that for the above learning tasks the subproblems are extremely well-behaved and can be solved efficiently.
5466 en Efficient MultiClass Maximum Margin Clustering This paper presents a cutting plane algorithm for multiclass maximum margin clustering (MMC). The proposed algorithm constructs a nested sequence of successively tighter relaxations of the original MMC problem, and each optimization problem in this sequence could be efficiently solved using the constrained concave-convex procedure (CCCP). Experimental evaluations on several real world datasets show that our algorithm converges much faster than existing MMC methods with guaranteed accuracy, and can thus handle much larger datasets efficiently.
5467 en Fast Solvers and Efficient Implementations for Distance Metric Learning In this paper we study how to improve nearest neighbor classification by learning a Mahalanobis distance metric. We build on a recently proposed framework for distance metric learning known as large margin nearest neighbor (LMNN) classification. Within this framework, we focus specifically on the challenges in scalability and adaptability posed by large data sets. Our paper makes three contributions. First, we describe a highly efficient solver for the particular instance of semidefinite programming that arises in LMNN classification; our solver can handle problems with billions of large margin constraints in a few hours. Second, we show how to reduce both training and testing times using metric ball trees; the speedups from ball trees are further magnified by learning low dimensional representations of the input space. Third, we show how to learn different Mahalanobis distance metrics in different parts of the input space. For large data sets, these mixtures of locally adaptive metrics lead to even lower error rates.
5468 en Nearest Hyperdisk Methods for High-Dimensional Classification In high-dimensional classification problems it is infeasible to include enough training samples to cover the class regions densely. Irregularities in the resulting sparse sample distributions cause local classifiers such as Nearest Neighbors (NN) and kernel methods to have irregular decision boundaries. One solution is to "fill in the holes" by building a convex model of the region spanned by the training samples of each class and classifying examples based on their distances to these approximate models. Methods of this kind based on affine and convex hulls and bounding hyperspheres have already been studied. Here we propose a method based on the bounding hyperdisk of each class -- the intersection of the affine hull and the smallest bounding hypersphere of its training samples. We argue that in many cases hyperdisks are preferable to affine and convex hulls and hyperspheres: they bound the classes more tightly than affine hulls or hyperspheres while avoiding much of the sample overfitting and computational complexity that is inherent in high-dimensional convex hulls. We show that the hyperdisk method can be kernelized to provide nonlinear classifiers based on non-Euclidean distance metrics. Experiments on several classification problems show promising results.
5469 en Fast Nearest Neighbor Retrieval for Bregman Divergences We present a data structure enabling efficient NN retrieval for bregman divergences. The family of bregman divergences includes many popular dissimilarity measures including KL-divergence (relative entropy), Mahalanobis distance, and Itakura-Saito divergence. These divergences present a challenge for efficient NN retrieval because they are not, in general, metrics, for which most NN data structures are designed. The data structure introduced in this work shares the same basic structure as the popular metric ball tree, but employs convexity properties of bregman divergences in place of the triangle inequality. Experiments demonstrate speedups over brute-force search of up to several orders of magnitude.
5470 en Deep Learning via Semi-Supervised Embedding We show how nonlinear embedding algorithms popular for use with shallow semi-supervised learning techniques such as kernel methods can be applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.
5471 en Localized Multiple Kernel Learning Recently, instead of selecting a single kernel, multiple kernel learning (MKL) has been proposed which uses a convex combination of kernels, where the weight of each kernel is optimized during training. However, MKL assigns the same weight to a kernel over the whole input space. In this paper, we develop a localized multiple kernel learning (LMKL) algorithm using a gating model for selecting the appropriate kernel function locally. The localizing gating model and the kernel-based classifier are coupled and their optimization is done in a joint manner. Empirical results on ten benchmark and two bioinformatics data sets validate the applicability of our approach. LMKL achieves statistically similar accuracy results compared with MKL by storing fewer support vectors. LMKL can also combine multiple copies of the same kernel function localized in different parts. For example, LMKL with multiple linear kernels gives better accuracy results than using a single linear kernel on bioinformatics data sets.
5472 en Composite Kernel Learning The Support Vector Machine (SVM) is an acknowledged powerful tool for building classifiers, but it lacks flexibility, in the sense that the kernel is chosen prior to learning. Multiple Kernel Learning (MKL) enables to learn the kernel, from an ensemble of basis kernels, whose combination is optimized in the learning process. Here, we propose Composite Kernel Learning to address the situation where distinct components give rise to a group structure among kernels. Our formulation of the learning problem encompasses several setups, putting more or less emphasis on the group structure. We characterize the convexity of the learning problem, and provide a general wrapper algorithm for computing solutions. Finally, we illustrate the behavior of our method on multi-channel data where groups correspond to channels.
5473 en Training SVM with Indefinite Kernels Similarity matrices generated from many applications may not be positive semidefinite, and hence can't fit into the kernel machine framework. In this paper, we study the problem of training support vector machines with an indefinite kernel. We consider a regularized SVM formulation, in which the indefinite kernel matrix is treated as a noisy observation of some unknown positive semidefinite one (proxy kernel) and the support vectors and the proxy kernel can be computed simultaneously. We propose a semi-infinite quadratically constrained linear program formulation for the optimization, which can be solved iteratively to find a global optimum solution. We further propose to employ an additional pruning strategy, which significantly improves the efficiency of the algorithm, while retaining the convergence property of the algorithm. In addition, we show the close relationship between the proposed formulation and multiple kernel learning. Experiments on a collection of benchmark data sets demonstrate the efficiency and effectiveness of the proposed algorithm.
5474 en Robust Matching and Recognition using Context-Dependent Kernels The success of kernel methods including support vector machines (SVMs) strongly depends on the design of appropriate kernels. While initially kernels were designed in order to handle fixed-length data, their extension to unordered, variable-length data became more than necessary for real pattern recognition problems such as object recognition and bioinformatics. We focus in this paper on object recognition using a new type of kernel referred to as "context-dependent". Objects, seen as constellations of local features (interest points,regions, etc.), are matched by minimizing an energy function mixing (1) a fidelity term which measures the quality of feature matching, (2) a neighborhood criteria which captures the object geometry and (3) a regularization term. We will show that the fixed-point of this energy is a "context-dependent" kernel ("CDK") which also satisfies the Mercer condition. Experiments conducted on object recognition show that when plugging our kernel in SVMs, we clearly outperform SVMs with "context-free" kernels.
5475 en An RKHS for Multi-View Learning and Manifold Co-Regularization Inspired by co-training, many multi-view semi-supervised kernel methods implement the following idea: find a function in each of multiple Reproducing Kernel Hilbert Spaces (RKHSs) such that (a) the chosen functions make similar predictions on unlabeled examples, and (b) the average prediction given by the chosen functions performs well on labeled examples. In this paper, we construct a single RKHS with a data-dependent “co-regularization” norm that reduces these approaches to standard supervised learning. The reproducing kernel for this RKHS can be explicitly derived and plugged into any kernel method, greatly extending the theoretical and algorithmic scope of co-regularization. In particular, with this development, the Rademacher complexity bound for co-regularization given in (Rosenberg & Bartlett, 2007) follows easily from well-known results. Furthermore, more refined bounds given by localized Rademacher complexity can also be easily applied. We propose a co-regularization based algorithmic alternative to manifold regularization (Belkin et al., 2006; Sindhwani et al., 2005a) that leads to major empirical improvements on semi-supervised tasks. Unlike the recently proposed transductive approach of (Yu et al., 2008), our RKHS formulation is truly semi-supervised and naturally extends to unseen test data.
5476 en A Distance Model for Rhythms Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce a model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.
5477 en A Reproducing Kernel Hilbert Space Framework for Pairwise Time Series Distances A good distance measure for time series needs to properly incorporate the temporal structure, and should be applicable to sequences with unequal lengths. In this paper, we propose a distance measure as a principled solution to the two requirements. Unlike the unconventional feature vector representation, our approach represents each time series with a summarizing smooth curve in a reproducing kernel Hilbert space (RKHS), and therefore translate the distance between time series into distances between curves. Moreover we propose to learn the kernel of this RKHS from a population of time series with discrete observations using Gaussian process-based non-parametric mixed-effect models. Experiments on two vastly different real-world problems show that the proposed distance measure leads to improved classification accuracy over the conventional distance measures.
5478 en Sequence Kernels for Predicting Protein Essentiality The problem of identifying the minimal gene set required to sustain life is of crucial importance in understanding cellular mechanisms and designing therapeutic drugs. This work describes several kernel-based solutions for predicting essential genes that outperform existing models while using less training data. Our first solution is based on a semi-manually designed kernel derived from the Pfam database, which includes several Pfam domains. We then present novel and general domain-based sequence kernels that capture sequence similarity with respect to several domains made of large sets of protein sequences. We show how to deal with the large size of the problem – several thousands of domains with individual domains sometimes containing thousands of sequences – by representing and efficiently computing these kernels using automata. We report results of extensive experiments demonstrating that they compare favorably with the Pfam kernel in predicting protein essentiality, while requiring no manual tuning.
5479 en Local Likelihood Modeling of Temporal Text Streams Temporal text data is often generated by a time-changing process or distribution. Such a drift in the underlying distribution cannot be captured by stationary likelihood techniques. We consider the application of local likelihood methods to generative and conditional modeling of temporal document sequences. We examine the asymptotic bias and variance and present an experimental study using the RCV1 dataset containing a temporal sequence of Reuters news stories.
5480 en Causal Modelling Combining Instantaneous and Lagged Effects: an Identifiable Model Based on Non-Gaussianity Causal analysis of continuous-valued variables typically uses either autoregressive models or linear Gaussian Bayesian networks with instantaneous effects. Estimation of Gaussian Bayesian networks poses serious identifiability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. We show that such a non-Gaussian model is identifiable without prior knowledge of network structure, and we propose an estimation method shown to be consistent. This approach also points out how neglecting instantaneous effects can lead to completely wrong estimates of the autoregressive coefficients.
5481 en Manifold Boost: Stagewise Function Approximation for Fully-, Semi- and Un-supervised Learning We describe a manifold learning framework that naturally accommodates supervised learning manifold learning, partially supervised learning and unsupervised clustering as particular cases. Our method chooses a function by minimizing loss subject to a manifold regularization penalty. This augmented cost is minimized using a greedy stagewise functional minimization procedure, as in Gradientboost. Each stage of boosting is fast and efficient. We demonstrate our approach using both radial basis function approximations and classification trees. The performance of our method is at the state of the art on standard problems.
5482 en Boosting with Incomplete Information In real-world machine learning problems, it is very common that part of the input feature vector is incomplete: either not available, missing, or corrupted. In this paper, we present a boosting approach that integrates features with incomplete information and those with complete information to form a strong classifier. By introducing hidden variables to model missing information, we form loss functions that combine fully labeled data with partially labeled data to effectively learn normalized and unnormalized models. The primal problems of the proposed optimization problems with these loss functions are provided to show their close relationships and the motivations behind them. We use auxiliary functions to bound the change of the loss functions and derive explicit parameter update rules for the learning algorithms. We demonstrate encouraging results on two real-world problems - visual object recognition in computer vision and named entity recognition in natural language processing - to show the effectiveness of the proposed boosting approach.
5483 en Maximum Likelihood Rule Ensembles We propose a new rule induction algorithm for solving classification problems via probability estimation. The main advantage of decision rules is their simplicity and good interpretability. While the early approaches to rule induction were based on sequential covering, we follow an approach in which a single decision rule is treated as a base classifier in an ensemble. The ensemble is built by greedily minimizing the negative loglikelihood which results in estimating the class conditional probability distribution. The introduced approach is compared with other decision rule induction algorithms such as SLIPPER, LRI and RuleFit.
5484 en Random Classification Noise Defeats All Convex Potential Boosters A broad class of boosting algorithms can be interpreted as performing coordinate-wise gradient descent to minimize some potential function of the margins of a data set. This class includes AdaBoost, LogitBoost, and other widely used and well-studied boosters. In this paper we show that for a broad class of convex potential functions, any such boosting algorithm is highly susceptible to random classification noise. We do this by showing that for any such booster and any nonzero random classification noise rate R, there is a simple data set of examples which is efficiently learnable by such a booster if there is no noise, but which cannot be learned to accuracy better than 1/2 if there is random classification noise at rate R. This negative result is in contrast with known branching program based boosters which do not fall into the convex potential function framework and which can provably learn to high accuracy in the presence of random classification noise.
5485 en Uncorrelated Multilinear Principal Component Analysis through Successive Variance Maximization Tensorial data are frequently encountered in various machine learning tasks today and dimensionality reduction is one of their most important applications. This paper extends the classical principal component analysis (PCA) to its multilinear version by proposing a novel dimensionality reduction algorithm for tensorial data, named as uncorrelated multilinear PCA (UMPCA). UMPCA seeks a tensor-to-vector projection that captures most of the variation in the original tensorial input while producing uncorrelated features through successive variance maximization. We evaluate the proposed algorithm on a second-order tensorial problem, face recognition, and the experimental results show its superiority, especially in low-dimensional spaces, through the comparison with three other PCA-based algorithms.
5486 en Expectation-Maximization for Sparse and Non-Negative PCA We study the problem of finding the dominant eigenvector of the sample covariance matrix, under additional constraints on its elements: a cardinality constraint limits the number of non-zero elements, and non-negativity forces the elements to have equal sign. This problem is known as sparse and non-negative principal component analysis (PCA), and has many applications including dimensionality reduction and feature selection. Based on expectation-maximization for probabilistic PCA, we present an algorithm for any combination of these constraints. Its complexity is at most quadratic in the number of dimensions of the data. We demonstrate significant improvements in performance and computational efficiency compared to the state-of-the-art, using large data sets from biology and computer vision.
5487 en ICA and ISA Using Schweizer-Wolff Measure of Dependence We propose a new algorithm for independent component and independent subspace analysis problems. This algorithm uses a contrast based on the Schweizer-Wolff measure of pairwise dependence, a non-parametric measure based on pairwise ranks of the variables. Our algorithm frequently outperforms state of the art ICA methods in the normal setting, is significantly more robust to outliers in the mixed signals, and performs well even in the presence of noise. Since pairwise dependence is evaluated explicitly, using Cardoso's conjecture, our method can be applied to solve independence subspace analysis (ISA) problems by grouping signals recovered by ICA methods. We provide an extensive empirical evaluation using simulated, sound, and image data.
5488 en Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be efficiently performed even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million user/movie ratings. The resulting models achieve significantly higher prediction accuracy than PMF models trained using MAP estimation.
5489 en Adaptive p-Posterior Mixture-Model Kernels for Multiple Instance Learning In multiple instance learning (MIL), how the instances determine the bag-labels is an essential issue, both algorithmically and intrinsically. In this paper, we show that the mechanism of how the instances determine the bag-labels is different for different application domains, and does not necessarily obey the traditional assumptions of MIL. We therefore propose an adaptive framework for MIL that adapts to different application domains by learning the domain-specific mechanisms merely from labeled bags. Our approach is especially attractive when we are encountered with novel application domains, for which the mechanisms may be different and unknown. Specifically, we exploit mixture models to represent the composition of each bag and an adaptable kernel function to represent the relationship between the bags. We validate on synthetic MIL datasets that the kernel function automatically adapts to different mechanisms of how the instances determine the bag-labels. We also compare our approach with state-of-the-art MIL techniques on real-world benchmark datasets.
5490 en Multiple Instance Ranking This paper introduces a novel machine learning model called multiple instance ranking (MIRank) that enables ranking to be performed in a multiple instance learning setting. The motivation for MIRank stems from the hydrogen abstraction problem in computational chemistry, that of predicting the grouping of hydrogen atoms from which a hydrogen is abstracted (removed) during metabolism. The model predicts the preferred hydrogen grouping within a molecule by ranking the groups, with the ambiguity of not knowing which hydrogen within the preferred grouping is actually abstracted. This paper formulates MIRank in its general context and proposes an algorithm for solving MIRank problems using successive linear programming. The method outperforms multiple instance classification models on several real and synthetic datasets.
5491 en Bayesian Multiple Instance Learning: Automatic Feature Selection and Inductive Transfer We propose a novel Bayesian multiple instance learning algorithm. This algorithm automatically identifies the relevant feature subset, and utilizes inductive transfer when learning multiple (conceptually related) classifiers. Experimental results indicate that the proposed baseline MIL method is more accurate than previous MIL algorithms and selects a much smaller set of useful features. Inductive transfer further improves the accuracy of the classifier as compared to learning each task individually.
5492 en Learning to Classify with Missing and Corrupted Features After a classifier is trained using a machine learning algorithm and put to use in a real world system, it often faces noise which did not appear in the training data. Particularly, some subset of features may be missing or may become corrupted. We present two novel machine learning techniques that are robust to this type of classification-time noise. First, we solve an approximation to the learning problem using linear programming. We analyze the tightness of our approximation and prove statistical risk bounds for this approach. Second, we define the online-learning variant of our problem, address this variant using a modified Perceptron, and obtain a statistical learning algorithm using an online-to-batch technique. We conclude with a set of experiments that demonstrate the effectiveness of our algorithms.
5493 en Multi-Classification by Categorical Features via Clustering We derive a generalization bound for multi-classification schemes based on grid clustering in categorical parameter product spaces. Grid clustering partitions the parameter space in the form of a Cartesian product of partitions for each of the parameters. The derived bound provides a means to evaluate clustering solutions in terms of the generalization power of a built-on classifier. For classification based on a single feature the bound serves to find a globally optimal classification rule. Comparison of the generalization power of individual features can then be used for feature ranking. Our experiments show that in this role the bound is much more precise than mutual information or normalized correlation indices.
5494 en Opening Remarks 
5497 en Learning Rules: From PCFGs to Adaptor Grammars 
5498 en Constraints as Prior Knowledge 
5499 en Some thoughts on prior knowledge, deep architectures and NLP 
5500 en Poster: Using Participant Role in Multiparty Meetings as Prior Knowledge for Nonparametric Topic Modeling In this paper we introduce our attempts to incorporate the participant role information in multiparty meetings for document modeling using the hierarchical Dirichlet process. The perplexity and automatic speech recognition results demonstrate that the participant role information is a promising prior knowledge source to be combined with language models for automatic speech recognition and interaction modeling for multiparty meetings.
5501 en Poster: Knowledge as a Constraint on Uncertainty for Unsupervised Classification: A Study in Part-of-Speech Tagging This paper evaluates the use of prior knowledge to limit or bias the choices of a classifer during otherwise unsupervised training and classifcation. Focusing on effects in the uncertainty of the model's decisions, we quantify the contributions of the knowledge source as a reduction in the conditional entropy of the label distribution given the input corpus. Allowing us to compare diffrent sets of knowledge without annotated data, we find that label entropy is highly predictive of final performance for a standard Hidden Markov Model (HMM) on the task of part-of-speech tagging. Our results show too that even basic levels of knowledge, integrated as labeling constraints, have considerable effect on classification accuracy, in addition to more stable and effcient training convergence. Finally, for cases where the model's internal classes need to be interpreted and mapped to a de- sired label set, we find that, for constrained models, the requirements for annotated data to make quality assignments are greatly reduced.
5502 en Poster: Using Prior Domain Knowledge to Build HMM-Based Semantic Tagger Trained on Completely Unannotated Data In this paper, we propose a robust statistical semantic tagging model trained on completely unannotated data. The approach relies mainly on prior domain knowledge to counterbalance the lack of semantically annotated treebank data. The proposed method encodes longer contextual information by grouping strongly related semantic concepts together into cohesive units. The method is based on hidden Markov model (HMM) and offers high ambiguity resolution power, outputs semantically rich information, and requires relatively low human effort. The approach yields high-performance models that are evaluated on two different corpora in two application domains in English and German.
5504 en Poster: Dirichlet Process Mixture Models for Verb Clustering In this work we apply Dirichlet Process Mixture Models to a learning task in natural language processing (NLP): lexical-semantic verb clustering. We assess the performance on a dataset based on Levin’s (1993) verb classes using the recently introduced V-measure metric. In, we present a method to add human supervision to the model in order to to influence the solution with respect to some prior knowledge. The quantitative evaluation performed highlights the benefits of the chosen method compared to previously used clustering approaches.
5505 en Incorporating Prior Knowledge into NLP with Markov Logic 
5506 en Expanding a Gazetteer-Based Approach for Geo-Parsing Disease Alerts Discovering in a text the geographic references it may contain, is a task that human readers perform using both their lexical and contextual knowledge. Using a gazetteer to label such targeted references in a dataset, this paper proposes an approach to learning the context in which they appear and by this means extending the prior knowledge encoded in the gazetteer. The present work was carried in the particular framework of a system for disease outbreak alerts detection and geo-indexing.
5507 en Bayesian Modeling of Dependency Trees Using Hierarchical Pitman-Yor Priors 
5508 en The Luckiness Principle 
5509 en Encoding prior knowledge in text processing 
5510 en Why isn’t linguistics more useful in NLP? 
5511 en Knowledge and Language 
5512 en Panel: Complex Model => Rich Prior Knowledge 
5513 en Panel: Priors, Deep Architectures and NLP: YOU ARE DOING EVERYTHING WRONG!! 
5514 en DRASO: Declaratively Regularized Alternating Structural Optimization Recent work has shown that Alternating Structural Optimization (ASO) can improve supervised learners by learning feature representations from unlabeled data. However, there is no natural way to include prior knowledge about features into this frame- work. In this paper, we present Declar- atively Regularized Alternating Structural Optimization (DRASO), a principled way for injecting prior knowledge into the ASO framework. We also provide some analysis of the representations learned by our method.
5515 en An Object-Oriented Representation for Efficient Reinforcement Learning Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs, and prove a polynomial bound in its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame.
5516 en Hierarchical Model-Based Reinforcement Learning Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their underlying structure. Model-based algorithms, which provided the first finite-time convergence guarantees for reinforcement learning, may also play an important role in coping with the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates modern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, R-maxq, inherits the efficient model-based exploration of the R-max algorithm and the opportunities for abstraction provided by the MAXQ framework. We analyze the sample complexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies and models.
5517 en On the Hardness of Finding Symmetries in Markov Decision Processes In this work we address the question of finding symmetries of a given MDP. We show that the problem is Isomorphism Complete, that is, the problem is polynomially equivalent to verifying whether two graphs are isomorphic. Apart from the theoretical importance of this result it has an important practical application. The reduction presented can be used together with any off-the-shelf Graph Isomorphism solver, which performs well in the average case, to find symmetries of an MDP. In fact, we present results of using NAutY (the best Graph Isomorphism solver currently available), to find symmetries of MDPs.
5518 en Reinforcement Learning in the Presence of Rare Events We consider the task of reinforcement learning in an environment in which rare significant events occur independently of the actions selected by the controlling agent. If these events are sampled according to their natural probability of occurring, convergence of standard reinforcement learning algorithms is likely to be very slow, and the learning algorithms may exhibit high variance. In this work, we assume that we have access to a simulator, in which the rare event probabilities can be artificially altered. Then, importance sampling can be used to learn with this simulation data. We introduce algorithms for policy evaluation, both using tabular and function approximation representation of the value function. We prove that in both cases, the reinforcement learning algorithms converge. In the tabular case, we also analyze the bias and variance of our approach compared to TD-learning. We evaluate empirically the performance of the algorithm on random Markov Decision Processes, as well as on a large network planning task.
5519 en Online Kernel Selection for Bayesian Reinforcement Learning Kernel-based Bayesian methods for Reinforcement Learning (RL) such as Gaussian Process Temporal Difference (GPTD) are particularly promising because they rigorously treat uncertainty in the value function and make it easy to specify prior knowledge. However, the choice of prior distribution significantly affects the empirical performance of the learning agent, and little work has been done extending existing methods for prior model selection to the online setting. This paper develops Replacing-Kernel RL, an online model selection method for GPTD using population-based search. Replacing-Kernel RL is compared to standard GPTD and tile-coding on several RL domains, and is shown to yield significantly better asymptotic performance for many different kernel families. Furthermore, the resulting kernels capture an intuitively useful notion of prior state covariance that may nevertheless be difficult to capture manually.
5521 en On-line Discovery of Temporal-Difference Networks We present an algorithm for on-line, incremental discovery of temporal-difference (TD) networks. The key contribution is the establishment of three criteria to expand a node in TD network: a node is expanded when the node is well-known, independent, and has a prediction error that requires further explanation. Since none of these criteria requires centralized calculation operations, they are easily computed in a parallel and distributed manner, and scalable for bigger problems compared to other discovery methods of predictive state representations. Through computer experiments, we demonstrate the empirical effectiveness of our algorithm.
5522 en Prediction with Expert Advice for the Brier Game We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.
5523 en Non-Parametric Policy Gradients: A Unified Treatment of Propositional and Relational Domains Policy gradient approaches are a powerful instrument for learning how to interact with the environment.Existing approaches have focused on propositional and continuous domains only. Without extensive feature engineering, it is difficult -- if not impossible -- to apply them within structured domains, in which e.g. there is a varying number of objects and relations among them. In this paper, we describe a non-parametric policy gradient approach -- called NPPG -- that overcomes this limitation. The key idea is to apply Friedmann's gradient boosting: policies are represented as a weighted sum of regression models grown in an stage-wise optimization. Employing off-the-shelf regression learners, NPPG can deal with propositional, continuous, and relational domains in a unified way. Our experimental results show that it can even improve on established results.
5524 en Space-indexed Dynamic Programming: Learning to Follow Trajectories We consider the task of learning to accurately follow a trajectory in a vehicle such as a car or helicopter. A number of dynamic programming algorithms such as Differential Dynamic Programming (DDP) and Policy Search by Dynamic Programming (PSDP), can efficiently compute non-stationary policies for these tasks --- such policies in general are well-suited to trajectory following since they can easily generate different control actions at different times in order to follow the trajectory. However, a weakness of these algorithms is that their policies are time-indexed, in that they apply different policies depending on the current time. This is problematic since 1) the current time may not correspond well to where we are along the trajectory and 2) the uncertainty over future states can prevent these algorithms from finding any good policies at all. In this paper we propose a method for space-indexed dynamic programming that overcomes both these difficulties. We begin by showing how a dynamical system can be rewritten in terms of a spatial index variable (i.e., how far along the trajectory we are) rather than as a function of time. We then use these space-indexed dynamical systems to derive space-indexed version of the DDP and PSDP algorithms. Finally, we show that these algorithms perform well on a variety of control tasks, both in simulation and on real systems.
5525 en Privacy-Preserving Reinforcement Learning Distributed reinforcement learning (DRL) has been studied as an approach to learn control policies thorough interactions between distributed agents and environments. The main emphasis of DRL has been put on the way to learn sub-optimal policies with the least or limited sharing of agents' perceptions. In this study, we introduce a new concept, privacy-preservation, into DRL. In our setting, agents' perceptions, such as states, rewards, and actions, are not only distributed but also are desired to be kept private. This can occur when agents' perceptions include private or confidential information. Conventional DRL algorithms could be applied to such problems, but do not theoretically guarantee privacy preservation. We design solutions that achieve optimal policies in standard reinforcement leering settings without requiring the agents to share their private information by means of well-known cryptographic primitive, secure function evaluation.
5526 en Learning All Optimal Policies with Multiple Criteria We describe an algorithm for learning in the presence of multiple criteria. Our technique generalizes previous approaches in that it can learn optimal policies for any linear preference assignment over the multiple reward criteria. The algorithm can be viewed as an extension to standard reinforcement learning for MDPs where instead of repeatedly backing up maximal expected rewards, we back up the set of expected rewards that are maximal for some set of linear preferences (given by a weight vector, w). We present the algorithm, along with a proof of correctness showing that our solution gives the optimal policy for any linear preference function. The solution reduces to the standard value iteration algorithm for a specific weight vector.
5527 en Active Reinforcement Learning When the transition probabilities and rewards of a Markov Decision Process (MDP) are known, the agent can obtain the optimal policy without any interaction with the environment. However, exact transition probabilities are difficult for experts to specify. One option left to an agent is a long and potentially costly exploration of the environment. In this paper, we propose another alternative: given initial (possibly inaccurate) specification of the MDP, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards. It then focuses its exploration on the regions of space to which the optimal policy is most sensitive. We show that the proposed exploration strategy performs well on several control and planning problems.
5528 en Reinforcement Learning with Limited Reinforcement: Using Bayes Risk for Active Learning in POMDPs Partially Observable Markov Decision Processes (POMDPs) have succeeded in planning domains because they optimally trade between actions that increase an agent's knowledge and actions that increase an agent's reward. Unfortunately, most POMDPs are defined with a large number of parameters which are difficult to specify only from domain knowledge. In this paper, we treat the POMDP model parameters as additional hidden state in a "model-uncertainty" POMDP and develop an approximate algorithm for planning in the this larger POMDP. The approximation, coupled with model-directed queries, allows the planner to actively learn good policies. We demonstrate our approach on several standard POMDP problems.
5529 en The Many Faces of Optimism: a Unifying Approach The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. Optimism in the face of uncertainty and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.
5530 en Transfer of Samples in Batch Reinforcement Learning The main objective of transfer learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving one or more source tasks. In this paper, we introduce a novel algorithm that transfers samples (i.e., experience tuples ) from source to target tasks. Under the assumption that tasks defined on the same environment often have similar transition models and reward functions, we propose a method to select samples from the source tasks that are mostly similar to the target task, and, then, to use them as input for batch reinforcement learning algorithms. As a result, the number of samples that the agent needs to collect from the target task to learn its solution is reduced. We empirically show that, following the proposed approach, the transfer of samples is effective in reducing the learning complexity, even when the source tasks are significantly different from the target task.
5531 en Exploration Scavenging We examine the problem of evaluating a policy in the contextual bandit setting using only observations collected during the execution of another policy. We show that policy evaluation can be impossible if the exploration policy chooses actions based on the side information provided at each time step. We then propose and prove the correctness of a principled method for policy evaluation which works when this is not the case, even when the exploration policy is deterministic, as long as each action is explored sufficiently often. We apply this general technique to the problem of offline evaluation of internet advertising policies. Although our theoretical results hold only when the exploration policy chooses ads independent of side information, an assumption that is typically violated by commercial systems, we show how clever uses of the theory provide non-trivial and realistic applications. We also provide an empirical demonstration of the effectiveness of our techniques on real ad placement data.
5532 en An Analysis of Linear Models, Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning We show that linear value function approximation is equivalent to a form of linear model approximation. We derive a relationship between the model approximation error and the Bellman error, and show how this relationship can guide feature selection for model improvement and/or value function improvement. We also show how these results give insight into the behavior of existing feature-selection algorithms.
5533 en An Analysis of Reinforcement Learning with Function Approximation We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsilis and Van Roy, 1996) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.
5534 en Apprenticeship Learning Using Linear Programming In apprenticeship learning, the goal is to learn a policy in a Markov decision process that is at least as good as a policy demonstrated by an expert. The difficulty arises in that the MDP's true reward function is assumed to be unknown. We show how to frame apprenticeship learning as a linear programming problem, and show that using an off-the-shelf LP solver to solve this problem results in a substantial improvement in running time over existing methods --- up to two orders of magnitude faster in our experiments. Additionally, our approach produces stationary policies, while all existing methods for apprenticeship learning output policies that are "mixed", i.e. randomized combinations of stationary policies. The technique used is general enough to convert any mixed policy to a stationary policy.
5535 en Preconditioned Temporal Difference Learning This paper extends many of the recent popular reinforcement learning (RL) algorithms to a generalized framework that includes least-squares temporal difference (LSTD) learning, least-squares policy evaluation (LSPE) and a variant of incremental LSTD (iLSTD). The basis of this extension is a preconditioning technique that tries to solve a stochastic model equation. This paper also studies three signicant issues of the new framework: it presents a new rule of step-size that can be computed online, provides an iterative way to apply preconditioning, and reduces the complexity of related algorithms to near that of temporal difference (TD) learning.
5536 en Automatic Discovery and Transfer of MAXQ Hierarchies We present an algorithm, HI-MAT (Hierarchy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by applying dynamic Bayesian network models to a successful trajectory from a source reinforcement learning task. HI-MAT discovers subtasks by analyzing the causal and temporal relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consistent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empirically that HI-MAT constructs compact hierarchies that are comparable to manually-engineered hierarchies and facilitate significant speedup in learning when transferred to a target task.
5537 en Sample-Based Learning and Search with Permanent and Transient Memories We present a reinforcement learning architecture, Dyna-2, that encompasses both sample-based learning and sample-based search, and that generalises across states during both learning and search. We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. In both cases, an estimate of the value function is formed, but in the first case it is transient, computed and then discarded after each move, whereas in the second case it is more permanent, slowly accumulating over many moves and games. The idea of Dyna-2 is for the transient planning memory and the permanent learning memory to remain separate, but for both to be based on linear function approximation and both to be updated by Sarsa. To apply Dyna-2 to 9x9 Computer Go, we use a million binary features in the function approximator, based on templates matching small fragments of the board. Using only the transient memory, Dyna-2 performed at least as well as UCT. Using both memories combined, it significantly outperformed UCT. Our program based on Dyna-2 achieved a higher rating on the Computer Go Online Server than any handcrafted or traditional search based program.
5538 en Efficiently Learning Linear-Linear Exponential Family Predictive Representations of State Exponential Family PSR (EFPSR) models capture stochastic dynamical systems by representing state as the parameters of an exponential family distribution over a short-term window of future observations. They are appealing from a learning perspective because they are fully observed (meaning expressions for maximum likelihood do not involve hidden quantities), but are still expressive enough to both capture existing models (such as POMDPs and linear dynamical systems) and predict new models. While learning algorithms based on maximizing exact likelihood exist, they are not computationally feasible. We present a new, computationally efficient, learning algorithm based on an approximate likelihood function. The algorithm can be interpreted as attempting to induce stationary distributions of observations, features and states which match their empirically observed counterparts. The approximate likelihood, and the idea of matching stationary distributions, may have application in other models.
5539 en A Semi-parametric Statistical Approach to Model-free Policy Evaluation Reinforcement learning (RL) methods based on least-squares temporal difference (LSTD) have been developed recently and have shown good practical performance. However, the quality of their estimation has not been well elucidated. In this article, we discuss LSTD based policy evaluation from the new viewpoint of semiparametric statistical inference. In fact, the estimator can be obtained from a particular estimating function which guarantees its convergence to the true value asymptotically, without specifying a model of the environment. Based on these observations, we 1) analyze the asymptotic variance of an LSTD-based estimator, 2) derive the optimal estimating function with the minimum asymptotic estimation variance, and 3) derive a suboptimal estimator to reduce the computational burden in obtaining the optimal estimating function.
5540 en Learning to Learn Implicit Queries from Gaze Patterns In the absence of explicit queries, an alternative is to try to infer users' interests from implicit feedback signals, such as clickstreams or eye tracking. The interests, formulated as an implicit query, can then be used in further searches. We formulate this task as a probabilistic model, which can be interpreted as a kind of transfer learning and meta-learning. The probabilistic model is demonstrated to outperform an earlier kernel-based method in a small-scale information retrieval task.
5541 en Multi-Task Learning for HIV Therapy Screening We address the problem of learning classifiers for a large number of tasks. We derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task. Our work is motivated by the problem of predicting the outcome of a therapy attempt for a patient who carries an HIV virus with a set of observed genetic properties. Such predictions need to be made for hundreds of possible combinations of drugs, some of which use similar biochemical mechanisms. Multi-task learning enables us to make predictions even for drug combinations with few or no training examples and substantially improves the overall prediction accuracy.
5542 en Manifold Alignment using Procrustes Analysis In this paper we introduce a novel approach to manifold alignment, based on Procrustes analysis. Our approach differs from "semi-supervised alignment" in that it results in a mapping that is defined everywhere - when used with a suitable dimensionality reduction method - rather than just on the training data points. We describe and evaluate our approach both theoretically and experimentally, providing results showing useful knowledge transfer from one domain to another. Novel applications of our method including cross-lingual information retrieval and transfer learning in Markov decision processes are presented.
5543 en No-Regret Learning in Convex Games Quite a bit is known about minimizing different kinds of regret in experts problems, and how these regret types relate to types of equilibria in the multiagent setting of repeated matrix games. Much less is known about the possible kinds of regret in online convex programming problems (OCPs), or about equilibria in the analogous multiagent setting of repeated convex games. This gap is unfortunate, since convex games are much more expressive than matrix games, and since many important machine learning problems can be expressed as OCPs. In this paper, we work to close this gap: we analyze a spectrum of regret types which lie between external and swap regret, along with their corresponding equilibria, which lie between coarse correlated and correlated equilibrium. We also analyze algorithms for minimizing these regret types. As examples of our framework, we derive algorithms for learning correlated equilibria in polyhedral convex games and extensive-form correlated equilibria in extensive-form games. The former is exponentially more efficient than previous algorithms, and the latter is the first of its type.
5545 en Strategy Evaluation in Extensive Games with Importance Sampling Typically agent evaluation is done through Monte Carlo estimation. However, stochastic agent decisions and stochastic outcomes can make this approach inefficient, requiring many samples for an accurate estimate. We present a new technique that can be used to simultaneously evaluate many strategies while playing a single strategy in the context of an extensive game. This technique is based on importance sampling, but utilizes two new mechanisms for significantly reducing variance in the estimates. We demonstrate its effectiveness in the domain of poker, where stochasticity makes traditional evaluation problematic.
5546 en MDL Tutorial We give a self-contained tutorial on the Minimum Description Length (MDL) approach to modeling, learning and prediction. We focus on the recent (post 1995) formulations of MDL, which can be quite different from the older methods that are often still called 'MDL' in the machine learning and UAI communities. In its modern guise, MDL is based on the concept of a `universal model'. We explain this concept at length. We show that previous versions of MDL (based on so-called two-part codes), Bayesian model selection and predictive validation (a variation of cross-validation) can all be interpreted as approximations to model selection based on 'universal models'. Modern MDL prescribes the use of a certain `optimal' universal model, the so-called `normalized maximum likelihood model' or `Shtarkov distribution'. This is related to (yet different from) Bayesian model selection with non-informative priors. It leads to a penalization of `complex' models that can be given an intuitive differential-geometric interpretation. Roughly speaking, the complexity of a parametric model is directly related to the number of distinguishable probability distributions that it contains. We also discuss some recent extensions such as the 'luckiness principle', which can be used if the Shtarkov distribution is undefined, and the 'switch distribution', which allows for a resolution of the AIC-BIC dilemma.
5547 en Fast computation of NML for Bayesian networks Bayesian networks are parametric models for multidimensional domains exhibiting complex dependencies between the dimensions (domain variables). A central problem in learning such models is how to regularize the number of parameters; in other words, how to determine which dependencies are significant and which are not. The normalized maximum likelihood (NML) distribution or code offers an information-theoretic solution to this problem. Unfortunately, computing it for arbitrary Bayesian network models appears to be computationally infeasible, but we show how it can be computed efficiently for certain restricted type of Bayesian networks.
5548 en Extensions to MDL denoising The minimum description length principle in wavelet denoising can be extended from the standard linear-quadratic setting in several ways. We describe briefly three extensions: soft thresholding, histogram modeling and a multicomponent approach. The MDL hard thresholding approach based on the normalized maximum likelihood universal modeling can be extended to include soft thresholding shrinkage, which can be considered to give better results in some applications. In MDL histogram denoising approach the assumptions of the parametric density models for the data can be relaxed. The informative and noise components of the data are modeled with equal bin width histograms. The method can cope with different noise distributions. In multicomponent approach more than one non-noise components are included in the model, because it is possible that in addition to the random noise there may be other disturbing signal elements, or that the informative signal is comprised of several different components which we may want to observe, separate or remove. In these cases adding informative components in the model may result result in better performance than in the NML denoising approach.
5549 en Nonparametric density estimation by switching According to standard MDL and Bayesian model selection, we should (roughly) prefer the model that minimises overall prediction error. But if the goal is to predict well, it may well depend on the sample size which model is most useful to predict the next outcome. By re-interpreting the Bayesian prediction strategies associated with the models as "experts", we can use the various algorithms for "expert tracking" to improve model selection for prediction without introducing a substantial computational overhead.
5550 en Sequential and factorized NML models Currently the most popular model selection criterion for learning Bayesian networks is the Bayesian mixture with a conjugate prior. This method has recently been reported to be very sensitive to the choice of prior hyper-parameters. On the other hand, the general model selection criteria, AIC and BIC are derived through asymptotics and their behavior is suboptimal for small sample sizes. In this work we introduce a new effective scoring criterion for learning Bayesian network structures, the factorized normalized maximum likelihood. This score features no tunable parameters thus avoiding the sensitivity problems of Bayesian scores. The new scoring method also suggests a parametrization of the Bayesian network that is based on the conditional normalized maximum likelihood predictive distribution.
5551 en Generalization theory of two-part code MDL estimator I will present a finite-sample generalization analysis of two-part code MDL estimator. This method selects a model that minimizes the sum of the model description length plus the data description length given the model. It can be shown that under various conditions, optimal rate of convergence can be achieved through an extended family of two-part code MDL that over-penalize the model description length. As an example, we apply MDL to learning sparse linear representations when the system dimension is much larger than the number of training examples. This is a problem that has attracted considerable attention in recent years. The generalization performance of a two-part code MDL estimator is calculated based on our theory, and it compares favorably to other methods such as 1-norm regularization.
5552 en Normalized maximum likelihood models in genomics The normalized maximum likelihood (NML) model (Rissanen, 1996; Rissanen, 2001, Shtarkov, 1987) for a class of Markov sources (Tabus and Korodi, 2008) was recently used for the compression of full genomes, obtaining for the human genome the best existing compression results (Korodi and Tabus, 2007). We show that one of the underlying biological features that the compression algorithm implicitly uncovers is the existence of approximate gene duplication. We proposed a refined method based on the same NML models for the segmentation of DNA sequences for uncovering gene duplications (Tabus, Yang, and Astola, 2008). Several analysis tasks in genomic sequences involve preliminary segmentation or clustering of the data, which can be performed by a number of techniques, based on various similarity measures. Here we review and further pursue the application of MDL techniques for genomic sequence analysis. The process of sequence matching will be used for solving the problem of uncovering gene duplications with the help of a preliminary segmentation of a complex DNA locus, known to have evolved through a series of duplications.
5553 en Information consistency of nonparametric Gaussian process methods We present information consistency results for nonparametric sequential prediction with Gaussian processes. The connection to nonparametric MDL is through the prequential approach, as detailed in Gruenwald's 2007 book, Sect. 13.5. Our proof technique is elementary, making use of a convex duality previously useful to obtain PAC-Bayesian bounds. We also obtain precise information consistency rates for a wide range of kernels and input distributions, using kernel eigenvalue asymptotics. In all these cases, the linear expert space is an infinite-dimensional function space, but still very reasonable rates are obtained.
5556 en Nonparametric Learning of Switching Autoregressive Processes Vector autoregressive (VAR) processes are useful in describing dynamical phenomena as diverse as speech, financial time-series, and the dancing of honey bees. However, such phenomena often exhibit structural changes over time and the VAR which describe them must also change. For example, the vocal tract of a speaker contracts; a country experiences a recession, a central bank intervention, or some national or global event; a honey bee changes from a waggle to a turn right dance. Some of these changes will appear fre- quently, while others are only rarely observed. In ad- dition, there is always the possibility of a previously unseen dynamic behavior. Thus, we propose a non- parametric approach for learning switching VAR pro- cesses, where we take the state sequence to be Markov....
5557 en Rank Minimization via Online Learning Minimum rank problems arise frequently in machine learning applications and are notoriously difficult to solve due to the non-convex nature of the rank objective. In this paper, we present the first online learning approach for the problem of rank minimization of matrices over polyhedral sets. In particular, we present two online learning algorithms for rank minimization - our first algorithm is a multiplicative update method based on a generalized experts framework, while our second algorithm is a novel application of the online convex programming framework [Zinkevich, 2003]. In the latter, we flip the role of the decision maker by making the decision maker search over the constraint space instead of feasible points, as is usually the case in online convex programming. A salient feature of our online learning approach is that it allows us to give the first provable approximation guarantees for the rank minimization problem over polyhedral sets. We demonstrate the effectiveness of our methods on synthetic examples, and on the real-life application of low-rank kernel learning.
5558 en The Asymptotics of Semi-Supervised Learning in Discriminative Probabilistic Models Semi-supervised learning aims at taking advantage of unlabeled data to improve the efficiency of supervised learning procedures. For discriminative models however, this is a challenging task. In this contribution, we introduce an original methodology for using unlabeled data through the design of a simple semi-supervised objective function. We prove that the corresponding semi-supervised estimator is asymptotically optimal. The practical consequences of this result are discussed for the case of the logistic regression model.
5563 en Design 
5564 en Purchasing 
5565 en Determine Price Method 
5566 en Cost Estimation 
5567 en Approve Check 
5568 en Create Project Plan 
5569 en Tooleast order management tutorial 
5570 en Tryout tutorial 
5571 en The Skew Spectrum of Graphs The central issue in representing graph-structured data instances in learning algorithms is designing features which are invariant to permuting the numbering of the vertices. We present a new system of invariant graph features which we call the skew spectrum of graphs. The skew spectrum is based on mapping the adjacency matrix to a function on the symmetric group and computing bispectral invariants. The reduced form of the skew spectrum is computable in O(n3) time, and experiments show that on several benchmark datasets it can outperform state of the art graph kernels.
5576 en Combining near-optimal feature selection with gSpan Graph classification is an increasingly important step in numerous application domains, such as function prediction of molecules and proteins, computerized scene analysis, and anomaly detection in program flows. Among the various approaches proposed in the literature, graph classification based on frequent subgraphs is a popular branch: Graphs are represented as (usually binary) vectors, with components indicating whether a graph contains a particular subgraph that is frequent across the dataset. On large graphs, however, one faces the enormous problem that the number of these frequent subgraphs may grow exponentially with the size of the graphs, but only few of them possess enough discriminative power to make them useful for graph classification. Efficient and discriminative feature selection among frequent subgraphs is hence a key challenge for graph mining. In this article, we propose an approach to feature selection on frequent subgraphs, called CORK, that combines two central advantages. First, it optimizes a sub modular quality criterion, which means that we can yield a near-optimal solution using greedy feature selection. Second, our sub modular quality function criterion can be integrated into gSpan, the state-of-the-art tool for frequent subgraph mining, and help to prune the search space for discriminative frequent subgraphs even during frequent subgraph mining.
5577 en Efficient Discriminative Training Method for Structured Predictions We propose an efficient discriminative training method for generative models under supervised learning. In our setting, fully observed instances are given as training examples, together with a specification of variables of interest for prediction. We formulate the training as a convex programming problem, incorporating the SVM-type large margin constraints to favor parameters under which the maximum a posteriori (MAP) estimates of the prediction variables, conditioned on the rest, are close to their true values given in the training instances. The resulting optimization problem is, however, more complex than its quadratic programming (QP) counterpart resulting from the SVM-type training of conditional models, because of the presence of non-linear constraints on the parameters. We present an efficient optimization method, which combines several techniques, namely, a data-dependent reparametrization of dual variables, restricted simplicial decomposition, and the proximal point algorithm. Our method extends the one for solving the aforementioned QP counterpart, proposed earlier by some of the authors.
5578 en Gray graph 
5579 en Learning for Control from Multiple Demonstrations We consider the problem of learning to follow a desired trajectory when given a small number of demonstrations from a sub-optimal expert. We present an algorithm that (i) extracts the---initially unknown---desired trajectory from the sub-optimal expert's demonstrations and (ii) learns a local model suitable for control along the learned trajectory. We apply our algorithm to the problem of autonomous helicopter flight. In all cases, the autonomous helicopter's performance exceeds that of our expert helicopter pilot's demonstrations. Even stronger, our results significantly extend the state-of-the-art in autonomous helicopter aerobatics. In particular, our results include the first autonomous tic-tocs, loops and hurricane, vastly superior performance on previously performed aerobatic maneuvers (such as in-place flips and rolls), and a complete airshow, which requires autonomous transitions between these and various other maneuvers.
5580 en Computers versus Common Sense It's way past 2001 now, where the heck is HAL? For several decades now we've had high hopes for computers amplifying our mental abilities not just giving us access to relevant stored information, but answering our complex, contextual questions. Even applications like human-level unrestricted speech understanding continue to dangle close but just out of reach.  What's been holding AI up? The short answer is that while computers make fine idiot savants, they lack common sense: the millions of pieces of general knowledge we all share, and fall back on as needed, to cope with the rough edges of the real world. I
5581 en The Science and Art of User Experience at Google Focus on the user and all else will follow. From its inception, Google has focused on providing the best user experience possible. Jen Fitzpatrick will take you through the art and science behind Google's design process and share examples of how design, usability and engineering come together in Google's unique culture to create great products
5582 en Git When you have hundreds of people simultaneously patching 25000 files of the Linux Kernel in sometimes conflicting ways, you might need some scheme or plan to sort all that out before you can build your next kernel and reboot.  The Linux team uses "git" for their source code repository management, a homegrown solution that is optimized for highly distributed development, working with huge sets of files, merging independent work at multiple levels, and seeing who broke what. (Git has also since been notably adopted by the Cairo, x.org, and Wine teams, and is being transitioned to by the Mozilla codebase.)  In my talk, I describe what "git"; is and isn't, and why you should use it instead of CVS, Subversion, SVK, Arch, Darcs, Mercurial, Monotone, Bazaar, and just about every other repository manager. I'll also walk though the basic concepts so that the manpages might start making sense. If I have time, I'll even do a live walkthrough, where you can watch how fast I make typos.
5584 en Opening Remarks 
5585 en Representative Subgraph Sampling using Markov Chain Monte Carlo Methods Bioinformatics and the Internet keep generating graph data with thousands of nodes. Most traditional graph algorithms for data analysis are too slow for analyzing these large graphs. One way to work around this problem is to sample a smaller ‘representative subgraph’ from the original large graph. Existing representative subgraph sampling algorithms either randomly select sets of nodes or edges, or they explore the vicinity of a randomly drawn node. All these existing approaches do not make use of topological properties of the original graph and provide good samples down to sample sizes of approximately 15% of the number of nodes in the original graph. In this article, we propose novel sampling methods for representative subgraph sampling, based on the Metropolis algorithm and Simulated Annealing. The key idea is to find a subgraph that preserves properties of the original graph that are efficient to compute or to approximate. In our experiments, we improve over the pioneering work of Leskovec and Faloutsos (KDD 2006), by producing representative subgraph samples that are both smaller and of higher quality than those produced by other methods from the literature.
5586 en Inferring the structure and scale of modular networks We present an efficient, principled, and interpretable technique for inferring module assignments and for identifying the optimal number of modules in a given network, based on variational Bayesian inference for stochastic block models. We show how our method extends previous work and addresses the “resolution limit problem”. We apply the technique to synthetic and real networks.
5587 en Min, Max and PTIME Anti-Monotonic Overlap Graph Measures The main contributions of this paper are: (1) We extend the anti-monotonicity results of Vanetik, Gudes and Shimony to all 24 combinations of iso-, homo-, or homeomorphism, on labeled or unlabeled, directed or undirected graphs, with edge- or vertex-overlap. (2) We show that (under reasonable assumptions) the maximum independent set measure (MIS) of Vanetik, Gudes and Shimony (2006) is the smallest anti-monotonic measure in the class of overlap-graph based frequency measures. We also introduce the new minimum clique partition measure (MCP) which represents the largest possible one. (3) In general, both theMIS and theMCP measure are NP-hard in the size of the overlap graph. We introduce the polynomial time computable Lovasz measure, which is is sandwiched between the former two, and show that is anti-monotonic.
5588 en Influence and Correlation in Social Networks In many online social systems, social ties between users play an important role in dictating users' behavior. One of the ways this can happen is through social influence, the phenomenon that the actions of a user can induce his/her friends to behave in a similar way. In systems where social influence exists, ideas, modes of behavior, or new technologies can diffuse through the network like an epidemic. Therefore, identifying and understanding social influence is of tremendous interest from both an analysis (e.g., predicting the future of the system) and a design (e.g., designing viral marketing strategies) point of view. In this talk, I will give a general overview of models for diffusion in social network, and then discuss the problem of identifying social influence in the data. This is a difficult task in general, since there are many other factors such as homophily or unobserved confounding variables that can induce statistical correlation between the actions of friends in a social network. Thus, distinguishing influence from those other factors is essentially the problem of distinguishing correlation from causality, a notoriously hard problem. Despite this, I will show how in an environment where the time stamp of the actions are observable, we can design simple statistical tests that distinguish between models of social influence and those that replicate the aforementioned sources of social correlation. I will sketch the proof of a theoretical justification of one of the tests, and present simulation results on randomly generated data and real tagging data from Flickr. The results exhibit that while there is significant social correlation in tagging behavior on this system, this correlation cannot be attributed to social influence.
5589 en Classification in Graphs using Discriminative Random Walks This paper describes a novel technique, called D-walks, to tackle semi-supervised classification problems in large graphs. We introduce here a betweenness measure based on passage times during random walks of bounded lengths in the input graph. The class of unlabeled nodes is predicted by maximizing the betweenness with labeled nodes. This approach can deal with directed or undirected graphs with a linear time complexity with respect to the number of edges, the maximum walk length considered and the number of classes. Preliminary experiments on the CORA database show that D-walks outperforms NetKit (Macskassy & Provost, 2007) as well as Zhou et al's algorithm (Zhou et al., 2005), both in classification rate and computing time.
5590 en An Online Algorithm for Learning a Labeling of a Graph This short report analyzes a simple and intuitive online learning algorithm - termed the graphtron - for learning a labeling over a fixed graph, given a sequence of labels. The contribution is twofold, (a) we give a theoretical characterization of the possible sequence of mistakes, and (b) we indicate the use for extremely large-scale problems due to sublinear space complexity and nearly linear time complexity. This work originated from numerous discussions with John, Mark and with Johan.
5591 en A New Kernel for Classification of Networked Entitiess Statistical machine learning techniques for data classification usually assume that all entities are i.i.d. (independent and identically distributed). However, real-world entities often interconnect with each other through explicit or implicit relationships to form a complex network. Although some graph-based classification methods have emerged in recent years, they are not really suitable for complex networks as they do not take the degree distribution of network into consideration. In this paper, we propose a new technique, Modularity Kernel, that can effectively exploit the latent community structure of networked entities for their classification. A number of experiments on hypertext datasets show that our proposed approach leads to excellent classification performance in comparison with the state-of-the-art methods.
5592 en Induction of Node Label Controlled Graph Grammar Rules Algorithms for inducing graph grammars from sets of graphs have been proposed before. An important class of such algorithms are those based on the Subdue graph mining system. But the rules learned by Subdue and its derivatives do not fit easily in any of the well-studied graph grammars formalisms. In this paper, we discuss how Subdue-like algorithms could be made to work in the context of NLC grammars, an important class of node replacement graph grammars. More specifically, we show how, given a set of occurrences of a subgraph, an NLC grammar rule can be induced such that the given occurrences could have been generated by it.
5593 en Poster Spotlights 
5594 en Structured Output Prediction with Structural SVMs This talk explores large-margin approaches to predicting graph-based objects like trees, clusterings, or alignments. Such problems arise, for example, when a natural language parser needs to predict the correct parse tree for a given sentence, when one needs to determine the co-reference relationships of noun-phrases in a document, or when predicting the alignment between two proteins. In particular, the talk will show how structural SVMs can learn such complex prediction rules, using the problems of supervised clustering, protein sequence alignment, and diversification in search engines as application examples. Furthermore, the talk will present new cutting-plane algorithms that allows training of structural SVMs in time linear in the number of training examples.
5595 en Structure and tie strengths in a mobile communication network We examine the communication patterns of millions of anonymized mobile phone users. Based on call records, we construct a communication network where vertices are subscribers and edge weights are defined as aggregated duration of calls, reflecting the strengths of social ties between callers. We observe a coupling between tie strengths and network topology: at the ”local” level, strong ties are associated with densely connected network neighborhoods, providing the first large-scale confirmation of the Granovetter hypothesis. Based on fragmentation analysis, weak ties are seen to play an important role at the network level, accounting for global connectivity. The observed coupling is shown to significantly slow down the spreading of random information, resulting in dynamic trapping of information in communities.
5596 en Improved Software Fault Detection with Graph Mining This work addresses the problem of discovering bugs in software development. We investigate the utilization of call graphs of program executions and graph mining algorithms to approach this problem. We propose a novel reduction technique for call graphs which introduces edge weights. Then, we present an analysis technique for such weighted call graphs based on graph mining and on traditional feature selection. Our new approach finds bugs which could not be detected so far. With regard to bugs which can already be localized, our technique also doubles the precision of finding them.
5597 en Biomine search engine for probabilistic graphs Biomine is a search engine prototype under development. It can be used to find biological entities that are (indirectly) related to given query entities, as well as to display and evaluate the relations. Biomine is based on an integrated index to a number of public biological databases. The representation is a probabilistic graph, where nodes correspond to biological entities (typically a record in a biological database) and edges to their relationships (typically a cross-reference between database records). Edges are annotated with probabilities that reflect the strength or the reliability of the relation. I will discuss research problems and challenges for search in such graphs.
5598 en Parameter Learning in Probabilistic Databases: A Least Squares Approach Probabilistic databases compute the success probabilities of queries. We introduce the problem of learning the parameters of the probabilistic database ProbLog. Given the observed success probabilities of a set of queries, we compute the probabilities attached to facts that have a low approximation error on the training data as well as on unseen examples. Assuming Gaussian error terms on the observed success probabilities, this naturally leads to a least squares optimization problem. Experiments on real world data show the usefulness and effectiveness of this least squares calibration of probabilistic databases.
5599 en Infinite mixtures for multi-relational categorical data Large relational datasets are prevalent in many fields. We propose an unsupervised component model for relational data, i.e., for heterogeneous collections of categorical co-occurrences. The co-occurrences can be dyadic or n-adic, and over the same or different categorical variables. Graphs are a special case, as collections of dyadic co occurrences (edges) over a set of vertices. The model is simple, with only one latent variable. This allows wide applicability as long as a global latent component solution is preferred, and the generative process fits the application. Estimation with a collapsed Gibbs sampler is straightforward. We demonstrate the model with graphs enriched with multinomial vertex properties, or more concretely, with two sets of scientific papers, with both content and citation information available.
5600 en Markov Logic Improves Protein ?-Partners Prediction Protein ?-partners prediction is an important problem in protein structure that can be naturally formulated as supervised link prediction. We show that prediction performance can be improved using a hybrid solution based on Markov logic networks with grounding-specific weights.
5601 en A Hilbert-Schmidt Dependence Maximization Approach to Unsupervised Structure Discovery In recent work by (Song et al., 2007), it has been proposed to perform clustering by maximizing a Hilbert-Schmidt independence criterion with respect to a predefined cluster structure Y, by solving for the partition matrix. We extend this approach here to the case where the cluster structure Y is not fixed, but is a quantity to be optimized and we use an independence criterion which has been shown to be more sensitive at small sample sizes (the Hilbert-Schmidt Normalized Information Criterion, or HSNIC (Fukumizu et al., 2008)). We demonstrate the use of this framework in two scenarios. In the first, we adopt a cluster structure selection approach in which the HSNIC is used to select a structure from several candidates. In the second, we consider the case where we discover structure by directly optimizing Y.
5602 en Four graph partitioning algorithms We will discuss four partitioning algorithms using eigenvectors, random walks, PageRank and their variations. In particular, we will examine local partitioning algorithms, which find a cut near a specified starting vertex, with a running time that depends on the size of the small side of the cut, rather than on the size of the input graph (which can be prohibitively large). Three of the four partitioning algorithms are local algorithms and are particularly appropriate for applications for massive data sets.
5603 en Inverting the Viterbi Algorithm: an Abstract Framework for Structure Design Probabilistic grammatical formalisms such as hidden Markov models (HMMs) and stochastic context-free grammars (SCFGs) have been extensively studied and widely applied in a number of fields. Here, we introduce a new algorithmic problem on HMMs and SCFGs that arises naturally from protein and RNA design, and which has not been previously studied. The problem can be viewed as an inverse to the one solved by the Viterbi algorithm on HMMs or by the CKY algorithm on SCFGs. We study this problem theoretically and obtain the first algorithmic results. We prove that the problem is NP-complete, even for a 3-letter emission alphabet, via a reduction from 3-SAT, a result that has implications for the hardness of RNA secondary structure design. We then develop a number of approaches for making the problem tractable. In particular, for HMMs we develop a branch-and-bound algorithm, which can be shown to have fixed-parameter tractable worst-case running time, exponential in the number of states of the HMM but linear in the length of the structure. We also show how to cast the problem as a Mixed Integer Linear Program.
5604 en An HDP-HMM for Systems with State Persistence The hierarchical Dirichlet process hidden Markov model (HDP-HMM) is a flexible, nonparametric model which allows state spaces of unknown size to be learned from data. We demonstrate some limitations of the original HDP-HMM formulation, and propose a sticky extension which allows more robust learning of smoothly varying dynamics. Using DP mixtures, this formulation also allows learning of more complex, multimodal emission distributions. We further develop a sampling algorithm that employs a truncated approximation of the DP to jointly resample the full state sequence, greatly improving mixing rates. Via extensive experiments with synthetic data and the NIST speaker diarization database, we demonstrate the advantages of our sticky extension, and the utility of the HDP-HMM in real-world applications.
5605 en Modeling Interleaved Hidden Processes Hidden Markov models assume that observations in time series data stem from some hidden process that can be compactly represented as a Markov chain. We generalize this model by assuming that the observed data stems from multiple hidden processes, whose outputs interleave to form the sequence of observations. Exact inference in this model is NP-hard. However, a tractable and effective inference algorithm is obtained by extending structured approximate inference methods used in factorial hidden Markov models. The proposed model is evaluated in an activity recognition domain, where multiple activities interleave and together generate a stream of sensor observations. It is shown to be more accurate than a standard hidden Markov model in this domain.
5606 en Beam Sampling for the Infinite Hidden Markov Model The infinite hidden Markov model is a nonparametric extension of the widely used hidden Markov model. Our paper introduces a new inference algorithm for the infinite hidden Markov model called beam sampling. Beam sampling combines slice sampling, which limits the number of states considered at each time step to a finite number, with dynamic programming, which samples whole state trajectories efficiently. Our algorithm typically outperforms the Gibbs sampler and is more robust. We present applications of iHMM inference using the beam sampler on changepoint detection and text prediction problems.
5607 en Efficiently Solving Convex Relaxations for MAP Estimation The problem of obtaining the maximum a posteriori (MAP) estimate of a discrete random field is of fundamental importance in many areas of Computer Science. In this work, we build on the tree reweighted message passing (TRW) framework of Kolmogorov and Wainwright et al. TRW iteratively optimizes the Lagrangian dual of a linear programming relaxation for MAP estimation. We show how the dual formulation of TRW can be extended to include linear cycle inequalities. We then consider the inclusion of some recently proposed second order cone (SOC) constraints in the dual. We propose efficient iterative algorithms for solving the resulting duals. Similar to the method described by Kolmogorov, these methods are guaranteed to converge. We test our algorithms on a large set of synthetic data, as well as real data. Our experiments show that the additional constraints (i.e. cycle inequalities and SOC constraints) provide better results in cases where the TRW framework fails (namely MAP estimation for non-submodular energy functions).
5608 en A Quasi-Newton Approach to Nonsmooth Convex Optimization We extend the well-known BFGS quasi-Newton method and its limited-memory variant (LBFGS) to the optimization of nonsmooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: The local quadratic model, the identification of a descent direction, and the Wolfe line search conditions. We apply the resulting sub(L)BFGS algorithm to L2-regularized risk minimization with binary hinge loss, and its direction-finding component to L1-regularized risk minimization with logistic loss. In both settings our generic algorithms perform comparable to or better than their counterparts in specialized state-of-the-art solvers.
5609 en Stopping Conditions for Exact Computation of Leave-One-Out Error in Support Vector Machines We propose a new stopping condition for a Support Vector Machine (SVM) solver which precisely reflects the objective of the Leave-One-Out error computation. The stopping condition guarantees that the output on an intermediate SVM solution is identical to the output of the optimal SVM solution with one data point excluded from the training set. A simple augmentation of a general SVM training algorithm allows one to use a stopping criterion equivalent to the proposed sufficient condition. A comprehensive experimental evaluation of our method shows consistent speedup of the exact LOO computation by our method, up to the factor of 13 for the linear kernel. The new algorithm can be seen as an example of constructive guidance of an optimization algorithm towards achieving the best attainable expected risk at optimal computational cost.
5610 en On Partial Optimality in Multi-label MRFs We consider the problem of optimizing multi-label MRFs, which is in general NP-hard and ubiquitous in low-level computer vision. One approach for its solution is to formulate it as an integer programming problem and relax the integrality constraints. The approach we consider in this paper is to first convert the multi-label MRF into an equivalent binary-label MRF and then to relax it. Our key contribution is a theoretical study of this new relaxation. We also show how this approach can be used in combination with recently developed optimization techniques based on roof-duality which have the desired property that a partial (or sometimes the complete) optimal solution of the binary MRF can be found. This property enables us to localize (restrict) the range of labels where the optimal label for any random variable of the multi-label MRF lies. In many cases these localizations lead to a partially optimal solution of the multi-label MRF. Further, running standard MRF solvers, e.g. TRW-S, on this restricted energy is much faster than running them on the original unrestricted energy. We demonstrate the use of our methods on challenging computer vision problems. Our experimental results show that methods derived from our study outperform competing methods for minimizing multi-label MRFs.
5611 en Democratic Approximation of Lexicographic Preference Models Previous algorithms for learning lexicographic preference models (LPMs) produce a "best guess" LPM that is consistent with the observations. Our approach is more democratic: we do not commit to a single LPM. Instead, we approximate the target using the votes of a collection of consistent LPMs. We present two variations of this method -- "variable voting" and "model voting" -- and empirically show that these democratic algorithms outperform the existing methods. We also introduce an intuitive yet powerful learning bias to prune some of the possible LPMs. We demonstrate how this learning bias can be used with variable and model voting and show that the learning bias improves the learning curve significantly, especially when the number of observations is small.
5612 en Unsupervised Rank Aggregation with Distance-Based Models The need to meaningfully combine sets of rankings often comes up when one deals with ranked data. Although a number of heuristic and supervised learning approaches to rank aggregation exist, they require domain knowledge or supervised ranked data, both of which are expensive to acquire. In order to address these limitations, we propose a mathematical and algorithmic framework for learning to aggregate (partial) rankings without supervision. We instantiate the framework for the cases of combining permutations and combining top-k lists, and propose a novel metric for the latter. Experiments in both scenarios demonstrate the effectiveness of the proposed formalism.
5613 en Learning Dissimilarities by Ranking: From SDP to QP We consider the problem of learning dissimilarities between points via formulations which preserve a specified ordering between points rather than the numerical values of the dissimilarities. Dissimilarity ranking (d-ranking) learns from instances like "A is more similar to B than C is to D" or "The distance between E and F is larger than that between G and H". Three formulations of d-ranking problems are presented and new algorithms are presented for two of them, one by semidefinite programming (SDP) and one by quadratic programming (QP). Among the novel capabilities of these approaches are out-of-sample prediction and scalability to large problems.
5614 en Optimizing Estimated Loss Reduction for Active Sampling in Rank Learning Learning to rank is becoming an increasingly popular research area in machine learning. The ranking problem aims to induce an ordering or preference relations among a set of instances in the input space. However, collecting labeled data is growing into a burden in many rank applications since labeling requires eliciting the relative ordering over the set of alternatives. In this paper, we propose a novel active learning framework for SVM-based and boosting-based rank learning. Our approach suggests sampling based on maximizing the estimated loss differential over unlabeled data. Experimental results on two benchmark corpora show that the proposed model substantially reduces the labeling effort, and achieves superior performance rapidly with as much as 30% relative improvement over the margin-based sampling baseline.
5615 en Bayes Optimal Classification for Decision Trees We present the first algorithm for exact Bayes optimal classification from the hypothesis space of decision trees satisfying leaf constraints. Our contribution is that we reduce this problem to the problem of finding a rule-based classifier with appropriate weights. We show that these rules and weights can be computed in linear time from the output of a modified frequent itemset mining algorithm, which means that we can compute the classifier in practice, despite the exponential worst-case complexity. We perform experiments in which we compare the Bayes optimal predictions with those of the maximum a posteriori hypothesis.
5616 en Detecting Statistical Interactions with Additive Groves of Trees Discovering additive structure is an important step towards understanding a complex multi-dimensional function because it allows the function to be expressed as the sum of lower-dimensional components. When variables interact, however, their effects are not additive and must be modeled and interpreted simultaneously. We present a new approach for the problem of interaction detection. Our method is based on comparing the performance of unrestricted and restricted prediction models, where restricted models are prevented from modeling an interaction in question. We show that an additive model-based regression ensemble, Additive Groves, can be restricted appropriately for use with this framework, and thus has the right properties for accurately detecting variable interactions.
5617 en Sparse Bayesian Nonparametric Regression One of the most common problems in machine learning and statistics consists of estimating the mean response X.beta from a vector of observations y assuming y=X.beta+epsilon where X is known, beta is a vector of parameters of interest and epsilon a vector of stochastic errors. We are particularly interested here in the case where the dimension K of beta is much higher than the dimension of y. We propose some flexible Bayesian models which can yield sparse estimates of beta. We show that as K tends to infinity, these models are closely related to a class of Levy processes. Simulations demonstrate that our models outperform significantly a range of popular alternatives.
5618 en Bolasso: Model Consistent Lasso Estimation through the Bootstrap We consider the least-square linear regression problem with regularization by the l1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.
5619 en The GroupLASSO for Generalized Linear Models: Uniqueness of Solutions and Efficient Algorithms The GroupLASSO method for finding important explanatory factors suffers from the potential non-uniqueness of solutions and also from high computational costs. We formulate conditions for the uniqueness of GroupLASSO solutions which lead to an easily implementable test procedure. In addition to merely detecting ambiguities in solutions, this testing procedure identifies all potentially active groups. These results are used to derive an efficient algorithm that can deal with input dimensions in the millions and can approximate the solution path efficiently. The derived methods are applied to large-scale learning problems where they exhibit excellent performance. We show that the proposed testing procedure helps to avoid misinterpretations of GroupLASSO solutions.
5620 en On the Chance Accuracies of Large Collections of Classifiers We provide a theoretical analysis of the chance accuracies of large collections of classifiers. We show that on problems with small numbers of examples, some classifier can perform well by random chance, and we derive a theorem to explicitly calculate this accuracy. We use this theorem to provide a principled feature selection criteria for sparse, high-dimensional problems. We evaluate this method on both microarray and fMRI datasets and show that it performs very close to the optimal accuracy obtained from an oracle. We also show that on the fMRI dataset this technique chooses relevant features successfully while another state-of-the-art method, the False Discovery Rate (FDR), completely fails at standard significance levels.
5621 en Autonomous Geometric Precision Error Estimation in Low-level Computer Vision Tasks Errors in map-making tasks using computer vision are sparse. We demonstrate this by considering the construction of digital elevation models that employ stereo matching algorithms to triangulate real-world points. This sparsity, coupled with a geometric theory of errors recently developed by the authors, allows for autonomous agents to calculate their own precision independently of ground truth. We connect these developments with recent advances in the mathematics of sparse signal reconstruction or compressed sensing. The theory presented here extends the autonomy of 3-D model reconstructions discovered in the 1990s to their errors.
5622 en Multi-Task Compressive Sensing with Dirichlet Process Priors Compressive sensing (CS) is an emerging field that, under appropriate conditions, can significantly reduce the number of measurements required for a given signal. In many applications, one is interested in multiple signals that may be measured in multiple CS-type measurements, where here each signal corresponds to a sensing "task". In this paper we propose a novel multi-task compressive sensing framework based on a Bayesian formalism, where a Dirichlet process (DP) prior is employed, yielding a principled means of simultaneously inferring the appropriate sharing mechanisms as well as CS inversion for each task. A variational Bayesian (VB) inference algorithm is employed to estimate the full posterior on the model parameters.
5623 en Compressed Sensing and Bayesian Experimental Design We relate compressed sensing (CS) with Bayesian experimental design and provide a novel efficient approximate method for the latter, based on expectation propagation. In a large comparative study about linearly measuring natural images, we show that the simple standard heuristic of measuring Wavelet coefficients top-down systematically outperforms CS methods using random measurements; the sequential projection optimisation approach of [Ji & Carin 2007] performs even worse. We also show that our own approximate Bayesian method is able to learn measurement filters on full images efficiently which outperform the Wavelet heuristic. To our knowledge, ours is the first successful attempt at {}"learning compressed sensing" for images of realistic size. In contrast to common CS methods, our framework is not restricted to sparse signals, but can readily be applied to other notions of signal complexity or noise models. We give concrete ideas how our method can be scaled up to large signal representations.
5624 en Efficient Projections onto the L1-Ball for Learning in High Dimensions We describe efficient algorithms for projecting a vector onto the L1-ball. We present two methods for projection. The first performs exact projection in O(n) time, where n is the dimension of the space. The second works on vectors k of whose elements are perturbed outside the L1-ball, projecting in O(k log(n)) time. This setting is especially useful for online learning in sparse feature spaces such as text categorization applications. We demonstrate the merits and effectiveness of our algorithms in numerous batch and online learning tasks. We show that variants of stochastic gradient projection methods augmented with our efficient projection procedures outperform state-of-the-art optimization techniques such as interior point methods. We also show that in online settings gradient updates with L1 projections outperform the EG algorithm while obtaining models with high degrees of sparsity.
5625 en Empirical Bernstein Stopping Sampling is a popular way of scaling up machine learning algorithms to large datasets. The question often is how many samples are needed. Adaptive stopping algorithms monitor the performance in an online fashion and make it possible to stop early, sparing valuable computation time. We concentrate on the setting where probabilistic guarantees are desired and demonstrate how recently-introduced empirical Bernstein bounds can be used to design stopping rules that are efficient. We provide upper bounds on the sample complexity of the new rules as well as empirical results on model selection and boosting in the filtering setting.
5626 en Pointwise Exact Bootstrap Distributions of Cost Curves Cost curves have recently been introduced as an alternative or complement to ROC curves in order to visualize binary classifiers performance. Of importance to both cost and ROC curves is the computation of confidence intervals along with the curves themselves so that the reliability of a classifier's performance can be assessed. Computing confidence intervals for the difference in performance between two classifiers allows to determine whether one classifier performs significantly better than another. A simple procedure to obtain confidence intervals for costs or the difference between two costs, under various operating conditions, is to perform bootstrap resampling of the testset. In this paper, we derive exact bootstrap distributions of these values and use these distributions to obtain confidence intervals, under various operating conditions. Performances of these confidence intervals are measured in terms of coverage accuracies. Simulations show excellent results.
5627 en An Empirical Evaluation of Supervised Learning in High Dimensions In this paper we perform an empirical evaluation of supervised learning methods on high dimensional data. We evaluate learning performance on three metrics: accuracy, AUC, and squared loss. We also study the effect of increasing dimensionality on the relative performance of the learning algorithms. Our findings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the various learning algorithms changes. To our surprise, the methods that seem best able to learn from high dimensional data are random forests and neural nets.
5628 en Cost-Sensitive Multi-class Classification from Probability Estimates For two-class classification, it is common to classify by setting a threshold on class probability estimates, where the threshold is determined by {ROC} curve analysis. An analog for multi-class classification is learning a new class partitioning of the multiclass probability simplex to minimize empirical misclassification costs. We analyze the interplay between systematic errors in the class probability estimates and cost matrices for multi-class classification. We explore the effect on the class partitioning of five different transformations of the cost matrix. Experiments on benchmark datasets with naive Bayes and quadratic discriminant analysis show the effectiveness of learning a new partition matrix compared to previously proposed methods.
5629 en A Least Squares Formulation for Canonical Correlation Analysis Canonical Correlation Analysis (CCA) is a well-known technique for finding the correlations between two sets of multi-dimensional variables. It projects both sets of variables into a lower-dimensional space in which they are maximally correlated. CCA is commonly applied for supervised dimensionality reduction, in which one of the multi-dimensional variables is derived from the class label. It has been shown that CCA can be formulated as a least squares problem in the binary-class case. However, their relationship in the more general setting remains unclear. In this paper, we show that, under a mild condition which tends to hold for high-dimensional data, CCA in multi-label classifications can be formulated as a least squares problem. Based on this equivalence relationship, we propose several CCA extensions including sparse CCA using 1-norm regularization. Experiments on multi-label data sets confirm the established equivalence relationship. Results also demonstrate the effectiveness of the proposed CCA extensions
5630 en Closed-form Supervised Dimensionality Reduction with Generalized Linear Models We propose a family of supervised dimensionality reduction (SDR) algorithms that combine feature extraction (dimensionality reduction) with learning a predictive model in a unified optimization framework, using data- and class-appropriate generalized linear models (GLMs), and handling both classification and regression problems. Our approach uses simple closed-form update rules and is provably convergent. Promising empirical results are demonstrated on a variety of high-dimensional datasets.
5631 en Subspace-based Learning with Grassmann Kernels In this paper we propose a discriminant learning framework for problems in which data consist of linear subspaces instead of vectors. By treating subspaces as basic elements, we can make learning algorithms adapt naturally to the problems with linear invariant structures. We propose a unifying view on the subspace-based learning method by formulating the problems on the Grassmann manifold, which is the set of fixed-dimensional subspaces of a Euclidean space. Previous methods on the problem typically adopt an inconsistent strategy: feature extraction is performed in the Euclidean space while non-Euclidean dissimilarity measures are used. In our approach, we treat each subspace as a point in the Grassmann space, and perform feature extraction and classification in the same space. We show feasibility of the approach by using the Grassmann kernel functions such as the Projection kernel and the Binet-Cauchy kernel. Experiments with real image databases show that the proposed method performs well compared with state-of-the-art algorithms.
5632 en Metric Embedding for Kernel Classification Rules In this paper, we consider a smoothing kernel-based classification rule and propose an algorithm for optimizing the performance of the rule by learning the bandwidth of the smoothing kernel along with a data-dependent distance metric. The data-dependent distance metric is obtained by learning a function that embeds an arbitrary metric space into a Euclidean space while minimizing an upper bound on the resubstitution estimate of the error probability of the kernel classification rule. By restricting this embedding function to a reproducing kernel Hilbert space, we reduce the problem to solving a semidefinite program and show the resulting kernel classification rule to be a variation of the k-nearest neighbor rule. We compare the performance of the kernel rule (using the learned data-dependent distance metric) to state-of-the-art distance metric learning algorithms (designed for k-nearest neighbor classification) on some benchmark datasets. The results show that the proposed rule has either better or as good classification accuracy as the other metric learning algorithms.
5633 en Extracting and Composing Robust Features with Denoising Autoencoders Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful itermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.
5634 en Large Scale Learning - Challenge With the exceptional increase in computing power, storage capacity and network bandwidth of the past decades, ever growing datasets are collected in fields such as bioinformatics (Splice Sites, Gene Boundaries, etc), IT-security (Network traffic) or Text-Classification (Spam vs. Non-Spam), to name but a few. While the data size growth leaves computational methods as the only viable way of dealing with data, it poses new challenges to ML methods. This workshop is concerned with the scalability and efficiency of existing ML approaches with respect to computational, memory or communication resources, e.g. resulting from a high algorithmic complexity, from the size or dimensionality of the data set, and from the trade-off between distributed resolution and communication costs.
5635 en Large Scale Learning Which Is Actually Useful 
5637 en Pascal Challenge: Linear Support Vector Machines We participate in the linear SVM Track of the Pascal Large Scale Learning Challenge at ICML 2008. We consider the LIBLINEAR package, which can handle L1- and L2-loss linear SVMs. The L1-SVM solver implemented in LIBLINEAR employes a coordinate descent method to solve the dual problem. This method is very useful for large sparse data with a huge number of instances and features. However, most data sets of this challenges have a quite small number of features. To work on the competition data, we slightly modify LIBLINEAR.
5638 en Parallel streaming decision trees A new algorithm for building decision tree classifiers is proposed. The algorithm is executed in a distributed environment and is especially designed for classifying large datasets and streaming data. It is empirically shown to be as accurate as standard decision tree classifiers, while being scalable to infinite streaming data and multiple processors.
5639 en CTJLSVM: Componentwise Triple Jump Acceleration for Training Linear SVM The triple jump extrapolation method is an effective approximation of Aitken’s acceleration for accelerating the convergence of many machine learning algorithms that can be formulated as fixedpoint iteration. In the remainder of this abstract, we briefly review the general idea of the triple jump method and then describe how to apply it to accelerate stochastic gradient descent (SGD) for training linear support vector machines (SVM).
5640 en An Efficient Parameter - Free Method for Large Scale Offline Learning With the rapid growth of computer storage capacities, available data and demand for scoring models both follow an increasing trend, sharper than that of the processing power. However, the main limitation to a wide spread of data mining solutions is the non-increasing availability of skilled data analysts, which play a key role in data preparation and model selection. In this paper we present a parameter-free scalable classification method, which is a step towards fully automatic data mining. The method is based on Bayes optimal univariate conditional density estimators, naive Bayes classification enhanced with a Bayesian variable selection scheme, and averaging of models using a logarithmic smoothing of the posterior distribution. We focus on the complexity of the algorithms and show how they can cope with datasets that are far larger than the available central memory. We finally report results on the Large Scale Learning challenge, where our method obtains state of the art performance within practicable computation time.
5641 en Training Support Vector Machines: Status and Challenges 
5642 en Interior Point SVM Support vector machine training can be represented as a large quadratic program. We present an efficient and numerically stable algorithm for this problem using primal-dual interior point methods. Reformulating the problem to exploit separability of the Hessian eliminates the main source of computational complexity, resulting in an algorithm which requires only O(n) operations per iteration. Extensive use of L3 BLAS functions enables good parallel efficiency on shared-memory processors. As the algorithm works in primal and dual spaces simultaneously, our approach has the advantage of obtaining the hyperplane weights and bias directly from the solver.
5643 en Large Scale Support Vector Machines 
5644 en LaRank, SGD-QN - Fast Optimizers for Linear SVM Originally proposed for solving multiclass SVM, the LaRank algorithm is a dual coordinate ascent algorithm relying on a randomized exploration inspired by the perceptron algorithm [Bordes05, Bordes07]. This approach is competitive with gradient based optimizers on simple binary and multiclass problems. Furthermore, very few LaRank passes over the training examples delivers test error rates that are nearly as good as those of the final solution. For this entry we ran several epochs of the LaRank algorithm until reaching the convergence criterion. The SGD-QN algorithm uses stochastic gradient descent modified using an efficient method to estimate the diagonal of the inverse Hessian. The estimation method is inspired oLBFGS [Schraudolph, 07]. Since there is a little need to update this estimated matrix at each iteration, this approximate second-order stochastic gradient method iterates nearly as fast than a classical stochastic gradient descent [Bottou98, Bottou07] but requires less iterations.
5645 en Large Scale Learning - Challenge: Discussion and Summary 
5646 en Averaging Support Vector Machines for Processing Large Data Sets The handling of large data sets by support vector machines (SVMs)(Vapnik, 1998) employing a nonlinear kernel suffers from the non-linear scaling of the numerical solution techniques for the underlying optimisation problem. This is in particular valid if the kernel matrix cannot be stored in the main memory anymore and therefore the evaluation of the kernel on given data points needs to be recomputed again and again. We investigate a simple approach to allow the processing of larger data sets: We separate the large data set into a number of smaller ones, each small enough to allow the caching of the kernel matrix, and learn a support vector machine for each of these data sets. For the evaluation on data points we then just simply average the results of the different SVMs.
5653 en Lecture 1: Atomic Theory of Matter **I. People in History**nnA. AristotlennB. DemocritusnnC. Continuum Modelnn##1. Robert Boylenn##2. Joseph Priestlynn##3. Antoine Lavoisiernn##4. Joseph Proustnn##5. John Dalton; Atomic Theory of Matternn**II. Scanning Tunneling Microscopy**nn**III. End of the 19^^th^^ Century**nnA. Major Advancesnn##1. Newtonian mechanicsnn##2. Thermodynamicsnn##3. Statistical Mechanicsnn##4. Classical ElectromagnetismnnB. Non-“Classical” Observationsnn##1. Discovery of electron and nucleusnn##2. Photoelectric effectnn**IV. Discovery of the Electron**
5654 en Lecture 2: Discovery of Nucleus **I. E. Rutherford, 1911**nnA. Discovery of the NucleusnnB. Backscattering experimentnn**II. Classical Description of Atom**nnA. Coulombic interactionnnB. Classical equation of motion: Newton’s Second Lawnn**III. Wave-Particle Duality of Matter and Radiation**nnA. Wave Nature of LightnnB. Electromagnetic Radiation – periodic variation of an electromagnetic field
5655 en Lecture 3: Wavelike Properties of Radiation **I. Light: Periodic Variation of Electromagnetic Field**nnA. Oscillation vs. propagationnnB. Calculating speed of wavennC. Visible lightnn**II. Wave Nature of Light**nnA. SuperpositionnnB. Constructive and destructive interferencenn**III. Wavelike Properties of Radiation**nnA. Young’s two slit experimentnnB. General condition for constructive interferencennC. General condition for destructive interference
5656 en Lecture 4: Particle-like Nature of Light **I. Photoelectric Effect**nnA. Threshold frequency, ?,,0,,nnB. Kinetic energy vs. frequencynn##1. Classical predictionsnn##2. Planck’s constantnn##3. E = h?nnC. Threshold energynn**II. Photon Momentum**nnA. p=h/?
5657 en Lecture 5: Matter As a Wave **I. Electron Diffraction Experiment (1927)**nnA. Wave-like properties of e-snnB. Calculating ? from ?nnC. de Broglie wavelengthnn**II. Schrödinger’s Equation – Equation of Motion for Matter Waves**
5658 en Lecture 6: The Hydrogen Atom **I. Binding energy of e- to nucleus**nnA. QuantizednnB. n quantum numbernnC. Binding energy, E,,n,,nn**II. Verification of Energy Levels for H Atom**nnA. Photon emissionnnB. ? of transitions between two statesnnC. Photon absorptionnn**III. Wavefuntions for H Atom**nnA. Stationary state wavefunctionnnB. Three quantum numbers to describe a wave in 3Dnn##1. Principle quantum number, nnn##2. Angular momentum quantum number, //l//nn##3. Magnetic quantum number, m
5659 en Lecture 7: Hydrogen Atom Wavefunctions **I. Hydrogen Atom Wavefunctions**nnA. OrbitalsnnB. Degeneracynn**II. Shapes of H Atom Orbitals**nnA. Probability densitynnB. Radial probability distributionnnC. s wavefunctionsnnD. Radial nodesnn**III. Bohr's Model and the Uncertainty Principle**
5660 en Lecture 8: P Orbitals **I. p Orbitals**nnA. Nodal planesnnB. Angular nodesnnC. Radial probability distributionsnn**II. Wavefunctions for Multielectron Atoms**nnA. Electron configurationnnB. Spin and the Pauli exclusion principlennC. One e^^-^^ wavefunctions for multielectron atoms
5661 en Lecture 9: Electronic Structure of Multielectron Atoms **I. Electron Configurations**nnA. Aufbau Principlenn##1. Pauli Exclusion Principlenn##2. Hund’s RulennB. Core electrons and valence electronsnn**II. Shielding and Z,,eff,,**nn**III. Electron Configurations of Ions**nn**IV. Photoelectron Spectroscopy**
5662 en Lecture 10: Periodic Trends in Elemental Properties **I. Periodic Table**nnA. HistorynnB. Periodic trendsnn##1. Ionization energynn##2. Electron affinitynn##3. Electronegativitynn##4. Atomic Sizesnnn**II. Isoelectronic**
5663 en Lecture 11: Covalent Bonds **I. Energy of Interaction**nnA. Nuclear-nuclear repulsionnnB. Electron-electron repulsionnnC. Electron-nuclear attraction
5664 en Lecture 12: Lewis Diagrams **I. Lewis Structures**nnA. Example 1: Cyanide Ion CN^^-^^nnB. Example 2: Thionyl Chloride SOCl,,2,,nn**II. Formal Charge**nnA. Assigning a formal charge to an atomnnB. SignificancennC. Formal charge vs. Oxidation numbernn**III. Skeletal Structure**nnA. Methyl groupsnnB. Chain moleculesnn**IV. Resonance**nnA. Example: NO,,3,,^^-^^nnB. Resonance hybrid
5665 en Lecture 13: Breakdown of Octet Rule **I. Breakdown of Octet Rule**nnA. Odd number of valence electronsnn##1. Example 1: •CH,,3,,nn##2. Example 2: •NOnnB. Octet deficient moleculesnnC. Valence shell expansionnn##1. Example 1: PCl,,5,,nn##2. Example 2: CrO,,4,,^^-2^^nn##3. Example 3: IF,,4,,^^-^^nn**II. Ionic Bonds – Classical Model and Mechanism**nnA. Harpoon MechanismnnB. Limitations of the modelnnC. Energy of Interaction vs. r,,e,,
5666 en Lecture 14: Molecular Orbital Theory **I. Linear combination of atomic orbitals (LCAO)**nnA. Molecular orbitals (MOs)nnB. Bonding orbitalnnC. Antibonding orbitalnnD. Electron configurationsnn##1. Example 1: H,,2,,nn##2. Example 2: He,,2,,nnE. Bond Ordernn**II. MOs formed by LCAO**nnA. MOs formed by LCAO – 2snn##1. Example 1: Li,,2,,nn##2. Example 2: Be,,2,,nnB. Bonding MOs formed by LCAO – 2p,,x,, and 2p,,y,,nn##1. Example 1: B,,2,,nn##2. Example 2: C,,2,,nnC. Bonding MO formed by LCAO – 2p,,z,,nnD. Antibonding MO formed by LCAO – 2p,,x,, and 2p,,y,,nnE. Antibonding MO formed by LCAO – 2p,,z,,nn**III. Molecular Orbital Theory: Heteronuclear Diatomics**
5667 en Lecture 15: Valence Bond Theory and Hybridization **I. sp^^3^^ Hybridization**nnA. Example 1: Methane CH,,4,,nnB. Example 2: Ethane C,,2,,H,,6,,nnC. Example 3: Ammonia NH,,3,,nnD. Example 4: Water H,,2,,Onn**II. sp2 Hybridization**nnA. Example 1: Borane BH,,3,,nnB. Example 2: Ethylene C,,2,,H,,4,,nnC. Example 3: Benzene C,,6,,H,,6,,nn**III. sp Hybridization**
5668 en Lecture 16: Hybridization and Chemical Bonding **I. Example: Methyl nitrate CH,,3,,ONO,,2,,**nnA. Finding the lowest energy Lewis structurennB. Bond symmetrynnC. Hybrid orbitalsnnD. Atomic orbitalsnn**II. Intramolecular Interactions: Origin of a Bad Hair Day**nnA. Hydrogen bondingnnB. WaternnC. Keratin
5669 en Lecture 17: Bond Energies / Bond Enthalpies **I. Bond Enthalpy**nA. Average bond enthalpiesnnnB. Enthalpy of chemical reactionsnnC. Endothermic vs. ExothermicnnD. Heat of Formation ?H°,,f,,nnE. State functionnnF. Hess’s Lawnn**II. Thermodynamics and Spontaneous Change**nnA. Gibbs free energy ?G°nnB. Entropy ?S° for Reactionsnn##1. 3,,rd ,,Law of Thermodynamicsnn##2. Internal degrees of freedom of reactants
5670 en Lecture 18: Free Energy of Formation ?Gof **I. Standard Gibbs Free Energy of Formation**nnA. Thermodynamic stabilitynnB. Calculating ?G° for a reactionnn**II. Second Law of Thermodynamics**nnA. Controlling spontaneity with temperaturennB. ?G at any pressure (ideal gases)nn**III. Chemical Equilibrium**nnA. Thermodynamic equilibrium constantnnB. Reaction quotient and direction of change
5671 en Lecture 19: Chemical Equilibrium **I. Nature of Chemical Equilibrium**nnA. Free energy, G vs. Progress of reactionnnB. Reaction quotientnn##1. Partial pressure (gas)n##2. Concentration (solute)nnC. Relationship between K and Qnn**II. Meaning of K**nnA. Concentration of products at equilibriumnnB. K > 1nnC. Relationship between ?G° and the magnitude of Knn**III. Relationship Between Equilibrium Expressions**nn**IV. External Effects on K**nnA. Principle of Le ChâteliernnB. Adding and removing reagentsnn##1. Adding more reactantnn##2. Adding more productnn##3. Removing product
5672 en Lecture 20: Chemical Equilibrium (cont.) **I. External Effects on K**nnA. Changing the volumennB. Adding inert gasnnC. Changing the temperaturenn##1. Temperature dependence of KnnD. Maximizing the yield of a reactionnnE. Le Châteier and hemoglobinnn**II. Significant Rules for Logs and Exponentials**
5674 en Lecture 21: Acid-Base Equilibrium **I. Classification of Acids and Bases**nnA. ArrheniusnnB. Brønsted-Lowrynn##1. Conjugate acid-base pairsnn##2. Amphoteric moleculesnnC. Lewis acid and basenn**I. Autoionization of Water**nn**III. pH Function**nn**IV. pOH Function**nn**V. Strength of Acids and Bases**nn##1. Acid in waternn##2. Base in waternn##3. Conjugate acids and basesnn##4. Relative strengths of acidsnn**VI. Types of Acid-Base Problems**nn**VII. Equilibrium Involving Weak Acids**
5675 en Lecture 22: Acid-Base Equilibrium (cont.) **I. Types of Acid-Base Problems**nn**II. Equilibrium Involving Weak Acids**nn**III. pH of Salt Solutions**nn**IV. Buffers**nnA. Acid buffernnB. Base buffernnC. Designing a buffernn##1. Henderson-Hasselbalch equation
5676 en Lecture 23: Acid-Base Equilibrium: Titrations **I. Titrations Involving a Strong Acid and a Strong Base**nnA. Equivalence pointnnB. End pointnn**II. Calculating Points on a pH Curve**nnA. Calculating pH before the equivalence pointnnB. Calculating volume of HCl needed to reach equivalence pointnnC. Calculating pH after the equivalence pointnn**III. Titration Curves for Weak Acid/Strong Base and for Weak Base/Strong Acid**
5678 en Lecture 24: Acid Base Titrations and Oxidation/Reduction **I. Acid Base Titration: How to Calculate pH**nnA. Weak acid / Strong basennB. Weak base / Strong acidnn**II. Oxidation/Reduction Reactions**nnA. Guidelines for assigning oxidation numbersnnB. Definitionsnn##1. Oxidationnn##2. Reductionnn##3. Oxidizing agentnn##4. Reducing agentnnC. Disproportionation reactionnnD. Balancing redox reactions
5679 en Lecture 25: Oxidation/Reduction **I. Electrochemical Cells**nnA. BatterynnB. Anode (oxidation)nnC. Cathode (reduction)nnD. Faraday’s LawnnE. Electrodesnn##1. Pt electrodenn##2. Hydrogen electrodenn**II. Relationship Between Cell Potential (?E,,cell,,) and Gibbs Free Energy**nnA. Standard states and cell potentialsnn##1. ?G°,,cell,,nn##2. ?E°,,cell,,nn##3. Galvanic cellnn##4. Electrolytic cellnnB. Standard reduction potential, E°
5681 en Lecture 26: Oxidation/Reduction (cont.) **I. Adding and Subtracting Half-Cell Reactions**nn**II. Nernst Equation**nnA. Example: Calculate ?E at 25°C of a cell in which the concentration of Zn^^2+^^ ions is 0.10 M and Cu^^2+^^ is 0.0010 M.nnB. Example: Pb^^2+^^ (aq) + Zn (s) ? Zn^^2+ ^^(aq) + Pb (s) What is K at 25°C?nnC. Reduction of vitamin B,,12,,
5682 en Lecture 27: Transition Metals **I. Formation of Coordination Complexes**nnA. Donor atoms: ligandsnnB. Acceptor atomsnnC. Coordination complexesnnD. Coordination numbernnE. Coordination complex notationnn**II. Structures of Coordination Complexes**nnA. Chelate effectnn##1. Vitamin B,,12,,nn##2. Ethylenediamine tetraacetic acid (EDTA)nnB. Geometric isomersnnC. Optical isomers (enantiomers)nn**III. d-Electron Counting in Coordination Complexes**nn**IV. d Orbitals**
5683 en Lecture 28: Transition Metals: Crystal Field Theory **I. Crystal Field and Ligand Field Theories**nn**II. Crystal Field Theory**nnA. Octahedral field splitting energy, ?,,o,,nnB. Octahedral crystal field splitting diagramnC. High spinnnD. Low spinnnE. Examples:nn##1. [Cr(NH,,3,,)3Br,,3v]nn##2. [Mn(H,,2,,O),,6v]Cl,,3nn##3. Co^^2+^^ in an octahedral coordination complex
5684 en Lecture 29: The Shapes of Molecules: VSEPR Theory **I. Valence Shell Electron Pair Repulsion (VSEPR) Theory**nnA. Prediction of geometrynnB. Nomenclaturenn**II. VSEPR Rules**nnA. Steric Number (SN)nnB. Resonance structuresnnC. Molecules without lone pairsnnD. Molecules with lone pairsnn##1. Attractive forces exerted by nucleinn##2. Repulsive forcesnn**III. Rationalization of shapes based on VSEPR theory**nnA. Atomic sizennB. Bond lengths
5686 en Lecture 30: Transition Metals **I. Crystal Field Theory: Tetrahedral Case**nnA. Tetrahedral crystal field splitting energy, ?,,T,,nnB. Orbital destabilizationnnC. ?,,T,, compared to ?,,Ov,,nn**II. Crystal Field Theory: Square Planar Case**nn**III. Spectrochemical Series**nnA. Strong field ligandsnnB. Weak field ligandsnn**IV. Color in octahedral coordination complexes**nn**V. Magnetism**nnA. ParamagneticnnB. Diamagnetic
5687 en Lecture 31: Kinetics **I. Rates of Chemical Reactions**nnA. KineticsnnB. Oxidation of glucosennC. Factors affecting rates of reactionsnnD. Measuring reaction ratesnnE. Rate expressionsnn**II. Rate Laws**nnA. Order of reaction in reactants / productsnnB. Overall reaction ordernnC. Units for knnD. Integrated rate lawsnn##1. First order half-life (t,,1/2,,)
5688 en Lecture 32: Kinetics (cont.) **I. Radioactive Decay**nnA. Nuclear kineticsnnB. Decay rate, activity (A)nnC. Becquerel (Bq)nnD. Medical usesnn**II. Second Order Integrated Rate Laws**nnA. Second order half-lifenn**III. Kinetics and Chemical Equilibrium**nnA. Equilibrium constantnnB. Elementary reactionsnnC. Example: decomposition of ozone
5689 en Lecture 33: Kinetics (cont.) **I. Investigating Reaction Mechanisms**nnA. RatennB. OrdernnC. MolecularitynnD. Steady-state approximationnnE. Rate determining step
5690 en Lecture 34: Kinetics (cont.) **I. Effect of Temperature on Reaction Rates**nnA. Gas-phasenn##1. Arrhenius equationnnB. Activation energy, E,,a,,nn##1. Using activation energy to predict a rate constantnnC. Reaction Coordinate and the Activation Complexnn##1. Activated complex / transition statenn##2. Relationship between E,,a,, and knn##3. Relationship between ?H and Knn##4. Rate constants and temperaturenn##5. Equilibrium constants and temperaturenn##6. Le Châtelier’s Principlenn##7. Large E,,a,, vs. small E,,a,,
5691 en Lecture 35: Kinetics: Catalysis **I. Kinetics of Catalysis**nnA. CatalystnnB. InhibitornnC. Types of Catalystsnn##1. Homogeneous catalystsnn##2. Heterogneous catalystsnn##3. Catalysts of life: enzymesnn**II. Enzyme Catalysis**nnA. SubstratesnnB. Active sitennC. Deriving the rate expression for E + S ? ES ? E + Pnn##1. Michaelis-Menten EquationnnD. Enzyme inhibition
5692 en Lecture 36: Review **I. Case Study: Methionine Synthase**nnA. Kineticsnn##1. Enzymenn##2. InhibitionnnB. Transition metalsnn##1. Tetradentate ligandnn##2. d-electron countnn##3. Chelate effectnnC. VSEPRnn##1. Steric numbernn##2. Geometrynn##3. AnglesnnD. Oxidation / Reductionnn**II. Review**nnA. Reducing agentnnB. Spontaneous reactionnnC. ?G°nn**III. Acid-Base Equilibrium**nnA. DeprotonationnnB. pKannC. Lewis acidnn**IV. Chemical Equilibrium**nnA. Conformational changes
5696 en Lecture 1: Periodic Phenomena (Oscillations, Waves) - SHO - Complex Notation - Differential Equations - Physical Pendulum 
5697 en Lecture 2: Beats - Damped Free Oscillations (Under- Over- and Critically Damped) - Quality Q 
5698 en Lecture 3: Forced Oscillations with Damping Information about the Tacoma Narrows Bridge Collapse Information about the Tacoma Narrows Bridge Collapse: http://www.pbs.org/wgbh/nova/bridge/tacoma3.html and nhttp://www.ketchum.org/bridgecollapse.html
5699 en Lecture 4: Forced Oscillations - Power at Resonance (Resonance Absorption, Resonance Width, Quality Q) - Transient Phenomena (General Solutions including Initial Conditions) 
5700 en Lecture 5: Coupled Oscillators 
5701 en Lecture 6: Driven Coupled Oscillators - Triple Pendulum - Steady State and Transient Solutions - Cramer's Rule 
5702 en Lecture 7: Many Coupled Oscillators - Wave Equation - Transverse Traveling Pulses and Waves 
5703 en Lecture 8: Traveling Waves - Boundary Conditions - Standing Waves - Sound (Longitudinal Waves) - Energy in Waves 
5704 en Lecture 9: Musical Instruments - Sound Cavities - Normal Modes 
5705 en Lecture 10: Exam 1 Review 
5706 en Lecture 11: Fourier Analysis - Time Evolution of Pulses on Strings 
5707 en Lecture 12: Dispersion - Phase Velocity - Group Velocity 
5708 en Lecture 13: Electromagnetic Waves - Plane Wave Solutions to Maxwell's Equations - Polarization - Malus' Law 
5709 en Lecture 14: Accelerated Charges - Poynting Vector - Power - Rayleigh Scattering - Polarization 
5710 en Lecture 15: Doppler Effect - Sound - EM Radiation - Binary Stars - Neutron Stars and Black Holes - Expanding Universe 
5711 en Lecture 16: Boundary Conditions at Perfect Conductors - Reflection - Standing EM Waves - Transmission Lines - Radiation Pressure 
5712 en Lecture 17: Wave Guides - Resonance Cavities of EM-radiation and Sound 
5713 en Lecture 18: Boundary Conditions for Dielectrics - Index of Refraction - Snell's Law - Total Internal Reflection - Fresnel Equations - Brewster Angle 
5714 en Lecture 19: Exam 2 Review 
5715 en Lecture 20: Huygens' Principle - Interference - Thin films - Soap - Oil - Light (double slit interference) 
5716 en Lecture 21: Diffraction - Gratings - Pin Holes - Angular Resolution 
5717 en Lecture 22: Rainbows - Haloes - Coronae - Glories 
5718 en Lecture 23: Farewell Special - Bring a Friend! 
5719 en Lecture 1: Introduction and lumped abstraction 
5720 en Lecture 2: Basic circuit analysis method (KVL and KCL mMethod) 
5721 en Lecture 3: Superposition, Thévenin and Norton 
5722 en Lecture 4: The digital abstraction 
5723 en Lecture 5: Inside the digital gate 
5724 en Lecture 6: Nonlinear analysis 
5725 en Lecture 7: Incremental analysis 
5726 en Lecture 8: Dependent sources and amplifiers 
5727 en Lecture 9: MOSFET amplifier large signal analysis 
5728 en Lecture 10: Amplifiers - small signal model 
5729 en Lecture 11: Small signal circuits 
5730 en Lecture 12: Capacitors and first-order systems 
5731 en Lecture 13: Digital circuit speed 
5732 en Lecture 14: State and memory 
5733 en Lecture 15: Second-order systems 
5734 en Lecture 16: Sinusoidal steady state 
5735 en Lecture 17: The impedance model 
5736 en Lecture 18: Filters 
5737 en Lecture 19: The operational amplifier abstraction 
5738 en Lecture 20: Operational amplifier circuits 
5739 en Lecture 21: Op amps positive feedback 
5740 en Lecture 22: Energy and power 
5741 en Lecture 23: Energy, CMOS 
5742 en Lecture 25: Violating the abstraction barrier 
5743 en Lecture 1: What holds our world together? Electric Charges (Historical), Polarization, Electric Force, Coulomb's Law //"I'm Walter Lewin.//nn//My lectures will in general not be a repeat of your book but they will be complementary to the book. The book will support my lectures. My lectures will support the book. You will not see any tedious derivations in my lectures. For that we have the book. But I will stress the concepts and I will make you see beyond the equations, beyond the concepts. I will show you whether you like it or not that physics is beautiful.//nn//And you may even start to like it. I suggest you do not slip up, not even one day, eight oh two is not easy. We have new concepts every week and before you know you may be too far behind. Electricity and magnetism is all around us. We have electric lights. Electric clocks. We have microphones, calculators, televisions, VCRs, radio, computers. Light itself is an electromagnetic phenomenon as radio waves are.//nn//The colors of the rainbow in the blue sky are there because of electricity. And I will teach you about that in this course. Cars, planes, trains can only run because of electricity. Horses need electricity because muscle contractions require electricity..."//
5744 en Lecture 2: Electric Field, Field Lines, Superposition, Inductive Charging, Dipoles, Induced Dipoles //"Today I'm going to work with you on a new concept and that is the concept of what we call electric field. We spend the whole lecture on electric fields. If I have a -- a charge, I just choose Q, capital Q and plus at a particular location and at another location I have another charge little Q, I think of that as my test charge. And there is a separation between the two which is R. The unit vector from capital Q to li- little Q is this vector. And so now I know that the two charges if they were positive -- let's suppose that little Q is positive, they would repel each other. Little Q is negative they would attract each other.//nn//And let this force be F and last time we introduced Coulomb's law that force equals little Q times capital Q times Coulomb's constant divided by R squared in the direction of R roof. The two have the same sign. It's in this direction. If they have opposite sign it's in the other direction. And now I introduce the idea of electric field for which we write the symbol capital E..."//
5745 en Lecture 3: Electric Flux, Gauss's Law, Examples //"Today we're going to work on a whole new concept and that is the concept of electric flux. We've come a long way. We started out with Coulomb's law. We got electric field lines. And now we have electric flux.//nn//Suppose I have an electric field which is like so and I bring in that electric field a surface, an open surface like a handkerchief or a piece of paper. And so here it is. Something like that. And I carve this surface up in very small surface elements, each with size DA, that's the area, teeny weeny little area, and let this be the normal, N roof, the normal on that surface.//nn//So now the local electric field say at that location would be for instance this. It's a vector..."//
5746 en Lecture 4: Electrostatic Potential, Electric Energy, eV, Conservative Field, Equipotential Surfaces //"We're going to talk about again some new concepts. And that's the concept of electrostatic potential electrostatic potential energy. For which we will use the symbol U and independently electric potential.//nn//Which is very different, for which we will use the symbol V. Imagine that I have a charge Q one here and that's plus, plus charge, and here I have a charge plus Q two and they have a distant, they're a distance R apart. And that is point P. It's very clear that in order to bring these charges at this distance from each other I had to do work to bring them there because they repel each other.//nn//It's like pushing in a spring. If you release the spring you get the energy back..."//
5747 en Lecture 5: E = -grad V, More on Equipotential Surfaces, Conductors, Electrostatic Shielding (Faraday Cage) //"So today no new concepts, no new ideas, you can release a little bit and I want to discuss with you the connection between electric potential and electric fields.//nn//Imagine you have an electric field here in space and that I take a charge Q in my pocket, I start at position A and I walk around and I return at that point A.//nn//Since these forces are conservative forces, if the electric field is a static electric field, there are no moving charges, but that becomes more difficult, then the forces are conservative forces and so the work that I do when I march around and coming back at point A must be zero. It's clear when you uh look at the equation number three that the potential difference between point A and point A is obviously zero. I g- start at point A and I end at point A and that is the integral in going from A back to point A of E dot DL and that then has to be zero.//nn//And we normally indicate such an integral with a circle which means you end up where you started. This is a line now this is not a closed surface as we had in equation one..."//
5748 en Lecture 6: High-Voltage Breakdown, Lightning, Sparks - St. Elmo's Fire //"Last time I mentioned to you that charge resides at the surface of solid conductors but that it's not uniformly distributed. Perhaps you remember that, unless it happens to be a sphere.//nn//And I want to pursue that today. If I had a solid conductor which say had this shape and I'm going to convince you today that right here the surface charge density will be higher than there. Because the curvature is stronger than it is here. And the way I want to approach that is as follows. Suppose I have here a solid conductor A which has radius R of A and very very far away, maybe tens of meters away, I have a solid conductor B with radius R of B and they are connected through a conducting wire.//nn//That's essential. If they are connected through a conducting wire, then it's equipotential. They all have the same potential. I'm going to charge them up until I get a charge distribution QA here and I get QB there. The potential of A is about the same that it would be if B were not there..."//
5749 en Lecture 7: Capacitance, Field Energy //"...assemble charges, I have to do work, we discussed that earlier. And we call that electrostatic potential energy. Today, I will look at this energy concept in a different way, and I will evaluate the energy in terms of the electric field. Suppose I have two parallel plates, and I charge this one with positive charge, which is the surface charge density times the area of the plate, and this one, negative charge, which is the surface charge density negative times the area of the plate. And let's assume that the separation between these two is H, and so we have an electric field, which is approximately constant, and the electric field here is sigma divided by epsilon zero.//nn//And now, I'm going to take the upper plate, and I'm going to move it up. And so as I do that, I have to apply a force, because these two plates attract each other, so I have to do work. And as I move this up, and I will move it up over distance X, I am creating here, electric field that wasn't there before...."//
5750 en Lecture 8: Polarization, Dielectrics, The Van de Graaff, More on Capacitors //"Electric fields can induce dipoles in insulators.//nn//Electrons and insulators are bound to the atoms and to the molecules, unlike conductors, where they can freely move, and when I apply an external field -- for instance, a field in this direction, then even though the molecules or the atoms may be completely spherical, they will become a little bit elongated in the sense that the electrons will spend a little bit more time there than they used to, and so this part become negatively charged and this part becomes positively charged, and that creates a dipole.//nn//I discussed that with you, already, during the first lecture, because there's something quite remarkable about this, that if you have an insulator -- notice the pluses and the minuses indicate neutral atoms -- and if now, I apply an electric field, which comes down from the top, then, you see a slight shift of the electrons, they spend a little bit more time up than down, and what you see now is, you see a layer of negative charge being created at the top, and a layer of positive charge being created at the bottom.//nn//That's the result of induction, we call that also, sometimes, polarization. You are polarizing, in a way, the electric charge..."//
5751 en Lecture 9: Currents, Resistivity, Ohm's Law //"When positive charges move in unintelligible directions, then per definition, we say the current goes in this direction. When negative charges go in this direction, we also say the current goes in that direction, that's just our convention. If I apply a potential difference over a conductor, then I'm going to create an electric field in that conductor. And the electrons -- there are free electrons in a conductor -- they can move, but the ions cannot move, because they are frozen into the solid, into the crystal.//nn//And so when a current flows in a conductor, it's always the electrons that are responsible for the current. The electrons fuel the electric fields, and then the electrons try to make the electric field zero, but they can't succeed, because we keep the potential difference over the conductor..."//
5752 en Lecture 10: Batteries, EMF, Energy Conservation, Power, Kirchhoff's Rules, Circuits, Kelvin Water Dropper 
5753 en Lecture 11: Magnetic field, Lorentz Force, Torques, Electric Motors (DC), Oscilloscope 
5754 en Lecture 12: Review Exam 1 (Secret Top!) 
5755 en Lecture 13: Moving Charges in B-fields, Cyclotron, Synchrotron, Mass Spectrometer, Cloud Chamber 
5756 en Lecture 14: Biot-Savart Law, Gauss' Law for Magnetic Fields, Revisit the "Leyden Jar", High-Voltage Power Lines 
5757 en Lecture 15: Ampere's Law, Solenoids, Revisit the Kelvin Water Dropper, Midterm Evaluation 
5758 en Lecture 16: Electromagnetic Induction, Faraday's Law, Lenz Law, Complete Breakdown of Intuition, Non-Conservative Fields 
5759 en Lecture 17: Motional EMF, Dynamos, Eddy Currents, Magnetic Braking 
5760 en Lecture 18: Displacement Current (Difficult Concept), Synchronous Motors, Induction Motors, Secret Top, How does it work? 
5761 en Lecture 19: How do Magicians levitate women? (with demo), Electric Shock Treatment (no demo), Electrocardiogram (with demo), Pacemakers, Superconductivity (with demo), Levitating Bullet Trains, Aurora Borealis 
5762 en Lecture 20: Inductance, RL Circuits, Magnetic Field Energy 
5763 en Lecture 21: Magnetic Materials, Dia-, Para-, and Ferromagnetism, Prize Ceremony of Motor Contest 
5764 en Lecture 22: Hysteresis, Electromagnets, Bohr Magneton, Maxwell's Equations, 600 daffodils 
5765 en Lecture 23: Review Exam 2 
5766 en Lecture 24: Transformers, Car Coils, RC Circuits 
5767 en Lecture 25: Driven LRC Circuits, Resonance, Metal Detectors (Beach/Airport) 
5768 en Lecture 26: Traveling Waves, Standing Waves, Musical Instruments 
5769 en Lecture 27: Resonance, Destructive Resonance, Electromagnetic Waves, Speed of Light, Radio - TV, Distance Determinations using Radar and Lasers Information about the Tacoma Narrows Bridge Collapse: nhttp://www.pbs.org/wgbh/nova/bridge/tacoma3.html and http://www.ketchum.org/bridgecollapse.html
5770 en Lecture 28: Index of Refraction, Poynting Vector, Oscillating Charges, Radiation Pressure, Comet Tails, Polarization (Linear, Elliptical, and Circular) 
5771 en Lecture 29: Snell's Law, Refraction, Total Reflection, Dispersion, Prisms, Huygens's Principle, The Illusion of Color, The Weird Benham Top, Land's Famous Demo 
5772 en Lecture 30: Polarizers, Malus's Law, Brewster Angle, Polarization by Reflection and Scattering, Why is the sky blue? Why are sunsets red? The sun will set in the lecture hall! 
5773 en Lecture 31: Rainbows, A modest rainbow will appear in the lecture hall! Fog Bows, Supernumerary Bows, Polarization of the Bows, Halos around the Sun and the Moon, Mock Suns 
5774 en Lecture 32: Review Exam 3 
5775 en Lecture 33: Double-Slit Interference, Interferometers 
5776 en Lecture 34: Gratings, Resolving Power, Single-Slit Diffraction, Angular Resolution, Human Eye, Telescopes 
5777 en Lecture 35: Doppler Effect, The Big Bang, Cosmology 
5778 en Lecture 36: Farewell Special, Bring a Friend! The charming MIT Muses surprise Walter Lewin during his last lecture, #36. (Photo by Markos Hankin, MIT Physics Department Lecture Demonstration Group. Used with permission of The Muses, MIT's all-women's a cappella group.)
5779 en Lecture 1: The Geometry of Linear Equations 
5780 en Lecture 1: Administrivia, Introduction, Analysis of Algorithms, Insertion Sort, Mergesort //"We're going to get started. Handouts are the by the door if anybody didn't pick one up. My name is Charles Leiserson. I will be lecturing this course this term, Introduction to Algorithms, with Erik Demaine. In addition, this is an SMA course, a Singapore MIT Alliance course which will be run in Singapore by David Hsu. And so all the lectures will be videotaped and made available on the Web for the Singapore students, as well as for MIT students who choose to watch them on the Web. If you have an issue of not wanting to be on the videotape, you should sit in the back row. OK? Otherwise, you will be on it..."//
5781 en Lecture 2: Asymptotic Notation, Recurrences, Substitution, Master Method //"My name is Erik Demaine. You should call me Erik. Welcome back to 6.046. This is Lecture 2. And today we are going to essentially fill in some of the more mathematical underpinnings of Lecture 1. So, Lecture 1, we just sort of barely got our feet wet with some analysis of algorithms, insertion sort and mergesort. And we needed a couple of tools. We had this big idea of asymptotics and forgetting about constants, just looking at the lead term. And so, today, we're going to develop asymptotic notation so that we know that mathematically. And we also ended up with a recurrence with mergesort, the running time of mergesort, so we need to see how to solve recurrences. And we will do those two things today. Question? Yes, I will speak louder. Thanks. Good..."//
5782 en Lecture 3: Divide-and-Conquer: Strassen, Fibonacci, Polynomial Multiplication //"Good morning everyone. Today we are going to do some algorithms, back to algorithms, and we are going to use a lot of the, well, some of the simpler mathematics that we developed last class like the master theorem for solving recurrences. We are going to use this a lot. Because we are going to talk about recursive algorithms today. And so we will find their running time using the master theorem. This is just the same as it was last time, I hope, unless I made a mistake. A couple of reminders. You should all go to recitation on Friday. That is required. If you want to, you can go to homework lab on Sunday. That may be a good excuse for you to actually work on your problem set a few hours early...//
5783 en Lecture 4: Quicksort, Randomized Algorithms //"OK. Today we are going to talk about a very interesting algorithm called Quicksort -- -- which was invented by Tony Hoare in 1962. And it has ended up being a really interesting algorithm from many points of view. And because of that, it turns out today's lecture is going to be both hard and fast. If you see the person next to you sleeping, you will want to say let's get going. It's a divide-and-conquer algorithm..."//
5784 en Lecture 5: Linear-time Sorting: Lower Bounds, Counting Sort, Radix Sort //"Today we're going to talk about sorting, which may not come as such a big surprise. We talked about sorting for a while, but we're going to talk about it at a somewhat higher level and question some of the assumptions that we've been making so far. And we're going to ask the question how fast can we sort? A pretty natural question. You may think you know the answer. Perhaps you do. Any suggestions on what the answer to this question might be? There are several possible answers. Many of them are partially correct. Let's hear any kinds of answers you'd like and start waking up this fresh morning. Sorry?...//
5785 en Lecture 6: Order Statistics, Median //"Today we're going to not talk about sorting. This is an exciting new development. We're going to talk about another problem, a related problem, but a different problem. We're going to talk about another problem that we would like to solve in linear time. Last class we talked about we could do sorting in linear time. To do that we needed some additional assumptions. Today we're going to look at a problem that really only needs linear time, even though at first glance it might look like it requires sorting. So this is going to be an easier problem. The problem is I give you a bunch of numbers..."//
5786 en Lecture 7: Hashing, Hash Functions //"Today starts a two-lecture sequence on the topic of hashing, which is a really great technique that shows up in a lot of places. So we're going to introduce it through a problem that comes up often in compilers called the symbol table problem. And the idea is that we have a table S holding n records where each record, just to be a little more explicit here. So each record typically has a bunch of, this is record x. x is usually a pointer to the actual data. So when we talk about the record x, what it usually means some pointer to the data. And in the data, in the record, so this is a record, there is a key called a key of x..."//
5787 en Lecture 8: Universal Hashing, Perfect Hashing //"Hashing. Today we're going to do some amazing stuff with hashing. And, really, this is such neat stuff, it's amazing. We're going to start by addressing a fundamental weakness of hashing. And that is that for any choice of hash function There exists a bad set of keys that all hash to the same slot. OK. So you pick a hash function. We looked at some that seem to work well in practice, that are easy to put into your code. But whichever one you pick, there's always some bad set of keys. So you can imagine, just to drive this point home a little bit..."//
5788 en Lecture 9: Relation of BSTs to Quicksort, Analysis of Random BST //"So, we're going to talk today about binary search trees. It's something called randomly built binary search trees. And, I'll abbreviate binary search trees as BST's throughout the lecture. And, you of all seen binary search trees in one place or another, in particular, recitation on Friday. So, we're going to build up the basic ideas presented there, and talk about how to randomize them, and make them good. So, you know that there are good binary search trees, which are relatively balanced, something like this. The height is log n..."//
5789 en Lecture 10: Red-black Trees, Rotations, Insertions, Deletions //"Good morning. It looks like 9:30 is getting earlier and earlier for everyone. Hello to all the people watching at home. I think there should be a requirement that if you're watching the video, you can only watch it 9:30-11:00 on Sunday, or at least start watching then just so you can all feel our mornings. Today, we're going to talk about balanced search trees. Now, we've hinted at this for a while. Our goal today is to get a search tree data structure, so we can insert, delete, and search all at log n time for operations. So, we want a tree that's guaranteed to be log n in height. So, that's a balanced search tree data structure..."//
5790 en Lecture 11: Augmenting Data Structures, Dynamic Order Statistics, Interval Trees //"Good morning. Today we're going to talk about augmenting data structures. And this is a -- Normally, rather than designing data structures from scratch, you tend to take existing data structures and build your functionality into them. And that is a process we call data-structure augmentation. And this also today marks sort of the start of the design phase of the class. We spent a lot of time doing analysis up to this point. And now we're still going to learn some new analytical techniques..."//
5791 en Lecture 12: Skip Lists //"Good morning. Today we're going to talk about it a balanced search structure, so a data structure that maintains a dynamic set subject to insertion, deletion, and search called skip lists. So, I'll call this a dynamic search structure because it's a data structure. It supports search, and it's dynamic, meaning insert and delete. So, what other dynamic search structures do we know, just for sake of comparison, and to wake everyone up? Shut them out, efficient, I should say, also good, logarithmic time per operation. So, this is a really easy question to get us off the ground..."//
5792 en Lecture 13: Amortized Algorithms, Table Doubling, Potential Method //"OK, good morning. So today we are going to, as I mentioned last week, we've started the part of the course where we are doing more things having to do with design than purely analysis. Today, we're actually going to do analysis, but it's the type of analysis that leads to really interesting design issues. And we're going to follow it up on Wednesday with an application of the methods we're going to learn today with a really interesting and practical problem. So we're talking today about amortized analysis..."//
5793 en Lecture 14: Competitive Analysis: Self-organizing //"And this is going to use some of the techniques we learned last time with respect to amortized analysis. And, what's neat about what we're going to talk about today is it's a way of comparing algorithms that are so-called online algorithms. And we're going to introduce this notion with a problem which is called self organizing lists. OK, and so the set up for this problem is that we have a list, L, of n elements. And, we have an operation. Woops, I've got to spell things right, access of x, which accesses item x in the list..."//
5794 en Lecture 15: Dynamic Programming, Longest Common Subsequence //"So, the topic today is dynamic programming. The term programming in the name of this term doesn't refer to computer programming. OK, programming is an old word that means any tabular method for accomplishing something. So, you'll hear about linear programming and dynamic programming. Either of those, even though we now incorporate those algorithms in computer programs, originally computer programming, you were given a datasheet and you put one line per line of code as a tabular method for giving the machine instructions as to what to do..."//
5795 en Lecture 16: Greedy Algorithms, Minimum Spanning Trees //"OK, today we're going to start talking about a particular class of algorithms called greedy algorithms. But we're going to do it in the context of graphs. So, I want to review a little bit about graphs, which mostly you can find in the textbook in appendix B. And so, if you haven't reviewed in appendix B recently, please sit down and review appendix B. It will pay off especially during our take-home quiz. So, just reminder, a digraph, what's a digraph? What's that short for? Directed graph, OK? Directed graph, G equals (V,E), OK, has a set, V, of vertices...//
5796 en Lecture 17: Shortest Paths I: Properties, Dijkstra's Algorithm, Breadth-first Search //"We're going to talk about shortest paths, and we're going to talk about shortest paths for three lectures. So, this is a trilogy. Today will be Shortest Paths One. I've been watching far too many versions of Star Wars this weekend. I saw the musical yesterday, matinee. That was an MIT musical. That was fun, of all three movies in about four hours. That was a bit long and then I saw the one-man show on Friday. One-man Star Wars: the original three movies in one hour. That was the opposite of too long. Both were fun. So I get my trilogy fix. All episodes, first we're going to start with The New Hope, and we're going to talk about the shortest paths problem and solve one particular problem of it, a very interesting version...//
5797 en Lecture 18: Shortest Paths II: Bellman-Ford, Linear Programming, Difference Constraints //"Good morning, everyone. Glad you are all here bright and early. I'm counting the days till the TA's outnumber the students. They'll show up. We return to a familiar story. This is part two, the Empire Strikes Back. So last time, our adversary, the graph, came to us with a problem. We have a source, and we had a directed graph, and we had weights on the edges, and they were all nonnegative. And there was happiness. And we triumphed over the Empire by designing Dijkstra's algorithm, and very efficiently finding single source shortest paths, shortest path weight from s to every other vertex...//
5798 en Lecture 19: Shortest Paths III: All-pairs Shortest Paths, Matrix Multiplication, Floyd-Warshall, Johnson //"-- shortest paths. This is the finale. Hopefully it was worth waiting for. Remind you there's a quiz coming up soon, you should be studying for it. There's no problem set due at the same time as the quiz because you should be studying now. It's a take-home exam. It's required that you come to class on Monday. Of course, you'll all come, but everyone watching at home should also come next Monday to get the quiz. It's the required lecture. So, we need a bit of a recap in the trilogy so far. So, the last two lectures, the last two episodes, or about single source shortest paths...//
5799 en Lecture 22: Advanced Topics //"We only have four more lectures left, and what Professor Demaine and I have decided to do is give two series of lectures on sort of advanced topics. So, today at Wednesday we're going to talk about parallel algorithms, algorithms where you have more than one processor whacking away on your problem. And this is a very hot topic right now because all of the chip manufacturers are now producing so-called multicore processors where you have more than one processor per chip. So, knowing something about that is good. The second topic we're going to cover is going to be caching, and how you design algorithms for systems with cache...//
5800 en Lecture 23: Advanced Topics (cont.) //"OK, good morning. So today, we're going to continue our exploration of multithreaded algorithms. Last time we talked about some aspects of scheduling, and a little bit about linguistics to describe a multithreaded competition. And today, we're going to actually deal with some algorithms. So, we're going to start out with a really simple, actually, what's fun about this, actually, is that everything I'm going to teach you today I could have taught you in week two, OK, because basically it's just taking the divide and conquer hammer, and just smashing problem after problem with it...//
5801 en Lecture 24: Advanced Topics (cont.) //"-- week of 6.046. Woohoo! The topic of this final week, among our advanced topics, is cache oblivious algorithms. This is a particularly fun area, one dear to my heart because I've done a lot of research in this area. This is an area co-founded by Professor Leiserson. So, in fact, the first context in which I met Professor Leiserson was him giving a talk about cache oblivious algorithms at WADS '99 in Vancouver I think. Yeah, that has to be an odd year. So, I learned about cache oblivious algorithms then, started working in the area, and it's been a fun place to play. But this topic in some sense was also developed in the context of this class. I think there was one semester, probably also '98-'99 where all of the problem sets were about cache oblivious algorithms...//
5802 en Lecture 25: Advanced Topics (cont.), Discussion of Follow-on Classes //"The last lecture of 6.046. We are here today to talk more about cache oblivious algorithms. Last class, we saw several cache oblivious algorithms, although none of them quite too difficult. Today we will see two difficult cache oblivious algorithms, a little bit more advanced. I figure we should do something advanced for the last class just to get to some exciting climax. So without further ado, let's get started. Last time, we looked at the binary search problem...//
5803 en Lecture 4: Naming 
5804 en Lecture 5: Fault Isolation with Clients and Servers 
5805 en Lecture 6: Virtualization, Virtual Memory 
5806 en Lecture 7: Virtual Processors: Threads and Coordination 
5807 en Lecture 8: Performance 
5808 en Lecture 9: Introduction to Networks 
5809 en Lecture 10: Layering and Link Layer 
5810 en Lecture 11: Network Layer, Routing 
5811 en Lecture 12: End-to-end Layer 
5812 en Lecture 13: Congestion Control 
5813 en Lecture 14: Distributed Naming 
5814 en Lecture 15: Reliability 
5815 en Lecture 16: Atomicity Concepts 
5816 en Lecture 17: Recoverability 
5817 en Lecture 18: Isolation 
5818 en Lecture 19: Transactions and Consistency 
5819 en Lecture 20: Multi-site Atomicity 
5820 en Lecture 21: Security Introduction 
5821 en Lecture 22: Authentication 
5822 en Lecture 23: Authorization and Confidentiality 
5823 en Lecture 24: Advanced Authentication 
5824 en Lecture 25: Complex, Trusted Systems 
5825 en Lecture 1: Positive definite matrices K = A'CA 
5826 en Lecture 2: One-dimensional applications: A = difference matrix 
5827 en Lecture 3: Network applications: A = incidence matrix 
5828 en Lecture 4: Applications to linear estimation: least squares 
5829 en Lecture 5: Applications to dynamics: eigenvalues of K, solution of Mu'' + Ku = F(t) 
5830 en Lecture 6: Underlying theory: applied linear algebra 
5831 en Lecture 7: Discrete vs. continuous: differences and derivatives 
5832 en Lecture 8: Applications to boundary value problems: Laplace equation 
5833 en Lecture 9: Solutions of Laplace equation: complex variables 
5834 en Lecture 10: Delta function and Green's function 
5835 en Lecture 11: Initial value problems: wave equation and heat equation 
5836 en Lecture 12: Solutions of initial value problems: eigenfunctions 
5837 en Lecture 13: Numerical linear algebra: orthogonalization and A = QR 
5838 en Lecture 14: Numerical linear algebra: SVD and applications 
5839 en Lecture 15: Numerical methods in estimation: recursive least squares and covariance matrix 
5840 en Lecture 16: Dynamic estimation: Kalman filter and square root filter 
5841 en Lecture 17: Finite difference methods: equilibrium problems 
5842 en Lecture 18: Finite difference methods: stability and convergence 
5843 en Lecture 19: Optimization and minimum principles: Euler equation 
5844 en Lecture 20: Finite element method: equilibrium equations 
5845 en Lecture 21: Spectral method: dynamic equations 
5846 en Lecture 22: Fourier expansions and convolution 
5847 en Lecture 23: Fast fourier transform and circulant matrices 
5848 en Lecture 24: Discrete filters: lowpass and highpass 
5849 en Lecture 25: Filters in the time and frequency domain 
5850 en Lecture 26: Filter banks and perfect reconstruction 
5851 en Lecture 27: Multiresolution, wavelet transform and scaling function 
5852 en Lecture 28: Splines and orthogonal wavelets: Daubechies construction 
5853 en Lecture 29: Applications in signal and image processing: compression 
5854 en Lecture 30: Network flows and combinatorics: max flow = min cut 
5855 en Lecture 31: Simplex method in linear programming 
5856 en Lecture 32: Nonlinear optimization: algorithms and theory 
5884 en Why Only an Atheist Can Believe: Politics Between Fear and Trembling ?i?ek addresses the complicated relationship between belief, or what we take to be belief, and our desire to see all. The lecture is followed by a brief period of questions and answers.
5885 en The Spectator's Malevolent Neutrality Slavoj ?i?ek is philosopher and Psychoanalyst from Ljubljana. His lecture on the specific roles of viewers and doers is entitled "The Spectator´s Malevolent Neutrality" and was held on June 8, 2004 during the Theaterformen festival in Brunswick (DE).
5886 en Lecture 2: Elimination with Matrices 
5887 en Lecture 3: Multiplication and Inverse Matrices 
5888 en Lecture 4: Factorization into A = LU 
5889 en Lecture 5: Transposes, Permutations, Spaces R^n 
5890 en Lecture 6: Column Space and Nullspace 
5891 en Lecture 7: Solving Ax = 0: Pivot Variables, Special Solutions 
5892 en Lecture 8: Solving Ax = b: Row Reduced Form R 
5893 en Lecture 9: Independence, Basis, and Dimension 
5894 en Lecture 10: The Four Fundamental Subspaces 
5895 en Lecture 11: Matrix Spaces; Rank 1; Small World Graphs 
5896 en Lecture 12: Graphs, Networks, Incidence Matrices 
5897 en Lecture 13: Quiz 1 Review 
5898 en Lecture 14: Orthogonal Vectors and Subspaces 
5899 en Lecture 15: Projections onto Subspaces 
5900 en Lecture 16: Projection Matrices and Least Squares 
5901 en Lecture 17: Orthogonal Matrices and Gram-Schmidt 
5902 en Lecture 18: Properties of Determinants 
5903 en Lecture 19: Determinant Formulas and Cofactors 
5904 en Lecture 20: Cramer's Rule, Inverse Matrix, and Volume 
5905 en Lecture 21: Eigenvalues and Eigenvectors 
5906 en Lecture 22: Diagonalization and Powers of A 
5907 en Lecture 23: Differential Equations and exp(At) 
5908 en Lecture 24: Markov Matrices; Fourier Series 
5909 en Lecture 24b: Quiz 2 Review 
5910 en Lecture 25: Symmetric Matrices and Positive Definiteness 
5911 en Lecture 26: Complex Matrices; Fast Fourier Transform 
5912 en Lecture 27: Positive Definite Matrices and Minima 
5913 en Lecture 28: Similar Matrices and Jordan Form 
5914 en Lecture 29: Singular Value Decomposition 
5915 en Lecture 30: Linear Transformations and Their Matrices 
5916 en Lecture 31: Change of Basis; Image Compression 
5917 en Lecture 32: Quiz 3 Review 
5918 en Lecture 33: Left and Right Inverses; Pseudoinverse 
5919 en Lecture 34: Final Course Review 
5920 en Politeness and Civility in the Function of Contemporary Ideology "Maybe we just need a different chicken..." Slavoj ?i?ek spoke at Powell's City of Books in downtown Portland, Oregon, on September 9, 2008
5925 en Agents and avatars: A new era of computational social science 
5926 en Emergence of cooperation by social interactions in space and time 
5927 en Adaptive networks - The intriguing interplay of dynamics ON and OF networks 
5928 en Correlations in complex systems 
5929 en The costs of interaction and how they shape the social network 
5930 en Econophysics and economic complexity What Is Econophysics (Via Wikipedia)nnEconophysics is an interdisciplinary research field, applying theories and methods originally developed by physicists in order to solve problems in economics, usually those including uncertainty or stochastic processes and nonlinear dynamics. Its application to the study of financial markets has also been termed statistical finance referring to its roots in statistical physics. Physicists’ interest in the social sciences is not new, Daniel Bernoulli, as an example, was the originator of utility-based preferences. One of the founders of neoclassical economic theory, former Yale University Professor of Economics Irving Fisher, was originally trained under the renowned Yale physicist, Josiah Willard Gibbs.
5931 en Five Principles for the Unification of the Behavioral Sciences 
5932 en New complex networks for social systems 
5933 en Taking the avatar approximation: throughput social-science in virtual worlds 
5934 en Agent-based simulation of emergence through monetary incentives and social pressure 
5935 en The saturation threshold of public opinion: Are agressive media campaigns always effective? 
5936 en Adressing the validation problem for social simulations 
5937 en Illusory and genuine control in optimizing games and financial markets 
5938 en Discussion on Modeling and Simulation Challenges 
5939 en Alien rule and its discontents 
5940 en The dangers of time-aggregation: A little knowledge is a dangerous thing: and sampling in social systems 
5941 en t.b.a. 
5942 en Group formation: Fragmentation transitions in network coevolution dynamics 
5943 en Human dynamics revealed through Web analytics 
5944 en Injecting Data into Simulation: Can Agent-Based Modelling Learn from Microsimulation? 
5945 en Data driven vs. hypothesis-driven research in the social sciences 
5946 en Measuring science: Fears, challenges and opportunities 
5947 en Models and tools for the analysis of human behaviour in third generation metropolis 
5948 en Observing society through tags: Using tags to help society 
5949 en Nationalism and its geopolitical consequences: A distributional perspective 
5950 en Discussion on Practical Challenges 
5951 en Patterns of daily movement 
5952 en The emotional content of large-scale texts: The happiness of bloggers, song lyrics, and presidents 
5953 en Modeling reality without sacrificing data: Inferentially tractable models for complex social systems 
5954 en How should the 21st century democracy be organised? Studying online communities to answer it 
5955 en Dynamics of biological systems 
5956 en "What on earth must we assume..." ...to explain increasing opinion differences between subgroups without assuming negative influence? 
5957 en Signalling models and experiments. A research perspective 
5958 en Discussion on challenges of Interdisciplinary research 
5963 en Symmetry in Graphs 
5965 en Transitive group actions on graphs 
5966 en Graph Embeddings and Symmetries 
5967 en Maps 
5968 en Representations of Graphs and their Symmetries 
5969 en Operations on Maps 
5971 en Lecture 1: Introduction **1. What is Film?**nn• Chemistry, Novelty, Manufactured object, Social formationnn**2. Think Away iPods**nn• The novelty of movement, Early films and early audiencesnn**3. The Fred Ott Principle**nn**4. Three Phases of Media Evolution**nn• Imitation, Technical Advance, Maturitynn**5. "And there was Charlie" - Film as a cultural form**nnRequired films:nnPorter, Edwin. S. [[http://memory.loc.gov/cgi-bin/query/r?ammem/papr:@filreq(@field(NUMBER+@band(edmp+2443s3))+@field(COLLID+edison))|The Great Train Robbery]] (view film)nnGriffith, D. W. [[http://www.tcf.ua.edu/classes/Jbutler/T112/Lonedale/index1.htm|The Lonedale Operator]] (view stills)nn<a href="http://www.surveymonkey.com/s/videolectures">Click here to take survey</a>
5972 en Lecture 2: Keaton **1. The Fred Ott Principle, continued**nn• The myth of technological determinism, A paradox: capitalism and the moviesnn**2. The Great Train Robbery (1903)**nn**3. The Lonedale Operator (1911)**nn**4. Buster Keaton**nn• Acrobat / actor, Technician / director, Metaphysician / artistnn**5. The multiplicity principle: entertainment vs. art**nn**6. The General (1927)**nn• "A culminating text", Structure, The Keaton hero: steadfast, muddlingnnRequired films:nnGriffith , D.W. [[http://www.youtube.com/watch?v=YMqGbEEP528&feature=related|A Beast at Bay]] (view film)nnKeaton, Buster. [[http://www.archive.org/details/OneWeek|One Week]] (view film)nnKeaton, Buster. [[http://www.archive.org/details/Cops|Cops]] (view film)nnKeaton, Buster. [[http://www.archive.org/details/TheGeneral|The General]] (view film)
5973 en Lecture 3: Chaplin **1. Movies before Chaplin**nn**2. Enter Chaplin**nn**3. Chaplin's career**nn• The multiplicity principle, continuednn**4. The Tramp as myth**nn**5. Chaplin's world - elemental themes**nnRequired films:nnChaplin, Charlie. [[http://www.archive.org/details/CC_1917_06_17_TheImmigrant |The Immigrant]] (view film)
5974 en Lecture 20: De Sica, Bicycle Thieves **1. Vittorio De Sica (1902-74)**nn• 1942 The Children Are Watching Usnn• 1946 Shoeshinenn• 1948 Bicycle Thievesnn• 1950 Miracle in Milannn• 1952 Umberto Dnn• 1960 Two Womennn• 1971 The Garden of the Finzi-Continisnn**2. Bicycle Thieves**nn• Structure: organic formnn• Social themesnn• Character: father and sonnn• The titlennRequired films:nnDe Sica, Vittorio. [[http://www.imdb.com/title/tt0040522/|Bicycle Thieves]]
5975 en Final Class Presentation For IAP 2007, the class design projects were focused on two student teams:nnVDS Vehicle Design SummitnnMITSET Space Elevator TeamnnThe two teams were assigned to:nn  * Define/pick the current baseline configuration.n  * Create a performance model of the baseline configuration.n  * Optimize for these factors:n     * VDS: miles-per-gallon [mpg]n     * MITSET: time-to-climb [sec]n  * Pick 4-5 most critical components and subsystems based on performance sensitivity.nnThe work of the students is presented below, along with video of their final class presentations. The deliverables are based upon the assignments from the class. All work is courtesy of the students named and used with permission.
5976 en Day 1: Introductory Lecture * 9:30-11:00: Morning Introduction and Shop Safety Rulesn * 11:00-11:30: Primer on Coursenno Define Goals of Classn *To give Students Experience in an Early Boat Design Technology, from Carving a Half-model by Eye, and then Translating that Shape into a Lines Plan which can be Analyzed, and then Developed into Building Plann*To give Students Experience in Understanding how Early Design Techniques Worked to Refine design by Intuition and "Boat Sense"n*Half Model a Readily Recognized and Interpretable Representation of the Performance of the Boatn*Beyond Simple Esthetic.n*A Trained Eye, Experience Sailor/Designer/Builder Could Look at Form and Consider it in a Dynamic State, Using Mind as Computational Tool to Understand How Features of the form Assimilaten* Process of Developing Lines More Opportunity to Refine the Shapen*Process of Lofting Yet Another Opportunity to Refine Formn*Essential that the Modeller Always Remain Skeptical and Critical of the Shape Their Creatingn*To Give Students an Awareness of How Design Methods Can Limit and Enable Developmentn*Development of Half Model Designing Techniques, Replacing Whole Molding and Rule of Thumb Design, Opened up Hull Design Development Once Adoptedn*System of Lifting Lines, Creating Lines Plan, and Lofting Allowed for Scientific Analysis of Hull, as well as Allowing Designer to Exert Great Influence on the Final Productn*But the System has Many Shortcomings: Time-consuming, Bad for Asymmetic Hulls, etc.n*What are Advantages and Shortcomings of CAD?nno Define Process of Class: Design your own Boat by Carving Half-model, Create Lines, Analyze Linesn*Goal is to Use the Half-model Method to Explore an Idea of a Designn*Show How the Half-model Design Process is Iterative, and How Ideas are Refined Each Step of Process - How  Thoughtfulness and Awareness of Process at Every Step Influences Outcomen*Grade Based on How Well the Students Use the Method to Challenge their Own Assumptions and Develop their Ideasn*Requirements are Attendance and Participation. Morning Lectures Every Day at 10:00 AM, Presence Requirednn*Morning Lecturenno Brief Overview of History of Boat Design Techniquesnno Give Basics on How the Form of a Half-model is Translated into Something that Can be Built From.n*My Jign*Herreshoff Methodsn*Mawing Model, Separating Liftsnno Begin Process, Going Around the Room and Extemporizing, of Developing Ideas for Design. Encourage Students to Start with, then Challenge, their Own Assumptionsn* Type of Boatn*Use of Boatn*Power of Boatn*Develop Idea of Essention Lines to Start Withn*Explain Usefulness of Different Sheer Shapesn*Bow Linesn*Stern Shapesn*Basically Do What we Can to Prime the Class to Make the Most of the Visit to the Model Roomnn*1:00-1:30: Lunch-advise People to Bring Lunchn*1:30-3:30: Getting Started on Designsnno Go to Half Model Roomn* Once Back, Begin Developing Essential Lines, Drawn onto the Blocks of Wood
5977 en Day 1: MIT Museum Nautical Collections and an Introductory Demonstration 
5978 en Day 2: Model Building *Morning Lecturen     *n      Demonstrate Carving Techniques and Tools.n     *n      Influence of Tools and Method on Form, Limitations of Ideasn        **n         Familiarity with Toolsn        **n         Understanding Intuitively How Tools will Help Create a Readily-built Formn     *n      Understanding Intuitively How the Resistance of the Wood Influences Shape
5979 en Day 4: Finishing Up and Drawing Up Plans *n   Morning Lecture:n     **n      Introduction to Lines Drawingn     **n      Techniques, Layout, Method, etc.n     **n      Opportunity to Re-think Designn  *n   Afternoon:n     **n      Pulling Shapes Off Modelsn     **n      Expanding Other Views on Papern     **n      Interpreting as You Go
5980 en Day 5: Conclusion, and More of the MIT Museum's Nautical Collections *n   Finish Lines Plansn  *n   Analyze One or Twon  *n   Discussion on How Method Informs Idean  *n   Discussion of Difference to Computer Draftingn  *n   Discussion in Model Roomn  *n   Discussion in Plans Collectionn  *n   Shop Cleanup
5981 en Lecture 3: MIT Center for Advanced Visual Studies Lecture Series Lost Highway Expedition will begin in Ljubljana, and travel through Zagreb, Novi Sad, Belgrade, Skopje, Pristina, Tirana, Podgorica to conclude in Sarajevo, comprised of two days of events at each city and one day of travel in between. The events may include guided tours, presentations and forums by local experts, workshops between the participating travelers and local participants, discussions, exhibitions, radio shows, picnics and other events that can be self-produced by the host cities. nnMembers of the Lost Highway Expedition do not have to travel or stay together and can enter and exit the expedition for any length of time and at any point. Participants are to self organize, support and realize their journey in the “Highway of Post Brotherhood and Non Unity.”nnEveryone is free to participate in any of the events or to bring their own to the highway. Word of the expedition will travel like a rumor from friends to friends, colleagues to colleagues.
5982 en Lecture 4: MIT Center for Advanced Visual Studies Lecture Series Artist, director, and performer John Malpede will talk with artist Harrell Fletcher about three decades of Malpede's aesthetically and politically uncompromising brand of art as social action.
5985 en Chapter 1: Maxwell's integral laws in free space 1.0 Introductionnn  * Overview of subjectnn1.1 The Lorentz law in free spacen1.2 Charge and current densitiesn1.3 Gauss' integral law of electric field densitynn  * Singular charge distributionsn  * Gauss' continuity conditionnn1.4 Ampere's integral lawnn  * Singular current distributionn  * Ampere's continuity conditionnn1.5 Charge conservation in integral formnn  * Charge conservation continuity conditionnn1.6 Faraday's integral lawnn  * Electric field intensity having no circulationn  * Electric field intensity with circulationn  * Faraday's continuity conditionnn1.7 Gauss' integral law of magnetic fluxnn  * Magnetic flux continuity conditionnn1.8 Summary
5986 en Chapter 4: Electroquasistatic fields: the superposition integral point of view 4.0 Introductionn4.1 Irrotational field represented by scalar potential: the gradient operator and the gradient integral theoremnn  * Visualization of two-dimensional irrotational fieldsnn4.2 Poisson's equationn4.3 Superposition principlen4.4 Fields associated with charge singularitiesnn  * Dipole at the originn  * Pair of charges at infinity having equal magnitude and opposite signn  * Other charge singularitiesnn4.5 Solution of Poisson's equation for specified charge distributionsnn  * Superposition integral for surface charge densityn  * Superposition integral for line charge densityn  * Two-dimensional charge and field distributionsn  * Potential of uniform dipole layer.nn4.6 Electroquasistatic fields in the presence of perfect conductorsnn  * Capacitancenn4.7 Method of imagesnn4.8 Charge simulation approach to boundary value problemsnn4.9 Summary
5987 en Chapter 5: Electroquasistatic fields from the boundary value point of view 5.0 Introductionnn5.1 Particular and homogeneous solutions to Poisson's and Laplace's equationsnn  * Superposition to satisfy boundary conditionsn  * Capacitance matrixnn5.2 Uniqueness of solutions of Poisson's equationnn5.3 Continuity conditionsnn5.4 Solutions to Laplace's equation in Cartesian coordinatesnn5.5 Modal expansions to satisfy boundary conditionsnn5.6 Solutions to Poisson's equation with boundary conditionsnn5.7 Solutions to Laplace's equation in polar coordinatesnn5.8 Examples in polar coordinatesnn  * Simple solutionsn  * Azimuthal modesn  * Radial modesnn5.9 Three solutions to Laplace's equation in spherical coordinatesnn5.10 Three-dimensional solutions to Laplace's equationnn  * Cartesian coordinate product solutionsn  * Modal expansion in Cartesian coordinatesn  * Modal expansion in other coordinatesnn5.11 Summary
5988 en Chapter 6: Polarization 6.0 Introductionnn6.1 Polarization densitynn6.2 Laws and continuity conditions with polarizationnn  * Polarization current density and Ampere's lawn  * Displacement flux densitynn6.3 Permanent polarizationnn6.4 Polarization constitutive lawsnn6.5 Fields in the presence of electrically linear dielectricsnn  * Capacitancen  * Induced polarization chargenn6.6 Piece-wise uniform electrically linear dielectricsnn  * Uniform dielectricsn  * Piece-wise uniform dielectricsnn6.7 Smoothly inhomogeneous electrically linear dielectricsnn6.8 Summary
5989 en Chapter 7: Conduction and electroquasistatic charge relaxation 7.0 Introductionnn7.1 Conduction constitutive lawsnn  * Ohmic conductionn  * Unipolar conductionnn7.2 Steady Ohmic conductionnn  * Continuity conditionsn  * Conductancen  * Qualitative view of fields in conductorsnn7.3 Distributed current sources and associated fieldsnn  * Distributed current source singularitiesn  * Fields associated with current source singularitiesn  * Method of imagesnn7.4 Superposition and uniqueness of steady conduction solutionsnn  * Superposition to satisfy boundary conditionsn  * The conductance matrixn  * Uniquenessnn7.5 Steady currents in piece-wise uniform conductorsnn  * Analogy to fields in linear dielectricsn  * Inside-outside approximationsnn7.6 Conduction analogsnn  * Mapping fields that satisfy Laplace's equationnn7.7 Charge relaxation in uniform conductorsnn  * Net charge on bodies immersed in uniform materialsnn7.8 Electroquasistatic conduction laws for inhomogeneous materialnn  * Evolution of unpaired charge densityn  * Electroquasistatic potential distributionn  * Uniquenessnn7.9 Charge relaxation in uniform and piece-wise uniform systemsnn  * Fields in regions having uniform propertiesn  * Continuity conditions in piece-wise uniform systemsn  * Nonuniform fields in piece-wise uniform systemsnn7.10 Summary
5990 en Chapter 8: Magnetoquasistatic fields: superposition integral and boundary value points of view 8.0 Introductionnn  * Vector field uniquely specifiednn8.1 The vector potential and the vector Poisson equationnn  * Two-dimensional current and vector potential distributionsnn8.2 The Biot-Savart superposition integralnn  * Stick model for computing fields of electromagnetnn8.3 The scalar magnetic potentialnn  * The scalar potential of a current loopnn8.4 Magnetoquasistatic fields in the presence of perfect conductorsnn  * Boundary conditions and evaluation of induced surface current densityn  * Voltage at the terminals of a perfectly conducting coiln  * Inductancenn8.5 Piece-wise magnetic fieldsn8.6 Vector potential and the boundary value point of viewnn  * Vector potential for two-dimensional fieldsn  * Vector potential for axisymmetric fields in spherical coordinatesn  * Boundary value solution by "Inspection"n  * Method of imagesn  * Two-dimensional boundary value problemsnn8.7 Summary
5991 en Chapter 9: Magnetization 9.0 Introductionnn9.1 Magnetization densitynn9.2 Laws and continuity conditions with magnetizationnn  * Faraday's law including magnetizationn  * Magnetic flux densityn  * Terminal voltage with magnetizationnn9.3 Permanent magnetizationnn9.4 Magnetization constitutive lawsnn9.5 Fields in the presence of magnetically linear insulating materialsnn  * Inductance in the presence of linearly magnetizable materialsn  * Induced magnetic charge: demagnetizationnn9.6 Fields in piece-wise uniform magnetically linear materialsnn  * Excitation in region of high permeabilityn  * Excitation in region of low permeabilitynn9.7 Magnetic circuitsnn  * Electrical terminal relations and characteristicsnn9.8 Summary
5992 en Chapter 10: Magnetoquasistatic relaxation and diffusion 10.0 Introductionnn10.1 Magnetoquasistatic electric fields in systems of perfect conductorsnn10.2 Nature of fields induced in finite conductorsnn10.3 Diffusion of axial magnetic fields through thin conductorsnn10.4 Diffusion of transverse magnetic fields through thin conductorsnn  * Response to a step in applied fieldnn10.5 Magnetic diffusion lawsnn  * Physical interpretationnn10.6 Magnetic diffusion transient responsenn  * Product solutions to the one-dimensional diffusion equationnn10.7 Skin effectnn10.8 Summar
5993 en Chapter 11: Energy, power flow, and forces 11.0 Introductionnn  * Power flow in a circuitn  * Overviewnn11.1 Integral and differential conservation statementsn11.2 Poynting's theoremnn  * Systems composed of perfect conductors and free spacenn11.3 Ohmic conductors with linear polarization and magnetizationnn  * An alternative conservation theorem for electroquasistatic systemsn  * Poynting power density related to circuit power inputn  * Poynting flux and electromagnetic radiationnn11.4 Energy storagenn  * Energy densitiesn  * Energy storage in terms of terminal variablesnn11.5 Electromagnetic dissipationnn  * Energy conservation for temporarily periodic systemsn  * Induction heatingn  * Dielectric heatingn  * Hysteresis lossesnn11.6 Electrical forces on macroscopic median11.7 Macroscopic magnetic forcesnn  * Reciprocity conditionsn  * Finding the coenergyn  * Evaluation of the forcen  * The torque of electrical originnn11.8 Forces on macroscopic electric and magnetic dipolesnn  * Force on an electric dipolen  * Force on electric charge derived from energy principlen  * Force on a magnetic charge and magnetic dipolen  * Comparison of Coulomb's force to the force on a magnetic dipolenn11.9 Macroscopic force densitiesnn  * The Lorentz force densityn  * The Kelvin polarization force densityn  * The Kelvin magnetization force densityn  * Alternative force densitiesnn11.10 Summary
5994 en Chapter 13: Electrodynamic fields: the boundary value point of view 13.0 Introductionnn13.1 Introduction to transverse electromagnetic (TEM) wavesnn  * The magnetoquasistatic (MQS) limitnnThe MQS approximationnn  * The electroquasistatic (EQS) limitn  * The EQS approximationnn13.2 Two-dimensional modes between parallel-platesnn13.3 Transverse (TE) and transverse magnetic (TM) standing waves between parallel platesnn13.4 Rectangular waveguide modesnn13.5 Dielectric waveguides: optical fibersnn13.6 Summar
6015 en SMEs Production Networks & Open Sourced Software 
6016 en Open Source Enterprise Resource Planning and Order Management System for Eastern European Tool and Die Making Workshops 
6017 en Software Tutorial - ToolEast Training Environment 
6018 en ToolEast portal presentation 
6023 en Lecture 2: MIT Center for Advanced Visual Studies Lecture Series Simon Starling is fascinated by the processes involved in transforming one object or substance into another. He makes objects, installations, and pilgrimage-like journeys which draw out an array of ideas About nature, technology and economics. Starling describes his work as ‘the physical manifestation of a thought process’, revealing hidden histories and relationships.nnFor Tabernas Desert Run 2004, Starling crossed the Tabernas desert in Spain on an improvised electric bicycle. The only waste product the vehicle produced was water, which he used to paint an illustration of a cactus. The contrast between the supremely efficient cactus and the contrived efforts of man is both comic and insightful, highlighting the commercial exploitation of natural resources in the region.nnShedboatshed (Mobile Architecture No 2) 2005 has a similar circularity. Starling dismantled a shed and turned it into a boat; loaded with the remains of the shed, the boat was paddled down the Rhine to a museum in Basel, dismantled and re-made into a shed.Both pilgrimages, provide a kind of buttress against the pressures of modernity, mass production and global capitalism.nnStarling’s new work One Ton, II 2005 focuses attention on energy consumption: the huge amounts of energy used to produce tiny quantities of platinum. One ton of ore, mined from the South African open cast mine pictured in the images, was needed to produce the five handmade platinum prints exhibited here.nnSimon Starling has been nominated for his solo exhibitions at The Modern Institute, Glasgow, and the Fundació Joan Miró, Barcelona.
6055 en Opening Remarks ICGI-2008 is the ninth in a series of successful biennial international conferences in the area of grammatical inference.n Grammatical inference has been extensively addressed by researchers in information theory, automata theory, language acquisition, computational linguistics, machine learning, pattern recognition, computational learning theory and neural networks.
6056 en Learning languages from bounded resources: the case of the DFA and the balls of strings 
6057 en On learning regular expressions and patterns via membership and correction queries 
6058 en A note on the relationship between different types of correction queries 
6059 en Polynomial time probabilistic learning of a subclass of linear languages with queries 
6060 en Learning context sensitive languages from linear structural information 
6061 en Grammatical Inference: news from the Machine Translation front 
6062 en Learning meaning before syntax 
6063 en Identification in the limit of $k,I$-substitutable context-free languages 
6064 en A polynomial algorithm for the inference of context free languages 
6065 en How to split recursive automata 
6066 en Learning commutative regular languages 
6067 en State-merging DFA induction algorithms with mandatory merge constraints 
6068 en Evaluation and Comparison of inferred regular grammars 
6069 en Towards feasible PAC-learning probabilistic deterministic finite automata 
6070 en Relevant representations for the inference of rational stochastic tree languages 
6071 en Unsupervised learning of probabilistic context-free grammar using iterative biclustering 
6072 en Bio-molecular computing of finite-state automata 
6073 en Learning right-to-left and left-to right iterative languages 
6074 en Learning bounded unions of Noetherian closed set systems via characteristic sets 
6075 en A learning algorithm for multi-dimensional trees, or:learning beyond context-freeness 
6076 en Polynomial distinguishability of timed automata 
6077 en Schema-guided induction of monadic queries 
6078 en Using Multiplicity automata to identify transducer Relations from membership and equivalence queries 
6079 en Should all Machine Learning be Bayesian? Should all Bayesian models be non-parametric? I'll present some thoughts and research directions in Bayesian machine learning. I'll contrast black-box approaches to machine learning with model-based Bayesian statistics. Can we meaningfully create Bayesian black-boxes? If so what should the prior be? Is non-parametrics the only way to go? Since we often can't control the effect of using approximate inference, are coherence arguments meaningless? How can we convert the pagan majority of ML researchers to Bayesianism? If the audience gets bored of these philosophical musings, I will switch to talking about our latest technical work on Indian buffet processes.
6080 en Introduction to BARK 2008 
6081 en On the relation between Bayesian inference and certain solvable problems of stochastic control Optimal control for nonlinear stochastic dynamical systems requires thesolution of a nonlinear PDE, the so - called Hamilton Jacobi Bellman equation.Recently, Bert Kappen and Emanuel Todorov have shown that for certain types of cost functions, this equationcan be transformed to a linear problem which is mathematically related to a Bayesian estimation problem. This has led to novel efficient algorithms for optimal control of such systems.nI will show a simple proof for this surprising result and discuss some possible implications.
6082 en Multi-task Learning with Gaussian Processes We consider the problem of multi-task learning, i.e. the setup where there are multiple related prediction problems (tasks), and we seek to improve predictive performance by sharing information across the different tasks. We address this problem using Gaussian process (GP) predictors, using a model that learns a shared covariance function on input-dependent features and a ``free-form'' covariance matrix that specifies inter-task similarity. We discuss the application of the method to a number of real-world problems such as compiler performance prediction and learning robot inverse dynamics.nJoint work with Kian Ming Chai, Edwin Bonilla, Stefan Klanke, Sethu Vijayakumar (Edinburgh)
6083 en Latent Force Models with Gaussian Processes We are used to dealing with the situation where we have a latent variable. Often we assume this latent variable to be independently drawn from a distribution, e.g. probabilistic PCA or factor analysis. This simplification is often extended for temporal data where tractable Markovian independence assumptions are used (e.g. Kalman filters or hidden Markov models).nIn this talk we will consider the more general case where the latent variable is a forcing function in a differential equation model. We will show how for some simple ordinary differential equations the latent variable can be dealt with analytically for particular Gaussian process priors over the latent force. In this talk we will introduce the general framework, present results in systems biology preview extensions.nnJoint work with Magnus Rattray, Mauricio Alvarez, Pei Gao, Antti Honkela, David Luengo, Guido Sanguinetti and Michalis Titsias.
6084 en Bayesian learning of sparse factor loadings Learning sparse structure is useful in many applications. For example, gene regulatory networks are sparsely connected since each gene is typically only regulated by a small number of other genes. In this case factor analysis models with sparse loading matrices have been used to uncover the regulatory network from gene expression data. In this talk I will examine the performance of sparsity priors, such as mixture and L1 priors, by calculating learning curves for Bayesian PCA in the limit of large data dimension. This allows us to address a number of questions e.g. how well can we estimate sparsity using the marginal likelihood when the prior is not well-matched to the data generating process?
6085 en Covariance functions and Bayes errors for GP regression on random graphs We consider GP learning of functions defined on the nodes of a random graph. Covariance functions proposed for this scenario, based on diffusion processes on the graph, are shown to have some counter-intuitive properties. In particular, on graphs with tree-like structure where loops can be neglected (as is typically the case for randomly generated graphs), the "obvious" limit of a large correlation length scale does not produce a constant covariance function.nIn the second part, we look at Bayes errors for GP regression on graphs and study how the learning curves depend on the size of the graph, its connectivity, and the number of training examples.nnJoint work with Camille Coti.
6087 en The role of mechanistic models in Bayesian inference I'll outline the role of mechanistic models, or simulators, in defining priors in a Bayesian inference setting. In particular I will focus on two main cases: 1) where process based understanding of the system allows us to construct a stochastic simulator for the system - which translates to inference in stochastic processes; 2) where an existing (typically) deterministic mechanistic model exists - which we can then emulate and treat 'correctly' in a Bayesian manner. I will pay special attention to the relation between the simulator and reality, since it is reality that typically is sampled to generate the observations used for inference in the model. I will outline ideas from emulation, and show the challenges I think remain to be solved.nThis is joint work with lots of people: Alexis Boukouvalas, Yuan Shen, Michael Vrettas, Manfred Opper and many others in the MUCM project.
6088 en Probabilistic models for ranking and information extraction I will summarize some current approaches to information extraction, which aims to obtain structured information from unstructured text sources such as the web. I will then discuss whether Bayesian modelling may be useful in this area and describe a first attempt at extracting class-attributes from web search query logs. If time remains I will move on to discuss various models for probabilistic ranking, and where possible appropriate Bayesian inference techniques.
6089 en Well-known shortcomings, advantages and computational challenges in Bayesian modelling: a few case stories Bayesian inference can be used to judge the data fit quantitatively through the marginal likelihood. In many practical cases only one model is considered and parameter averaging is simply used to avoid overfitting. I show such an example for a large data set of genomic sequence tags where we want to predict how many new unique tags we will find if we perform new sequencing. The two parameter Yor-Pitman process is used and the results illustrate a few well-known facts: parameter averaging can be crucial and large data sets will expose the inadequacy of the model as seen by unrealistically narrow error-bars on (cross-validated) predictions. This indicates that we should come up with better models and being able to calculate the marginal likelihood for these models to perform model selection. In the second part of the talk I will discuss some of the computational challenges of calculating marginal likelihoods. Gaussian process classification is used as an example to illustrate that this is hard even for a uni-modal posterior.
6090 en Variational Model Selection for Sparse Gaussian Process Regression Model selection for sparse Gaussian process (GP) models is an important problem that involves the selection of both the inducing/active variables and the kernel parameters. We describe an auxiliary variational method for sparse GP regression that jointly learns the inducing variables and kernel parameters by minimizing the Kullback-Leibler divergence between an approximate distribution and the true posterior over the latent function values. The variational distribution is parametrized using an unconstrained distribution over inducing variables and a conditional GP prior. This framework allows us to compute a lower bound of the true log marginal likelihood which can be reliably maximized over the inducing inputs and the kernel parameters. We will show how we can reformulate several of the most advanced sparse GP methods, such as the subset of data (SD), DTC, FITC and PITC method, based on the above framework.
6091 en Negotiated Interaction : Iterative Inference and Feedback of Intention in HCI I will talk about an approach to human-computer interaction which makes the uncertainty in the computer's interpretation of the user's intentions tangible, supporting efficient and enjoyable interaction. I will present a 'liquid cursor' demonstration, as an example of making Bayesian inference concrete and visible, as evidence flows between the user and computer. I will present some current research challenges which I hope the BARK audience can engage with, including the use of complex models to shape interaction dynamics, and in measures of interaction between agents. Application examples from mobile interaction and Brain Computer Interaction will be used.nn
6092 en Introduction to the MIT 8.02 course //8.02 is largely about Electricity and Magnetism and at the heart of Electricity and Magnetism are the famous four equations we call the Maxwell's Equations. Its quite a difficoult course for students and I go out of my way to also introduce many phenomena that they see around them and make those phenomena connect with electricity and magnetism.//
6094 en Introduction to the MIT 8.03 course //"8.03 is the Physics of Vibrations and Waves, the third course of physics. It is not a general institute requrement ofcourse except physics majors will have to take the course. We covered the traditional materials that you find in all Vibrations and Waves. Oscilators, resonance phenomenon, mechanical oscilations, electro magnetic oscilations, and over and above I try wherever possible to make students see also throught 8.03 the familiar world around them atleast the world that they have heard about..."//
6095 en Embedded Machine Learning: Using Support Vector Machines in Wireless Sensor Networks using TinyOS and Lego Mindstorms NXT The tutorial on embedded machine learning will present a case study of implementing and using a binary support vector machine in wireless sensor networks. It will use a very popular operating system for wireless sensor networks called TinyOS and the new exciting open hardware/software platform Lego Mindstorms NXT from LEGO. Outline of the tutorial (structured list of topics):nThe tutorial provides an overview of embedded machine learning and an Overview of wireless sensor networks, TinyOS and the programming language nesC. It provides an introduction to LEGO MINDSTORMS and the main hardware items needed to engage the problem in a meaningful way. The mapping of the binary support vector machine to the constraints of the embedded machine learning problem is given (memory, battery, little CPU).
6096 en Knowledge Discovery from Evolving Data Data mining has traditionally concentrated on the analysis of a static world, in which data instances are collected, stored and analyzed to derive models and take decisions according to them. More recent research on stream mining has put forward the need to deal with data that cannot be collected and stored statically but must be analyzed on the fly. At the same time, the need to store, maintain, query and update models derived from the data has been recognized and advocated [LT08]. However, these are only two aspects of the dynamic world that must be analyzed with data mining: The world is changing and so do the accumulating data and, ultimately, the models derived from them.nThe challenges for Knowledge Discovery in a changing world have two forms: (a) adapting the patterns to the changes in the population and (b) capturing, understanding and highlighting the changes.nIn this tutorial, we discuss the topics associated with data mining for changing environments and elaborate on research advances in this area. Relevant research comes among else from the fields of incremental mining, stream mining, temporal mining and change detection. Since this is a very wide field, we concentrate on the second challenge, the understanding of change, and we organize research contributions in this context.
6097 en Ecml/Pkdd 2008 Opening and Awards Ceremony 
6098 en Data Clustering: 50 Years Beyond K-means The practice of classifying objects according to perceived similarities is the basis for much of science. Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms in to taxonomic ranks: domain, kingdom, phylum, class, etc.). Cluster analysis is the formal study of algorithms and methods for grouping objects according to measured or perceived intrinsic characteristics. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes cluster analysis (unsupervised learning) from discriminant analysis (supervised learning). The objective of cluster analysis is to simply find a convenient and valid organization of the data, not to establish rules for separating future data into categories. The development of clustering methodology has been a truly interdisciplinary endeavor. Taxonomists, social scientists, psychologists, biologists, statisticians, engineers, computer scientists, medical researchers, and others who collect and process real data have all contributed to clustering methodology. According to JSTOR, data clustering first appeared in the title of a 1954 article dealing with anthropological data. One of the most well-known, simplest and popular clustering algorithms is K-means. It was independently discovered by Steinhaus (1955), Lloyd (1957), Ball and Hall (1965) and McQueen (1967)! A search via Google Scholar found 22,000 entries with the word clustering and 1,560 entries with the words data clustering in 2007 alone. Among all the papers presented at CVPR, ECML, ICDM, ICML, NIPS and SDM in 2006 and 2007, 150 dealt with clustering. This vast literature speaks to the importance of clustering in machine learning, data mining and pattern recognition. A cluster is comprised of a number of similar objects grouped together. While it is easy to give a functional definition of a cluster, it is very difficult to give an operational definition of a cluster. This is because objects can be grouped into clusters with different purposes in mind. Data can reveal clusters of different shapes and sizes. Thus the crucial problem in identifying clusters in data is to specify or learn a similarity measure. In spite of thousands of clustering algorithms that have been published, a user still faces a dilemma regarding the choice of algorithm, distance metric, data normalization, number of clusters, and validation criteria. A familiarity with the application domain and clustering goals will certainly help in making an intelligent choice. This talk will provide background, discuss major challenges and key issues in designing clustering algorithms, summarize well known clustering methods, and point out some of the emerging research directions, including semi-supervised clustering that exploits pairwise constraints, ensemble clustering that combines results of multiple clusterings, learning distance metrics from side information, and simultaneous feature selection and clustering.
6099 en Learning language from its perceptual context Current systems that learn to process natural language require laboriously constructed human-annotated training data. Ideally, a computer would be able to acquire language like a child by being exposed to linguistic input in the context of a relevant but ambiguous perceptual environment. As a step in this direction, we present a system that learns to sportscast simulated robot soccer games by example. The training data consists of textual human commentaries on Robocup simulation games. A set of possible alternative meanings for each comment is automatically constructed from game event traces. Our previously developed systems for learning to parse and generate natural language (KRISP and WASP) were augmented to learn from this data and then commentate novel games. The system is evaluated based on its ability to parse sentences into correct meanings and generate accurate descriptions of game events. Human evaluation was also conducted on the overall quality of the generated sportscasts and compared to human-generated commentaries.
6100 en Improving Classification with Pairwise Constraints: A Margin-based Approach In this paper, we address the semi-supervised learning problem when there is a small amount of labeled data augmented with pairwise constraints indicating whether a pair of examples belongs to a same class or different classes. We introduce a discriminative learning approach that incorporates pairwise constraints into the conventional margin-based learning framework. We also present an efficient algorithm, PCSVM, to solve the pairwise constraint learning problem. Experiments with 15 data sets show that pairwise constraint information significantly increases the performance of classification.
6101 en Exact and Approximate Inference for Annotating Graphs with Structural SVMs Training processes of structured prediction models such as structural SVMs involve frequent computations of the maximum-a-posteriori (MAP) prediction given a parameterized model. For specific output structures such as sequences or trees, MAP estimates can be computed efficiently by dynamic programming algorithms such as the Viterbi algorithm and the CKY parser. However, when the output structures can be arbitrary graphs, exact calculation of the MAP estimate is an NP-complete problem. In this paper, we compare exact inference and approximate inference for labeling graphs. We study the exact junction tree and the approximate loopy belief propagation and sampling algorithms in terms of performance and ressource requirements.
6102 en A Fast Method for Training Linear SVM in the Primal We propose a new algorithm for training a linear Support Vector Machine in the primal. The algorithm mixes ideas from non smooth optimization, subgradient methods, and cutting planes methods. This yields a fast algorithm that compares well to state of the art algorithms. It is proved to require $O(1/{lambdaepsilon})$ iterations to converge to a solution with accuracy $epsilon$. Additionally we provide an exact shrinking method in the primal that allows reducing the complexity of an iteration to much less than $O(N)$ where $N$ is the number of training samples.
6103 en Cascade RSVM in Peer-to-Peer Networks The goal of distributed learning in P2P networks is to achieve results as close as possible to those from centralized approaches. Learning models of classification in a P2P network faces several challenges like scalability, peer dynamism, asynchronism and data privacy preservation. In this paper, we study the feasibility of building SVM classifiers in a P2P network. We show how cascading SVM can be mapped to a P2P network of data propagation. Our proposed P2P SVM provides a method for constructing classifiers in P2P networks with classification accuracy comparable to centralized classifiers and better than other distributed classifiers. The proposed algorithm also satisfies the characteristics of P2P computing and has an upper bound on the communication overhead. Extensive experimental results confirm the feasibility and attractiveness of this approach.
6104 en Sequence Labelling SVMs Trained in One Pass This paper proposes an online solver of the dual formulation of support vector machines for structured output spaces. We apply it to sequence labelling using the exact and greedy inference schemes. In both cases, the per-sequence training time is the same as a perceptron based on the same inference procedure, up to a small multiplicative constant. Comparing the two inference schemes, the greedy version is much faster. It is also amenable to higher order Markov assumptions and performs similarly on test. In comparison to existing algorithms, both versions match the accuracies of batch solvers that use exact inference after a single pass over the training examples.
6105 en The Boolean Column and Column-Row Matrix Decompositions Matrix decompositions are used for many data mining purposes. One of these purposes is to find a concise but interpretable representation of a given data matrix. Different decomposition formulations have been proposed for this task, many of which assume a certain property of the input data (e.g., nonnegativity) and aim at preserving that property in the decomposition. In this paper we propose new decomposition formulations for binary matrices, namely the Boolean CX and CUR decompositions. They are natural combinations two previously presented decomposition formulations. We consider also two subproblems of these decompositions and present a rigorous theoretical study of the subproblems. We give algorithms for the decompositions and for the subproblems, and study their performance via extensive experimental evaluation. We show that even simple algorithms can give accurate and intuitive decompositions of real data, thus demonstrating the power and usefulness of the proposed decompositions.
6106 en A Unified View of Matrix Factorization Models We present a unified view of matrix factorization that frames the differences among popular methods, such as NMF, Weighted SVD, E-PCA, MMMF, pLSI, pLSI-pHITS, Bregman co-clustering, and many others, in terms of a small number of modeling choices. Many of these approaches can be viewed as minimizing a generalized Bregman divergence, and we show that (i) a straightforward alternating projection algorithm can be applied to almost any model in our unified view; (ii) the Hessian for each projection has special structure that makes a Newton projection feasible, even when there are equality constraints on the factors, which allows for matrix co-clustering; and (iii) alternating projections can be generalized to simultaneously factor a set of matrices that share dimensions. These observations immediately yield new optimization algorithms for the above factorization methods, and suggest novel generalizations of these methods such as incorporating row/column biases, and adding or relaxing clustering constraints.
6107 en Improving Maximum Margin Matrix Factorization Collaborative filtering is a popular method for personalizing product recommendations. Maximum Margin Matrix Factorization (MMMF) has been proposed as one successful learning approach to this task and has been recently extended to structured ranking losses. In this paper we discuss a number of extensions to MMMF by introducing offset terms, item dependent regularization and a graph kernel on the recommender graph. We show equivalence between graph kernels and the recent MMMF extensions by Mnih and Salakhutdinov. Experimental evaluation of the introduced extensions showimproved performance over the original MMMF formulation.
6108 en Learning Bidirectional Similarity for Collaborative Filtering Memory-based collaborative filtering aims at predicting the utility of a certain item for a particular user based on the previous ratings from similar users and similar items. Previous studies in finding similar users and items are based on user-defined similarity metrics such as Pearson Correlation Coefficient or Vector Space Similarity which are not adaptive and optimized for different applications and datasets. Moreover, previous studies have treated the similarity function calculation between users and items separately. In this paper, we propose a novel adaptive bidirectional similarity metric for collaborative filtering. We automatically learn similarities between users and items simultaneously through matrix factorization. We show that our model naturally extends the memory based approaches. Theoretical analysis shows our model to be a novel generalization of the SVD model. We evaluate our method using three benchmark datasets, including MovieLens, EachMovie and Netflix, through which we show that our methods outperform many previous baselines.
6109 en Mining Edge-Weighted Call Graphs to Localise Software Bugs An important problem in software engineering is the automated discovery of noncrashing occasional bugs. In this work we address this problem and show that mining of weighted call graphs of program executions is a promising technique. We mine weighted graphs with a combination of structural and numerical techniques. More specifically, we propose a novel reduction technique for call graphs which introduces edge weights. Then we present an analysis technique for such weighted call graphs based on graph mining and on traditional feature selection schemes. The technique generalises previous graph mining approaches as it allows for an analysis of weights. Our evaluation shows that our approach finds bugs which previous approaches cannot detect so far. Our technique also doubles the precision of finding bugs which existing techniques can already localise in principle.
6110 en Exceptional Model Mining In most databases, it is possible to identify small partitions of the data where the observed distribution is notably different from that of the database as a whole. In classical subgroup discovery, one considers the distribution of a single nominal attribute, and exceptional subgroups show a surprising increase in the occurrence of one of its values. In this paper, we introduce Exceptional Model Mining (EMM), a framework that allows for more complicated target concepts. Rather than finding subgroups based on the distribution of a single target attribute, EMM finds subgroups where a model fitted to that subgroup is somehow exceptional. We discuss regression as well as classification models, and define quality measures that determine how exceptional a given model on a subgroup is. Our framework is general enough to be applied to many types of models, even from other paradigms such as association analysis and graphical modeling.
6111 en Tight Optimistic Estimates for Fast Subgroup Discovery Subgroup discovery is the task of finding subgroups of a population which exhibit both distributional unusualness and high generality. Due to the non monotonicity of the corresponding evaluation functions, standard pruning techniques cannot be used for subgroup discovery, requiring the use of optimistic estimate techniques instead. So far, however, optimistic estimate pruning has only been considered for the extremely simple case of a binary target attribute and up to now no attempt was made to move beyond suboptimal heuristic optimistic estimates. In this paper, we show that optimistic estimate pruning can be developed into a sound and highly effective pruning approach for subgroup discovery. Based on a precise definition of optimality we show that previous estimates have been tight only in special cases. Thereafter, we present tight optimistic estimates for the most popular binary and multi-class quality functions, and present a family of increasingly efficient approximations to these optimal functions. As we show in empirical experiments, the use of our newly proposed optimistic estimates can lead to a speed up of an order of magnitude compared to previous approaches.
6112 en Multiple Manifold Learning Framework based on Hierarchical Mixture Density Model Several manifold learning techniques have been developed to learn, given a data, a single lower dimensional manifold providing a compact representation of the original data. However, for complex data sets containing multiple manifolds of possibly of different dimensionalities, it is unlikely that the existing manifold learning approaches can discover all the interesting lower-dimensional structures. We therefore introduce a hierarchical manifolds learning framework to discover a variety of the underlying low dimensional structures. The framework is based on hierarchical mixture latent variable model, in which each submodel is a latent variable model capturing a single manifold. We propose a novel multiple manifold approximation strategy used for the initialization of our hierarchical model.The technique is first verified on artificial data with mixed 1-, 2- and 3-dimensional structures. It is then used to automatically detect lower-dimensional structures in disrupted satellite galaxies.
6113 en Industrial data mining, Challenges and perspectives Business Intelligence is a very active sector in all industrial domains. Classical techniques (reporting and Olap), mainly concerned with presenting data, are already widely deployed. Meanwhile, Data Mining has long been used in companies as a niche-technique, reserved for experts only and for very specific problems (credit scoring, fraud detection for example). But with the increasing availability of large data volumes (in particular, but not only, from the Web), companies are more and more turning to data mining to provide them with high added-value predictive analytics. However producing models in large numbers, making use of large data volumes in an industrial context can only happen if solutions to challenges, both theoretic and operational, are found: we need algorithms which can be used to produce models when datasets have thousands of variables and millions of observations; we need to learn how to run and control the correct execution of hundreds of models; we need ways to automate the data mining process.nI will present these constraints in industrial contexts and show how KXEN has exploited theoretical results (coming from Vladimir Vapnik's work) to provide answers to the above-mentioned challenges. I will give a few examples of real-life applications and will conclude with some remarks on the future of data mining in the industrial domain.
6114 en Fraud Risk Management in Practice ATOS Worldline performs Fraud Risk Management on a wide range of products like Debit and Credit Cards, for Issuers, Acquirers and Petrol Companies, accross all fraud types like Theft, Counterfeit, and Internet Fraud. During our talk, we will highlight the different components needed for efficient Fraud Risk Management like creation of awareness, an efficient and flexible infrastructure, the right skills, the right support, etc. We will particularly focus of the role and position of data mining in this process and the relation between expert and data driven fraud detection. All will be amply illustrated with real fraud case examples.
6117 en Semantic Unification in a Real-World Data Flow for Delivering Business Intelligence MDC Partners is based in Belgium and delivers business and market intelligence to pharma and medical device companies. As heterogenous public databases and the internet are our main data sources, semantic unification of concepts is the necessary driving force behind our data gathering en data storage platforms. In this talk we discuss a number of design philosophies and technical challenges in building a semantic data gathering, mining and reporting engine. Specifically on the semantic unification side, we will discuss alternative schemes for semantisizing text fragments, based on bootstrapped probabilistic grammars, reference ontologies and factual semantic data. The impact on the total semantic engine, from incoming data to reported queries, of alternative approaches will be discussed.
6118 en Learning Decision Trees for Unbalanced Data Learning from unbalanced datasets presents a convoluted problem in which traditional learning algorithms may perform poorly. The objective functions used for learning the classifiers typically tend to favor the larger, less important classes in such problems. This paper compares the performance of several popular decision tree splitting criteria ? information gain, Gini measure, and DKM ? and identifies a new skew insensitive measure in Hellinger distance. We outline the strengths of Hellinger distance in class imbalance, proposes its application in forming decision trees, and performs a comprehensive comparative analysis between each decision tree construction method. In addition, we consider the performance of each tree within a powerful sampling wrapper framework to capture the interaction of the splitting metric and sampling. We evaluate over this wide range of datasets and determine which operate best under class imbalance.
6119 en One-class Classification by Combining Density and Class Probability Estimation One-class classification has important applications such as outlier and novelty detection. It is commonly tackled using density estimation techniques or by adapting a standard classification algorithm to the problem of carving out a decision boundary that describes the location of the target data. In this paper we investigate a simple method for one-class classification that combines the application of a density estimator, used to form a reference distribution, with the induction of a standard model for class probability estimation. In this method, the reference distribution is used to generate artificial data that is employed to form a second, artificial class. In conjunction with the target class, this artificial class is the basis for a standard two-class learning problem. We explain how the density function of the reference distribution can be combined with the class probability estimates obtained in this way to form an adjusted estimate of the density function of the target class. Using UCI datasets, and data from a typist recognition problem, we show that the combined model, consisting of both a density estimator and a class probability estimator, can improve on using either component technique alone when used for one-class classification. We also compare the method to one-class classification using support vector machines.
6120 en Ranking the Uniformity of Interval Pairs We study the problem of finding the most uniform partition of a label distribution on an interval. This problem occurs, e.g., in discretization of continuous features, where evaluation heuristics need to find the location of the best place to split the current feature. The weighted average of empirical entropies of the interval label distributions is often used for this task. We observe that this rule is sub-optimal, because it prefers short intervals too much. Therefore, we proceed to study alternative approaches. A solution that is based on compression turns out to be the best in our empirical experiments. We also study how these alternative methods affect the performance of classification algorithms.
6121 en The role of hierarchies in exploratory data mining In a broad range of data mining tasks, the fundamental challenge is to efficiently explore a very large space of alternatives. The difficulty is two-fold: first, the size of the space raises computational challenges, and second, it can introduce data sparsity issues even in the presence of very large datasets. In this talk, we'll consider how the use of hierarchies (e.g., taxonomies, or the OLAP multi-dimensional model) can help mitigate the problem.
6122 en A Genetic Algorithm for Text Classification Rule Induction This paper presents a Genetic Algorithm, called Olex-GA, for the induction of rule-based text classifiers of the form ``classify document $d$ under category $c$ if $t_1 in d$ or ... or $t_n in d$ and not ($t_{n+1} in d$ or ... or $t_{n+m} in d$) holds'', where each $t_i$ is a term. Olex-GA relies on an efficient emph{several-rules-per-individual} binary representation and uses the $F$-measure as the fitness function.nnThe proposed approach is tested over the standard test sets Reuters and Ohsumed and compared against several classification algorithms (namely, Naive Bayes, Ripper, C4.5, SVM). Experimental results demonstrate that it achieves very good performance on both data collections, showing to be competitive with (and indeed outperforming in some cases) the evaluated classifiers.nn; Note:n:nA prototype of the ruleninduction system Olex-GA described in that paper is available at the addressn[[http://www.mat.unical.it/Olex-GA]]
6123 en A Joint Topic and Perspective Model for Ideological Discourse Polarizing discussions on political and social issues are common in mass and user-generated media. However, computer-based understanding of ideological discourse has been considered too difficult to undertake. In this paper we propose a statistical model for ideology discourse. By ideology we mean ``a set of general beliefs socially shared by a group of people.'' For example, Democratic and Republican are two major political ideologies in the United States. The proposed model captures lexical variations due to an ideological text's topic and due to an author or speaker's ideological perspective. To cope with the non-conjugacy of the logistic-normal prior we derived a variational inference algorithm for the model. We evaluate the proposed model on synthetic data as well as a written and a spoken political discourse. Experimental results strongly support that ideological perspectives are reflected in lexical variations.
6124 en Towards Machine Learning of Grammars and Compilers of Programming Languages Polarizing discussions on political and social issues are common in mass and user-generated media. However, computer-based understanding of ideological discourse has been considered too difficult to undertake. In this paper we propose a statistical model for ideology discourse. By ideology we mean ``a set of general beliefs socially shared by a group of people.'' For example, Democratic and Republican are two major political ideologies in the United States. The proposed model captures lexical variations due to an ideological text's topic and due to an author or speaker's ideological perspective. To cope with the non-conjugacy of the logistic-normal prior we derived a variational inference algorithm for the model. We evaluate the proposed model on synthetic data as well as a written and a spoken political discourse. Experimental results strongly support that ideological perspectives are reflected in lexical variations.
6125 en A Joint Segmenting and Labeling Approach for Chinese Lexical Analysis This paper introduces an approach which jointly performs a cascade of segmentation and labeling subtasks for Chinese lexical analysis, including word segmentation, named entity recognition and part-of-speech tagging. Unlike the traditional pipeline manner, the cascaded subtasks are conducted in a single step simultaneously, therefore error propagation could be avoided and the information could be shared among multi-level subtasks. In this approach, Weighted Finite State Transducers (WFSTs) are adopted. Within the unified framework of WFSTs, the models for each subtask are represented and then combined into a single one. Thereby, through one-pass decoding the joint optimal outputs for multi-level processes will be reached. The experimental results show the effectiveness of the presented joint processing approach, which significantly outperforms the traditional method in pipeline style.
6126 en Bootstrapping Information Extraction from Semi-structured Web Pages We consider the problem of extracting structured records from semi-structured web pages with no human supervision required for each target web site. Previous work on this problem has either required significant human effort for each target site or used brittle heuristics to identify semantic data types. Our method only requires annotation for a few pages from a few sites in the target domain. Thus, after a tiny investment of human effort, our method allows automatic extraction from potentially thousands of other sites within the same domain. Our approach extends previous methods for detecting data fields in semi-structured web pages by matching those fields to domain schema columns using robust models of data values and contexts. Annotating 2-5 pages for 4-6 web sites yields an extraction accuracy of 83.8% on job offer sites and 91.1% on vacation rental sites. These results significantly outperform a baseline approach.
6127 en Large-Scale Clustering through Functional Embedding We present a new framework for large-scale data clustering. The main idea is to modify functional dimensionality reduction techniques to directly optimize over discrete labels using stochastic gradient descent. Compared to methods like spectral clustering our approach solves a single optimization problem, rather than an ad-hoc two-stage optimization approach, does not require a matrix inversion, can easily encode prior knowledge in the set of implementable functions, and does not have an out-of-sample problem. Experimental results on both artificial and real-world datasets show the usefulness of our approach.
6128 en Mixed Bregman Clustering with Approximation Guarantees Two recent breakthroughs have dramatically improved the scope and performance of k-means clustering: squared Euclidean seeding for the initialization step, and Bregman clustering for the iterative step. In this paper, we first unite the two frameworks by generalizing the former improvement to Bregman seeding - a biased randomized seeding technique using Bregman divergences - while generalizing its important theoretical approximation guarantees as well. We end up with a complete Bregman hard clustering algorithm integrating the distortion at hand in both the initialization and iterative steps. Our second contribution is to further generalize this algorithm to handle mixed Bregman distortions, which smooth out the asymetricity of Bregman divergences. In contrast to some other symmetrization approaches, our approach keeps the algorithm simple and allows us to generalize theoretical guarantees from regular Bregman clustering.
6129 en Clustering Distributed Sensor Data Streams In this work we study the problem of continuously maintain a cluster structure over the data points generated by a sensor network. We propose DGClust, a new distributed algorithm which reduces both the dimensionality and the communication burdens, by allowing each local sensor to keep an online discretization of its data stream. Each new data point triggers a cell in this univariate grid, reflecting the current state of the data stream at the local site. Whenever a local site changes its state, it notifies the central server about the new state it is in. The central site keeps a small list of counters of the most frequent global states. A simple adaptive partitional clustering algorithm is applied to the frequent states central points, providing an anytime definition of the clusters centers. The approach is evaluated in the context of distributed sensor networks, presenting empirical and theoretical evidence of its advantages.
6130 en Data Streaming with Affinity Propagation This paper proposed StrAP (Streaming AP), extending Affinity Propagation (AP) to data steaming. AP, a new clustering algorithm, extracts the data items, or exemplars, that best represent the dataset using a message passing method. Several steps are made to build StrAP. The first one (Weighted AP) extends AP to weighted items with no loss of generality. The second one (Hierarchical WAP) is concerned with reducing the quadratic AP complexity, by applying AP on data subsets and further applying Weighted AP on the exemplars extracted from all subsets. Finally StrAP extends Hierarchical WAP to deal with changes in the data distribution. Experiments on artificial datasets, on the Intrusion Detection benchmark (KDD99) and on a real-world problem, clustering the stream of jobs submitted to the EGEE grid system, provide a comparative validation of the approach.
6131 en Parameter Learning in Probabilistic Databases: A Least Squares Approach We introduce the problem of learning the parameters of the probabilistic database ProbLog. Given the observed success probabilities of a set of queries, we compute the probabilities attached to facts that have a low approximation error on the training examples as well as on unseen examples. Assuming Gaussian error terms on the observed success probabilities, this naturally leads to a least squares optimization problem. Our approach, called LeProbLog, is able to learn both from queries and from proofs and even from both simultaneously. This makes it flexible and allows faster training in domains where the proofs are available. Experiments on real world data show the usefulness and effectiveness of this least squares calibration of probabilistic databases.
6132 en A Simple Model for Sequences of Relational State Descriptions Artificial intelligence aims at developing agents that learn and act in complex environments. Realistic environments typically feature a variable number of objects, relations amongst them, and non-deterministic transition behavior. Standard probabilistic sequence models provide efficient inference and learning techniques, but typically cannot fully capture the relational complexity. On the other hand, statistical relational learning techniques are often too inefficient. In this paper, we present a simple model that occupies an intermediate position in this expressiveness/efficiency trade-off. It is based on CP-logic, an expressive probabilistic logic for modeling causality. However, by specializing CP-logic to represent a probability distribution over sequences of relational state descriptions, and employing a Markov assumption, inference and learning become more tractable and effective. We show that the resulting model is able to handle probabilistic relational domains with a substantial number of objects and relations.
6133 en Extracting Semantic Networks from Text via Relational Clustering Extracting knowledge from text has long been a goal of AI. Initial approaches were purely logical and brittle. More recently, the availability of large quantities of text on the Web has led to the development of machine learning approaches. However, to date these have mainly extracted ground facts, as opposed to general knowledge. Other learning approaches can extract logical forms, but require supervision and do not scale. In this paper we present an unsupervised approach to extracting semantic networks from large volumes of text. We use the TextRunner system [1] to extract tuples from text, and then induce general concepts and relations from them by jointly clustering the objects and relational strings in the tuples. Our approach is defined in Markov logic using four simple rules. Experiments on a dataset of two million tuples show that it outperforms three other relational clustering approaches, and extracts meaningful semantic networks.
6134 en Data Mining for Anomaly Detection Anomaly detection corresponds to discovery of events that typically do not conform to expected normal behavior. Such events are often referred to as anomalies, outliers, exceptions, deviations, aberrations, surprise, peculiarities or contaminants in different application domains Detection of anomalies is a common problem in many domains, such as detecting fraudulent credit card transactions, insurance and tax fraud detection, intrusion detection for cyber security, failure detection, direct marketing, and medical diagnostics.nAlthough anomalies are by definition infrequent, in many examples their importance is quite high compared to other events, making their detection extremely important.nThis tutorial will provide an overview of the research done in the increasingly important field of anomaly detection. The tutorial will cover the existing literature from a variety of perspectives, such as nature of input/output, and the availability of supervision.nAnomalies will be divided into three broad groups: (i) Point anomalies, (ii) Contextual anomalies, and (iii) Structural anomalies, and a wide variety of anomaly detection methods appropriate for each type of anomaly will be presented. Additionally, the tutorial will discuss several application domains, such as intrusion detection, fraud detection, industrial damage detection, healthcare informatics, where anomaly detection plays a central role.
6135 en The Regularization Frontier in Machine Learning Machine Learning algorithms often involve the joint optimization of several objective functions for achieving good generalization performance. Well known examples are Support Vector Machines for regression, classification and novelty detection or the Lasso problem where one objective function is related to the perfect fit of the data and the second one concerns particular desirable properties such as smoothness or sparsity of the target model. These two goals being antagonist, a trade-off needs to be achieved. Hence, the learning process can be cast in a multi-objective optimisation problem. The aim of this tutorial is to bridge the gap between the multi-objective optimization literature and the machine learning community by providing an insight on the Pareto frontier, the efficient computation of this frontier using regularization path algorithms. The connection between these algorithms and parametric optimisation problems will be highlighted as well as issues related to sparsity, model selection and numerical implementation.
6136 en Nature, not human activity, rules the climate The science is settled: Evidence clearly demonstrates that Carbon dioxidencontributes insignificantly to Global Warming and is therefore not an'pollutant.'nnThis fact has not yet been widely recognized, and irrationalnGlobal Warming fears continue to distort energy policies and economic policy. Allnefforts to curtail CO,,2,, emissions, whether global or at the state level, arenpointless -- and in any case, ineffective and very costly.nnOn the whole, a warmer climate is beneficial.
6137 en Ontology construction from text 
6138 en The Schelling model of urban segregation The later economics Nobel laureate Schelling published in 1971 a model for the spontaneous segregation of people of two different groups (ethnic, religious, ...). Recent simulations of the Ising-type model are reviewed.
6139 en Facilitation, competition, and vegetation patchiness: From scale free distribution to patterns A new technique for the modeling of perennial vegetation patchiness in the arid/semiarid climatic zone is suggested. Incorporating the stochasticity that affects life history of seedlings and the deterministic dynamics of soil moisture and biomass, this model is flexible enough to yield qualitatively different forms of spatial organization. In the facilitation-dominated regime, scale free distribution of patch sizes is observed, in correspondence with recent field studies. In the competition controlled case, on the other hand, power-law statistics is valid up to a cutoff, and an intrinsic length scale appears.
6140 en Nowak-Vallacher's Mouse Paradigm - a tool for measuring the dynamics of thought Traditional approaches in social psychology attribute attitude change to external factors disregarding the intrinsic dynamics of information processing in the brain. Vallacher and Nowak proposed a method for measuring momentary changes in the stream of thought, the Mouse Paradigm. In this approach, momentary state of one's feelings about an object corresponds to the perceived distance from this object. By tracking the computer mouse movements produced by subjects thinking about the object, it is possible to find temporal patterns of attitude change. In the presentation it will be argued that the Mouse Paradigm can be a valuable tool for measuring social judgment and self-esteem.
6141 en Association between genetic polymorphisms for vasopressin and oxytocin receptors and pro-social behavior in economic decision tasks Human altruism is a widespread phenomenon that has puzzled evolutionary biologists since Darwin. Economic games illustrate human altruism by demonstrating that behavior deviates from economic predictions of selfish utility maximization. A game that most plainly demonstrates this altruistic tendency is the Dictator Game. We hypothesized that human pro-social behavior is to some extent hardwired and that two likely candidate genes that may contribute to individual differences in altruistic behavior are the arginine vasopressin receptor (AVPR1a) and the oxytocin receptor (OXTR). nnGenes that in some mammals such as the vole have been shown to have a profound impact on affiliative behaviors. Multiple studies have shown how the two closely related neuropeptides facilitate social communication, and cognition across mammals; in the current investigation, we demonstrate that AVPR1a and OXTR polymorphisms predict pro-social allocation of funds in two economic games that measure altruistic and pro-social behavior, the Dictator Game and Social Value Orientations (SVO). 203 college students participated in both a one-time online version of the Dictator game and SVO. Subjects and their parents were also genotyped for the AVPR1a and OXTR. Using a family-based method, we observed preferential transmission of individual alleles for the Dictator game and the SVO.
6142 en Fluctuation scaling in complex systems: Taylor's law and beyond Complex systems consist of many interacting elements which participate in some dynamical process. The activity of various elements is often different and the fluctuation in the activity of an element grows monotonically with the average activity. This relationship is generically of the form fluctuations ≈ const.\times average^?, where the exponent ? is predominantly in the range [1/2, 1]. This power law has been observed in a very wide range of disciplines, ranging from population dynamics through the Internet to the stock market and it is often treated under the names Taylor's law or fluctuation scaling. We attempt to show how general the above scaling relationship is by surveying the literature, as well as by reporting some new empirical data and model calculations. We also show some basic principles that can underlie the generality of the phenomenon.
6143 en Energy policy: complex? Energy generation is comparatively simple, energy policy is not. Can complex systems science help us to understand the national and international policies past and present? Can it contribute to a sensible resolution of the problems caused by our low cost energy economies and resulting carbon emissions? If so, how? if not, why not?
6144 en Stabilization of metapopulation cycles: Toward a classification scheme The stability of population oscillations in ecological systems is considered. Experiments suggest that in many cases the single patch dynamics of predator prey or host-parasite systems is extinction prone and stability is achieved only when the spatial structure of the population is expressed via desynchronization between patches. A few mechanisms have been suggested so far to explain the inability of dispersal to synchronize the system. We suggest a classification scheme that allows for either a-priori (based on the system parameters) or a posteriori (based on local measurements) identification of the dominant process that yields desynchronization.
6145 en Linking pattern formation and biodiversity in Dryland Vegetation Ecological processes generally involve different levels of organization, starting with a singlenspecies individual, through a population of many individuals of a given species, and up to andiverse community consisting of large populations of many different species interacting among themselves and with their physical environment. Upscaling low-level attributes, such as species traits and biomass-resource feedbacks, to community level properties, such as vegetation patterns and species diversity, is a highly challenging goal for theorists. In this talk I will describe recentnprogress our group has made towards achieving this goal in the context of dryland plant communities.
6146 en Principal Component Analysis and Clustering Reveal Human Maternal Ancestry from Complete Mitochondrial Sequences We develop a simple, direct method to infer the phylogenetic tree for the maternal lineage of all humans usingnprincipal component analysis and consensus ensemble clustering. Unlike standard methods such as parsimony and maximum likelihood, our method is fast, gives a unique tree, makes no a-priori assumptions, uses all polymorphisms in the data and has high internal branch consensus. It confirms that modern humans came from Africa in at least two migrations and that the common maternal ancestor of humans or "mitochondrial Eve" lived in Africa ~200,000 years ago. It also suggests that the so called "R Clade", usually defined by a polymorphism at locus 12705 is too heterogeneous to have derived from a single common ancestor and places haplogroups B/R5/F in the Asian branch of the N Clade in agreement with their current location.
6147 en The Markovian Patch-Occupancy (MPO) framework in Community Ecology Ecological research over the years has pointed to the existence of a wide spectrum of 'semi-universal' patterns of species diversity, found over very different life forms and ecosystems (e.g., the species-area relationship, the productivity-diversity relationship, the local-regional diversity relationship, etc.). We present the Markovian Patch-Occupancy (MPO) framework as a powerful platform for analyzing the mechanisms underlying these patterns. The MPO framework uses a stochastic individual-based model of an ecological community, based on the theory of Markov processes. The analytically-tractable model is both general and highly flexible, and can easily incorporate a wide spectrum of ecological factors including the effects of area, geographical isolation, habitat loss, habitat heterogeneity, life-history characteristics and trade-offs, density dependence, community-level carrying capacity, competition for space, various forms of dispersal (random dispersal, preference for unoccupied sites, preference for suitable habitats), and complete flexibility in the demographic rates of individual species. The MPO framework can be used to formulate and solve modern models of the neutral theory, and is capable of explaining a surprisingly wide spectrum of the 'semi-universal' patterns of species diversity. The generality, high flexibility and analytic tractability of the MPO framework make it a powerful platform for other research fields as well.
6148 en Global Features Of Species Network A simple model for competition induced speciation is presented and analyzed. Logistic growth with nonlocal interaction is studied on regular and random networks, and the large scale structure of the emerging genomic frequencies is examined. The neutrality assumption is violated if the network is random and the competition is nonlocal. Instead, "Hubs" in the sequence space are suppressed by the competition more than nodes of lower degree. Thus, speciation is unavoidable for large scale free networks. The emerging genetic mixture depends strongly on the initial conditions. The frequency of hubs is much larger when the population evolves from a single nucleation event, in comparison with populations that recover from a catastrophe.
6149 en Dynamic networks at the edge of chaos A network of coupled phase oscillators is considered. Interactions between the oscillators are characterized by phase shifts, effectively taking into account interaction delays. We show that in this simple model coherent collective dynamics can emerge. Alternatively, chaos can develop when interaction phase shifts are large enough. Introducing a global feedback, chaotic behavior can be suppressed, giving rise to localized structures in the network with complex dynamical behavior. This transition scenario is analyzed, and special attention is paid to the dynamical properties of self-organized structures.
6150 en Dynamical properties of evolving networks Networks are usually studied as static objects, through the properties of a single snapshot of the network. The network generating mechanism are then deduced from the statistical properties of this snapshot. We propose a methodology to directly study the network evolution from dynamic data. This method joint with an appropriate Markov model for edge and node addition provides a direct insight on the network generation mechanism. The Markov model permits the quantitative comparison of the contribution of each generation mechanism to specific network properties.
6151 en Excess covariance in financial I discuss the properties of correlations between returns of financial assets, both static and dynamic, and the challenges they pose to understanding the dynamics of financial markets. Next I will discuss how these questions can be addressed in theoretical models, showing how these features are related to traders' behavior.
6152 en Lévy-stable distribution in economics In this lecture I will provide an assessment of the role played by the class of Lévy-stable distributions in modern macroeconomics and econometrics. Some emphasis will be put on the distributional features of sectoral-level productivity growth rates, and their implications for macroeconomic theory.
6153 en The alphabet model for rare events in social dynamics In most of social sciences prediction is based on extrapolation of central tendencies. In reality, almost everything that is important is the consequence of a rare event. The alphabet model describes how seemingly unimportant rare events may govern social dynamics.
6154 en Community structure in graphs Identifying communities in networks is an open challenge of fundamental importance in several disciplines. Here I discuss the main aspects of the problem, from the definition of community to the problem of hierarchy, including the crucial issue of testing methods of community detection.
6155 en Leibniz, Complexity and Incompleteness I will discuss Leibniz's ideas on complexity (Discours de metaphysique, 1686), leading to modern work on program-size complexity, the halting probability and incompleteness. Leibniz's principle of sufficient reason asserts that if anything is true it is true for a reason. But the bits of the numerical value of the halting probability are mathematical truths that are true for no reason. More precisely, as I will explain, they are irreducible mathematical truths, that is, true for no reason simpler than themselves.
6156 en The broken symmetries of financial markets Various types of irregularities of financial markets will be discussed, together with the methods needed to characterise them and the models that reproduce them. A particular emphasis will be put on time reversal asymmetry.
6157 en Information feedback mechanism and market dominance in a percolation model of eco-innovation diffusion New technologies often enter the market at a competitive disadvantage. While they may seem to promise future advantages such as lower costs, environmental friendliness, or higher performance, initially they may be significantly more expensive than incumbent technologies or face teething problems. The adoption of a new technology may also be affected by consumers’ uncertainty on its performance. In such an environment, information feedbacks are likely to arise and could allow a certain technology to be the dominant one in the market. Drawing on recent percolation models of diffusion that combine the contagion aspect of agents distributed on a network, with the heterogeneity of agent characteristics, we develop a complex-dynamics model of new technology diffusion. By using a multinomial decision mechanism to model each adopter’s choice on a portfolio of new available technologies we show the effect of information feedbacks on market dominance. Using agent-based simulations we explore when a limited subsidy policy, combined with a power-law learning curve for the price as a function of the cumulative number of adopters, can trigger a self-sustained diffusion of a certain technology.
6158 en Common Welfare, Strong Currencies and the Globalization Process The so called “globalization” process (i.e. the inexorable integration of markets, currencies, nation-states, technologies andthe intensification of consciousness of the world as a whole) has a behavior exactly equivalent to a system that is tending to a maximum entropy state. This globalization process obeys a collective welfare principle in where the maximum payoff is given by the equilibrium of the system and its stability by the maximization of the welfare of the collective besides the individual welfare. This let us predict the apparition of big common markets and strong common currencies. They will reach the “equilibrium” by decreasing its number until they reach a state characterized by only one common currency and only one big common community around the world.
6159 en The coupling of strain evolution and disease dynamics Influenza A is characterized by seasonal outbreaks and a gradual genetic, yet discontinuous antigenic evolution (antigenic "cluster jumps" occuring every 4-7 years, with a large seasonal epidemic as a result of lacking immunity among the population).The interplay between viral mutations and an age-group-specific human immune response is modelled here, trying to account for the observed phenomena and to predict next season's epidemic strain from the dynamics of the previous year.
6160 en Self-Organization of Sound Systems In the framework of Complex Networks The sound inventories of the world's languages show a considerable extent of symmetry. It has been postulated that this symmetry is a reflection of the human physiological, cognitive and societal factors. Although the organization of the vowel systems has been satisfactorily explained for smaller inventories, the structure of the consonant inventories is an open problem since 1939. We reformulate the problem in the light of statistical physics, more precisely complex networks, and observe that the distribution of the occurrence and co-occurrence of the phonemes (consonants and vowels) over languages are scale-free. The co-occurrence network exhibits strong community structures, where the driving forces behind the community formation are the human articulatory and perceptual factors. In order to validate the above principle, we introduce an information theoretic definition of these factors - feature entropy and feature distance - and show that the natural language inventories are significantly different in these terms from the randomly generated ones. A preferential attachment based growth model can lead to the emergence of similar topologies as that of the real networks. Furthermore, in a separate study, we observe that spectral analysis of the co-occurrence network of consonants helps us in the induction of linguistic typologies.
6161 en Random trees and genealogies The talk will review some of the statistical properties of the trees which represent the ancestry of evolving populations, both for neutral models of asexual and sexual reproduction. It will in particular show how the ages of the first common ancestors depend on the population size.
6162 en Statistical physics and complex networks Statistical physics approaches are developed and applied successfully in recent years to understand the topology, robustness and function of complex networks. We will show how ideas and tools from percolation theory lead to novel results on the robustness, immunization strategies, optimal paths and minimum spanning trees. These results are relevant to many real world systems ranging from the Internet to social systems and climate.
6163 en Complexity: What are we talking about This field of physics was originally identified as Solid state Physics, then P.W. Anderson coined the term Condensed Matter Physics and more recently it has merged with Statistical Physics to lead to the Physics of Complex Systems.nThe study of complex systems refers to the emergency of collective properties in systems with a large number of parts in interaction among them. These elements can be atoms or macromolecules in a physical or biological context, but also people, machines or companies in a socio-economic context. The science of complexity tries to discover the nature of the emerging behavior of complex systems, often invisible to the traditional approach, by focusing on the structure of the interconnections and the general architecture of systems, rather than on the individual components.nIt is a change of perspective in the forma mentis of scientists rather than a new scientific discipline. Traditional science is based on a reductionistic reasoning for which, if one knows the basic elements of a system, it is possible to predict its behavior and properties. It is easy to realize, however, that for a cell or for the socio-economic dynamics one faces a new situation in which the knowledge of the individual parts is not sufficient to describe the global behavior of the structure. We can represent this situation as the study of the architecture of matter and nature. It depends in some way from the individual elements (bricks) but then it shows fundamental laws and properties which cannot be derived from these elements. Starting from the simplest physical systems, like critical phenomena in which order and disorder compete, these emergent behaviors can be identified in many other systems, from ecology to the immunitary system, to the social behavior and economics. The science of complexity has the objective of understand the properties of these systems. Which rules govern their behavior? How they adapt to changing conditions? How they learn efficiently and how they optimize their behavior?nThe development of the science of complexity cannot be reduced to a single theoretical or technological innovation but it implies a novel scientific approach with enormous potentialities to influence deeply the scientific activities, social, economic and technological.nn
6164 en Emergence of complexity in biological networks: from selection to tinkering Recent work has been searching for general principles of organization and evolution of natural and artificial systems changing through local rules based on reuse of previously existing substructures. Such a process of "tinkering" makes a big difference (at least in principle) when comparing biological structures and man-made artifacts. As pointed out by the French biologist François Jacob, the engineer is able to foresee the future use of the artifact (i.e. it acts as a designer) whereas evolution does not. The first can ignore previous designs, whereas the second is based on changes taking place by using available structures.nIn spite of its apparent drawbacks, tinkering has been able to generate most complex structures observable in the real world (including some in the technological world). Very often, the resulting structures share common principles of organization, suggesting that convergent evolution towards a limited number of basic plans is inevitable. How innovations emerge through evolution is one of the key problems in complexity. Recent work on evolved complex networks suggests that tinkering is a main driving force shaping complex systems and that several desirable properties, including modularity, might emerge for free under tinkered evolution.
6165 en Adaptation and organization in the economy of living matter Metabolic networks guarantee the supply of energy and building blocks necessary for the maintenance of life. Using genomic information, mathematical models, and optimality criteria, one can learn about their evolutionary history and organization principles.
6166 en Agent Based Models in Economics and Complexity A crucial aspect of the complexity approach is how interacting elements produce aggregate patterns that those elements in turn react to. This leads to the emergence of aggregate properties and structures that cannot be guessed by looking only at individual behaviour. Explicitly considering how heterogeneous elements dynamically develop their behaviour through interaction is a hard task analytically, the equilibrium analysis of mainstream (neoclassical) economics being a not neutral shortcut. On the other hand, explicitly considering the dynamics of the process started to be a feasible alternative only when computer power became widely accessible. The computational study of heterogeneous interaction agents is called agent-based modelling (ABM). Interestingly, among its first applications a prominent role was given to economic models, although it was quickly found of value in other disciplines too. Goal of this lecture is to motivate the use of the complexity approach and agent-based modelling in economics, by discussing the weaknesses of the traditional paradigm of mainstream economics, and then explain what ABM is and which research and policy questions it can help to analyse.nn
6167 en Systems Modelling ; Approaches for making and exploring decisions Previous systems based analyses, predictions and decisions include conflict , environmental pollution, disease, organisational changes/issues and the mathematical methods involved - The big questions for system modellers working with decision makers are how to define and focus on key issues of optimal combination of micro and macro modelling
6168 en Complexity science in the 21st century: Keeping purpose in a random world 
6179 en Hard and easy science Using examples of the application of quantitative ideas in social science (hard sciences) and recalling some approaches to complex problems in natural science (easy sciences) we suggest a path to developing a fundamental foundation for the analysis of social problems. A roadmap of suggested strategies over a few decades with agreed on metrics of successful activities would give guidance tonquantitative approaches to these hard scientific problems.
6180 en Financial crises and risk management The scientific study of complex systems has transformed a wide range of disciplines in recent years, enabling researchers in both the natural and social sciences to model and predict phenomena as diverse as the failure of materials, earthquakes, global warming, demographic patterns, and financial crises. In this talk, Didier Sornette describes a simple, powerful, and general theory of how, why, and when stock markets crash.nMost attempts to explain market failures seek to pinpoint triggering mechanisms that occur hours, days, or weeks before the collapse.nSornette proposes a radically different view: the underlying cause can be sought months and even years before the abrupt, catastrophic event in the build-up of cooperative speculation, into an accelerating rise of the market price, otherwise known as a "bubble." This view implies the possibility of predicting such events and Sornette will describe the current status of predictions that he and his collaborators have made for events in various markets.
6181 en Individual Creativity and Radical Breakthroughts 
6182 en Creativity and radical breakthroughs in Science 
6183 en Interview 
6184 en Communication across Scientific Disciplines 
6185 en Welcome and Opening Address by main sponsors 
6186 en Applications of Statistical Physics to Understanding Complex Systems 
6187 en Complexities: Synchronic and Diachronic 
6188 en Social science 2.0? Social science is often concerned with the emergence of collective behavior out of the interactions of large numbers of individuals; but in this regard it has long suffered from a severe measurement problem—namely that interactions between people are hard to measure, especially at scale, over time, and at the same time as observing behavior. In this talk, I will argue that the technological revolution of the Internet is beginning to lift this constraint. To illustrate, I will describe three examples of research that would have been extremely difficult, or even impossible, to perform just a decade ago: (1) using email exchange to track social networks evolving in time; (2) using a web-based experiment to study the collective consequences of social influence on decision making; and (3) using a social networking site to study the difference between perceived and actual homogeneity of attitudes among friends. Although internet-based research still faces serious methodological and procedural obstacles, I propose that the ability to study truly “social” dynamics at individual-level resolution will have dramatic consequences for social science.
6189 en Genealogies in models of evolution with selection Simples mean field models of evolution in presence of selection will be discussed.nThe effect of selection is to modify the statistical properties of genealogical trees: while in absence of selection, the trees are randomly distributed as in Kingman's coalescent, theirnstatistics, in presence of selection, are very reminisecent of Parisi's theory of mean field spin glasses.
6190 en Minimal agent based model for the origin and self-organization of financial markets We introduce a minimal Agent Based Model which includes the following elements (first considered by Lux and Marchesi):n- Fundamentalists (F: stabilizing tendency)n- Chartists (C: destabilizing tendency)n- Herding effect (tendency to follow the others)n- Price behavior (analysis of the price time series according to F or C criteria)nThe novelty of our model is a substantial simplification and corresponding reduction of the number of parameters. This leads to a detailed understanding of the origin of the Stylized Facts (SF) like the fat tails and volatility clustering. The SF are shown to correspond to finite size effects (with respect to time and to the number of agents N) which, however, can be active at different time scales. This implies that universality cannot be expected in describing these properties in terms of effective critical exponents.nA basic question is then the self-organization: why the system chooses to stay in this narrow range of parameters corresponding also to a finite value of N?nWe show that the introduction of a threshold in the agents’ action (small price fluctuations lead to no action) triggers the self-organization towards the quasi-critical state and to a finite average value of N (which depends on the other parameters). Non stationarity in the number of active agents and in their action plays a fundamental role. The interpretation of N as the number of effective independent agents is non trivial and deserves further studies. The model can be easily generalized to more realistic variants in a systematic way.
6191 en Systemic Mechanism Detection in Tumor Formation 
6192 en Scale-free topologies emerging from a dynamical entrainment of a complex network I show that the topology and dynamics of a network of unsynchronized Kuramoto oscillators can be simultaneously controlled by means of a forcing mechanism which yields a phase locking of the oscillators to that of an external pacemaker in connection with the reshaping of the network's degree distribution. The entrainment mechanism is based on the addition, at regular time intervals, of unidirectional links from oscillators that follow the dynamics of a pacemaker to oscillators in the pristine graph whose phases hold a prescribed phase relationship. Such a dynamically based rule in the attachment process leads to the emergence of a power-law shape in the final degree distribution of the graph whenever the network is entrained to the dynamics of the pacemaker. I show that the arousal of a scale-free distribution in connection with the success of the entrainment process is a robust feature, characterizing different network's initial configurations and parameters.
6193 en Towards a physics of society Statistical physics has proven to be an invaluable tool to describe and understand the properties of systems formed by a large number of elementary units. A big challenge is whether the tools and techniquesnof statistical physics are suitable to explore large scale social phenomena. Most attempts ofnthe literature focus on simple microscopic models, with little or no contact to real social dynamics. A validation of this approach is still lacking and must rely on quantitative evidence about real social systems. Finding regularities on real data is a crucial step in this direction. We will show that voting and citing behaviors are both characterized by scaling and universality. The statistical distribution of the number of votes/cites, suitably normalized, is independent of the particular system considered. This opens the way to a simple modeling of the observed phenomenology.
6194 en Demography - Who pays my pension ? Simple computer simulations indicate in many European countries serious problems for old-age pensions around 2030 or later: Many more old peoplenand much less young people. Possible remedies are immigration and increase in retirement ages. In Algeria the present situation seems betternbalanced, in the Palestinian Territories it is the opposite, for the nextnfew decades.nn
6195 en Complex networks: theory and applications Complex systems which are composed of many interacting entities can be represented, analyzed and better understood using a network representation, where the entities are represented by nodes and the interactions by links. In recent years it was realized that the topology of many real networks is very different from that of the classical graph theory. In particular, while classical graphs were assumed to be homogeneous with every node having a typical number of links (degree) real networks are usually heterogeneous (e.g., the Internet) with nodes having very different degrees. Thus, many properties of networks were not understood and many open questions were asked. The finding of the new topology led, in recent years, to the emergence of a active field of complex networks where new suitable theories and approaches are developed. I will discuss these developments as well as many recent applications, such as robustness, effective immunization strategies and optimal transport in real world networks.nReferences:n[1] Transport in weighted networks: partition into superhighways and roads Z. Wu, L.A. Braunstein, S. Havlin, H.E. Stanley Phys. Rev. Lett. 96, 148702 (2006)n[2] Limited path percolation in complex networks E. Lopez, R. Parshani, R. Cohen, S. Carmi, S. Havlin Phys. Rev. Lett. 99, 188701 (2007)n[3] A model of Internet topology using k-shell decomposition S. Carmi, S. Havlin, S. Kirkpatrick, Y. Shavitt, E. Shir PNAS 104, 11150 (2007)n[4] Climate networks around the globe are significantly affected by El Nino K. Yamasaki, A. Gozolchiani, S. Havlin Phys. Rev. Lett. 100, 228501 (2008)n[5] Finding a Better Immunization Strategy Y. Chen, G. Paul, S. Havlin, F. Liljeros, and H. E. Stanley Phys. Rev. Lett. 101, 058701 (2008)
6196 en Changing the world of web search Heavy-tailed distributions have been observed in many phenomena involving consumer behavior, especially on the internet. We consider the problem of assembling a search engine of the future. We examine the impact of such heavy-tailed consumer behavior on the design choice for such an engine, and in the process highlight our poor understanding (and the need for much further research) into many concrete aspects of economically viable search engine operation.
6197 en Can nature solve hard problems? Nature certainly poses a lot of hard problems to the physicist or the chemist.But can natural systems, or nature-inspired artifacts, be used to solve hard problems? This general question can be made a bit more precise by focusing on combinatorial optimization problems. Simple attempts at solving problems like Hamiltonian path or Steiner tree seem far from the goal, and in many case it seems that the occurence of a glassy transition, which freezes the relaxation in metastable states, is the major obstacle.nRecently, some of the hardest constraint satisfaction problems have been addressed successfully through message passing methods, with algorithms which partly get around the freezing transition. The talk will review this approach and discuss its limitations. Message passing, which is a purely local strategy based on the exchange of simple probabilistic messages along a graph of constraints, is not only very powerful, but also appealing since its basic ingredients share some similarity with neural networks. This points to alternative, ``natural'' ways of solving hard problems, through artifacts which get around the basic physical limitations of usual thermal systems.
6198 en Computational and mathematical challenges involved in estimating Phylogenetic inference presents enormous computational and mathematical challenges, but these are particularly exacerbated when dealing with very large datasets (containing thousands of sequences) or when sequences evolve under complex evolutionary processes (ranging from simple "indel" models, to horizontal gene transfer, to genome rearrangement events). In this talk, I will describe some of the recent progress on evolutionary history reconstructing under complex evolutionary processes, focusing in particular on multiple sequence alignment and its implications for large-scale phylogenetics.nRelated Link: http://www.phylo.org - CIPRES project webpage.
6199 en Human Mobility Patterns Despite their importance for urban planning, traffic forecasting and the spread of biological and mobile viruses, our understanding of the basic laws governing human motion remains limited owing to the lack of tools to monitor the time-resolved location of individuals. We study the trajectory of anonymized mobile phone users, finding that, in contrast with the random trajectories predicted by the prevailing Le´vy flight and random walk models, human trajectories show a high degree of temporal and spatial regularity, each individual being characterized by a time independent characteristic travel distance and a significant probability to return to a few highly frequented locations. Afterncorrecting for differences in travel distances and the inherent anisotropy of each trajectory, the individual travel patterns collapse into a single spatial probability distribution, indicating that,ndespite the diversity of their travel history, humans follow simple reproducible patterns. This inherent similarity in travel patterns could impact all phenomena driven by human mobility, from epidemic prevention to emergency response, urban planning and agent-based modeling.
6200 en Modeling social networks on large scale Recent development in information and communication technology has enabled to study networks of social interactions of unprecedented size. Such systems include email or phone networks and e-communities. In contrast to the traditional, questionnaire-based investigations, in these cases a natural quantitative measure of the strength of the interactions is present (like the frequency or duration of calls) leading to weighted network representations. One important observationnis that this strength of the interactions varies over many orders of magnitude. A natural conclusion is that the weights play important roles both in the evolution of the networks and in the dynamics of the processes on them. Based on simple rules borrowed from sociology wenconstruct a model, where the emergence of the community structure is a consequence of the interplay between topology and weights. We show that the model reflects well the observations made on a huge call network.nReferences:n[1] J.M. Kumpula, J.-P. Onnela, J. Saramäki, K. Kaski, J. Kertész: Emergence of communities in weighted networks, Phys. Rev. Lett. 99, 228701 (2007)n[2] J.-P. Onnela, J. Saramäki, J. Hyvonen, G. Szabó, D. Lazer, K. Kaski, J. Kertész, A.-L. Barabási: Structure and tie strengths in mobile communication networks, PNAS 104, 7332-7336 (2007)n[3] J.-P. Onnela, J. Saramäki, J. Hyvonen, G. Szabó, M. Argollo de Menezes, K. Kaski, A.-L. Barabási, J. Kertész: Analysis of a large-scale weighted network of one-to-one human communication, New J. Phys. 9, 179 (2007)
6201 en Prefrontal cortex and decision-making The prefrontal cortex has been known to participate in working memory processes. Tonic sustained activation observed during the delay period (delay-period activity) has been considered as a neural correlate of the mechanism for active maintenance of information. Neurophysiological studies revealed that delay-period activity represents retrospective information (e.g., sensory events) as well as prospective information (e.g., forthcoming motor information). This suggests that delay-period activity participates in decision processes regarding motor performances based on the sensory information. To examine how delay-period activity participates in the decision of a motor behavior, we analyzed prefrontal activity while monkeys performed two tasks: ODR and S-ODR tasks. In the ODR task, monkeys were required to make a memory-guided saccade to the cue location after a 3-s delay. In the S-ODR task, four identical visual cues were presented simultaneously during the cue period and monkeys were required to make a saccade in any one direction after the delay. Delay-period activity was observed in both tasks in the same neuron with similar directional preferences. Neurons with delay-period activity were classified into several groups based on the temporal pattern of the activity itself and of the strength of the directional selectivity. Among these, neurons with an increasing type of delay-period activity with persistent directional selectivity throughout the delay period in the ODR task also showed directional delay-period activity in the S-ODR task. These results indicate that an increasing type of delay-period activity, which is thought to represent motor information, plays an important role in generating and enhancing directional bias in the S-ODR task and therefore contributes significantly to the decision process of the saccade direction in the S-ODR task.
6202 en The case of the altruist meme Altruism elicits in humans very powerful and diverse feelings. Its paradoxical nature makes it mysterious and challenging to understand. It is difficult to understand why a rational being would sacrifice its own interests for somebody else, especially if one's reproduction fitness depends on them. It is also difficult to believe that such a trait would survive natural selection. In fact it was shown by rigorous theorems in a wide range of conditions that the Nash stable strategy is to be selfish and not to share. We show that the resources sharing phenotype is highly favored and wins natural selection dramatically if instead of incurring fixed gain and losses, the players gain or loose fixed fractions of their current resources. Thus the solution of the sharing paradox resides in recognizing the random multiplicative (rather then the usual game-theory "additive") character of the real life, society, economic and cultural "games". In this talk I will present analytical and numerical results for different variations of the model along with results from intensive computer simulations that support and clarify these results
6203 en Dynamic Decisions, Multiple Equilibria and Complexity Agent-based models represent the interaction between a multiplicity of agents through dynamic systems, often giving rise to intricate and complex dynamics. We can extend this type of economic analysis by emphasizing that economic agents have memories, form expectations, and are guided by intentional behavior within the context of a certain decision horizon. However, it is important to note that the agents' decisions and actions change the economic environment and affect the system dynamics. The interaction of agents is often stylized as predator-prey, competitive and cooperative interactions in the context of Lotka-Volterra systems. We start with these types of systems and show that economic agents' decisions can be understood as a perturbation term in the general system dynamics. The dynamics of a model with zero time-horizon, which has small effects on the system dynamics, can often be studied analytically and taken as starting point for a numerical analysis with a longer time horizon. Since, due to nonlinearities, multiple equilibria frequently arise, this generates path dependency and complex dynamics. We solve these types of models by using dynamic programming with a flexible grid size that can capture multiple equilibria and threshold and bifurcation behavior. Heterogeneity of agents and multiple attractors predict a bimodal distribution of outcomes which can empirically be verified using Markov transition matrices. We give a number of examples from resource economics, development economics, investment theory, industrial organization, imperfect capital markets, growth, distribution, and climate change. We give prototype examples and illustrate economic mechanisms wherein such complicated dynamics, e.g., those with threshold and bifurcation behavior, can occur. These types of models can not only be empirically tested, but have strong policy implications in the sense that policy can tilt the dynamics toward superior equilibria and increase the domain of attraction for preferable equilibria.
6204 en Strong random correlations in complex systems Complex systems (living organisms, the brain, society, the economy, etc.) seem to depend on a huge number of details which makes them nearly irreducible, so that they cannot be described in terms of a small number of variables. This poses fundamental difficulties for the modeling of such systems and the parametrization or calibration of any model that we may propose to describe them. Furthermore, this irreducibility also implies the existence of strong random correlations between a large number of the components of the system that are not necessarily close neighbours in a geometric sense, or not necessarily linked by strong, direct interactions. This makes the system sensitive to changes in the external control parameters, to boundary conditions, etc., and poses a serious challenge to computer simulations. These ideas are illustrated on some toy models: a spin glass, a random cellular automaton, and a game theoretical model.
6205 en A Paradigmatic Complex System: The Immune System The immune system provides accessible data about the evolution, the organizational operation and the end function of a complex system; immunology exemplifies the evolution, the organizational sociology and the end function of complex systems’ research. I shall discuss select issues related both to the system and to the science that studies the system.nThe system: Evolution (from innate to adaptive immunity); Organization (degeneracy of recognitions and interactions; pleiotropic and redundant agents); End function (body protection or body maintenance). The science: Evolution (from clonal selection to systems biology; the quest for optimums); Organization (molecular biology versus physiology); End function (prevention and cure of disease).
6206 en Rule Rationality vs. Act Rationality 
6207 en A Century of Controversy over the Foundations of Mathematics I'll tell the dramatic story of the recent disputes over the foundations of mathematics. I'll start with the problems in Cantor's theory of infinite sets and then discuss the work of Bertrand Russell, David Hilbert, Kurt Godel and Alan Turing, and finally my own work using complexity.nThis complexity-based analysis of the foundations of mathematics suggests to me that perhaps mathematics is more similar to physics and to biology than is commonly believed, and should sometimes be carried out quasi-empirically, that is, more in the spirit of an experimental science.
6208 en The Architecture of Ecological Interactions: Patterns and Principles Descriptions of complex feeding relationships among species in ecosystems first appeared more than a century ago, and the quantitative analysis of the network structure of “food webs” dates back several decades. Improvements in food-web data collection, analysis, and modeling, coupled with a resurgence of interdisciplinary research on the topology of many kinds of “real-world” networks, have resulted in renewed interest in ecological network structure. Recent research suggests that food webs display universal scale-dependent patterns in how trophic links and roles are distributed among species in ecological communities. The fundamental ways in which feeding interactions are organized appears to have become established very early in the history of multicellular life on earth. Understanding the principles that underlie robust food-web patterns represents an important frontier of ecological research.
6209 en Artifacts and Organization: A Complexity Perspective on Innovation and Social Change Human sociocultural life is impossible to conceive without two fundamental ingredients: artifacts and organizations. Just about everything we do involves interactions with artifacts, from the clothes we wear and the buildings we inhabit, to the devices through which we communicate with one another and the tools and technologies we use to make ever more artifacts. And almost all of our interactions depend for their setting, purpose and rules on organizations, whether they be churches, businesses, government agencies, political parties, law courts, police forces, armies, social clubs – or even friendship networks on internet.nWe human beings didn’t invent either artifacts or organizations: biological evolution did. Both fashioning artifacts and deploying collective action are evolutionary strategies that have been around a long time. But even if we didn’t invent them, nothing in biology remotely compares with the use that we human beings have made of these two strategies. The number and complexity of the artifacts we have developed over the millennia, and in particular over the past few centuries, and the variety of activities we have organized around these artifacts, has no counterpart in the pre-human world. If three million years ago, our ancestors had essentially one kind of artifact, and fifty thousand years ago, maybe several hundred, today’s inhabitant of New York City can choose among 1010 different bar-coded items, not to mention a host of other material, informational or performative artifacts currently produced by human beings for the use of human beings! Even more unprecedented are the diversity of forms and the scale of the organizations we have created, through which we collectively carry out political, economic, social and cultural functions that seem far removed from the overriding biological functional imperatives of survival and reproduction.nOver the past several years, my colleagues and I have been working out a complexity-based theory of innovation that is intended to explain how human beings have managed to generate the explosion of artifacts and the new functionalities they make possible. The theory starts from the premise that all artifacts have a history, as do the modes of interaction among people in which artifacts figure. The aim of the theory is to describe and analyze the processes through which artifact histories are realized:n• How do new artifact types come into being?n• How do their tokens proliferate and become incorporated into patterns of human interaction?n• And how are new patterns of interaction among human beings and artifacts generated?nAs I will argue in the talk, we cannot begin to answer these questions without developing simultaneously a theory of sociocultural organizations: what they are, how they come into being, how they transform themselves.nThe main conclusion of the talk is that our species has developed a new modality of innovation, in which artifacts and organization are inextricably linked: human beings generate new artifacts that they embed in new collective activities, which are in turn supported by new organizations and sustained by new values. Over time, this new innovation modality gave rise to a positive feedback dynamic, which we call exaptive bootstrapping. Exaptive bootstrapping explains how we have generated so many transformations in our selves, our societies, our culture and our environment.
6210 en nformation processing in cortical neural networks with dynamic synaptic connections Synaptic transmission in the cortex is characterized by the activity-dependent short-term plasticity (STP), which can be broadly classified as synaptic depression and synaptic facilitation. As recent experiments indicate, different cortical areas exhibit variable mixes of facilitation and depression, which are also specific for connections between different types of neurons. In the first half of my presentation, I will describe the basics of dynamic synaptic transmission, its biophysical underpinnings and the ways it can be captured in biophysically motivated phenomenological models. I will also discuss some immediate implications of STP on information transmission between ensembles of neocortical neurons. In the second half of the presentation, I will focus on the effects of STP on the dynamics of recurrent networks and resulting neural computation. I will introduce the 'population spikes' (PSs), which are brief epochs of highly ynchronized activity that emerge in recurrent networks with dominating synaptic depression between excitatory neurons. PSs can underlie some of the response properties of neurons in the auditory cortex. I will then describe the recently introduced idea that synaptic facilitation could be utilized in order to maintain information about the incoming stimuli in the facilitation level of recurrent connections between the targeted neurons, thus providing an effective mechanism for short-term memory for a period of several seconds after the termination of the stimulus.
6211 en Predicting Climate Change? I will review the different components of the climate system and try to demonstrate why its behavior is very hard to predict. I will then continue discussing the relative roles of anthropogenic and solar climate driving, and its implications to future climate change (which is not as bleak as most often promoted!)
6212 en Marked to Market Leverage with Zero-Intelligent Agents In liquid markets real-time mark-to-market portfolio valuations may not be expected to impact prices. However during illiquid periods with leveraged trading such settlement can have a significant impact on price volatility and trading-choices. While the assumption of efficient markets confers speculative traders with stabilizing attributes, liquidity or settlement trades can have destabilizing properties in the short run: traders buy when prices rise, and sell when prices fall, to meet collateral requirements. If there are a lot of traders in the market, this situation can become cumulative, producing positive autocorrelation in returns and volatility clustering. Such price dynamics, and a corresponding drying up of market liquidity, can occur even when traders are zero-intelligent, that is, their price expectations or risk aversion does not change in response to price changes.
6213 en Decoding mental states from human brain activity Is it possible to predict what a person is thinking of - or even what they are planning to do - based alone on their current brain activity? Recent advances have made it possible to decode and predict a person's thoughts from functional magnetic resonance imaging (fMRI) data. The key is that each thought is associated with a unique brain activation pattern that can be used as a signature or for that specific thought. It is possible to train pattern classifiers to recognize these characteristic signatures and thus read out a person's thoughts from their brain activity alone. This research can reveal how neural representations of mental contents are stored and transformed in the brain. It also gives rise to many potential applications, as for example in the control of computers and artificial prostheses by brain activity or in the detection of concealed mental states.
6214 en Dynamics of Information and Evaluation on Social Networks The lecture will concern dynamics of two different processes occurring in social networks: the flow of information and the process of evaluation. Similarities and differences in how networks structure shapes the spread of information and governs social influence in the process of evaluation will be discussed. Research in social psychology suggests that information is not only acquired but also evaluated and interpreted in the process of interaction, as individuals construct common social representation. Both simulation and empirical data show that transmission of information and evaluations operate in a very different ways. Empirical data concerning the structure of selected social networks and dynamics of information on the networks will be presented.
6215 en Complex Patterns in Reactive-Wetting Interface Dynamics Reactive-wetting interface dynamics exhibits complex spatio-temporal patterns during the kinetic roughening process of the triple line. This could seemingly be described by scaling (growth and roughness) exponents. However, we show that the non-linear interface dynamics is much more complex. Using extreme value statistics, in particular the persistence measure, we demonstrate the difficulties to associate a given universality class to this complex system. Our reactive-wetting system, which is the only known system in room temperature, consists of small mercury droplets (150m in diameter) spreading on thin silver films (2000 – 4000 A). The process is monitored using an optical microscope.nnnIn this talk we discuss the growth and roughness exponents of the propagating interface, the temporal interface width fluctuations during a single growth process, and the lateral correlation length along the triple line – all as a function of the silver substrate roughness and the temperature of the system. We then introduce the persistence measure in order to demonstrate the complexity of the system, and suggest several numerical models to obtain better insights regarding the microscopic physical mechanisms that play a role in the process.
6216 en Non-Genetic Individuality in a Predator-Prey system Isogenic bacteria can exhibit a range of phenotypes, even in homogeneous environmental conditions. Such non-genetic individuality has been observed in a wide range of biological processes, including differentiation and stress response(1). A striking example is the heterogeneous response of bacteria to antibiotics, whereby a small fraction of drug-sensitive bacteria can persist under extensive antibiotic treatments. Recently, a renewed interest in the persistence phenomenon has revealed that non-genetic heterogeneity might be one of the main reasons for the failure of antibiotic treatment in infections such as tuberculosis, where a single persistent bacterium can re-start an infection(2). Persistence is typically observed through the monitoring of the survival fraction of a bacterial population exposed to antibiotics. The initially rapid killing of the bacteria is followed by a significantly reduced killing rate which indicates the presence of a persistent sub-population. When cells grown from this persistent sub-population are subjected again to antibiotics, the same bi-phasic killing curve is obtained, suggesting that the persistent sub-population is not genetically different from the original population. We have previously shown that persistent bacteria enter a phenotypic state, identified by slow growth or dormancy, which protects them from the lethal action of antibiotics(3). Here we studied the effect of persistence on the interaction between Escherichia coli and phage lambda. We focused on two different variations of this well-studied predator-prey system: (a) a phage that is present in the genome of each bacterium and can cause bacterial death by a process called "prophage induction" and (b) a lytic phage that attacks bacteria from the outside, infects them and them kills them. The effect of the persistent phenotype was studied in those systems and the experimental results obtained were then implemented in a mathematical description of these interactions(4).nWe used long-term time-lapse microscopy to follow the expression of GFP under the phage lytic promoter, as well as cellular fate, in single infected bacteria. We found that dormancy that protects bacteria under antibiotic treatments also protects them against prophage induction. A competition experiment run between a low persistence population and a high persistence one demonstrated a clear advantage to the latter. This suggests that persistence might have evolved under the evolutionary pressure of prophage stress. Intriguingly, we found that, while persistent bacteria are protected from prophage induction, they are not protected from lytic infection. Quantitative analysis of gene expression revealed that the expression of lytic genes is suppressed in persistent bacteria. However, when persistent bacteria switch to normal growth, the infecting phage resumes the process of gene expression, ultimately causing cell lysis.nDespite its mild effect on the short-term survival of the population, the delayed cell lysis of persistent bacteria needs to be taken in account. Using a mathematical model for this predator-prey interaction, we found that the bacteria's non-genetic individuality can significantly affect the population dynamics, and might be relevant for understanding the co-evolution of bacterial hosts and phages.nReferencesn1. Rando, O. J. & Verstrepen, K. J. (2007) Timescales of genetic and epigenetic inheritance Cell 128, 655-668.n2. Stewart, G. R., Robertson, B. D., & Young, B. D. (2003) Tuberculosis: A problem with persistence Nature Reviews: Microbiology 1, 97-105.n3. Balaban, N. Q., Merrin, J., Chait, R., Kowalik, L., & Leibler, S. (2004) Bacterial persistence as a phenotypic switch Science 305, 1622-1625.n4. Pearl, S., Gabay, C., Kishony, R., Oppenheim, A., & Balaban, N. Q. (2008) Nongenetic individuality in the host-phage interaction PLoS Biol 6, e120.
6217 en Epigenomics and Morphodynamics The substrate for heredity, DNA, is chemically rather inert. However, it bears one of the elements of information that specify the form of the organism. How can a form be specified, starting from DNA ? Recent observations indicate that the dynamics of transcription — the process that decodes the hereditary information — can imprint forms of a certain topological class onto DNA. This topology allows both to optimize transcription and to facilitate the concerted change of the transcriptional status in response to environmental modifications. To the best of our knowledge, this morphogenetic event is first on the path from DNA to organism.
6219 en Neural mechanisms of working memory in the prefrontal cortex Working memory is a mechanism for short-term active maintenance of information as well as for processing maintained information. The dorsolateral prefrontal cortex (DLPFC) has been known to participate in working memory. The analysis of task-related DLPFC activity while monkeys performed a variety of working memory tasks revealed that delay-period activity is a neural correlate of a mechanism for temporary active maintenance of information, because this activity persisted throughout the delay period, showed selectivity to a particular visual feature, and was related to correct behavioral performances. On the other hand, information processing can be considered as a change of the information represented by a population of neurons during the progress of the trial. Using population vectors calculated by a population of task-related DLPFC activities, we demonstrated the temporal change of information represented by a population of DLPFC neurons during performances of spatial working memory tasks. Cross-correlation analysis using spike firings of simultaneously isolated pairs of neurons reveals widespread functional interactions among neighboring neurons, especially neurons having delay-period activity, and their dynamic modulation depending on the context of the trial. Functional interactions among neurons and their dynamic modulation could be a mechanism of information processing in the working memory processes.
6220 en Can attractor network models account for the statistics of firing during persistent activity in prefrontal cortex? Persistent activity observed in neurophysiological experiments in monkeys is thought to be the neuronal correlate of working memory. Over the last decade, network modelers have strived to reproduce the main features of these experiments. In particular, attractor network models have been proposed in which there is a coexistence between a non-selective attractor state with low background activity with selective attractor states in which sub-groups of neurons fire at rates which are higher (but not much higher) than background rates. A recent detailed statistical analysis of the data seems however to challenge such attractor models: the data indicates that firing during persistent activity is highly irregular (with an average CV larger than 1), while models predict a more regular firing process (CV smaller than 1). I will discuss how this feature can be reproduced in a network of excitatory leakly integrate-and-fire neurons.
6221 en The computational limitations of balanced networks Computation in neural networks relies crucially on non-linearity. In neural networks in the balanced state the non-linearity of the neuronal transfer function becomes functionally unimportant. The disappearance of this non- linearity strongly limits the computational power of balanced networks. I will show in examples of balanced networks for associative memory how one can try to circumvent this limitation of balanced networks and discuss the problems with these solutions.
6222 en Balanced spatial working memory Neural activity persisting for several seconds is thought to be the neural correlate of working memory in cortex. It was found recently that during persistent activity spike trains are highly irregular, even more irregular than in spontaneous activity. We show that this apparently innocuous feature raises a fundamental difficulty if one holds that neuronal nonlinearities combined with recurrent excitation underly activity persistence as usually assumed. Instead, we argue that the key nonlinearities involved are synaptic and not neuronal. We assess this proposal in the framework of a network model representing a circuit in prefrontal cortex involved in spatial working memory. This lead us to suggest that short term plasticity recently discovered in synapses made by pyramidal cells in prefrontal cortex is crucial in spatial working memory.
6223 en Active memory maintenance with short-term synaptic facilitatio Current theoretical framework holds that information is actively maintained in working memory through enhanced firing rates (delay activity). This would be achieved either via persistent activity reverberation within selective neural populations or as a result of intrinsic single-cell properties (i.e. bi-stability). Electrophysiological studies show, however, that delay activity increase can be modest, sometimes completely disappearing during part of the delay period. We therefore propose a new theoretical framework whereby working memory is sustained by calcium-mediated synaptic facilitation in the recurrent connections of neocortical networks. In this account, the presynaptic residual calcium is used as a 'buffer' which is loaded, refreshed and read-out by spiking activity. Due to the long time constants of calcium kinetics, the refresh rate can be very low, which results in a mechanism that is metabolically efficient and resistant to external interferences. The duration and stability of working memory can be effectively regulated by modulating the spontaneous activity in the network. Joint work with: Omri Barak and Misha Tsodyks
6224 en Working memory for saccadic eye movements in the parietal cortex Area LIP of the parietal cortex is related to saccades, to visual attention, and additional related cognitive processes, and contains neurons that show persistent activity in memory-guided saccades. The talk will focus on comparisons of the neuronal activity (1) during memory-saccades towards versus opposite the target's direction (prosaccades and antisaccades), and (2) during memory-saccades comprising unguided choice versus guided-choice. We will consider computational problems and other implications arising from the results.
6225 en The Computational Principles and Neural Mechanism Underlying Contraction Bias It is well established that the estimated magnitude of memorized stimuli is biased: small magnitudes are overestimated and large magnitudes are underestimated, a phenomenon known as 'contraction bias.' In a previous study monkeys were trained to memorize the frequency of a vibrotactile stimulus (Base) and compare it with the frequency of a second stimulus (Comparison) while single unit activity was recorded in their prefrontal cortex (Romo et al. (1999), Nature, 399:470-473). We identified that the pattern of errors made by the monkeys is consistent with the contraction bias, providing an opportunity to study this phenomenon both at the level of behavior and at the level of neural activity. Here we address two questions: (1) What are the computational principles and (2) the neural mechanisms underlying the contraction bias?n(1) We show that contraction bias is consistent with Bayesian inference, in which a noisy measurement is combined with a-priori knowledge about the distribution of Base magnitudes in order to improve performance. According to the Bayesian hypothesis, increasing the level of uncertainty in the magnitude of the memorized stimulus enhances the bias. This uncertainty is a function of the delay between the Base and Comparison frequencies, as the performance level of the monkey decreases with the duration of the delay. Indeed, as expected from the Bayesian hypothesis, the longer the delay between the Base and Comparison frequencies, the greater the bias. According to the Bayesian hypothesis, monkeys utilize the prior distribution of Base frequencies in their decision making process. In order to study how the monkeys estimate this prior distribution, we analyzed the dependence of the monkeys' decisions on the recent history of stimuli presented to them in the experiment. We show that the estimated prior distribution depends mostly on the recent history of several previous trials. (2) The firing rate of many prefrontal cortex neurons during the delay period is a monotonic function of the Base stimulus frequency. It has been suggested that decisions in the discrimination task are made by comparing this activity with the neural representation of the Comparison frequency. Thus, the contraction of the memorized frequencies should be reflected in the activity of the prefrontal cortex neurons, resulting in the biased decisions. By studying how past trials affect neural activity in the prefrontal cortex, we seek to identify the neural correlate of the contraction bias.
6226 en Mechanisms of mGluR mediated plateau potentials in entorhinal cortex neurons It is well established that the estimated magnitude of memorized stimuli is biased: small magnitudes are overestimated and large magnitudes are underestimated, a phenomenon known as 'contraction bias.' In a previous study monkeys were trained to memorize the frequency of a vibrotactile stimulus (Base) and compare it with the frequency of a second stimulus (Comparison) while single unit activity was recorded in their prefrontal cortex (Romo et al. (1999), Nature, 399:470-473). We identified that the pattern of errors made by the monkeys is consistent with the contraction bias, providing an opportunity to study this phenomenon both at the level of behavior and at the level of neural activity. Here we address two questions: (1) What are the computational principles and (2) the neural mechanisms underlying the contraction bias?n(1) We show that contraction bias is consistent with Bayesian inference, in which a noisy measurement is combined with a-priori knowledge about the distribution of Base magnitudes in order to improve performance. According to the Bayesian hypothesis, increasing the level of uncertainty in the magnitude of the memorized stimulus enhances the bias. This uncertainty is a function of the delay between the Base and Comparison frequencies, as the performance level of the monkey decreases with the duration of the delay. Indeed, as expected from the Bayesian hypothesis, the longer the delay between the Base and Comparison frequencies, the greater the bias. According to the Bayesian hypothesis, monkeys utilize the prior distribution of Base frequencies in their decision making process. In order to study how the monkeys estimate this prior distribution, we analyzed the dependence of the monkeys' decisions on the recent history of stimuli presented to them in the experiment. We show that the estimated prior distribution depends mostly on the recent history of several previous trials. (2) The firing rate of many prefrontal cortex neurons during the delay period is a monotonic function of the Base stimulus frequency. It has been suggested that decisions in the discrimination task are made by comparing this activity with the neural representation of the Comparison frequency. Thus, the contraction of the memorized frequencies should be reflected in the activity of the prefrontal cortex neurons, resulting in the biased decisions. By studying how past trials affect neural activity in the prefrontal cortex, we seek to identify the neural correlate of the contraction bias.
6227 en Low-dimensional network models for data from the prefrontal cortex During short-term memory maintenance, different neurons in prefrontal cortex (PFC), recorded under identical conditions, show a wide variety of temporal dynamics and response properties [1]. These data are a specific example of the more general finding that neural recordings from frontal cortices often reveal that different neurons have very different response characteristics. Modeling this complexity of responses has been difficult. Most commonly, some features of the responses are focused on, and models that fit those reduced features are built. But can the full complexity of responses be easily captured ? Here we attack the problem by fitting simple recurrent neural network models to the data.nFollowing the traditional approach, we first group neurons into different classes. When selecting neurons from a single class the estimation procedure yields a connectivity matrix with two populations of neurons coupled by mutual inhibition and self-excitation. The connectivity matrix has rank one and approximately agrees with a model we proposed earlier [2]. When selecting neurons from two classes, a connectivity matrix similar to that of the ring attractor network emerges, with a rank of two. The full complexity and richness of the observed neural dynamics, however, can only be captured when estimating a network architecture from the full set of neurons. In this case, the resulting connectivity matrix has rank five and its structure is dominated by randomness. Simulations of the resulting network reproduce the full data set. We show that several of the eigenvalues of the connectivity matrix are close zero, so that the network dynamics has either a constant or integrating flow along the respective dimensions. Finally, we discuss the consistency of the estimated connectivity matrices with the measured noise correlations. [1] Timing and Neural Encoding of Somatosensory Parametric Working Memory in Macaque Prefrontal Cortex. C.D. Brody, A. Hernandez, A. Zainos, and R. Romo, Cereb. Cortex 13:1196-1207, 2003. [2] Flexible control of mutual inhibition: a neural model of two- interval discrimination. C.K. Machens, R. Romo, and C.D. Brody, Science, 307:1121-1124, 2005.
6228 en Mechanism for Top-down Control of Working Memory Capacity Working memory relies on the activation of both prefrontal and parietal cortex, possibly with prefrontal cortex exerting top-down control. However, there is still no mechanistic description of either capacity limitations or top-down control of maintenance of information in working memory. Here, we propose that lateral inhibition in parietal cortex limits mnemonic capacity. However, at high loads, this inhibition can be counteracted by excitatory input from prefrontal cortex, thus boosting parietal capacity. We formulate this computationally in a biophysical cortical microcircuit model, and conceptualize it in a mathematical equation. Predictions from the model were confirmed in an fMRI study. The model provides a mechanistic framework for understanding top-down control of working memory, and specifies two different contributions of prefrontal and parietal cortex to working memory capacity.
6230 en Memory blends How do perceptual memories interact? I will describe results from psychophysical experiments, using sequences of visual stimuli, showing that similarity between subsequent stimuli has a critical impact on the generated memory.
6231 en Short-term memory effects on visual perception Memory traces, stored in the form of attractors or appearing as the result of recent perceptual experience, can actively shape processing and categorization of visual stimuli. Electrophysiology in monkey IT cortex and computational modeling indicate the existence of categorical boundaries in the dynamics of cortical networks. Our psychophysical experiments in humans show that these boundaries can be shifted following recent visual experience, in adaptation and priming paradigms.
6232 en Short term memory traces in neural networks Critical cognitive phenomena such as planning and decision making rely on the ability of the brain to hold information in working memory. Many proposals exist for the maintenance of such memories in persistent activity that arises from stable fixed point attractors in the dynamics of recurrent neural networks. However such fixed points are incapable of storing temporal sequences of recent events. An alternate, and relatively less explored paradigm, is the storage of arbitrary temporal input sequences in the transient responses of a recurrent neural network. Such a paradigm raises a host of important questions. Are there any fundamental limits on the duration of such transient memory traces? How do these limits depend on the size of the network? What patterns of synaptic connections yield good performance on generic working memory tasks? To what extent do these traces degrade in the presence of noise? We use the theory of Fisher information to construct of novel measure of memory traces in neural networks. By combining Fisher information with dynamical systems theory, we find precise answers to the above questions for general linear neural networks. We prove that the temporal duration of a memory trace in any network is at most proportional to the number of neurons in the network. However, memory traces in generic recurrent networks have a short duration even when the number of neurons in the network is large. Networks that exhibit good working memory performance must have a (possibly hidden) feedforward architecture, such that the signal entering at the first layer is amplified as it propagates from one layer to the next. We prove that networks subject to a saturating nonlinearity, can achieve memory traces whose duration is proportional to the square root of the number of neurons. These networks have a feedforward architecture with divergent connectivity. By spreading excitation across many neurons in each layer, such networks achieve signal amplification without saturating single neurons.
6233 en The costs of visual working memory The capacity of visual working memory has been extensively characterized, but little work has investigated how occupying visual memory influences other aspects of cognition and perception. Here we show a novel effect: maintaining an item in visual working memory slows processing of similar visual stimuli during the maintenance period. Subjects judged the gender of computer rendered faces or the naturalness of body postures while maintaining different visual memory loads. We found that when stimuli of the same class (faces or bodies) were maintained in memory, perceptual judgments were slowed. Our results suggest there is interference between visual working memory and perception, caused by visual similarity between new perceptual input and items already encoded in memory.nn
6234 en Mixed neuronal selectivity is important in recurrent neural networks implementing context dependent tasks Higher order animals show the remarkable ability to flexibly adapt their behavior according to the context. The execution of complex cognitive tasks can be modeled as a series of event driven transitions between mental states, each encoding a certain disposition to behavior or a specific sensori-motor decision. In this work, we hypothesize that these mental states are instantiated neuronally by recurrent circuit dynamics, in the form of stable attractors of the neural activity. We show that the mathematical conditions for the attractors and the event driven transitions can be satisfied only if neurons are selective to combinations of internal mental states and sensory stimuli. One possible way to generate such mixed selectivity is to introduce neurons whose afferent connections have random synaptic strengths. This approach has at least three highly desirable features. First, in spite of the combinatorial explosion of possible neurons with mixed selectivity, the number of needed randomly connected neurons grows only linearly with the number of relevant task events and contexts, which makes a reasonably sized network able to execute extremely complex cognitive tasks. Second, the firing patterns of neurons of the simulated proposed network, capture several aspects of the activity recorded in prefrontal cortex and other brain areas involved in a complex cognitive processes. The activity is self-sustaining in the absence of events, rule selective, and highly heterogeneous. Third, the introduction of randomly connected neurons accelerates the convergence of learning algorithms and it can be exploited to rapidly learn complex behavioral tasks. In conclusion we think that mixed selectivity, so widely observed in the living brain, can be an important and general functional principle for executing complex cognitive tasks.nn
6235 en Unconscious determinants of free decisions in the human brain There has been a long debate whether subjectively "free" decisions are determined by brain activity ahead of time. Previous claims that subjective decisions are preceded by brain activity have been highly criticized as inaccuracies in the participants' subjective reports. Also, it has remained unclear whether an intention to act is initiated in motor-related brain regions, or if high-level brain areas are involved. Here we use a combination of statistical pattern recognition and fMRI to show that the outcome of decisions can be decoded from brain activity in prefrontal and parietal cortex even up to ten seconds before they enter awareness. This delay is too long to be accounted for by inaccuracies in measuring the onset of conscious intentions. Instead it presumably reflects the operation of a network of high-level control areas that operate at a slow timescale and begin to prepare an upcoming decision long before it enters awareness. This suggests that our free choices can be determined by brain activity much earlier than commonly appreciated.
6258 en Learning Patterns of the Brain: Machine Learning Challenges of fMRI Analysis Functional Magnetic Resonance Imaging (fMRI) has given neuroscientists and cognitive psychologists incredible power to analyze the deep mysteries of the human brain. With this powerful imaging technology, however, many new challenges have arisen for the statistics and machine learning communities. In this talk, I will present an overview of fMRI and some of the current machine learning challenges. I will discuss recent work on hierarchical Bayesian methods for dealing with high dimensional, sparse data. I will also discuss the application of classical order statistics to the problem of feature selection. Finally, I will show some of our latest results combining a large text corpus with fMRI to produce a generative model of neuro-activation for arbitrary words in the English language.
6259 en Feature Selection via Block-Regularized Regression Identifying co-varying causal elements in very high dimensional feature space with internal structures, e.g., a space with as many as millions of linearly ordered features, as one typically encounters in problems such as whole genome association (WGA) mapping, remains an open problem in statistical learning. We propose a block-regularized regression model for sparse variable selection in a high-dimensional space where the covariates are linearly ordered, and are possibly subject to local statistical linkages (e.g., block structures) due to spacial or temporal proximity of the features.nnOur goal is to identify a small subset of relevant covariates that are not merely from random positions in the ordering, but grouped as contiguous blocks from large number of ordered covariates. Following a typical linear regression framework between the features and the response, our proposed model employs a sparsity-enforcing Laplacian prior for the regression coefficients, augmented by a 1st-order Markovian process along the feature sequence that "activates" the regression coefficients in a coupled fashion. We describe a sampling-based learning algorithm and demonstrate the performance of our method on simulated and biological data for marker identification under WGA.
6260 en Exploiting document structure and feature hierarchy for semi-supervised domain adaptation In this work we try to bridge the gap often encountered by researchers who find themselves with few or no labeled examples from their desired target domain, yet still have access to large amounts of labeled data from other related, but distinct source domains, and seemingly no way to transfer knowledge from one to the other.nnExperimentally, we focus on the problem of extracting protein mentions from academic publications in the field of biology, where the source domain data are abstracts labeled with protein mentions, and the target domain data are wholly unlabeled captions. We mine the large number of such full text articles freely available on the Internet in order to supplement the limited amount of annotated data available.nnBy exploiting the explicit and implicit common structure of the different subsections of these documents, including the unlabeled full text, we are able to generate robust features that are insensitive to changes in marginal and conditional distributions of classes and data across domains. We supplement these domain-insensitive features with automatically obtained high-confidence positive and negative predictions on the target domain to learn extractors that generalize well from one section of a document to another. Similarly, we develop a novel hierarchical prior structure over the features motivated by the common structure of feature spaces for this task across natural language data sets. Finally, lacking labeled target testing data, we employ comparative user preference studies to evaluate the relative performance of the proposed methods with respect to existing baselines.
6261 en An Agent-based Model of Employment, Production and Consumption 
6262 en Breakdown of the mean-field approximation in a wealth distribution model 
6263 en Dissecting the Canon: Visual Subject Co-Popularity Networks in Art Research 
6264 en Triplet Extraction from Sentences In this paper we present a machine learning approach tonextract subject-predicate-object triplets from Englishnsentences. nnSVM is used to train a model on humannannotated triplets, and the features are computed fromnthree parsers.
6265 en Semantic Graphs Derived from Triplets with Application in Document Summarization Information nowadays has become more and morenaccessible, so much as to give birth to an informationnoverload issue. Yet important decisions have to benmade, depending on the available information.nAs it is impossible to read all the relevant content thatnhelps one stay informed, a possible solution would bencondensing data and obtaining the kernel of a text bynautomatically summarizing it.nnWe present an approach to analyzing text andnretrieving valuable information in the form of ansemantic graph based on subject-verb-object tripletsnextracted from sentences. Once triplets have beenngenerated, we apply several techniques in order tonobtain the semantic graph of the document: coreferencenand anaphora resolution of named entitiesnand semantic normalization of triplets. Finally, wendescribe the automatic document summarizationnprocess starting from the semantic representation ofnthe text.nnThe experimental evaluation carried out step by stepnon several Reuters newswire articles shows ancomparable performance of the proposed approachnwith other existing methodologies. For the assessmentnof the document summaries we utilize an automaticnsummarization evaluation package, so as to show anranking of various summarizers.
6266 en Stochastic Subgradient Approach for Solving Linear Support Vector Machines This paper is an overview of a recent approach fornsolving linear support vector machines (SVMs), thenPEGASOS algorithm. The algorithm is based on antechnique called the stochastic subgradient descent andnemploys it for solving the optimization problem posednby the soft margin SVM - a very popular classifier. nnWe briefly introduce the SVM problem and one of thenwidely used solvers, SVM light, then describe thenPEGASOS algorithm and present some experiments.nnWe conclude that the algorithm efficiently discoversnsuboptimal solutions to large scale problems within anmatter of seconds.
6267 en Fuzzy Clustering of Documents This paper presents a short overview of methods for fuzzy clustering and states desired properties for an optimal fuzzy document clustering algorithm. Based on these criteria we chose one of the fuzzy clustering most prominent methods – the c-means, more precisely probabilistic c-means. nnThis algorithm is presented in more detail along with some empirical results of the clustering of 2-dimensional points and documents. For the needs of documents clustering we implemented fuzzy c-means in the TextGarden environment. We show few difficulties with the implementation and their possible solutions. nnAs a conclusion we also propose further work that would be needed in order to fully exploit the power of fuzzy document clustering in TextGarden.
6268 en A Functional Programming Approach to Distance-based Machine Learning Distance-based algorithms for both clustering andnprediction are popular within the machine learningncommunity. These algorithms typically deal with attributevaluen(single-table) data. The distance functions used arentypically hard-coded.nnWe are concerned here with generic distance-basednlearning algorithms that work on arbitrary types ofnstructured data. In our approach, distance functions are notnhard-coded, but are rather first-class citizens that can benstored, retrieved and manipulated. In particular, we cannassemble, on-the-fly, distance functions for complexnstructured data types from pre-existing components.nnTo implement the proposed approach, we use thenstrongly typed functional language Haskell. Haskell allowsnus to explicitly manipulate distance functions. We havenproduced a SW library/application with structured data typesnand distance functions and used it to evaluate the potential ofnHaskell as a basis for future work in the field of distancebasednmachine learning.
6269 en Improving Morphosyntactic Tagging of Slovene by Tagger Combination Part-of-speech (PoS) or, better, morphosyntactic tagging is the process of assigning morphosyntacticncategories to words in a text, an important pre-processing step for most human language technologynapplications. PoS-tagging of Slovene texts is a challenging task since the size of the tagset is over onenthousand tags (as opposed to English, where the size is typically around sixty) and the state-of-the-artntagging accuracy is still below levels desired. The paper describes an experiment aimed at improvingntagging accuracy for Slovene, by combining the outputs of two taggers – a proprietary rule-basedntagger developed by the Amebis HLT company, and TnT, a tri-gram HMM tagger, trained on a handannotatedncorpus of Slovene. The two taggers have comparable accuracy, but there are many casesnwhere, if the predictions of the two taggers differ, one of the two does assign the correct tag. Weninvestigate training a classifier on top of the outputs of both taggers that predicts which of the twontaggers is correct. We experiment with selecting different classification algorithms and constructingndifferent feature sets for training and show that some cases yield a meta-tagger with a significantnincrease in accuracy compared to that of either tagger in isolation.
6270 en Extending Ontologies for Annotating Business News Ontologies are commonly used for annotating textual datanmainly based on human language technologies [1]. Thisnresearch focuses on manual extensions of ontologies tonsupport the annotation of business news. Experiments werenconducted on a well known Cyc ontology and using Cycnannotator on two business news datasets. We show that thenproposed extensions of ontology results in annotation withnbetter coverage of terms that are relevant for the businessndomain. nnThe results of identifying financial terms innbusiness news using the original Cyc ontology show thenaverage precision of 56% and recall of 41% in case ofnReuters news and the average precision of 69% and thenrecall of 57% in case of Yahoo financial news. Using thenproposed extension results with increased performance, thenaverage precision of 82% and average recall of 73% fornYahoo financial news and average precision of 84% andnaverage recall of 63% for Reuters news.
6271 en Semantic Modeling, Translation and Matching of QoS The variety of access and transport technologiesnavailable in modern computer networks pose significantnchallenges related to compatibility and quality of servicen(QoS) related issues. Applications and services can havenmany different and unique requirements towards thentransportation services (TSs) they use to interconnect.nnTraditionally, applications are required to specify theirnQoS requirements in the language which the TSsnunderstand. This results in reformulation of intuitivenparameters (i.e. desired video resolution) to parametersnunderstood by the TSs (i.e. required bandwidth).nnpaper presents techniques for (a) automaticnmatchmaking of application requirements to the offersnby TSs providers and (b) automatic translation ofnapplication requirements into the TSs QoSnrequirements. To this end semantic technologies,nnamely OpenCyc, are used for ontological modeling,ntranslation and matchmaking. We present relevantnexamples on how semantic technologies can be used innthe context of communication networks.
6272 en Hierarchical Annotation of Medical Images In this paper, we describe an approach for the automatic medical annotation task of the 2008 CLEF cross-language image retrieval campaign (ImageCLEF). The data comprise 12076 fully annotated images according to the IRMA code. This work is focused on the process of feature extraction from images and hierarchical multi-label classification. To extract features from the images we used a technique called: local distribution of edges. nnWith this techniques each image was described with 80 variables. The goal of the classification task was to classify an image according to the IRMA code. The IRMA code is organized hierarchically. Hence, as classifer we selected an extension of the predictive clustering trees (PCTs) that is able to handle this type of data. nnFurther more, we constructed ensembles (Bagging and Random Forests) that use PCTs as base classifiers.
6273 en The Statistical Interpretation of Simulated Emergency Breaking Event Time Series Data Over three hundred four-second / 40hz time series datasetsn(from simulated emergency braking manoeuvres at Englishnfatal accident sites and field trials) werenclassified using key characteristics of the braking sequencesnextracted for each event.nnThese characteristics were thenntested for significant difference between road surface typesnand braking system types. One key marker, averagendeceleration, was also compared against existingnbenchmark ‘typical’ values for acceptable performance asnfound in the literature.
6274 en Churn Prediction Model in Retail Banking Using Fuzzy C-Means Clustering The paper presents model based on fuzzy methods for churn prediction in retail banking. The study was done on the real, anonymised data of 5000 clients of a retail bank. Real data are great strength of the study, as a lot of studies often use old, irrelevant or artificial data. Canonical discriminant analysis was applied to reveal variables that provide maximal separation between clusters of churners and non-churners. Combination of standard deviation, canonical discriminant analysis and k-means clustering results were used for outliers detection.nnDue to the fuzzy nature of practical customer relationship management problems it was expected, and shown, that fuzzy methods performed better than the classical ones. According to the results of the preliminary data exploration and fuzzy clustering with different values of the input parameters for fuzzy c-means algorithm, the best parameter combination was chosen and applied to training data set. Four different prediction models, called prediction engines, have been developed. The definitions of clients in the fuzzy transitional conditions and the distance of k instances fuzzy sums were introduced.nnThe prediction engine using these sums performed best in churn prediction, applied to both balanced and non-balanced test sets.
6275 en Text Mining, Information and Fact Extraction (TMIFE) communities (medical informatics, security, blog and news analysis, business information analysis, legal informatics, etc.). ?Still, today it is a somewhat fragmented subfield of human language technologies and information retrieval where the themes of (often forgotten) old-style pattern-based IE and more recent machine learning techniques, as applied in medical informatics, opinion mining and blog extraction, are scattered in various conferences and sessions (computational linguistics, artificial intelligence, machine learning, Web technologies, semantic computing).nThe aim of this tutorial is to explain important technologies from handcrafted patterns to learning, and especially focus on how they blend together in order to suit the needs of current information systems that retrieve or mine information, or that make decisions and solve problems based on the extracted information. This unified perspective also entails valuable insights into the role of traditional pipelined system architectures and more recent probabilistic inference techniques.nProbabilistic extraction, by which text is translated into a variety of semantic labels, pe"../slides/rfectly integrates with probabilistic retrieval models that naturally combine surface text features and semantic labels in ranking computations, among which are the popular language retrieval models. Finally, information extraction alleviates the knowledge acquisition bottleneck in expert and question answering systems technology that operate in more restricted subject domains.nWe conclude with some pointers to new challenges among which are the recognition of complex semantic concepts (e.g., narrative scripts, or issues such as medical malpractice or competitiveness) in texts.nBecause of the reconciling aspects of the many techniques and application domains, the tutorial will attract students and researchers with different backgrounds.
6276 en Content Based Image Retrieval (CBIR) This course will give an overview of the main tasks and methods in the content based image retrieval (CBIR) field.nFirstly we will address a question of image retrieval methods application in real life: what are the real-world problems that can be solved using these methods, where it can be used by human.nThen we will consider the traditional architecture of CBIR-systems and the main problems which should be solved by the developers of such a system. This includes the discussion of image preprocessing and feature extraction, multidimensional indexing, design of user interface and data visualization. Special attention will be given to low-level feature processing (color, texture and shape) and construction of feature vectors. The classification of known feature vectors will be presented and results of experimental comparison will be discussed for some of them.nIn the final part of the course we will briefly review some of the existing CBIR systems (both commercial and research ones) and we will analyze and discuss their advantages and disadvantages.
6277 en IR in Social Media (IRSM) We define Social Media as a user-generated content on a Web. Social Media includes but not limited to: blogs, usenet, forums. The first part of a tutorial is pretty technical and hands-on. We will show specifics of a data acquisition from blogs, microblogs, usenet. We will present our existing data sets and show how to use them. In the second part we will talk about specifics of using obtained data. We will cover keyword extraction and other data mining techniques.nSpam has become a major problem for Internet users and covers web search as well as most aspects of communication including email, IM, discussion forums. The recent popularity of blogging has spurned a surge in blog spam, with many flavors including splogs, comment spam, trackback spam and ping spam. In this talk we will discuss the differences and commonalities of combating spam in the blog medium vs. other types of spam. The exposition will be supported by results and examples based on real data.
6278 en Data Structures in IR (DSIR) The course presents an overview of theoretical and practical approaches to implementation of information retrieval systems. It is mainly focused on classic big and large-scale search problems but also includes brief description of structures applicable for other IR tasks. The course covers a wide range of questions from a high-level theoretical view on data structures design to particular questions of implementation. It includes such important practical problems, which are poorly presented in available educational literature, as parallelization, lossy compressions techniques, and relevant modern hardware features.nThe course contains a discussion of known open source and commercial systems implementations. Some considered examples are based on lecturer’s practical experience from his participation in IR systems development projects.nThe course can be interesting for students who want to know details of IR system implementation or tailoring existing systems for a specific data scale or IR task. It was presented at internal seminars for employees at Ask.com in 2007 and 2008.
6279 en Hands-on Natural Language Processing for Information Access Applications (NLPIAA) This course focus on the development of practical applications which involve the use of natural language technology. The course will introduce NLP concepts which will be reinforced by the development, testing, and evaluation of technology in demonstration sessions. Applications to be studied in the course include: Information Extraction, Question Answering, and Text Summarization. None of the applications will be studied in detail, the main objective of the course is to promote the use of NLP and to facilitate access to available technology which can be adapted to specific application domains so that students can go home motivated to develop their own tools/systems.nDetailed content:n– Overview of Natural Language Processing technologies including parts of speech tagging, named entity recognition, parsing, semantic interpretation and coreference resolution.n– Natural Language Technology for Information access: existent systems and projects combining advanced NLP will be presented (e.g. Cubreporter project).n– Information Extraction: named entity recognition, relation extraction, event extraction, rule-based and machine learning approaches, evaluation, MUC.n– Question Answering: QA architecture, questions and answers, passage selection, answer identification, evaluation, TREC/QA.n– Text Summarization: sentence extraction, superficial features for sentence extraction, feature combination, multi-document summarization, evaluation, Document Understanding Conference.
6281 en Young Scientics Presenatations 
6284 en PhD Thesis Defense: Dynamics of large networks A basic premise behind the study of large networks is that interaction leads to complexncollective behavior. In our work we found very interesting and counterintuitive patterns forntime evolving networks, which change some of the basic assumptions that were made in thenpast. We then develop models that explain processes which govern the network evolution,nfit such models to real networks, and use them to generate realistic graphs or give formalnexplanations about their properties. In addition, our work has a wide range of applications:nit can help us spot anomalous graphs and outliers, forecast future graph structure and runnsimulations of network evolution.nnAnother important aspect of our research is the study of “local” patterns and structuresnof propagation in networks. We aim to identify building blocks of the networks and findnthe patterns of influence that these blocks have on information or virus propagation over thennetwork. Our recent work included the study of the spread of influence in a large personto-nperson product recommendation network and its effect on purchases. We also model thenpropagation of information on the blogosphere, and propose algorithms to efficiently findninfluential nodes in the network.nnA central topic of our thesis is also the analysis of large datasets as certain network propertiesnonly emerge and thus become visible when dealing with lots of data. We analyze thenworld’s social and communication network of Microsoft Instant Messenger with 240 millionnpeople and 255 billion conversations. We also made interesting and counterintuitive observationsnabout network community structure that suggest that only small network clusters exist,nand that they merge and vanish as they grow.
6287 en Motivation 
6288 en Incidence Geometry 
6289 en Incidence Geometry Constructions 
6290 en Residuals and Sections 
6291 en Polarity with Respect to a Circle Configurations - Euclidean Plane, Affine Plane, Projective Plane 
6292 en (V3) Configuration 
6293 en Haar Graphs and Cyclic Configurations 
6294 en Realizations of configurations 
6295 en Celestial and Polycycle Graphs 
6296 en Polycyclic configurations 
6462 en Atmospheric Science in the human dominated era of the Anthropocene Profesor Paul Crutzen z in?tituta Max-Planck v Nem?iji se ukvarja z raziskavami vpliva stratosferske in troposferske kemije na biogeokemijsko kro?enje elementov in spojin ter njene vloge pri klimatskih spremembah. Razlo?il je katalitsko vlogo du?ikovih oksidov pri reakciji z ozonom in tako pojasnil prvo v nizu stratosferskih fotokemijskih reakcij, ki povzro?ajo tanj?anje ozonskega pla??a. Po odkritju pomena klorofluoroogljikovih spojin (CFC) pri pojavu ozonske luknje, je prof. Crutzen s teorijo heterogenih reakcij na povr?ini trdnih delcev v polarnih stratosferskih oblakih razlo?il, zakaj je tanj?anje ozonskega pla??a najbolj izrazito ob za?etku polarnega poletja nad Antarktiko.nnPoleg tega se je ukvarjal tudi s problematiko ozona v troposferi, kjer prihaja zaradi onesna?evanja ozra?ja do povi?anih koncentracij. Tudi na tem podro?ju je bil prof. Crutzen z razlagami reakcijskih mehanizmov in meritvami koncentracij ozona vodilni znanstvenik v svetovnem merilu. Njegovo delo je nagrajeno s ?tevilnimi uglednimi mednarodnimi priznanji.nnZa pionirske dose?ke na podro?ju nastanka in razgradnje ozona v atmosferi je leta 1995 skupaj z Mariom Molino in Sherwoodom Rowlandom prejel Nobelovo nagrado za kemijo.n;n:**//Professor Crutzen starts his talk at the time of 3:50 in the video.//**
6467 en Reflections - looking back at 40 years as an airplane designer 
6468 en "I remember you but not your name" - The brain and proper name retreival / "Spomnim se te, a ne vem, kako se kli?e?" - Mo?gani in priklic lastnih imen Proper names are important in every day life. Forgetting them is a very common source of embarrassment, increasingly more frequent with aging. Their difference with common names has been matter of philosophical speculations and linguistic theories. Until two decades ago, the mechanisms underlying their production were largely unknown and undistinguished from those underlying the production of common names.nnIn this talk, proper name retrieval is compared to common name retrieval. The following questions are addressed:nn  * Are PN (proper names) and common names (CN) processed separately?n  * Can PN processing be located in the brain?n  * Is PN processing more difficult than CN processing?n  * Does PN processing change with age differently with respect to common names?n  * Can neuropsychology of PN be used for clinical purposes?nnThe answer to these questions is sought via the methods of cognitive neuropsychology. Findings in selected clinical cases are shown to be consistent with both philosophical and linguistic theories on the difference between PN and CN. A theoretical information processing model of proper name retrieval, derived from neuropsychological findings, is proposed. The psychological reality of some linguistic theories is demonstrated. n----nLastna imena so pomembna v vsakdanjem ?ivljenju. Pozabljenje lastnih imen je zelo pogost vir zadrege, ki s starostjo postaja vse pogostej?i. Razlike med lastnimi in ob?imi imeni so predmet ?tevilnih filozofskih ?pekulacij in jezikoslovnih teorij. ?e vse do pred dobrima dvema desetletjema pa so bili mehanizmi njihove produkcije prete?no nepoznani in ena?eni s tistimi, ki uravnavajo produkcijo ob?ih imen.nnV tem predavanju, ki primerja priklic lastnih imen s priklicem ob?ih imen, se bo sku?alo odgovoriti na naslednja vpra?anja:nn  * Ali so lastna imena v mo?ganih procesirana lo?eno od ob?ih imen?n  * Ali lahko znotraj mo?gan lociramo predel, ki se ukvarja s procesiranjem lastnih imen?n  * Ali je procesiranje lastnih imen zahtevnej?e od procesiranja ob?ih imen?n  * Ali se procesiranje lastnih imen s starostjo spreminja druga?e kot procesiranje ob?ih imen?n  * Ali je nevropsihologija lastnih imen uporabna tudi v klini?ne namene?nnNa ta vpra?anja bomo odgovorili z metodami kognitivne nevropsihologije. Ugotovitve nekaj klini?nih ?tudij so konsistentne tako s filozofskimi kot z jezikoslovnimi teorijami o razliki med lastnimi in ob?imi imeni. Predstavljen bo informacijsko procesni model priklica lastnih imen, izpeljan iz nevropsiholo?kih ugotovitev. S tem bo dokazana psiholo?ka realnost nekaterih jezikoslovnih teorij.n;n:**//Professor Semenza starts his talk in English at the time of 2:15 within video//**
6471 en A journey to the International Space Station, space radiation and light flashes / Pot na Mednarodno vesoljsko postajo, kozmi?no sevanje in svetlobni bliski Dr. Christer Fuglesang was born on March 18, 1957 in Stockholm, Sweden. He studied at the University of Stockholm, where he obtained his doctorate in Experimental Particle Physics in 1987. He continued his career as an experimental high energy physicist and chiefly worked at CERN (European Organization for Nuclear Research) in Geneva. At CERN, he worked on the UA5 experiment, which studied proton-antiproton collisions at the energy of 540 GeV.nnIn 1980s he became a Fellow of the CPLEAR, through which he also collaborated with Slovene scientists presently working at University of Nova Gorica. The aforementioned international group of scientists studied the violation of most basic symmetries in nature. The group’s most remarkable achievement was the discovery of the arrow of time in the microcosm. He continued his career by working on the project regarding the construction of the world largest hadron collider LHC.nnIn 1990, Dr. Fuglesang returned to Sweden and obtained a position at the Manne Siegbahn Institute of Physics, Stockholm. Two years later, however, he decided to continue his career as an astronaut and joined the Astronaut Corps of the European Space Agency (ESA) in May 1992. In 1995, he was selected member of the backup crew for the Euromir 95 mission. Besides obtaining a licence for a qualified astronaut of the American agency NASA in 1998, he was also awarded the Russian 'Soyuz Return Commander' certificate. In the meantime, Dr. Fuglesang continued with his research and was involved in the SilEye experiment, which investigated light flashes in astronauts’ eyes on the international space station MIR. The same type of research is being continued on the International Space Station (ISS).nnDr. Fuglesang has also initiated the DESIRE project to simulate and estimate the radiation environment inside the ISS.nnEven prior to the accident of the space shuttle Columbia, he was selected crew member of the STS-116, which was with the expected delay launched toward the ISS on December 7 2006. Dr. Fuglesang’s assigned duties within the STS-116 mission are mainly of scientific and technical nature.nnDuring his successful career, Dr. Christer Fuglesang has been presented numerous international awards. Moreover, an Honorary Doctorate from Umeå University in Sweden was conferred upon him in 1999. n----nDr. Christer Fuglesang je ?tudiral na Univerzi v Stockholmu, kjer je leta 1987 doktoriral iz fizike. Svojo kariero je nadaljeval kot eksperimentalni fizik visokih energij in ve?ino ?asa delal v Evropskem laboratoriju za fiziko delcev CERN v ?enevi. Po vrnitvi na ?vedsko se je ?e kmalu odlo?il, da svojo strokovno pot nadaljuje kot astronavt. Maja 1992 se je pridru?il astronavtom Evropske vesoljske agencije ESA. Leta 1995 je postal ?lan rezervne posadke za misijo Euromir 95. Leta 1998 je dobil ?e licenco astronavta ameri?ke agencije NASA in licenco poveljnika ruskega programa Soyuz. V tem ?asu je nadaljeval z raziskovalnim delom.nnDelal je na eksperimentu Sil Eye, kjer je preu?eval svetlobne bliske v astronavtovih o?eh na mednarodni postaji MIR. S tem delom nadaljuje tudi na mednarodni vesoljski postaji ISS. ?e pred nesre?o raketoplana Columbia je postal ?lan posadke STS-116, ki je po pri?akovanih zamudah poletela proti ISS 7. decembra 2006. Njegove naloge v misiji STS-116 so bile predvsem znanstvene in tehni?ne narave.n;n:**//Dr. Fuglesang starts his talk in English at the time 6:35//**
6474 en What an emerging language can tell us about language evolution / Kaj nam nastajajo?i jezik lahko pove o razvoju jezika Al-Sayyid Bedouin Sign Language (ABSL) is the sign language of a small, insular, endogamous community in the Negev desert with a high incidence of genetically recessive profound neurosensory deafness. The first deaf individuals were born about 75 years ago and the number of deaf members of the community now numbers over 100 (in a population of about 3500). ABSL appears to have developed with little or no influence from either neighboring sign languages or the surrounding spoken languages; it is widely used in the community, with at least as many hearing as deaf users; and neither the language nor the deaf signers are stigmatized in the community. For several years, a team of linguists from Israel and the United States has been analyzing the structure of ABSL. Prof. Aronoff will review their work to date, discuss the linguistic structure of ABSL and the implications of their findings for current theories of the nature and evolution of human language.n----nZnakovni jezik Al-Sayyid beduinov je nastal v okviru majhne zaprte skupnosti v pu??avi Negev z velikim odstotkom genetsko pogojene gluhosti. Tekom zadnjih 50 let je skupnost razvila samosvoj znakovni jezik skoraj brez vpliva drugih znakovnih jezikov ali sosednjih govorjenih jezikov, zaradi ?esar je jezik pomemben za razumevanje nastanka in razvoja ?love?kega jezika ter narave ?lovek?ega uma.
6477 en Cloning and embryonic stem cells for regenerative medicine / Kloniranje in embrionalne mati?ne celice za regenerativno medicino Parkinsons, Alzheimers, diabetes and other degenerative diseases could be treated with cell transplantation of new and healthy cells. During the past years the field of regenerative medicine has been focusing on development of therapeutically useful cells from embryonic stem cells. While these cells hold a great potential, their ethicaly controversial derivation has revived research using adult stem cells. Recently, novel technologies have been developed that allow derivation of pluripotent cells from adult somatic tissues. This would allow for derivation of patient-specific, autologous cells for cell therapy. This presentation will discuss the possibilities for the future development of strategies for regenerative medicine.n----nPodro?je regenerativne medicine, ki prou?uje mo?nost zdravljenja Parkinsonove, Alzheimerjeve, sladkorne ter drugih degenerativnih bolezni s presaditvijo novih, zdravih celic, se je v zadnjih letih osredoto?ilo na razvoj celi?nih terapij iz embrionalnih mati?nih celic. Kljub potencialu jih zaradi eti?no spornega izvora nadome??amo z mati?nimi celicami iz odraslih tkiv. V zadnjem ?asu smo razvili nove tehnologije, ki omogo?ajo pridobivanje terapevtskih celic iz odraslih, somatskih virov. To nam bo omogo?ilo razvoj celi?nih terapij, ki bodo razvite iz, in presajene v istega pacienta. Predavanje bo predstavilo mo?nosti za prihodnost celi?ne terapije.
6488 en General introduction to the Tyrosafe project 
6489 en Warm-up presentation on policies of skid resistance 
6490 en Question-starting presentation on policies of skid resistance 
6491 en Policies of noise emissions and of rolling resistance presentation 
6492 en Debate about policies of noise emissions and of rolling resistance 
6493 en Presentation on policy aspects of measurement harmonisation 
6494 en Debate about presentation on policy aspects of measurement harmonisation 
6495 en Concluding remarks - 1st TYROSAFE Workshop 
6496 en The handling of dangerous goods in Switzerland 
6497 en GOOD ROUTE: The aim, the approach and the outcomes 
6498 en EURIDICE Project 
6499 en GOOD ROUTE project 
6500 en TraSer Open Source solution for Tracking and Tracing 
6501 en SMARTFREIGHT 
6502 en Panel discussion: Towards intelligent cargo Panelists:nn* Evangelos Bekiaris, CERTH-HIT (EL) GOOD ROUTEn* Paolo Paganelli, Insiel (IT) EURIDICEn* Jan Tore Pedersen, BMT Ltd. (NO) FREIGHTWISEn* Hans Westerheim SINTEF (NO) SMARTFREIGHTn* Hans Wortmann, University of Groningen (NL) TRASER
6503 en Tracking & Tracing as the Basis for new Logistic Services 
6504 en Current European research in ICT in Freight and Logistics and prospects for the rest of the 7th FP 
6505 en A service based approach to ICT for logistics 
6506 en Outlook on the usage of Active and passive RFID in Logistics 
6507 en M2M connectivity applications for the Logistics sector 
6508 en The integration of freight transport ICT 
6509 en Smart Freight – past and current research 
6510 en INTERSYS – Using RFID for identification and control of shipments, load units a wagons in intermodal transport system 
6511 en Using Multi-Agent Systems for Sustainable Logistics Results from the Dutch Transumo Research Project 
6512 en ICT Support for Regional logistics platforms 
6516 en Introduction 
6517 en Re-thinking scientific teams: competition, conflict and collaboration In this talk, Dr. Gadlin will discuss the organizational barriers and breakdown of collaboration in the scientific community while focusing on recurring themes in collaborative disputes. He then presents means for resolving and responding to conflict confidentially through dispute resolution programs and pre-nuptial agreements between collaborators.
6518 en Nanotechnology Innovation--Two Aspects In this talk, Dr. Kesan will discuss the challenges and issues posed by nanotechnology innovation for patent policy and for granting patent rights commensurate with innovation. Second, he will discuss how the insights from value-sensitive design can be applied to vindicate societal choices and preferences in emerging nanotechnologies.
6519 en Question and Answers for session 1 
6520 en Investment and interpretation: nanotechnology, financial journalism and practical epistemology Studies of the ways in which the media report nanotechnologies, and particularly the ways in which they frame their interpretation, are crucial to an understanding of the formation of public perceptions of what are highly technical areas of scientific endeavour. This talk reports on a research project which, whilst broadly located within this area of concern, looked at the related question of the financial understanding of science: how, in a field characterised by high levels of commercialisation, potential investors get information and make judgments about particular applications, and the significance of the roles played by journalists and other mediators in this process.nnThe focus here is on the practical epistemological strategies that scientific and financial journalists employ to make sense of nanotechnologies. Drawing on interview data, the paper considers the way that these journalists assess claims made about scientific validity and investment potential, and how they negotiate such narrative dilemmas as balancing the need for scepticism in a rhetorically inflated context with the professional requirement to produce an interesting story. It is argued that this analytic focus – on journalists as active interpreters and as actors for whom the understanding of nanotechnologies is a pressing practical problem – provides an important complement both to studies of the framing effects of journalistic copy, and to studies of public understandings of what remains, for most of the public, a relatively arcane field. Moreover, in focusing on where the action currently is, it may inform our knowledge of not just the commercial development of nanotechnologies, but also the formation and development of public opinion.
6521 en Predicting the Future: How Ordinary People Make Sense of Emerging Nanotechnologies This presentation will briefly review quantitative and qualitative data that suggest the general tenor of the current public opinion climate for nanotechnologies, and then identify the key factors that can be expected to affect how people cope with information about any new technology. These include their own underlying values, their levels of trust in key social actors, and the connections they identify with technologies previously encountered, as well as information from media accounts. Public conceptions of potential risks are often broader than those commonly identified in formal risk assessments, encompassing "social risks" such as disruption, displacement, privacy, distribution, regulation, and so on, as well as risks to human health and environmental integrity. While media are only one influence among many, they are regularly accused of exaggerating some risks while ignoring others. Progress toward developing a theory that might predict when and explain why this occurs will be reviewed.
6522 en Question and Answers for session 2 
6523 en Perceiving Nanoscale Phenomena: Interpreting and Disseminating Nanoscale Images Scientific imaging techniques have played an increasingly significant role in nanoscale research. But how should the resulting images be interpreted? What kind of information do they offer about their targets (the objects and relations they are about)? And how should that information be understood and disseminated (in particular, how reliable should the information be)? Given the plurality of images found in nanoscale research, it is unlikely that a single unified account can be articulated that accommodates all the images in question. However, this doesn’t mean that a general framework that helps us address central issues in the interpretation of nanoscale imaging cannot be provided.nnIn this paper, I offer such a framework. It provides a new conceptualization of nanoscale images and their content, by highlighting the following aspects: (a) In order to understand a nanoscale image, it’s crucial to determine which kind of image we are dealing with: How was the image obtained? And which sort of information is it intended to convey? (b) We should then examine the sources of bias that the images may contain: What kind of artifacts may be included in the image? (c) Finally, nanoscale images are often used as exemplars in the domain from which they emerged: How can such images be used as inferential devices that allow researchers to generalize the information provided in a particular image to other samples (whether in the same domain or in related ones)?nnAfter motivating and articulating such a framework, I provide some case studies illustrating how the proposal works, by considering a variety of nanoscale images from microscopy (including probe and electron microscopy).
6524 en Nanotechnology, development and public policy In this talk, Dr. Armstrong will discuss five points to think about in regards to Nanotechnology when dealing with development and public policy issues. Dr. Armstrong focuses on the vagueness of the term nanotechnology, the abstract nature in which it is conceptualized and what level of abstraction would be appropriate when conceptualizing it.
6525 en The Culture of the American University in the Age of Neoliberalism Universities in the United States and across the globe are changing. What is the nature of this change? For nearly twenty years, much of the scholarship and work by journalists on the United States has highlighted increases in conflict of interest, secrecy, proprietary research, loss of unbiased public interest analysts, and distortion of research agendas associated with university-industry research relationships. While these concerns are not entirely misplaced, I argue that the focus on what are egregious violations of academic norms—on dramatic cases—fails to capture a deeper and more difficult to police transformation of the US university. Instead, I believe a fundamental transformation of the culture of university life and academic science, especially, is underway. In this paper, I explore the claims of some of the most high profile recent work on the commercialization of the American university, and I point to a set of examples and indicators that suggest we are seeing a deep transformation of academic culture in the United States.
6526 en Why Managing Research is Not Managing Science In this talk, Dr. Rjeski will discuss the increase and penetration of new nano products into the market and how this is a measure of its success. He will then address the need for responsibility and education to address the public’s desire for full-disclosure, pre-market testing, and third-party testing and research. Finally he will discuss the nanotechnology concerns for the future.
6527 en Question and Answers for session 4 
6528 en Welcome from PIARC Slovenia's first delegate 
6530 en Welcome from european research area 
6531 en Introduction to the symposium from the technical committee 
6532 en Slovenia it's roads and its expertise 
6533 en Research on sustainable paving/surfacing for low volume rural roads in Vietnam 
6534 en Macrotexture and drainability in friction courses: modeling and experiments 
6535 en Feasibility of using Deflectograph data to review drainage network in the UK 
6536 en Surface characteristics of asphalt pavements with synthetic lightweight aggregate 
6537 en Low dry friction - Measurement and imaging 
6538 en Samples of singular detailed texture and skid resisance analysis 
6539 en Evaluation of the effect of grooving of the coeficient of friction of wet runways 
6541 en Tire/Road noise relationshipwith viscoelastic properties of paving materials 
6542 en Thin asphalts surface layers for highways - optimised for low tyre/road noise 
6543 en Statistical properties of road traffic noise emission measurements 
6544 en The new Austrian skid resistance evaluation background based on the correlation of skid resistance values roadstar and braking deceleration of passenger cars 
6545 en Model for assessing risks of road infrastructure 
6549 en Pavement surface defects: Classification and quantification over a road network 
6550 en Modeling the ups and downs of the grip resistance of road surface 
6551 en The weighted longitudinal profile (WLP) technical specification and experiences of Austria and Germany 
6552 en Development of a Half-Car based rutting index 
6553 en Developing the automatic measurement of surface condition on local roads 
6554 en Development of a new 3D transverse laser profiling system for the automatic measurement of road cracks 
6555 en Routine measurements of pavement surface cracks 
6556 en Debate about assement of the pavement (cracking measurement) 
6557 en Protocol Applications of the Automated Distress Analyzer (ADA) 
6558 en From theoretical acoustics studies to implementation on a worksite : a major step towards rolling noise reduction 
6559 en Evaluation expérimentale des regles de conception des virages par l'analyse des interactions véhicule/chaussée 
6560 en Debate about vehicle/road interactions 
6561 en Prediction of Deterioration of Asphalt Pavements by Empirical Mechanistic Model 
6562 en Relationship between surface skid resistance and rate of accidents at major intersections in Dar es Salaam 
6563 en The possibilities of utilising data gathered from laser measurements of pavement surface texture 
6564 en Advanced analysis for singular longitudinal profiles 
6565 en Debate about data analysis tools 
6566 en Quantifying the Impact of Jointed Concrete Pavement Curling and Warping on Pavement Unevenness 
6568 en Application of the Cross-Correlation Technique to Evaluate Profile Data 
6569 en Implementation of a Grinding Simulation Tool in the Profile Viewing and Analysis (ProVAL) Software Tool 
6570 en Debate about data analysis tools 2 
6572 en The long term skid resistance performance of three artificial aggregates used in chipseal surfaces in New Zealand 
6573 en Roadex III: Health issues related to poorly maintained road networks 
6574 en May new road be less safe for a while? Initial skid resistance of wet and dry asphalt pavements in the Netherlands 
6577 en Characteristics of today's concrete surfaces 
6578 en Maintenance practices and standards on pavements at Mexican airports 
6579 en Tyrosafe project 
6580 en Measuring Pavement Condition in Developing Countries: The World Bank's Experience 
6581 en Debate about assessment of the pavements 
6582 en Debate about assessment of the pavements (friction) 
6589 en NSPARQL: A Navigational Language for RDF Navigational features have been largely recognized as fundamental for graph database query languages. This fact has motivated several authors to propose RDF query languages with navigational capabilities. In particular, we have argued in a previous paper that nested regular expressions are appropriate to navigate RDF data, and we have proposed the nSPARQL query language for RDF, that uses nested regular expressions as building blocks. In this paper, we study some of the fundamental properties of nSPARQL concerning expressiveness and complexity of evaluation. Regarding expressiveness, we show that nSPARQL is expressive enough to answer queries considering the semantics of the RDFS vocabulary by directly traversing the input graph. We also show thatnnesting is necessary to obtain this last result, and we study the expressiveness of the combination of nested regular expressions and SPARQL operators. Regarding complexity of evaluation, we prove that the evaluation of a nested regular expression E over an RDF graph G can be computed in time O(|G| · |E|).
6590 en An Experimental Comparison of RDF Data Management Approaches in a SPARQL Benchmark Scenario Efficient RDF data management is one of the cornerstones in realizing the Semantic Web vision. In the past, different RDF storage strategies have been proposed, ranging from simple triple stores to morenadvanced techniques like clustering or vertical partitioning on the predicates. We present an experimental comparison of existing storage strategies on top of the SP2Bench SPARQL performance benchmark suite and put the results into context by comparing them to a purely relational model of the benchmark scenario. We observe that (1) in terms of performance and scalability, a simple triple store built on top of a column-store DBMS is competitive to the vertically partitioned approach when choosing a physical (predicate, subject, object) sort order, (2) in our scenario with real-world queries, none of the approaches scales to documents containing tens of millions of RDF triples, and (3) none of the approaches can compete with a purely relational model. We conclude that future research is necessary to further bring forward RDF data management.
6591 en Anytime Query Answering in RDF through Evolutionary Algorithms We present a technique for answering queries over RDF data through an evolutionary search algorithm, using fingerprinting and Bloom filters for rapid approximate evaluation of generated solutions. Our evolutionary approach has several advantages compared to traditional database-nstyle query answering. First, the result quality increases monotonically and converges with each evolution, offering “anytime” behaviour with arbitrary trade-off between computation time and query results; in addition, the level of approximation can be tuned by varying the size of the Bloom filters. Secondly, through Bloom filter compression we can fit large graphs in main memory, reducing the need for disk I/O during query evaluation. Finally, since the individuals evolve independently, parallel execution is straightforward. We present our prototype that evaluates basic SPARQL queries over arbitrary RDF graphs and show initial results over large datasets.
6592 en The Expressive Power of SPARQL This paper studies the expressive power of SPARQL. The main result is that SPARQL and non-recursive safe Datalog with negation have equivalent expressive power, and hence, by classical results, SPARQL is equivalent from an expressive point of view to Relational Algebra. We present explicit generic rules of the transformations in both directions. Among other findings of the paper are the proof that negationncan be simulated in SPARQL, that non-safe filters are superfluous, and that current SPARQL W3C semantics can be simplified to a standard compositional one.
6593 en An OWL 2 Far? The definition of OWL, the ontology language underlying the Semantic Web, is based on formal representation methods. This provides benefits, in that tools have a firm definition of what they are supposed to do, but can have problems, due to difficulty or expense of building tools or mismatch with needs. The panel will discuss whether the general idea of designing standard Semantic Web languages with steadily increasing power (e.g., the progression from RDF to RDFS to OWL to OWL 2 to …) all based on formal methods is the right way to support the Semantic Web. What level of expressive power does the Semantic Web need? How should standard Semantic Web languages be designed? Does the Semantic Web even need formality?
6594 en Comparing ontology distances: preliminary results There are many reasons for measuring a distance between ontologies. In particular, it is useful to know quickly if two ontologies are close or remote before deciding to match them. To that extent, a distance between ontologies must be quickly computable. We present constraints applying to such measuresnand several possible ontology distances. Then we evaluate experimentally some of them in order to assess their accuracy and speed.
6595 en Folksonomy-based collabulary learning The growing popularity of social tagging systems promises to alleviate the knowledge bottleneck that slows the full materialization of the Semantic Web, as these systems are cheap, extendable, scalable and respond quickly to user needs. However, for the sake of knowledge workflow, one needs to find a compromise between the ungoverned nature of folksonomies and the controlled vocabulary of domain-experts. In this paper, we address this concern by first devising a method that automatically combines folksonomies with domain-expert ontologies resulting in an enriched folksonomy. We then introduce a new algorithm based on frequent itemsets mining that efficiently learns an ontology over the concepts present in the enriched folksonomy. Moreover, we propose a new benchmark for ontology evaluation, which is used in the context of information finding, since this is one of the leading motivations for using ontologies in social tagging systems, to quantitatively assess our method. We conduct experiments on real data and empirically show the effectiveness of our approach.
6596 en Learning Concept Mappings from Instance Similarity Finding mappings between compatible ontologies is an important but difficult open problem. Instance-based methods for solving this problem have the advantage of focusing on the most active parts of the ontologies and reflect concept semantics as they are actually being used. However such methods have not at present been widely investigated in ontology mapping, compared to linguistic and structural techniques. Furthermore, previous instance-based mapping techniques were only applicable to cases where a substantial set of instances was available that was doubly annotated with both vocabularies. In this paper we approach the mapping problem as a classification problem based on the similarity between instances of concepts. This has the advantage that no doubly annotated instances are required, so that the method can be applied to any two corpora annotated with their own vocabularies. We evaluate the resulting classifiers on two real-world use cases, one with homogeneous and one with heterogeneous instances. The results illustrate the efficiency and generality of this method.
6597 en Instanced-based mapping between thesauri and folksonomies The emergence of web based systems in which users can annotate items, raises the question of the semantic interoperability between vocabularies originating from collaborative annotation processes, often called folksonomies, and keywords assigned in a more traditional way. If collections are annotated according to two systems, e.g. with tags and keywords, the annotated data can be used for instance based mapping between the vocabularies. The basis for this kind of matching is an appropriate similarity measure between concepts, based on their distribution as annotations. In this paper we propose a new similarity measure that can take advantage of some special properties of user generated metadata. We have evaluated this measure with a set of articles from Wikipedia which are both classified according to the topic structure of Wikipedia and annotated by users of the bookmarking service del.icio.us. The results using the new measure are significantly better than those obtained using standard similarity measures proposed for this task in the literature, i.e., it correlates better with human judgments. We argue that the measure also has benefits for instance based mapping of more traditionally developed vocabularies.
6598 en Collecting Community-Based Mappings in an Ontology Repository Several ontology repositories provide access to the growing collection of ontologies on the Semantic Web. Some repositories collect ontologies automatically by crawling the Web; in other repositories, users submit ontologies themselves. In addition to providing search across multiple ontologies, the added value of ontology repositories lies in the metadata that they may contain. This metadata may include information provided by ontology authors, such as ontologies’ scope and intended use; feedback provided by users such as their experiences in using the ontologies or reviews of the content; and mapping metadata that relates concepts from different ontologies. In this paper, we focus on the ontology-mapping metadata and on community-based method to collect ontology mappings. More specifically, we develop a model for representing mappings collected from the user community and the metadata associated with the mapping. We use the model to bring together more than 30,000 mappings from 7 sources. We also validate the model by extending BioPortal–a repository of biomedical ontologies that we have developed—to enable users to create single concept-to-concept mappings in its graphical user interface, to upload and download mappings created with other tools, to comment on the mappings and to discuss them, and to visualize the mappings and the corresponding metadata.
6599 en Algebras of ontology alignment relations Correspondences in ontology alignments relate two ontology entities with a relation. Typical relations are equivalence or subsumption. However, different systems may need different kinds of relations. We propose to use the concepts of algebra of relations in order to express the relations between ontology entities in a general way. We show the benefits in doing so in expressing disjunctive relations, merging alignments in different ways, amalgamating alignments with relations of different granularity, and composing alignments.
6600 en RDF123: from Spreadsheets to RDF We describe RDF123, a highly flexible open-source tool for translating spreadsheet data to RDF. Existing spreadsheet-to-rdf tools typically map only to star-shaped RDF graphs, i.e. each spreadsheet row is an instance, with each column representing a property. RDF123, on the other hand, allows users to define mappings to arbitrary graphs, thus allowing much richer spreadsheet semantics to be expressed. Further, each row in the spreadsheet can be mapped with a fairly different RDF scheme. Two interfaces are available. The first is a graphical application that allows users to create their mapping in an intuitive manner. The second is a Web service that takes as input a URL to a Google spreadsheet or CSV file and an RDF123 map, and provides RDF as output.
6601 en Evaluating long-term use of the Gnowsis Semantic Desktop for PIM The Semantic Desktop is a means to support users in Personal Information Management (PIM). Using the open source software prototype Gnowsis, we evaluated the approach in a two month case study in 2006 with eight participants. Two participants continued using the prototype and were interviewed after two years in 2008 to show their long-term usage patterns. This allows us to analyse how the system was used for PIM. Contextual interviews gave insights on behaviour, while questionnaires and event logging did not. We discovered that in the personal environment, simple has-Part and is-related relations are sufficient for users to file and re-find information, and that the personal semantic wiki was used creatively to note information.
6602 en Bringing the IPTC News Architecture into the Semantic Web For easing the exchange of news, the International Press Telecommunication Council (IPTC) has developed the NewsML Architecture (NAR), an XML-based model that is specialized into a number of languages such as NewsML G2 and EventsML G2. As part of this architecture, specific controlled vocabularies, such as the IPTC News Codes, are used to categorize news items together with other industry-standard thesauri. While news is still mainly in the form of text-based stories, these are often illustrated with graphics, images and videos. Media-specific metadata formats, such as EXIF, DIG35 and XMP, are used to describe the media. The use of different metadata formats in a single production process leads to interoperability problems within the news production chain itself. It also excludes linking to existing web knowledge resources and impedes the construction of uniform end-user interfaces for searching and browsing news content.nIn order to allow these different metadata standards to interoperate within a single information environment, we design an OWL ontology for the IPTC News Architecture, linked with other multimedia metadata standards. We convert the IPTC NewsCodes into a SKOS thesaurus and we demonstrate how the news metadata can then be enriched using natural language processing and multimedia analysis and integrated with existing knowledge already formalized on the Semantic Web. We discuss the method we used for developing the ontology and give rationale for our design decisions. We provide guidelines for re-engineering schemas into ontologies and formalize their implicit semantics. In order to demonstrate the appropriateness of our ontology infrastructure, we present an exploratory environment for searching and browsing news items.
6603 en Semantic Web Service Choreography: Contracting and Enactment The emerging paradigm of service-oriented computing requires novel techniques for various service-related tasks. Along with automated support for service discovery, selection, negotiation, and composition, support for automated service contracting and enactment is crucial for any large scale service environment, where large numbers of clients and service providers interact. Many problems in this area involve reasoning, and a number of logic-based methods to handle these problems have emerged in the field of Semantic Web Services. In this paper, we build upon our previous work where we used Concurrent Transaction Logic (CTR) to model and reason about service contracts. We significantly extend the modeling power of the previous work by allowing iterative processes in the specification of service contracts, and we extend the proof theory of CTR to enable reasoning about such contracts. With this extension, our logic-based approach is capable of modeling general services represented using languages such as WS-BPEL.
6604 en Formal Model for Semantic-Driven Service Execution Integration of heterogeneous services is often hard-wired in service or workflow implementations. In this paper we define an execution model operating on semantic descriptions of services allowing flexible integration of services with solving data and process conflicts where necessary. We implement the model using our WSMO technology and a case scenario from the B2B domain of the SWS Challenge.
6605 en Efficient Semantic Web Service Discovery in Centralized and P2P Environments Efficient and scalable discovery mechanisms are critical for enabling service-oriented architectures on the Semantic Web. The majority of currently existing approaches focuses on centralized architectures, and deals with efficiency typically by pre-computing and storing the results of the semantic matcher for all possible query concepts. Such approaches, however, fail to scale with respect to the number of service advertisements and the size of the ontologies involved. On the other hand, this paper presents an efficient and scalable index-based method for Semantic Web service discovery that allows for fast selection of services at query time and is suitable for both centralized and P2P environments. We employ a novel encoding of the service descriptions, allowing the match between a request and an advertisement to be evaluated in constant time, and we index these representations to prune the search space, reducing the number of comparisons required. Given a desired ranking function, the search algorithm can retrieve the top-k matches progressively, i.e., better matches are computed and returned first, thereby further reducing the search engine’s response time. We also show how this search can be performed efficiently in a suitable structured P2P overlay network. The benefits of the proposed method are demonstrated through experimental evaluation on both real and synthetic data.
6606 en ELP: Tractable Rules for OWL 2 We introduce ELP as a decidable fragment of the SemanticnWeb Rule Language (SWRL) that admits reasoning in polynomialntime. ELP is based on the tractable description logicnEL++, and encompasses an extended notion of the recentlynproposed DL rules for that logic. Thus ELP extends EL++nwith a number of features introduced by the forthcomingnOWL 2, such as disjoint roles, local reflexivity, certain rangenrestrictions, and the universal role.We present a reasoning algorithmnbased on a translation of ELP to Datalog, and thisntranslation also enables the seamless integration of DL-safenrules into ELP.While reasoning with DL-safe rules as such isnalready highly intractable, we show that DL-safe rules basednon the Description Logic Programming (DLP) fragment ofnOWL 2 can be admitted in ELP without losing tractability.
6607 en Term Dependence on the Semantic Web A large amount of terms (classes and properties) have been published on the Semantic Web by various parties, to be shared for describing resources. Terms are defined based on other terms, and thus a directed dependence relation is formed. The study of term dependence is a foundation work and is important for many other tasks, such as ontology maintenance, integration, and distributed reasoning on the Web scale. In this paper, we analyze the complex network characteristics of the term dependence graph and the induced vocabulary dependence graph. The graphs analyzed in the experiments are constructed from a large data set that contains 1,278,233 terms in 3,039 vocabularies. The results characterize the current status of schemas on the Semantic Web in many aspects, including degree distributions, reachability, and connectivity.
6608 en Semantic Relatedness Measure Using Object Properties in an Ontology This paper presents a new semantic relatedness measure on ontologies which considers especially the object properties between the concepts. Our approach relies on two hypotheses. Firstly, using only concept hierarchy and object properties, only a few paths can be considered as “semantically corrects” and these paths obey to a given set of rules. Secondly, following a given edge in a path has a cost (represented as a weight), which depends on its type ( $is\mbox{-}a$ , $part\mbox{-}of$ , etc.), its context in the ontology and its position in this path. We propose an evaluation of our measure on the lexical base WordNet using $part\mbox{-}of$ relation with two different benchmarks. We show that, in this context, our measure outperforms the classical semantic measures.
6609 en A Process Catalog for Workflow Generation As AI developers increasingly look to workﬂow technologies to perform complex integrations of individual software components, there is a growing need for the workﬂow systems to have expressive descriptions of those components. They must know more than just the types of a component’s inputs and outputs; instead, they need detailed characterizations that allow them to makenﬁne-grained distinctions between candidate components and between candidate workﬂows. This paper describes PROCAT, an implemented ontology-based cata log for components, conceptualized as processes, that captures and communicates this detailed information. PROCAT is built on a layered representation that allows reasoning about processes at varying levels of abstraction, from qualitative con straints reﬂecting preconditions and effects, to quantitative predictions about output data and performance. PROCAT employs Semantic Web technologies RDF,OWL, and SPARQL, and builds on SemanticWeb services research.We describe PROCAT’S approach to representing and answering queries about processes, discuss some early experiments evaluating the quantitative predictions, and report on our experience using PROCAT in a system producing workﬂows for intelligencenanalysis.
6610 en Inference Web in Action: Lightweight Use of the Proof Markup Language The Inference Web infrastructure for web explanations together with its underlying Proof Markup Language (PML) for encoding justification and provenance information has been used in multiple projects varying from explaining the behavior of cognitive agents to explaining how knowledge is extracted from multiple sources of information in natural language. The PML specification has increased significantly since its inception in 2002 in order to accommodate a rich set of requirements derived from multiple projects, including the ones mentioned above. In this paper, we have a very different goal than the other PML documents: to demonstrate that PML may be effectively used by simple systems (as well as complex systems) and to describe lightweight use of language and its associated Inference Web tools. We show how an exemplar scientific application can use lightweight PML descriptions within the context of an NSF-funded cyberinfrastructure project. The scientific application is used throughout the paper as a use case for the lightweight use of PML and the Inference Web and is meant to be an operational prototype for a class of cyberinfrastructure applications.
6611 en Supporting Ontology-based Dynamic Property and Classification in WebSphere Metadata Server Metadata management is an important aspect of today’s enterprise information systems. Metadata management systems are growing from toolspecific repositories to enterprise-wide metadata repositories. In this context, one challenge is the management of the evolving metadata whose schema or meta-model itself may evolve, e.g., dynamically-added properties, which are often hard to predict upfront at the initial meta-model design time; another challenge is to organize the metadata by semantically-rich classification schemes. In this paper, we present a practical system which provides support for users to dynamically manage semantically-rich properties and classifications in the IBM WebSphere Metadata Server (MDS) by integrating an OWL ontology repository. To enable the smooth acceptance of Semantic Web technologies for developers of commercial software which must run 24 hours/day, 7 days/week, the system is designed to consist of integrated modeling paradigms, with an integrated query language and runtime repository. Specifically, we propose the modeling of dynamic properties on structured metadata as OWL properties and the modeling of classification schemes as OWL ontologies for metadata classification. We present a natural extension to OQL (Object Query Language)-like query language to embrace dynamic properties and metadata classification. We also observe that hybrid storage, i.e., horizontal tables for structured metadata and vertical triple tables for dynamic properties and classification, is suitable for the storage and query processing of co-existing structured metadata and semantic metadata. We believe that our study and experience are not specific to MDS, but are valuable for the community trying to apply Semantic Web technologies to the structured data management area.nn
6612 en Towards a Multimedia Content Marketplace Implementation Based on Triplespaces A Multimedia Content Marketplace can support innovative business models in the telecommunication sector. This marketplace has a strong need for semantics, co-ordination and a service-oriented architecture. Triple Space Computing is an emerging semantic co-ordination paradigm for Web services, for which the marketplace is an ideal implementation scenario. This paper introduces the developed Triple Space platform and our planned evaluation of its value to our telecommunication scenario.
6613 en Requirements Analysis Tool: A Tool for Automatically Analyzing Software Requirements Documents We present a tool, called the Requirements Analysis Tool that performs a wide range of best practice analyses on software requirements documents. The novelty of our approach is the use of user-defined glossaries to extract structured content, and thus support a broad range of syntactic and semantic analyses, while allowing users to write requirements in the stylized natural language advocated by expert requirements writers. Semantic Web technologies are then leveraged for deeper semantic analysis of the extracted structured content to find various kinds of problems in requirements documents.
6614 en OntoNaviERP: Ontology-supported Navigation in ERP Software Documentation The documentation of Enterprise Research Planning (ERP) systems is usually (1) extremely large and (2) combines various views from the business and the technical implementation perspective. Also, a very specific vocabulary has evolved, in particular in the SAP domain (e.g. SAP Solution Maps or SAP software module names). This vocabulary is not clearly mapped to business management terminology and concepts. It is a well-known problem in practice that searching in SAP ERP documentation is difficult, because it requires in-depth knowledge of a large and proprietary terminology. We propose to use ontologies and automatic annotation of such large HTML software documentation in order to improve the usability and accessibility, namely of ERP help files. In order to achieve that, we have developed an ontology and prototype for SAP ERP 6.0. Our approach integrates concepts and lexical resources from (1) business management terminology, (2) SAP business terminology, (3) SAP system terminology, and (4) Wordnet synsets. We use standard GATE/KIM technology to annotate SAP help documentation with respective references to our ontology. Eventually, our approach consolidates the knowledge contained in the SAP help functionality at a conceptual level. This allows users to express their queries using a terminology they are familiar with, e.g. referring to general management terms. Despite a widely automated ontology construction process and a simplistic annotation strategy with minimal human intervention, we experienced convincing results. For an average query linked to an action and a topic, our technology returns more than 3 relevant resources, while a naïve term-based search returns on average only about 0.2 relevant resources.
6615 en Market Blended Insight: modeling propensity to buy with the Semantic Web Market Blended Insight (MBI) is a project with a clear objective of making a significant performance improvement in UK business to business (B2B) marketing activities in the 5-7 year timeframe. The web has created a rapid expansion of content that can be harnessed by recent advances in Semantic Web technologies and applied to both Media industry provision and company utilization of exploitable business data and content. The project plans to aggregate a broad range of business information, providing unparalleled insight into UK business activity and develop rich semantic search and navigation tools to allow any business to ’place their sales proposition in front of a prospective buyer’ confident of the fact that the recipient has a propensity to buy.
6616 en DogOnt – Ontology Modeling for Intelligent Domotic Environments Home automation has recently gained a new momentum thanks to the ever-increasing commercial availability of domotic components. In this context, researchers are working to provide interoperation mechanisms and to add intelligence on top of them. For supporting intelligent behaviors, house modeling is an essential requirement to understand current and future house states and to possibly drive more complex actions. In this paper we propose a new house modeling ontology designed to fit real world domotic system capabilities and to support interoperation between currently available and future solutions. Taking advantage of technologies developed in the context of the Semantic Web, the DogOnt ontology supports device/network independent description of houses, including both “controllable” and architectural elements. States and functionalities are automatically associated to the modeled elements through proper inheritance mechanisms and by means of properly defined SWRL auto-completion rules which ease the modeling process, while automatic device recognition is achieved through classification reasoning.
6618 en A Semantic Data Grid for Satellite Mission Quality Analysis The combination of Semantic Web and Grid technologies and architectures eases the development of applications that share heterogeneous resources (data and computing elements) that belong to several organisations. The Aerospace domain has an extensive and heterogeneous network of facilities and institutions, with a strong need to share both data and computational resources for complex processing tasks. One such task is monitoring and data analysis for Satellite Missions. This paper presents a Semantic Data Grid for satellite missions, where flexibility, scalability, interoperability, extensibility and efficient development have been considered the key issues to be addressed.
6619 en Semantic Wikis: Fusing the two strands of the Semantic Web 
6620 en Internet of Services 
6621 en Semantic Web @ BBN 
6622 en Data Intelligence Fourth of a set of industry talks at ISWC. “Data Intelligence”. Today, lots of research is inhibited because of data that cannot be made available to the research community (this has been one of my gripes for a while, so great to be hearing from MS). Big issue of course is data privacy. Goldcorp challenge. Goldmining company. Provided all their survey data to the public online, let challenge participants register and seek gold, offer prizes. Very successful in identifying good mining targets. In our field, innovation inhibited by inability to diseminate info due to privacy concerns. Many newsworthy privacy violations (cracking of anonymized search logs, anonymized health records, anonymized video ratings) discourage data release.  She proposes a framework for specifying how data can be used, so that scientists can sign licenses on the data they are getting. I’m still in favor of instead focusing on ways to let users release some subset of information about themselves unconditionally—I think that for most users, deciding what subset is unconditionally save is a much easier job than deciding the restricted conditions (under arbitrary unimaginable circumstances) under which all their data is safe.
6623 en Semantic Web in Asia: Example Use Cases Tony Lee, the President and CEO of Saltlux Inc, one of the global leader in semantic web technology headquartered in Seoul, Korea, will deliver a speech at the ISWC 2008 being held at Karlsruhe, Germany under the title of “Semantic Web in Asia: Example use cases”nTony will introduce about the current status and the developmental possibilities of the Asian semantic web technology market by reviewing the commercialization use example cases that Saltlux carried out recently including Artificial Intelligence Mobile Service for KTF, u-City for Samsung SDS, Semantic search system for Korea National Archives, and other significant cases.nSome of the invited speakers are from Yahoo, Microsoft, and SAP.nSaltlux is participating to the ISWC 2008 as one of the Silver Sponsors.
6624 en Making the Web searchable 
6625 en SemanticWeb from an industry perspective 
6626 en Thesaurus-based search in large heterogeneous collections n cultural heritage, large virtual collections are coming into existence. Such collections contain heterogeneous sets of metadata and vocabulary concepts, originating from multiple sources. In the context of the E-Culture demonstrator we have shown earlier that such virtual collections can be effectively explored with keyword search and semantic clustering. In this paper we describe the design rationale of ClioPatria, an open-source system which provides APIs for scalable semantic graph search. The use of ClioPatria’s search strategies is illustrated with a realistic use case: searching for ”Picasso”. We discuss details of scalable graph search, the required OWL reasoning functionalities and show why SPARQL queries are insufficient for solving the search problem.
6628 en Creating and Using Organisational Semantic Webs in Large Networked Organisations Modern knowledge management is based on the orchestration of dynamic communities that acquire and share knowledge according to customized schemas. However, while independence of ontological views is favoured, these communities must also be able to share their knowledge with the rest of the organization. In this paper we introduce K-Forms and K-Search, a suite of Semantic Web tools for supporting distributed and networked knowledge acquisition, capturing, retrieval and sharing. They enable communities of users to define their own domain views in an intuitive way (automatically translated into formal ontologies) and capture and share knowledge according to them. The tools favour reuse of existing ontologies; reuse creates as side effect a network of (partially) interconnected ontologies that form the basis for knowledge exchange among communities. The suite is under release to support knowledge capture, retrieval and sharing in a large jet engine company.
6629 en An architecture for semantic navigation and reasoning with patient data - experiences of the Health-e-Child project Medical ontologies have become the standard means of recording and accessing conceptualized biological and medical knowledge. The expressivity of these ontologies goes from simple concept lists through taxonomies to formal logical theories. In the context of patient information, their application is primarily annotation of medical (instance) data. To exploit higher expressivity, we propose an architecture which allows for reasoning on patient data using OWL DL ontologies. The implementation is carried out as part of the Health-e-Child platform prototype. We discuss the use case where ontologies establish a hierarchical classification of patients which in turn is used to aid the visualization of patient data. We briefly discuss the treemap-based patient viewer which has been evaluated in the Health-e-Child project.
6630 en Involving Domain Experts in Authoring OWL Ontologies This demonstration presents ROO, a tool that facilitates domain experts' definition of ontologies in OWL by allowing them to author the ontology in a controlled natural language called Rabbit. ROO guides users through the ontology construction process by following a methodology geared towards domain experts’ involvement in ontology authoring, and exploiting intelligent user interfaces techniques. An experimental study with ROO was conducted to examine the usability and usefulness of the tool, and the quality of the resultant ontologies. The findings of the study will be presented in a full paper at the ISWC08 research track.n
6631 en Supporting Collaborative Ontology Development in Protege Ontologies are becoming so large in their coverage that no single person or a small group of people can develop them effectively and ontology development becomes a community-based enterprise. We present Collaborative Protégé—an extension of the Protégé ontology editor that we have designed specifically to support the collaboration process for a community of users. During the ontology-development process, Collaborative Protégé allows users to hold discussions about the ontology components and changes using typed annotations; it tracks the change history of the ontology entities; it provides a chat and search functionality. Users edit simultaneously an ontology stored in a common repository. All changes made by a user are seen immediately by other users. Collaborative Protégé is open source and distributed with the full installation of Protégé.n
6632 en Identfying Potentiallcy Important Conepts and Relations in an Ontology More and more ontologies have been published and used widely on the web. In order to make good use of an ontology, especially a new and complex ontology, we need methods to help understand it first. Identifying potentially important concepts and relations in an ontology is an intuitive but challenging method. In this paper, we first define four features for potentially important concepts and relation from the ontological structural point of view. Then a simple yet effective Concept-And-Relation-Ranking (CARRank) algorithm is proposed to simultaneously rank the importance of concepts and relations. Different from the traditional ranking methods, the importance of concepts and the weights of relations reinforce one another in CARRank in an iterative manner. Such an iterative process is proved to be convergent both in principle and by experiments. Our experimental results show that CARRank has a similar convergent speed as the PageRank-like algorithms, but a more reasonable ranking result.
6633 en RoundTrip Ontology Authoring Controlled Language (CL) for Ontology Editing tools offer an attractive alternative for naive users wishing to create ontologies, but they are still required to spend time learning the correct syntactic structures and vocabulary in order to use the Controlled Language properly. This paper extends previous work (CLOnE) which uses standard NLP tools to process the language and manipulate an ontology. Here we also generate text in the CL from an existing ontology using template-based (or shallow) Natural Language Generation (NLG). The text generator and the CLOnE authoring process combine to form a RoundTrip Ontology Authoring environment: one can start with an existing imported ontology or one originally produced using CLOnE, (re)produce the Controlled Language, modify or edit the text as required and then turn the text back into the ontology in the CLOnE environment. Building on previous methodology we undertook an evaluation, comparing the RoundTrip Ontology Authoring process with a well-known ontology editor; where previous work required a CL reference manual with several examples in order to use the controlled language, the use of NLG reduces this learning curve for users and improves on existing results for basic ontology editing tasks.
6634 en Integrating Object-Oriented and Ontological Representations: A Case Study in Java and OWL The Web Ontology Language (OWL) provides a modelling paradigm that is especially well suited for developing models of large, structurally complex domains such as those found in Health Care and the Life Sciences. OWL’s declarative nature combined with powerful reasoning tools has effectively supported the development of very large and complex anatomy, disease, and clinical ontologies. OWL, however, is not a programming language, so using these models in applications necessitates both a technical means of integrating OWL models with programs and considerable methodological sophistication in knowing how to integrate them. In this paper, we present an analytical framework for evaluating various OWL-Java combination approaches. We have developed a software framework for what we call hybrid modelling, that is, building models in which part of the model exists and is developed directly in Java and part of the model exists and is developed directly in OWL. We analyse the advantages and disadvantages of hybrid modelling both in comparison to other approaches and by means of a case study of a large medical records system.
6635 en Extracting Semantic Constraint from Description Text for Semantic Web Service Discovery Various semantic web service discovery techniques have been proposed, many of which perform the profile based service signature (I/O) matching. However, the service I/O concepts are not sufficient to discover web services accurately. This paper presents a new method to enhance the semantic description of semantic web service by using the semantic constraints of service I/O concepts in specific context. The semantic constraints described in a constraint graph are extracted automatically from the parsing results of the service description text by a set of heuristic rules. The corresponding semantic web service matchmaker performs not only the profile’s semantic matching but also the matching of their semantic constraints with the help of a constraint graph based matchmaking algorithm. The experiment results are encouraging when applying the semantic constraint to discover semantic web services on the service retrieval test collection OWLS-TC v2.
6636 en Enhancing Semantic Web Services with Inheritance Currently proposed Semantic Web Services technologies allow the creation of ontology-based semantic annotations of Web services so that software agents are able to discover, invoke, compose and monitor these services with a high degree of automation. The OWL Services (OWL-S) ontology is an upper ontology in OWL language, providing essential vocabularies to semantically describe Web services. Currently OWL-S services can only be developed independently; if one service is unavailable then finding a suitable alternative would require an expensive and difficult global search/match. It is desirable to have a new OWL-S construct that can systematically support substitution tracing as well as incremental development and reuse of services. Introducing inheritance relationship (IR) into OWL-S is a natural solution. However, OWL-S, as well as most of the other currently discussed formalisms for Semantic Web Services such as WSMO or SAWSDL, has yet to define a concrete and self-contained mechanism of establishing inheritance relationships among services, which we believe is very important for the automated annotation and discovery of Web services as well as human organization of services into a taxonomy-like structure. In this paper, we extend OWL-S with the ability to define and maintain inheritance relationships between services. Through the definition of an additional “inheritance profile”, inheritance relationships can be stated and reasoned about. Two types of IRs are allowed to grant service developers the choice to respect the “contract” between services or not. The proposed inheritance framework has also been implemented and the prototype will be briefly evaluated as well.
6637 en Using Semantic Distances for Reasoning with Inconsistent Ontologies Re-using and combining multiple ontologies on the Web is bound to lead to inconsistencies between the combined vocabularies. Even many of the ontologies that are in use today turn out to be inconsistent once some of their implicit knowledge is made explicit. However, robust and efficient methods to deal with inconsistencies are lacking from current Semantic Web reasoning systems, which are typically based on classical logic. In earlier papers, we have proposed the use of syntactic relevance functions as a method for reasoning with inconsistent ontologies. In this paper, we extend that work to the use of semantic distances. We show how Google distances can be used to develop semantic relevance functions to reason with inconsistent ontologies. In essence we are using the implicit knowledge hidden in the Web for explicit reasoning purposes. We have implemented this approach as part of the PION reasoning system. We report on experiments with several realistic ontologies. The test results show that a mixed syntactic/semantic approach can significantly improve reasoning performance over the purely syntactic approach. Furthermore, our methods allow to trade-off computational cost for inferential completeness. Our experiment shows that we only have to give up a little quality to obtain a high performance gain.
6638 en Statistical Learning for Inductive Query Answering on OWL Ontologies A novel family of parametric language-independent kernel functions defined for individuals within ontologies is presented. They are easily integrated with efficient statistical learning methods for inducing linear classifiers that offer an alternative way to perform classification w.r.t. deductive reasoning. A method for adapting the parameters of the kernel to the knowledge base through stochastic optimization is also proposed. This enables the exploitation of statistical learning in a variety of tasks where an inductive approach may bridge the gaps of the standard methods due the inherent incompleteness of the knowledge bases. In this work, a system integrating the kernels has been tested in experiments on approximate query answering with real ontologies collected from standard repositories.
6639 en Optimization and Evaluation of Reasoning in Probabilistic Description Logic: Towards a Systematic Approach This paper describes the first steps towards developing a methodology for testing and evaluating the performance of reasoners for the probabilistic description logic P- ${\ensuremath{\mathcal{SHIQ}}(D)}$ . Since it is a new formalism for handling uncertainty in DL ontologies, no such methodology has been proposed. There are no sufficiently large probabilistic ontologies to be used as test suites. In addition, since the reasoning services in P- ${\ensuremath{\mathcal{SHIQ}}(D)}$ are mostly query oriented, there is no single problem (like classification or realization in classical DL) that could be an obvious candidate for benchmarking. All these issues make it hard to evaluate the performance of reasoners, reveal the complexity bottlenecks and assess the value of optimization strategies. This paper addresses these important problems by making the following contributions: First, it describes a probabilistic ontology that has been developed for the real-life domain of breast cancer which poses significant challenges for the state-of-art P- ${\ensuremath{\mathcal{SHIQ}}(D)}$ reasoners. Second, it explains a systematic approach to generating a series of probabilistic reasoning problems that enable evaluation of the reasoning performance and shed light on what makes reasoning in P- ${\ensuremath{\mathcal{SHIQ}}(D)}$ hard in practice. Finally, the paper presents an optimized algorithm for the non-monotonic entailment. Its positive impact on performance is demonstrated using our evaluation methodology.
6640 en Combining a DL Reasoner and a Rule Engine for Improving Entailment-Based OWL Reasoning We introduce the notion of the mixed DL and entailment-based (DLE) OWL reasoning, defining a framework inspired from the hybrid and homogeneous paradigms for integration of rules and ontologies. The idea is to combine the TBox inferencing capabilities of the DL algorithms and the scalability of the rule paradigm over large ABoxes. Towards this end, we define a framework that uses a DL reasoner to reason over the TBox of the ontology (hybrid-like) and a rule engine to apply a domain-specific version of ABox-related entailments (homogeneous-like) that are generated by TBox queries to the DL reasoner. The DLE framework enhances the entailment-based OWL reasoning paradigm in two directions. Firstly, it disengages the manipulation of the TBox semantics from any incomplete entailment-based approach, using the efficient DL algorithms. Secondly, it achieves faster application of the ABox-related entailments and efficient memory usage, comparing it to the conventional entailment-based approaches, due to the low complexity and the domain-specific nature of the entailments.
6641 en Improving an RCC-Derived Geospatial Approximation by OWL Axioms An approach to improve an RCC-derived geospatial approximation is presented which makes use of concept inclusion axioms in OWL. The algorithm used to control the approximation combines hypothesis testing with consistency checking provided by a knowledge representation system based on description logics. Propositions about the consistency of the refined ABox w.r.t. the associated TBox when compared to baseline ABox and TBox are made. Formal proves of the divergent consistency results when checking either of both are provided. The application of the approach to a geospatial setting results in a roughly tenfold improved approximation when using the refined ABox and TBox. Ways to further improve the approximation and to automate the detection of falsely calculated relations are discussed.
6642 en OWL Datatypes: Design and Implementation We analyze the datatype system of OWL and OWL 2, and discuss certain nontrivial consequences of its definition, such as the extensibility of the set of supported datatypes and complexity of reasoning. We also argue that certain datatypes from the list of normative datatypes in the current OWL 2 Working Draft are inappropriate and should be replaced with different ones. Finally, we present an algorithm for datatype reasoning. Our algorithm is modular in the sense that it can handle any datatype that supports certain basic operations. We show how to implement these operations for number and string datatypes.
6643 en Laconic and Precise Justifications in OWL A justification for an entailment in an OWL ontology is a minimal subset of the ontology that is sufficient for that entailment to hold. Since justifications respect the syntactic form of axioms in an ontology, they are usually neither syntactically nor semantically minimal. This paper presents two new subclasses of justifications—laconic justifications and precise justifications. Laconic justifications only consist of axioms that do not contain any superfluous “parts”. Precise justifications can be derived from laconic justifications and are characterised by the fact that they consist of flat, small axioms, which facilitate the generation of semantically minimal repairs. Formal definitions for both types of justification are presented. In contrast to previous work in this area, these definitions make it clear as to what exactly “parts of axioms” are. In order to demonstrate the practicability of computing laconic, and hence precise justifications, an algorithm is provided and results from an empirical evaluation carried out on several published ontologies are presented. The evaluation showed that laconic/precise justifications can be computed in a reasonable time for entailments in a range of ontologies that vary in size and complexity. It was found that in half of the ontologies sampled there were entailments that had more laconic/precise justifications than regular justifications. More surprisingly it was observed that for some ontologies there were fewer laconic justifications than regular justifications.
6644 en Scalable Grounded Conjunctive Query Evaluation over Large and Expressive Knowledge Bases Grounded conjunctive query answering over OWL-DL ontologies is intractable in the worst case, but we present novel techniques which allow for efficient querying of large expressive knowledge bases in secondary storage. In particular, we show that we can effectively answer grounded conjunctive queries without building a full completion forest for a large Abox (unlike state of the art tableau reasoners). Instead we rely on the completion forest of a dramatically reduced summary of the Abox. We demonstrate the effectiveness of this approach in Aboxes with up to 45 million assertions.n
6645 en A Kernel Revision Operator for Terminologies - Algorithms and Evaluation Revision of a description logic-based ontology deals with the problem of incorporating newly received information consistently. In this paper, we propose a general operator for revising terminologies in description logic-based ontologies. Our revision operator relies on a reformulation of the kernel contraction operator in belief revision. We first define our revision operator for terminologies and show that it satisfies some desirable logical properties. Second, two algorithms are developed to instantiate the revision operator. Since in general, these two algorithms are computationally too hard, we propose a third algorithm as a more efficient alternative. We implemented the algorithms and provide evaluation results on their efficiency, effectiveness and meaningfulness in the context of two application scenarios: Incremental ontology learning and mapping revision.
6646 en Description Logic Reasoning with Decision Diagrams: Compiling SHIQ to Disjunctive Datalog We propose a novel method for reasoning in the description logic $\mathcal{SHIQ}$ . After a satisfiability preserving transformation from $\mathcal{SHIQ}$ to the description logic $\mathcal{ALCI}b$ , the obtained $\mathcal{ALCI}b$ Tbox $\mathcal{T}$ is converted into an ordered binary decision diagram (OBDD) which represents a canonical model for $\mathcal{T}$ . This OBDD is turned into a disjunctive datalog program that can be used for Abox reasoning. The algorithm is worst-case optimal w.r.t. data complexity, and admits easy extensions with DL-safe rules and ground conjunctive queries.nSupported by the European Commission under contracts 027595 NeOn and 215040 ACTIVE, and by the Deutsche Forschungsgemeinschaft (DFG) under the ReaSem project.
6648 en An Interface-Based Ontology Modularization Framework for Knowledge Encapsulation In this paper, we present a framework for developing ontologies in a modular manner, which is based on the notions of interfaces and knowledge encapsulation. Within the context of this framework, an ontology can be defined and developed as a set of ontology modules that can access the knowledge bases of the others through their well-defined interfaces. An important implication of the proposed framework is that ontology modules can be developed completely independent of each others’ signature and language. Such modules are free to only utilize the required knowledge segments of the others. We describe the interface-based modular ontology formalism, which theoretically supports this framework and present its distinctive features compared to the exiting modular ontology formalisms. We also describe the real-world design and implementation of the framework for creating modular ontologies by extending OWL-DL and modifying the Swoop interfaces and reasoners.
6649 en On the Semantics of Trust and Caching in the Semantic Web The Semantic Web is a distributed environment for knowledge representation and reasoning. The distributed nature brings with it failing data sources and inconsistencies between autonomous knowledge bases. To reduce problems resulting from unavailable sources and to improve performance, caching can be used. Caches, however, raise new problems of imprecise or outdated information. We propose to distinguish between certain and cached information when reasoning on the semantic web, by extending the well known $\mathcal{FOUR}$ bilattice of truth and knowledge orders to $\mathcal{FOUR-C}$ , taking into account cached information. We discuss how users can be offered additional information about the reliability of inferred information, based on the availability of the corresponding information sources. We then extend the framework towards $\mathcal{FOUR-T}$ , allowing for multiple levels of trust on data sources. In this extended setting, knowledge about trust in information sources can be used to compute, how well an inferred statement can be trusted and to resolve inconsistencies arising from connecting multiple data sources. We redefine the stable model and well founded semantics on the basis of $\mathcal{FOUR-T}$ , and reformalize the Web Ontology Language OWL2 based on logical bilattices, to augment OWL knowledge bases with trust based reasoning.
6650 en Exploring Semantic Social Networks using Virtual Reality We present Redgraph, the first generic virtual reality visualization program for Semantic Web data. Redgraph is capable of handling large data-sets, as we demonstrate on social network data from the U.S. Patent Trade Office. We develop a Semantic Web vocabulary of virtual reality terms compatible with GraphXML to map graph visualization into the Semantic Web itself. Our approach to visualizing Semantic Web data takes advantage of user-interaction in an immersive environment to bypass a number of difficult issues in 3-dimensional graph visualization layout by relying on users themselves to interactively extrude the nodes and links of a 2-dimensional graph into the third dimension. When users touch nodes in the virtual reality environment, they retrieve data formatted according to the data’s schema or ontology. We applied Redgraph to social network data constructed from patents, inventors, and institutions from the United States Patent and Trademark Office in order to explore networks of innovation in computing. Using this data-set, results of a user study comparing extrusion (3-D) vs. no-extrusion (2-D) are presented. The study showed the use of a 3-D interface by subjects led to significant improvement on answering of fine-grained questions about the data-set, but no significant difference was found for broad questions about the overall structure of the data. Furthermore, inference can be used to improve the visualization, as demonstrated with a data-set of biotechnology patents and researchers.
6651 en Semantic Grounding of Tag Relatedness in Social Bookmarking Systems Collaborative tagging systems have nowadays become important data sources for populating semantic web applications. For tasks like synonym detection and discovery of concept hierarchies, many researchers introduced measures of tag similarity. Even though most of these measures appear very natural, their design often seems to be rather ad hoc, and the underlying assumptions on the notion of similarity are not made explicit. A more systematic characterization and validation of tag similarity in terms of formal representations of knowledge is still lacking. Here we address this issue and analyze several measures of tag similarity: Each measure is computed on data from the social bookmarking system del.icio.us and a semantic grounding is provided by mapping pairs of similar tags in the folksonomy to pairs of synsets in Wordnet, where we use validated measures of semantic distance to characterize the semantic relation between the mapped tags. This exposes important features of the investigated similarity measures and indicates which ones are better suited in the context of a given semantic application.
6652 en Semantic Modelling of User Interests Based on Cross-Folksonomy Analysis The continued increase in Web usage, in particular participation in folksonomies, reveals a trend towards a more dynamic and interactive Web where individuals can organise and share resources. Tagging has emerged as the de-facto standard for the organisation of such resources, providing a versatile and reactive knowledge management mechanism that users find easy to use and understand. It is common nowadays for users to have multiple profiles in various folksonomies, thus distributing their tagging activities. In this paper, we present a method for the automatic consolidation of user profiles across two popular social networking sites, and subsequent semantic modelling of their interests utilising Wikipedia as a multi-domain model. We evaluate how much can be learned from such sites, and in which domains the knowledge acquired is focussed. Results show that far richer interest profiles can be generated for users when multiple tag-clouds are combined.
6653 en Introduction to the Semantic Web 
6654 en A Semantic Multimedia Web: Create, Annotate, Present and Share your Media The success of content-centered (social) Web 2.0 services contributes to an ever growing amount of digital multimedia content available on the Web. Video advertisement is becoming more and more popular and films, music and videoclips are largely consumed from legacy commercial databases. Re-using such multimedia material is, however, still a hard problem. Why is it so difficult to find appropriate multimedia content, to reuse and repurpose content previously published and to adapt interfaces to these content according to different user needs?nnThis tutorial proposes to cover these questions. Based on established media workflow practices, we describe a small number of fundamental processes of media production. We explain how multimedia metadata can be represented, attached to the content it describes, and benefits from the web that contains more and more formalized knowledge (the Web of linked data). We show how web applications can benefit from semantic metadata for creating, searching and presenting multimedia content.
6655 en RDFa - Bridging the Web of Documents and the Web of Data RDFa is the bridge between the Web of Documents, targeting at human users, and the Web of Data, focusing on machines. Not only due to the recent uptake of RDFa (Digg, Yahoo!, etc.), learning how and where to use RDFa is essential. This tutorial will introduce the usage of RDFa in real-world use cases and will enable the attendees to work with RDFa both on the client as on the server side. We will create, publish and consume RDFa-marked-up data in the course of the tutorial and discuss advanced aspects, such as dynamic content handling. There are no pre-requisites for participation in the tutorial other than a familiarity with the basics of the (Semantic) Web such as URIs, RDF, XHTML, and SPARQL.
6656 en Knowledge Representation and Extraction for Business Intelligence Business Intelligence (BI) requires the acquisition and aggregation of key pieces of knowledge from multiple sources in order to provide business analysts with valuable information or feed statistical BI models and tools. The massive amount of textual and multimedia information available to business analysts makes information extraction and semantic-based digital tools key enablers for the acquisition and management of semantic information. The role of Ontologies is important here, since they promote interoperability and uniform and standardized access to heterogeneous sources and software components. In addition they encode rules for deduction of new knowledge from extracted data.nnThe tutorial will give an overview of approaches to identify, extract, and consolidate semantic information for business intelligence, also stressing the role of temporal information. The tutorial will take a practical hands-on approach in which theoretical concepts and approaches are presented together with case studies on semantic-based tools in the context of the 6th Framework Programme Musing Integrated Project which is targeting three different vertical domains: Financial Risk Management; Internationalisation; and IT Operational Risk Management.
6657 en Reasoning for Ontology Engineering and Usage We will provide a brief introduction to OWL, in fact OWL2, and the underlying Description Logic, clarifying the semantics and providing examples to help the understanding of this admittedly complex formalism. In particular, we will discuss common misunderstandings around OWL and OWL2, explain the open world assumption, inferences, and the functionality of reasoners. We will use the RacerPro reasoner to demonstrate the benefit of using reasoning for query answering over ontologies. Scalability issues with respect to expressive ontologies as well as huge assertional knowledge bases are discussed.
6658 en Realizing a Semantic Web Application You are aware of the Semantic Web, but you haven’t got time to develop a Semantic Web application yourself? During this tutorial we challenge the Semantic Web technologies on the Web 2.0 ground of realizing a mash-up that reuses, transforms and combines existing data taken from the open Web.nnRSWA tutorial explains how to develop step-by-step a Semantic Web application that expects a music style as an input; retrieves data from online music archives and event databases; merges them and let the users explore events related to artists that practice the required style. The result is a Semantic Web Application we named Music Event Explorer or shortly meex; try it out at http://swa.cefriel.it/meex!
6659 en How to Publish Linked Data on the Web The Web is increasingly understood as a global information space consisting not just of linked documents, but also of Linked Data. The Linked Data principles provide a basis for realizing this Web of Data, or Semantic Web. Since early 2007 numerous data sets have been published on the Web according to these principles, in domains as broad as music, books, geographical information, films, people, events, reviews and photos. In combination these data sets consist of over 2 billion RDF triples, interlinked by more than 3 million triples that cross data sets. As this Web of Linked Data continues to grow, and an increasing number of applications are developed that exploit these data sets, there is a growing need for data publishers, researchers, developers and Web practitioners to understand Linked Data principles and practice. Run by some of the leading members of the Linked Data community, this tutorial will address those needs, and provide participants with a solid foundation from which to begin publishing Linked Data on the Web, as well as to implement applications that consume Linked Data from the Web.
6660 en Semantic Web for Health Care and Life Sciences The W3C Semantic Web in Health Care and Life Sciences Interest Group (HCLSIG) has used RDF tools to integrate several large biological and clinical databases. This has simplified access to relational and hierarchical data and enabled third party additions to the database. HCLSIG demonstrates the use of Semantic Web technologies to access data on a web scale, taking advantage of OWL and rules to allow queries to re-purpose data without the need to coordinate with the data custodian.nnThis tutorial will introduce OWL and rule mappings of databases, as well as introduce good practices for data modeling and publication. The use of SKOS for terminologies will also be described. Attendees will learn possible applications of Semantic Web tools to share data between and within organizations and solve large scale data integration problems.nnThis tutorial will discuss how publishers of biological and clinical data can use OWL and rules to model their data and how users of Semantic Web tools can access this more diverse data. Attendees should be familiar with the Semantic Web languages RDF, Turtle, SPARQL and be introduced to OWL. These materials will be covered in the earlier RSWA tutorial.
6661 en Free Semantic Content: Using OpenCyc in Semantic Web Applications OpenCyc will be more accessible and Semantic Web interoperability will be enhanced if users are able to access just the parts of OpenCyc they need. The tutorial will describe how Semantic Web researchers and practitioners can benefit from integrating their representations with the extensive upper and middle level ontological content of the free and unrestricted OpenCyc knowledge base, and other integrative vocabularies like Okkam. The syntax of OpenCyc will be described both in raw form, and as mapped onto Semantic Web standard languages, and the content of the knowledge base will be described in overview. Based on that, we’ll show how to extend the OpenCyc KB for user applications, and how to make use of it in a web-services environment to support knowledge integration, and simple machine learning applications. Finally, we’ll demonstrate the use of the OpenCyc vocabulary to support a broad-applicability knowledge capture application, illustrative of the transition from Web2.0 to Web3.0. Hands on exercises will be used to illustrate knowledge use and construction, use of OpenCyc with inference, and use for semantic search over text in a web services environment.
6662 en Opening Ceremony 
6663 en Multimedia Semantic Web The Capture, Storage, Sharing, Organizing, Retrieval, and Use of knowledge dominate most socio-economic activities in our society. Most of the knowledge in the world is initially captured and stays in the form of experiences in different sensing modalities. Current technology can address knowledge in text because in text the experiential data is converted to symbols by humans. Converting sensory data to symbols in computer systems has been difficult, primarily due to our inability to formally represent and effectively model the appropriate context within which the multimedia sensory data should be interpreted. This problem of integrating continuous multimedia and symbolic data becomes more urgent as multimedia data is becoming common, and the resulting social applications on the Web are required to deal with semantics of multimedia data. Clearly, the collection of searchable multimedia experiences will facilitate progress in sciences and the quality of human life in every part of the world, across all economies and cultures. In this paper, we will present challenges offered by semantics of multimedia data, review emerging semantic web approaches towards addressing these challenges, and present some example applications that are being developed to address these emerging challenges.
6664 en Freebase: An Open, Writable Database of the World’s Information Freebase is an open database of the world’s information, built by a global community and free for anyone to query, contribute to, and build applications on. Drawing from large open data sets like Wikipedia, MusicBrainz, GNIS, EDGAR etc., Freebase is curated by a passionate community of users and contains structured information on millions of topics such as people, places, music, film, food, science, historical events, and more.nnPart of what makes this open database unique is that it spans domains, but requires that a particular topic exist only once in Freebase. Thus freebase is an identity database with a user contributed schema which spans multiple domains. For example, Arnold Schwarzenegger may appear in a movie database as an actor, a political database as a governor, and in a bodybuilder database as Mr. Universe. In Freebase, however, there is only one topic for Arnold Schwarzenegger that brings all these facets together. The unified topic is a single reconciled identity, which makes it easier to find and contribute information about the linked world we live in.
6665 en Message in a Bottle or: How can the Semantic Web Community be more convincing? Enormous resources are poured into projects like the Large Hadron Collider, the Hubble space telescope, or the Iter fusion reactor. Computer science resources pale in comparison – the European Semantic Web effort is tiny compared to those projects. Why is this the case? Does the Semantic Web (or computer science in general) promise less impact or relevance than those Physics projects? In my talk I will argue that the Physicists are much better in formulating an engaging mission and message. Especially the Semantic Web community has not been very good in coming up with a convincing mission directed to the public. We need to and we can do better. I will formulate requirements and a starting point for such a message and investigate ongoing seemingly unrelated research areas and trends on the Semantic Web like Semantic Sensor Networks, Social Semantic Desktop and Semantic Publishing and how they contributes to a better conveyable mission.
6666 en Semantic Web Challenge & Billion Triple Challenge The central idea of the Semantic Web is to extend the current human-readable web by encoding some of the semantics of resources in a machine-processable form. Moving beyond syntax opens the door to more advanced applications and functionality on the Web. Computers will be better able to search, process, integrate and present the content of these resources in a meaningful, intelligent manner.nnThe core technological building blocks are now in place and widely available: ontology languages, flexible storage and querying facilities, reasoning engines, etc. Standards and guidelines for best practice are being formulated and disseminated by the W3C.nnThe Semantic Web Challenge offers participants the chance to show the best of the Semantic Web. The Challenge thus serves several purposes:nn  * Helps us illustrate to society what the Semantic Web can providen  * Gives researchers an opportunity to showcase their work and compare it to othersn  * Stimulates current research to a higher final goal by showing the state-of-the-art every yearn
6667 en Lightning Talks This year, ISWC will include a session of “Lightning Talks”. The session provides an opportunity for participants to present ideas, comments, calls for collaboration, scathing polemic criticisms,… controversy and discussion are positively encouraged!! We would particularly welcome observations or comments arising from material presented during the conference.
6668 en Closing Ceremony and Best Paper Awards Outlook on ISWC2009
6669 en Assesment of surface ravelling using traffic speed survey techniques 
6670 en A method for developing preliminary friction deterioration model on runway 
6671 en Evaluation of the international friction index coefficients for various devices 
6672 en Seasonal variationsof pavement surface frictional properties 
6673 en How to assess a payment adjustment when both surface and mechanical defects are involved? A sinergy study on theory and experiments 
6674 en Debate about pavement management 
6675 en Debate about pavement management 
6726 en Is it SUSY? -first steps after an LHC discovery A missing energy discovery is possible at the LHC in the first year of running. The origin of such a signal could be any of a huge number of models of supersymmetry, or non-supersymmetric models with extra dimensions or "little Higgs". Recently we have developed a realistic strategy to rapidly narrow the list of candidate theories at, or close to, the moment of discovery. The strategy is based on robust ratios of inclusive counts of simple physics objects. We studied specific cases showing discrimination of look- alike models in simulated data sets that are at least 10 to 100 times smaller than used in previous studies. We discriminate supersymmetry models from non-supersymmetric look-alikes with only 100 pb-1 of simulated data, using combinations of observables that trace back to differences in spin.
6728 en Opening of ETVC'08 
6729 en Detection of Symmetries and Repeated Patterns in 3D Point Cloud Data Digital models of physical shapes are becoming ubiquitous in our economy and life. Such models are sometimes designed ab initio using CAD tools, but more and more often they are based on existing real objects whose shape is acquired using various 3D scanning technologies. In most instances, the original scanner data is just a set, but a very large set, of points sampled from the surface of the object. We are interested in tools for understanding the local and global structure of such large-scale scanned geometry for a variety of tasks, including model completion, reverse engineering, shape comparison and retrieval, shape editing, inclusion in virtual worlds and simulations, etc. This talk will present a number of point-based techniques for discovering global structure in 3D data sets, including partial and approximate symmetries, shared parts, repeated patterns, etc. It is also of interest to perform such structure discovery across multiple data sets distributed in a network, without actually ever bring them all to the same host.
6730 en Discrete Curvature Flow for Surfaces and 3-Manifolds This talk introduce the concepts, theories and algorithms for discrete curvature flows for surfaces with arbitrary topologies. Discrete curvature flow for hyperbolic 3-manifolds with geodesic boundaries are also explained. Curvature flow method can be used to design Riemannian metrics by prescribed curvatures, and applied for parameterization in graphics, shape registration and comparison in vision and brain mapping in medical imaging, spline construction in computer aided geometric design, and many other engineering fields.
6731 en Certified Mesh Generation Given a domain D, the problem of mesh generation is to construct a simplicial complex that approximates D in both a topological and a geometrical sense and whose elements satisfy various constraints such as size, aspect ratio or anisotropy. The talk will cover some recent results on triangulating surfaces and volumes by Delaunay refinement, anisotropic mesh generation and surface reconstruction. Applications in medical images, computer vision and geology will be discussed.
6732 en Information-Theoretic Algorithms for Diffusion Tensor Imaging Concepts from Information Theory have been used quite widely in Image Processing, Computer Vision and Medical Image Analysis for several decades now. Most widely used concepts are that of KL-divergence, minimum description length (MDL), etc. These concepts have been popularly employed for image registration, segmentation, classification etc. In this chapter we review several methods, mostly developed by our group at the Center for Vision, Graphics & Medical Imaging in the University of Florida, that glean concepts from Information Theory and apply them to achieve analysis of Diffusion-Weighted Magnetic Resonance (DW-MRI) data. This relatively new MRI modality allows one to non-invasively infer axonal connectivity patterns in the central nervous system. The focus of this chapter is to review automated image analysis techniques that allow us to automatically segment the region of interest in the DWMRI image wherein one might want to track the axonal pathways and also methods to reconstruct complex local tissue geometries containing axonal fiber crossings. Implementation results illustrating the algorithm application to real DW-MRI data sets are depicted to demonstrate the effectiveness of the methods reviewed.
6733 en Statistical Computing on Manifolds for Computational Anatomy Computational anatomy is an emerging discipline that aims at analyzing and modeling the individual anatomy of organs and their biological variability across a population. The goal is not only to model the normal variations among a population, but also discover morphological diferences between normal and pathological populations, and possibly to detect, model and classify the pathologies from structural abnormalities. Applications are very important both in neuroscience, to minimize the inuence of the anatomical variability in functional group analysis, and in medical imaging, to better drive the adaptation of generic models of the anatomy (atlas) into patient-specic data. nnHowever, understanding and modeling the shape of organs is made di- cult by the absence of physical models for comparing diferent subjects, the complexity of shapes, and the high number of degrees of freedom implied. Moreover, the geometric nature of the anatomical features usually extracted raises the need for statistics and computational methods on objects like curves, surfaces and deformations that do not belong to standard Euclidean spaces. We investigate in this chapter the Riemannian metric as a basis for developing generic algorithms to compute on manifolds. nnWe show that few computational tools derive this structure can be used in practice as the basic atoms to build more complex generic algorithms such as mean computation, Mahalanobis distance, interpolation, ltering and anisotropic difusion on elds of geometric features. This computational framework is illustrated with the analysis of the shape of the scoliotic spine and the modeling of the brain variability from sulcal lines where the results suggest new anatomical findings.
6734 en Large-Scale Object Recognition Systems This paper introduces recent methods for large scale image search. State-of-the-art methods build on the bag-of-features image representation. We first analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based onn* Hamming embedding (HE) andn* weak geometric consistency constraints (WGC).nHE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within the inverted file and are efficiently exploited for all images, even in the case of very large datasets.nnExperiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric consistency constraints, as well as their efficiency. Estimation of the full geometric transformation, i.e., a re-ranking step on a short list of images, is complementary to our weak geometric consistency constraints and allows to further improve the accuracy.
6735 en Recovering Shape and Motion from Video Sequences In recent years, because cameras have become inexpensive and ever more prevalent, there has been increasing interest in video-based modeling of shape and motion. This has many potential applications in areas such as electronic publishing, entertainment, sports medicine and athletic training. It, however, is an inherently difficult task because the image-data is often incomplete, noisy, and ambiguous. In our work, we focus on the recovery of deformable and articulated 3D motion from single video sequences. nnIn this talk, I will present the models we have developed for this purpose and demonstrate the applicability of our technology for Augmented Reality and human body tracking purposes. Finally, I will present some open research issues and discuss our plans for future developments.
6736 en Computational Geometry from the Viewpoint of Affine Differential Geometry Incidence relations (configurations of vertexes, edges, etc.) are important in computational geometry. Incidence relations are invariant under the group of affine transformations. On the other hand, affine differential geometry is to study hypersurfaces in an affine space that are invariant under the group of affine transformation. Therefore affine differential geometry gives a new sight in computational geometry. nnFrom the viewpoint of affine differential geometry, algorithms of geometric transformation and dual transformation are discussed. The Euclidean distance function is generalized by a divergence function in affine differential geometry. A divergence function is an asymmetric distance-like function on a manifold, and it is an important object in information geometry. For divergence functions, the upper envelope type theorems on statistical manifolds are given. Voronoi diagrams determined from divergence functions are also discussed.
6737 en Unifying Subspace and Distance Metric Learning with Bhattacharyya Coefficient for Image Classification In this talk, we propose a unified scheme of subspace and distance metric learning under the Bayesian framework for image classification. According to the local distribution of data, we divide the k-nearest neighbors of each sample into the intra-class set and the inter-class set, and we aim to learn a distance metric in the embedding subspace, which can make the distances between the sample and its intra-class set smaller than the distances between it and its inter-class set. To reach this goal, we consider the intra-class distances and the inter-class distances to be from two different probability distributions respectively, and we model the goal with minimizing the overlap between two distributions. nnInspired by the Bayesian classification error estimation, we formulate the objective function by minimizing the Bhattachyrra coefficient between two distributions. We further extend it with the kernel trick to learn nonlinear distance metric. The power and generality of the proposed approach are demonstrated by a series of experiments on the CMU-PIE face database, the extended YALE face database, and the COREL-5000 nature image database.
6738 en Non-standard Geometries and Data Analysis Traditional data mining starts with the mapping from entities to points in a Euclidean space. The search for patterns and structure is then framed as a geometric search in this space. Concepts like principal component analysis, regression, clustering, and centrality estimation have natural geometric formulations, and we now understand a great deal about manipulating such (typically high dimensional) spaces. For many domains of interest however, the most natural space to embed data in is not Euclidean. nnData might lie on curved manifolds, or even inhabit spaces endowed with different distance structures than l_p spaces. How does one do data analysis in such domains ? In this talk, I'll discuss two specific domains of interest that pose challenges for traditional data mining and geometric methods. One space consists of collections of distributions, and the other is the space of shapes. In both cases, I'll present ongoing work that attempts to interpret and understand clustering in such spaces, driven by different applications.
6760 en From spinwaves to Giant Magnetoresistance and beyond Peter Grünberg is joint winner of the 2007 Nobel Prize in Physics. The talk is be based on his Nobel lecture in 2007.
6761 en PS10: First attempt to commercial solar energy in the world The field of solar energy research has reached a level of maturity where truly commercial applications can be envisaged. nThe fascinating story of one of the first prototypes will be presented.
6762 en Prospect of Particle Physics in China The Beijing Electron Positron Collider (BEPC) finished its running July 2005, with great success in both the Tau-Charm physics experiment and the synchrotron radiation light source. The latest Charm physics results from BEPC are reviewed, including the observation of the new resonance of X1835 with a possible explanation of the PPbar bound state. The major upgrade of BEPC into a double ring collider, so called BEPCII, will increase its luminosity by two orders of magnitude. nnThe physics window of BEPCII is mainly the precision measurements in the Charm physics and the search for new phenomena. The construction of BEPCII is finished. The tuning of the storage ring goes smoothly. The synchrotron radiation facility of BEPCII opened to users with high performance since the end of 2006. nnThe new detector BESIII has been moved into the interaction region June, and the joint commissioning started. The non-accelerator experiments in China are promoted with great efforts, including the neutrino physics experiments, the cosmic ray measurements, and the particle astrophysics experiments in Space. nnThe reactor neutrino experiment at Daya Bay could reach the sensitivity of 0.01 on the measurement of the neutrino mixing parameter sin22 ?13. The medium and long term plan of the Chinese particle physics experiments is also discussed.
6763 en X-rays from comets - a surprising discovery Comets are kilometre-size aggregates of ice and dust, which remained from the formation of the solar system. It was not obvious to expect X-ray emission from such objects. Nevertheless, when comet Hyakutake (C/1996 B2) was observed with the ROSAT X-ray satellite during its close approach to Earth in March 1996, bright X-ray emission from this comet was discovered. This finding triggered a search in archival ROSAT data for comets, which might have accidentally crossed the field of view during observations of unrelated targets. To increase the surprise even more, X-ray emission was detected from four additional comets, which were optically 300 to 30 000 times fainter than Hyakutake. nnFor one of them, comet Arai (C/1991 A2), X-ray emission was even found in data which were taken six weeks before the comet was optically discovered. These findings showed that comets represent a new class of celestial X-ray sources. The subsequent detection of X-ray emission from several other comets in dedicated observations confirmed this conclusion. nnThe talk will summarise the highlights of the series of discoveries and provide an explanation for this unexpected phenomenon.
6764 en The LHC is safe 
6765 en Accelerators for Hadrontherapy Hadrontherapy was born in 1938, when neutron beams were used in cancer therapy, but it has become an accepted therapeutical modality only in the last fifteen years. Fast neutrons are still in use, even if their limitations are now apparent. Charged hadron beams are more favourable, since the largest specific energy deposition occurs at the end of their range in matter. The most used hadrons are at present protons and carbon ions, which allow a dose deposition which conforms to the tumour target. nnRadiobiological experiments and the results of the first clinical trials indicate that carbon ions have, besides this macroscopic property, a different way of interacting with cell at the microscopic level. There are thus solid hopes to use carbon beams of about 4500 MeV to control tumours which are radioresistant both to X-rays and to protons. nnAfter discussing these macroscopic and microscopic properties and presenting the work carried out at CERN in the framework of the Proton Ion Medical Machine Study (PIMMS), the hospital-based facilities in the world, running or under construction, will be reviewed.
6779 en Quantum gravity in three dimensions 
6780 en String theory and cosmology 
6781 en LHC machine status 
6782 en Status of the LHC detectors: design, construction, commissioning 
6783 en Planning to Learn with a Knowledge Discovery Ontology Assembly of optimized knowledge discovery workfows requires awareness of and extensive knowledge about the principles and mutual relations between diverse data processing and mining algorithms.We aim at alleviating this burden by automatically proposing workfows for the given type of inputs and required outputs of the discovery process. The methodology adopted in this study is to define a formal conceptualization of knowledge types and data mining algorithms and design a planning algorithm, which extracts constraints from this conceptualization for the given user's input-output requirements. We demonstrate our approach in two use cases, one from scientific discovery in genomics and another from advanced engineering.
6785 en Information Geometry and Its Applications Information geometry emerged from studies on invariant properties of a manifold of probability distributions. It includes convex analysis and its duality as a special but important part. Here, we begin with a convex function, and construct a dually flat manifold. The manifold possesses a Riemannian metric, two types of geodesics, and a divergence function. The generalized Pythagorean theorem and dual projections theorem are derived therefrom.We construct alpha-geometry, extending this convex analysis. In this review, geometry of a manifold of probability distributions is then given, and a plenty of applications are touched upon. Appendix presents an easily understable introduction to differential geometry and its duality.
6786 en Information Geometry: Duality, Convexity and Divergences In this talk, I explore the mathematical relationships between duality in information geometry, convex analysis, and divergence functions. nFirst, from the fundamental inequality of a convex function, a family of divergence measures can be constructed, which specializes to the familiar Bregman divergence, Jenson difference, beta-divergence, and alpha-divergence, etc. nnSecond, the mixture parameter turns out to correspond to the alpha <-> -alpha duality in information geometry (which I call "referential duality", since it is related to the choice of a reference point for computing divergence). nnThird, convex conjugate operation induces another kind of duality in information geometry, namely, that of biorthogonal coordinates and their transformation (which I call "representational duality", since it is related to the expression of geometric quantities, such as metric, affine connection, curvature, etc of the underlying manifold). Under this analysis, what is traditionally called "+1/-1 duality" and "e/m duality" in information geometry reflect two very different meanings of duality that are nevertheless intimately interwined for dually flat spaces.
6787 en Computational Photography: Epsilon to Coded Imaging Computational photography combines plentiful computing, digital sensors, modern optics, actuators, and smart lights to escape the limitations of traditional cameras, enables novel imaging applications and simplifies many computer vision tasks. However, a majority of current Computational Photography methods involve taking multiple sequential photos by changing scene parameters and fusing the photos to create a richer representation. The goal of Coded Computational Photography is to modify the optics, illumination or sensors at the time of capture so that the scene properties are encoded in a single (or a few) photographs. We describe several applications of coding exposure, aperture, illumination and sensing and describe emerging techniques to recover scene parameters from coded photographs.
6788 en The Intrinsic Geometries of Learning In a seminal paper, Amari (1998) proved that learning can be made more efcient when one uses the intrinsic Riemannian structure of the algorithms' spaces of parameters to point the gradient towards better solutions. In this paper, we show that many learning algorithms, including various boosting algorithms for linear separators, the most popular top-down decision-tree induction algorithms, and some on-line learning algorithms, are spawns of a generalization of Amari's natural gradient to some particular non-Riemannian spaces. nnThese algorithms exploit an intrinsic dual geometric structure of the space of parameters in relationship with particular integral losses that are to be minimized. We unite some of them, such as AdaBoost, additive regression with the square loss, the logistic loss, the top-down induction performed in CART and C4.5, as a single algorithm on which we show general convergence to the optimum and explicit convergence rates under very weak assumptions. As a consequence, many of the classication calibrated surrogates of Bartlett et al. (2006) admit efficient minimization algorithms.
6789 en Applications of Information Geometry to Radar Signal Processing Main issue of High Resolution Doppler Imagery is related to robust statistical estimation of Toeplitz Hermitian positive definite covariance matrices of sensor data time series (e.g. in Doppler Echography, in Underwater acoustic, in Electromagnetic Radar, in Pulsed Lidar). We consider this problem jointly in the framework of Riemannian symmetric spaces and the framework of Information Geometry. Both approaches lead to the same metric, that has been initially considered in other mathematical domains (study of Bruhat-Tits complete metric Space & Upper-half Siegel Space in Symplectic Geometry). Based on Frechet-Karcher barycenter definition & geodesics in Bruhat-Tits space, we address problem of N Covariance matrices Mean estimation. Our main contribution lies in the development of this theory for Complex Autoregressive models (maximum entropy solution of Doppler Spectral Analysis). nnSpecific Blocks structure of the Toeplitz Hermitian covariance matrix is used to define an iterative & parallel algorithm for Siegel metric computation. Based on Affine Information Geometry theory, we introduce for Complex Autoregressive Model, Kohler metric on reflection coefficients based on Kohler potential function given by Doppler signal Entropy. The metric is closely related to Kohler-Einstein manifold and complex Monge-Ampere Equation. Finally, we study geodesics in space of Kohler potentials and action of Calabi & Kohler-Ricci Geometric Flows for this Complex Autoregressive Metric. nnWe conclude with different results obtained on real Doppler Radar Data in HF & X bands : X-band radar monitoring of wake vortex turbulences, detection for Coastal X-band & HF Surface Wave Radars.
6790 en Constant-Working-Space Algorithms for Image Processing This talk surveys recent progress in constant-working-space algorithms for problems related to image processing. An extreme case is when an input image is given as read-only memory in which reading an array element is allowed but writing any value at any array element is prohibited, and also the number of working storage cells available for algorithms is at most some constant. This chapter shows how a number of important fundamental problems can be solved in such a highly constrained situation.
6791 en Sparse Geometric Super-Resolution What is the maximum signal resolution that can be recovered from partial noisy or degraded data ? This inverse problem is a central issue, from medical to satellite imaging, from geophysical seismic to HDTV visualization of Internet videos. Increasing an image resolution is possible by taking advantage of "geometric regularities", whatever it means. Super-resolution can indeed be achieved for signals having a sparse representation which is "incoherent" relatively to the measurement system. nnFor images and videos, it requires to construct sparse representations in redundant dictionaries of waveforms, which are adapted to geometric image structures. Signal recovery in redundant dictionaries is discussed, and applications are shown in dictionaries of bandlets for image super-resolution.
6792 en Sparse Sampling: Variations on a Theme by Shannon Sampling is not only a beautiful topic in harmonic analysis, with an interesting history, but also a subject with high practical impact, at the heart of signal processing and communications and their applications. The question is very simple: when is there a one-to-one relationship between a continuous-time function and adequately acquired samples of this function? A cornerstone result is of course Shannon's sampling theorem, which gives a sufficient condition for reconstructing the projection of a signal onto the subspace of bandlimited functions, and this by taking inner products with a sinc function and its shifts. Many variations of this basic framework exist, and they are all related to a subspace structure of the classes of objects that can be sampled. Recently, this framework has been extended to classes of non-bandlimited sparse signals, which do not have a subspace structure. Perfect reconstruction is possible based on a suitable projection measurement. This gives a sharp result on the sampling and reconstruction of sparse continuous-time signals, namely that 2K measurements are necessary and sufficient to perfectly reconstruct a K-sparse continuous-time signal. In accordance with the principle of parsimony, we call this sampling at Occam's rate. nnWe first review this result and show that it relies on structured Vandermonde measurement matrices, of which the Fourier matrix is a particular case. It also uses a separation into location and value estimation, the first being non-linear, while the second is linear. Because of this structure, fast, O(K^3) methods exist, and are related to classic algorithms used in spectral estimation and error correction coding. We then generalize these results to a number of cases where sparsity is present, including piecewise polynomial signals, as well as to broad classes of sampling or measurement kernels, including Gaussians and splines. Of course, real cases always involve noise, and thus, retrieval of sparse signals in noise is considered. That is, is there a stable recovery mechanism, and robust practical algorithms to achieve it. Lower bounds by Cramer-Rao are given, which can also be used to derive uncertainty relations with respect to position and value of sparse signal estimation. Then, a concrete estimation method is given using an iterative algorithm due to Cadzow, and is shown to perform close to optimal over a wide range of signal to noise ratios. This indicates the robustness of such methods, as well as their practicality. nnNext, we consider the connection to compressed sensing and compressive sampling, a recent approach involving random measurement matrices, a discrete set up, and retrieval based on convex optimization. These methods have the advantage of unstructured measurement matrices (actually, typically random ones) and therefore a certain universality, at the cost of some redundancy. We compare the two approaches, highlighting differences, similarities, and respective advantages. Finally, we move to applications of these results, which cover wideband communications, noise removal, and superresolution imaging, to name a few. We conclude by indicating that sampling is alive and well, with new perspectives and many interesting recent results and developments.nnJoint work with Thierry Blu (CUHK), Lionel Coulot, Ali Hormati (EPFL), Pier-Luigi Dragotti (ICL) and Pina Marziliano (NTU).
6793 en Image Retrieval via Kullback Divergence of Patches of Wavelets Coefficients in the k-NN Framework This talk presents a framework to define an objective measure of the similarity (or dissimilarity) between two images for image processing. The problem is twofold:n* define a set of features that capture the information contained in the image relevant for the given task andn* define a similarity measure in this feature space.nnIn this paper, we propose a feature space as well as a statistical measure on this space. Our feature space is based on a global description of the image in a multiscale transformed domain. After decomposition into a Laplacian pyramid, the coefficients are arranged in intrascale/ interscale/interchannel patches which reflect the dependencies of neighboring coefficients in presence of specific structures or textures. At each scale, the probability density function (pdf) of these patches is used as a description of the relevant information. Because of the sparsity of the multiscale transform, the most significant patches, called Sparse Multiscale Patches (SMP), describe efficiently these pdfs.nnWe propose a statistical measure (the Kullback-Leibler divergence) based on the comparison of these probability density function. Interestingly, this measure is estimated via the nonparametric, k-th nearest neighbor framework without explicitly building the pdfs. This framework is applied to a query-by-example image retrieval method. Experiments on two publicly available databases showed the potential of our SMP approach for this task. In particular, it performed comparably to a SIFT-based retrieval method and two versions of a fuzzy segmentation-based method (the UFM and CLUE methods), and it exhibited some robustness to different geometric and radiometric deformations of the images.
6794 en Machine learning and kernel methods for computer vision Kernel methods are a new theoretical and algorithmic framework for machine learning. By representing data through well defined dot-products, referred to as kernels, they allow to use classical linear supervised machine learning algorithms to non linear settings and to non vectorial data. A major issue when applying these methods to image processing or computer vision is the choice of the kernel. I will present recent advances in the design of kernels for images that take into account the natural structure of images.
6795 en 3D Visibility and Lines in Space Computing visibility information in a 3D environment is crucial to many applications such as computer graphics, vision and robotics. Typical visibility problems include computing the view from a given point, determining whether two objects partially see each other, and computing the umbra and penumbra cast by a light source. In a given scene, two points are visible if the segment joining them does not properly intersect any obstacle in the scene. The study of visibility is thus intimately related to the study of the set of free line segments in a scene. In this talk, I will review some recent combinatorial and algorithmic results related to non-occluded segments tangent to up to four objects in three dimensional scenes.
6796 en Procedural Modeling of Architectures: Towards Large Scale Visual Reconstruction Three-dimensional content is a novel modality used in numerous domains like navigation, post production & cinematography, architectural modeling and urban planning. These domains have benefited from the enormous progress has been made on 3D reconstruction from images. Such a problem consists of building geometric models of the observed environment. State of the art methods can deliver excellent results in a small scale but suffer from being local and cannot be considered in a large scale reconstruction process since the assumption of recovering images from multiple views for an important number of buildings is rather unrealistic. nnOn the other hand several efforts have been made in the graphics community towards content creation with city engines. Such models are purely graphics-based and given a set of rules (grammars) as well as dictionary of architectures (buildings) can produce virtual cities. Such engines could become far more realistic through the use of actual city models as well as knowledge of building architectures. Developing 3D models/rules/grammars that are image-based and coupling these models with actual observations is the greatest challenge of urban modeling. nnSolving the large-scale geometric modeling problem from minimal content could create novel means of world representation as well as novel markets and applications. In this talk, we will present some preliminary results on large scale modeling and reconstruction through architectural grammars.
6797 en 3D Video: A Fusion of Graphics and Vision In recent years 3-dimensional video has received a significant attention both in research and in industry. Applications range from special effects in feature films to the analysis of sports events. 3D video is concerned with the computation of virtual camera positions and fly-throughs of a scene given multiple, conventional 2D video streams. The high-quality synthesis of such view-independent video representations poses a variety of technical challenges including acquisition, reconstruction, processing, compression, and rendering. nnIn this talk I will outline the research in this area carried out at ETH over the past years. I will discuss various concepts for passive and active acquisition of 3D video using combinations of multiple cameras and projectors. Furthermore, I will address topics related to the representation and processing of the massive amount data arising from such multiple video streams. nnI will highlight the underlying technical concepts and algorithms that draw upon knowledge both from graphics and from vision. Finally I will demonstrate some commercial applications targeting at virtual replays for sports broadcasts.
6798 en Semantic Technologies for Enterprise: Adding Value to RTD Better marketing of innovative ICT solutions to the world of business requires links to be forged between researchers, ICT industry players, and corporate users, and a greater understanding of the market to be reached. This session will bring together actors involved in developing and marketing ‘Semantic Technologies for the Enterprise’ (STE), in implementing semantics in their productive processes and in applied research. These participants will be presented with market research on the conditions for uptake of STE in key economic sectors, along with real-life experiences of turning RTD output into profitable products and services. Opportunity will be given to discuss these results, particularly focusing on ways to identify and fill gaps in the market.
6799 en The Value-IT Support Action 
6800 en From the Lab to the Market 
6801 en Ovum’s Approach and Interim Analysis 
6802 en Semantic Technologies for Enterprise: Semantic Open Innovation (A Success Story) 
6803 en Open discussion 
6804 en Semantics in digital content: From multimedia to emerging 3D Media This session aims to solicit discussions, exchange ideas and establish new partnerships in the area of semantic media technologies and semantic based modelling and processing in general but will focus on 3D content in particular, according to the missions of the SMaRT society and the Coordination Action FOCUS K3D, which address the “Intelligent Content and Semantics” strategic objective of the FP7 ICT theme.nIn this session we will invite experts & representatives of projects and institutions dealing with the topic of semantics in Multimedia analysis and retrieval, in order to discuss challenges in this field and how to address them.
6805 en Introduction to SMaRT 
6806 en K-Space: Bridging the semantic gap 
6807 en MESH Project Overview 
6808 en RUSHES Project Overview 
6809 en Semantics in digital content: Open discussion 01 
6810 en FOster the Comprehension and USe of Knowledge intensive technologies for coding and sharing 3D media content 
6811 en AWG CAD/CAE and Virtual Product Modeling 
6812 en AWG Gaming and Simulation 
6813 en Medicine and Bioinformatics 
6814 en Archaeology & Cultural Heritage AWG 
6815 en Semantics in digital content: Open discussion 02 
6816 en The Expanding Information Universe: New Trends, New Forms, New Usages 
6817 en Filling the pipes – the brave new visual world 
6818 en The Information Universe of the (Near) Future 
6819 en The Expanding Information Universe: Open Discussion 
6820 en An Introduction To Collective Intelligence 
6821 en Semantics through Collective Intelligence 
6822 en Mining the Web 2.0 to Improve Search 
6823 en Social Network Analysis and CI (Collective Intelligence) 
6825 en User Interfaces and Interaction for CI 
6827 en Services infrastructure 
6828 en M2M communication platform 
6829 en Cargo Intelligence 
6830 en ICT services for goods mobility: Scenario and vision 
6831 en TraSer - Identity-Based Tracking and Web Services for SMEs 
6832 en XX 
6833 en ICT Services for Sustainable Freight Transport: Open Discussion 
6834 en Complex Metallic Alloys: Concept, Properties and Perspective The conference will focus at a few examples of the atypical behavior ofncomplex metallic alloys, including quasicrystals as the ultimate state ofnstructural complexity in a crystal made of metals. The main issue in thisntopic is to understand why Nature selects complex compounds, whereas mostnconstituent metals embody simple versions of the densest possible packing ofnhard spheres.nn In turn, complexity entails atypical physical properties that significantlyndepart from the ones known for simple metals and their alloys. Examples arentransport properties, surface electronic structure, surface energy, wettingnand friction.nn The view of the author is that complex metallic alloys help us revisitnancient, and probably also well-established problems in metal physics, thusnimproving our understanding of the basic properties of condensed matter,nwhile in parallel potential applications may be sorted out. Examples will bengiven.n----
6835 en Developments of high performance n-type carbon nanotube field-effect transistors As scaling down with Moore's law, the modern silicon complementary metal?oxide?semiconductor (CMOS) technology is facing great challenges and people are looking for alternatives. Carbon nanotube (CNT), due to its novel structure and properties, has been regarded as one of the most promising building blocks for the future integrated circuits. Since the first CNT field?effect transistor (CNTFET) was designed in 1998, device performance hasbeen continually improved. By using palladium (Pd) electrodes and high?k materials (which are less prone to current leakage) as gate dielectrics, p?type CNTFETs have now surpassed the capabilities ofnstate?of?the?art silicon p?MOSFETs. nnHowever, the development of n?type CNTFETs has lagged behind. This is mainly due to the difficulty of fabricating a Schottky barrier?free contact between metal electrodes and the conduction band of the CNT. The slow progress in producing n?CNTFETs hasngreatly hindered the development of CNT?based integrated circuits. We Recently discovered that scandium (Sc) can be used to generate an ohmic contact with thenconduction band of a CNT and high performance n?type CNTFETs can be easily fabricated. nnBased on this discovery, we proposed an CNT?based doping?free CMOS technology and pushed the limits of n?ntype CNTFETs. We also demonstrated a design of whole carbon nanotube circuits by integrating Multi?Walled CNTs with the Single?Walled CNTFETs which serve as interconnects. All of our results show a prospective future of CNT?based integrated circuits.nn----
6837 en Incorporating optical techniques in electron microscopy for comprehensive characterization of individual nanostructures The purpose of this informal talk is to introduce briefly some new additions to the research group at Peking University and seek more future collaborations with JSI.nnOptical techniques (e.g., luminescence and Raman spectroscopy) can provide rich information on semiconductor properties  (band structure, phonon structure,  confinements, etc.), which are complementary to electron microscopy techniques. Initial efforts have been carried out to combine submicron optical techniques and in situ nanoprobe technique in electron microscopy to carry out comprehensive characterization of individual semiconductor nanostructures.nnIn the first approach, we attach individual suspended semiconductor nanowires or nanorods to nanometer sized metal tips, which are compatible for different instruments, such as scanning electronnmicroscope (SEM), transmission electron microscope (TEM) and microphotoluminescence (PL).nThus optical (PL), microstructural (SEM and TEM) and electrical (nanoprobe technique inside SEM) characterization can be carried out on the same 1?D nanostructure. Our results on in situ annealed ZnO nanowires show conclusive correlations among defect?related green emission, redshift of the nearnband edge emission, carrier density and oxygen deficiency. This highly flexible technique also enables angular dependent microphotoluminescence measurements on individual suspended ZnO nanorods.nIn the second approach, we combine optical fiber probe with the nanoprobe technique in SEM to achieve comprehensive characterization of optoelectronic nanotructures inside a single chamber. nnThe nanoprobe technique, employing sharp metal tips, is used for nanostructure manipulation and electrical measurement. The fiber probe, coupled to a spectrometer or a laser and controlled by a nanomanipulator, allows local optical detection or excitation. Using in situ light emitter and photodetector based on individual nanostructures, we demonstrate that above technique with high flexibility and efficiency can play an important role in building prototype optoelectronic devices and selectingsuitable nanostructures for device purposes.n----
6839 en Neural control – Layers, Loops, Learning 
6840 en Semantic Web-based E-commerce 
6843 en Real-Time Information Processing 
6851 en An overview of the United States government's space and science policy-making process A brief overview of the basic elements of the US space and science policy-making apparatus will be presented, focussing on insights into the interactions among the principal organizations, policy-making bodies and individual participants and their respective impact on policy outcomes. Several specific examples will be provided to illustrate the points made, and in the conclusion there will be some observations on current events in the US that may shape the outcome for the near-term future of US space and science policy in several areas.
6861 en Variable selection in nonparametric additive models We consider a nonparametric additive model of a conditional mean function nin which the number of variables and additive components may be much nlarger than the sample size but the number of non-zero additive compo- nnents is small relative to the sample size. The statistical problem is to ndetermine which additive components are non-zero. The additive compo- nnents are approximated by truncated series expansions with B-spline bases. nThe adaptive group LASSO is used to select non-zero components. We ngive conditions under which this procedure selects the non-zero components ncorrectly with probability approaching one as the sample size increases. Fol- nlowing model selection, oracle-e?cient, asymptotically normal estimators of nthe non-zero components can be obtained by using existing methods. The nresults of Monte Carlo experiments show that the adaptive group LASSO nprocedure works well with samples of moderate size.
6862 en The prediction error in functional regression The talk considers functional linear regression, where scalar responses Y nare modeled in dependence of random functions. We propose a smoothing nsplines estimator for the functional slope parameter based on a slight modi- nﬁcation of the usual penalty. Theoretical analysis concentrates on the error nin an out-of-sample prediction of the response for a new random function. nIt is shown that rates of convergence of the prediction error depend on the nsmoothness of the slope function and on the structure of the predictors. We nthen prove that these rates are optimal in the sense that they are minimax nover large classes of possible slope functions and distributions of the predic- ntive curves. For the case of models with errors-in-variables the smoothing nspline estimator is modiﬁed by using a denoising correction of the covari- nance matrix of discretized curves. The methodology is then applied to a real ncase study where the aim is to predict the maximum of the concentration of nozone by using the curve of this concentration measured the preceding day.
6863 en Inverse problems in empirical risk attitudes Supported by several recent investigations the empirical pricing kernel nparadox might be considered as a stylized fact. In Chabi-Yo, Garcia & nRenault (2008) simulation studies have been presented which suggest that nthis paradox might be caused by regime switching of stock prices in ﬁnan- ncial markets. Alternatively, we want to emphasize a microeconomic view. nBased on an economic model with state dependent utilities for the ﬁnan- ncial investors we succeed in explaining the paradox by changes of the risk nattitudes. Theoretically, the change behaviour is compressed by the pric- ning kernels. As a starting point for empirical insights we shall develop and ninvestigate an inverse problem in terms of data ﬁts for estimated basic val- nues of the pricing kernel. Also numerical solutions of this problem will be npresented. nn
6864 en Variational Inference and Experimental Design for Sparse Linear Models Sparsity is a fundamental concept in modern statistics, and often the nonly general principle available at the moment to address novel learning ap- nplications with many more variables than observations. Despite the recent nadvances of the theoretical understanding and the algorithmics of sparse npoint estimation, higher-order problems such as covariance estimation or noptimal data acquisition are seldomly addressed for sparsity-favouring mod- nels, and there are virtually no scalable algorithms. nWe provide an approximate Bayesian inference algorithm for sparse lin- near models, that can be used with hundred thousands of variables. Our nmethod employs a convex relaxation to variational inference and settles an nopen question in continuous Bayesian inference: The Gaussian lower bound nrelaxation is convex for a class of super-Gaussian potentials including the nLaplace and Bernoulli potentials. nOur algorithm reduces to the same computational primitives used for nsparse estimation methods, but requires Gaussian marginal variance esti- nmation as well. We show how the Lanczos algorithm from numerical math- nematics can be employed to compute the latter. nWe are interested in Bayesian experimental design, a powerful framework nfor optimizing measurement architectures. We have applied our framework nto problems of magnetic resonance imaging design and reconstruction.
6865 en Groupwise sparsity enforcing estimators for solving the EEG/MEG inverse problem Cerebral current ﬂows are directly related to information transfer in the nbrain and thus an excellent means for studying the mechanisms of cogni- ntive processing. Electro- and Magneto-encephalography, EEG and MEG, nare noninvasive measures of these electric currents (EEG) or their respec- ntive accompanying magnetic ﬁelds (MEG). The reconstruction of the cere- nbral current density from EEG/MEG measurements is an ill-posed inverse nproblem. As the forward mapping from the current sources to the external nsensors is linear, the inverse problem may be formulated as a highly un- nder determined linear system of equations, which has no unique solution. nThe common strategy to deal with this ambiguity is regularization, i.e. ﬁt- nting the data with an additional penalization of the sources. Both l2-norm nand l1-norm based penalties have been proposed based on the neurophys- niologically motivated assumptions of smoothness and sparsity, respectively.
6866 en Nonparametric estimation of the error distribution in software testing We introduce an estimation procedure for the error distribution based non additive relations in a nonparametric setting with application to software ntesting. Therein, we face a nonlinear statistical inverse problem, which can nbe solved by Fourier methods. We derive exact conﬁdence intervals and nprove rate-optimality for the derived method. nn
6867 en Kernel Representations and Kernel Density Estimation There has been a great deal of attention in recent times particularly in nmachine learning to representation of multivariate data points x by K(x, n·) nwhere K is positive and symmetric and thus induces a reproducing kernel nHilbert space.The idea is then to use the matrix n||K(Xi , Xj )|| as a substitute for the empirical covariance matrix of a sample X1 , . . . , Xn for PCA nand other inference.(Jordan and Fukumizu(2006) for instance. Nadler et. nal(2006) connected this approach to one based on random walks and diffusion limits and indicated a connection to kernel density estimation.By nmaking at least a formal connection to a multiplication operator on a function space we make further connection and show how clustering results of nBeylkin ,Shih and Yu (2008) which apparently di?er from Nadler et al. can nbe explained. nn
6868 en Consistency of random forests and other averaging classiﬁers In the last years of his life, Leo Breiman promoted random forests for nuse in classiﬁcation. He suggested using averaging as a means of obtaining ngood discrimination rules. The base classiﬁers used for averaging are simple nand randomized, often based on random samples from the data. He left a nfew questions unanswered regarding the consistency of such rules. In this ntalk, we give a number of theorems that establish the universal consistency nof averaging rules.
6869 en Nonnegative garrote in additive models using P-splines The nonnegative garrote method was proposed as a variable selection nmethod by Breiman (1995). In this talk we consider additive modeling and napply the nonnegative garrote method for selecting among the d independent nvariables. For initial estimation of the d unknown univariate functions, we nuse P-splines estimation (Eilers & Marx (1996)) and backﬁtting is applied nto deal with the additive modeling. We compare the pro- posed method ninvolving P-splines with some other methods for additive models. The ﬁnite sample performance of the procedure is investigated via a simulation study nand an illustration with real data is provided. nn
6870 en Statistical performances of SVM Regularization in Classiﬁcation 
6871 en Approximation of Random Fields in High Dimension We consider the ?-approximation by n-term partial sums of the Karhunen- nLo`eve expansion to d-parametric random ﬁelds of tensor product-type in the naverage case setting. We investigate the behavior, as d n? ∞, of the informa- ntion complexity of approximation with error not exceeding a given level ?. nIt was recently shown that for this problem one observes the curse of dimen- nsionality (intractability) phenomenon. We aim to give the exact asymptotic nexpression for the information complexity.
6872 en Some methods of sparse recovery We suggest some new methods of sparse recovery in deterministic and nstatistical models. Examples include problems with missing data, con- nstrained minimization and other. We prove sparsity oracle inequalities both nfor the case of exact sparse model and for approximately sparse solutions.
6873 en Stability Selection for High-Dimensional Data Despite remarkable progress over the past 5 years, estimation of high- ndimensional structure, such as in graphical modeling, cluster analysis or nvariable selection in (generalized) regression, remains di?cult. Among the nmain problems are: (i) the choice of an appropriate amount of regularization; n(ii) a potential lack of stability of a solution and quantiﬁcation of evidence nor signiﬁcance of a selected structure or of a set of selected variables. nWe introduce the new method of stability selection which addresses these ntwo ma jor problems for high-dimensional structure estimation, both from a npractical and theoretical point of view. Stability selection is based on sub- nsampling in combination with (high-dimensional)selection algorithms. As nsuch, the method is extremely general and has a very wide range of ap- nplicability. Stability selection provides ﬁnite sample control for some error nrates of false discoveries and hence a transparent principle to choose a proper namount of regularization for structure estimation or model selection. Maybe neven more importantly, results are typically remarkably insensitive to the nchosen amount of regularization. Another property of stability selection is nthe empirical and theoretical improvement over pre-speciﬁed selection meth- nods. We prove for randomized Lasso that stability selection will be model nselection consistent even if the necessary conditions needed for consistency of nthe original Lasso method are violated. We demonstrate stability selection nfor variable selection, Gaussian graphical modeling and clustering, using real nand simulated data. nThis is joint work with Nicolai Meinshausen. nn
6874 en The incoherence condition in additive models We extend the idea of regularization using the Lasso, to the case of an nadditive model with p components, p being larger than the sample size n. nOur method has a group Lasso type structure, and penalizes non-smoothness nof the components in the additive model. To arrive at a sparsity oracle in- nequality, we need an incoherence condition which generalizes the incoherence nconditions used for the Lasso. Bickel et al. (2008) impose the “restricted neigenvalue assumption”, which is closely related to the “compatibility con- ndition” in van de Geer (2007), which we simply call “Condition C”. We will nformulate a version of such a “Condition C” for additive models. To verify nit, we discuss the case of random design. We prove new results for weighted nempirical processes, which make the transition from random to ﬁxed design npossible, and which only requires a population version of “Condition C”. A nconsequence is that the sparsity oracle property of our procedure holds when nthe variables are independent,andthatalsovariousdependencystructuresare allowed. nn
6875 en Sparse Canonical Correlation Analysis We present a novel method for solving Canonical Correlation Analy- nsis (CCA) in a sparse convex framework using a least squares approach. nThe presented method focuses on the scenario when one is interested in (or nlimited to) a primal representation for the ﬁrst view while having a dual rep- nresentation for the second view. Sparse CCA (SCCA) minimises the number nof features used in both the primal and dual pro jections while maximising nthe correlation between the two views. The method is demonstrated on two npaired corpuses of English-French and English-Spanish for mate-retrieval. nWe are able to observe, in the mate-retreival, that when the number of the noriginal features is large SCCA outperforms Kernel CCA (KCCA), learning nthe common semantic space from a sparse set of features. nn
6876 en Matching pursuit algorithms in machine learning I will describe a generic matching pursuit algorithm that can be used in nmachine learning for regression, subspace methods (kernel PCA and kernel nCCA) and classiﬁcation (given time). I will also describe some generalisa- ntion error bounds upper bounding their loss. Some of these bounds will nbe formed using standard sample compression bounds whilst others will be namalgamations of traditional learning theory techniques such as VC theory nand Rademacher complexities. This is joint work with John Shawe-Taylor. nn
6878 en The purpose of Academic Freedom Today Lecture by the President of the Republic, Dr Danilo Türk, delivered at the ceremony marking the 20th anniversary of the Magna Charta Universitatum.
6879 en The European Union and Its Global Role: The Partnerships That Are Needed Preisdent of the Republic, Dr Danilo Türk delivered a lecture entitled "The European Union and Its Global Role: The Partnerships That Are Needed" at Columbia University as part of the World Leaders Forum.
6880 en Europe and Islam: Coexistence or Integration? The President of the Republic of Slovenia, Dr Danilo Türk, visited the Oxford Centre for Islamic Studies, where he delivered a lecture entitled "Europe and Islam: Coexistence or Integration?”
6883 en E4 Promo Video 
6884 en E4 Framework 
6885 en E4 Customer Relationship Management (CRM) 
6886 en E4 Quality Control Management (QCM) 
6887 en E4 Project management - Add new timesheet task 
6888 en E4 Project management - Create incident 
6889 en E4 Project management - Reply to the incident 
6890 en E4 Project management - Tasks 
6891 en E4 Project management - Create expense 
6892 en E4 Project management - Send forum notifications 
6893 en E4 Project management - Receive timesheet task 
6894 en E4 Project management - Add new component 
6895 en E4 Project management - Company user setup 
6896 en E4 CKM - Advanced search 
6897 en E4 QCM - Using E4 Best Network 
6898 en E4 QCM Final 
6909 en Opening of the conference SAMT´08 
6910 en Tracking the Progress of Multimedia Semantics - from MPEG-7 to Web 3.0 This paper will describe the next generation of hybrid scalable classification systems that combine social tagging, machine-learning and traditional library classification approaches. It will also discuss approaches to the related challenge of aggregating light-weight community-generated tags with complex MPEG-7 descriptions and discipline-specific ontologies through common, extensible upper ontologies - to enhance discovery and re-use of multimedia content across disciplines and communities.
6911 en Automatic Summarization of Rushes Video using Bipartite Graphs In this paper we present a new approach for automatic summarization of rushes, or unstructured video. Our approach is composed of three major steps. First, based on shot and sub-shot segmentations, we filter sub-shots with low information content not likely to be useful in a summary. Second, a method using maximal matching in a bipartite graph is adapted to measure similarity between the remaining shots and to minimize inter-shot redundancy by removing repetitive retake shots common in rushes video. Finally, the presence of faces and motion intensity are characterised in each sub-shot. A measure of how representative the sub-shot is in the context of the overall video is then proposed. Video summaries composed of keyframe slideshows are then generated. In order to evaluate the effectiveness of this approach we re-run the evaluation carried out by TREC, using the same dataset and evaluation metrics used in the TRECVID video summarization task in 2007 but with our own assessors. Results show that our approach leads to a significant improvement in terms of the fraction of the TRECVID summary ground truth included and is competitive with other approaches in TRECVID 2007.
6912 en Performing Content-based Retrieval of Humans using Gait Biometrics In order to analyse surveillance video, we need to efficiently explore large datasets containing videos of walking humans. At this resolution, the human walk (their gait) can be determined automatically more readily than other features, such as the face. Analysis can rely retrieval of video data which has been enriched using semantic annotations. A manual annotation process is time-consuming and prone to error due to subject bias. We explore the content-based retrieval of videos containing walking subjects using semantic queries. We evaluate current biometric research using gait, unique in their effectiveness at recognising people at a distance. We introduce a set of semantic traits discernible _by humans_ at a distance, outlining their psychological validity. Working under the premise that similarity of the chosen gait signature implies similarity of certain semantic traits we perform a set of semantic retrieval experiments using popular latent semantic analysis techniques from the information retrieval community.
6913 en On Video Abstraction Systems' Architectures and Modelling Abstract Nowadays the huge amount of video material stored in multimedia repositories makes the search and retrieval of such content a very slow and usually difficult task. The existing video abstraction systems aim to ease the multimedia access problem by providing short versions of the original content which ease the search and navigation process by reducing the time spent in content browsing. There are many video abstraction system variations providing different kinds of output (video skims, keyframe-based summaries, etc). This paper presents a generic video abstraction architecture which aims characterize the stages required for a generic abstraction process as well as the definition of the theoretical aspects and requirements for the modelling of video abstraction systems which is a first step before building abstraction systems with specific characteristics.
6914 en Intelligent Information Management 
6915 en Context as a non-ontological determinant of semantics This paper is a theoretical analysis of formal annotation and ontology for the expression of the semantics of document. They are found wanting in this respect, not only for technical reasons, but because they embody a fundamentally misunderstood model of the process of signification. I propose an alternative model in which the interpretation context plays a fundamental rôle in the definition of an _activity game_ that includes all actions performed on a document, including accessing external data. I briefly discuss it and its current technical embodiment.
6916 en Meta-Metadata: An extensible semantic architecture for multimedia metadata definition, extraction and presentation This paper introduces a new extensible architecture for defining the metadata of multimedia from different sources, extracting this metadata from documents, and representing it to users. We introduce meta-metadata, semantic data structures that guide the extraction and manipulation of strongly typed metadata, including visual representations, from diverse documents. Meta-metadata declarations are automatically translated into metadata class definitions. Both are defined using the ecologylab.xml information binding framework. Extensions to the framework support manipulation of instances of the generated metadata classes with generic field accessor objects, enabling information extraction, information visualization, contextual metadata presentation, editing, and interaction. We show how meta-metadata and the metadata it generates are used in the mixed-initiative information composition information discovery support tool, combinFormation.
6917 en A System of Ontologies and Services for Hypermedia Authoring on the Web Manual authoring of hypermedia from collections of media and metadata is an hardly manageable task for non-professional users. The use of semantic annotation and intelligent authoring systems allows the automatic generation of stereotyped hypermedia presentations with effective communication power by entrusting to the intelligent system the management of constraints, content structuring and design decisions. In this work we present a software platform supporting the development of hypermedia authoring applications. The platform is composed by several web services based on a combination of logic programming and semantic web technologies. Every services share a common underlying semantic layer describing media assets, concepts representations and attributes of the authoring process. Two sample applications illustrate the usage of the platform: the Semantic Image Gallery service which allows to semantically search and virtually organize collections of images and the Hyper Atlas service which realizes the automatic generation of hypermedia presentations navigable according to a hierarchy of concepts.
6918 en A Semantic Model for the Authorisation of Context-Aware Content Adaptation Nowadays, users want to be able to access their computing resources and all kind of content using different types of devices, from wireless portable devices to stationary devices connected to local area networks in a way that context-based content adaptation has become essential for Universal Multimedia Access (UMA) scenarios. This content adaptation represents the modification of an object possibly subjected to the Intellectual Property (IP) law, and the original author or rights holder should be able to retain the possibility of vetoing or restricting the operation. Whereas MPEG-21 addresses the adaptation in the part 7 of the standard (DIA, Digital Item Adaptation), and the Rights Expressions Language (REL) in the part 5, it still lacks of an integrated approach of them.nThis paper considers the authorisation of context-aware content adaptation in a generic multimedia scenario based on the integration of two new standard-based ontologies providing a bridge between them at a semantic level. First, it is presented the Context Aware Ontology (CAO), which models the Universal Environment Descriptor (UED) tool contained in the MPEG-21 DIA standard. Then, it is introduced the Adaptation Authoriser based on the RRDOnto (Represent Rights Data) ontology, which grants that IP rights will be respected along the Value Chain while supporting the MPEG-21 License model. Finally, the integration of both is described, providing a joint model that allows the adaptation to be controlled by Content Creators and Content Distributors.
6920 en Tag Suggestr: Automatic Photo Tag Expansion using Visual Information for Photo Sharing Websites In this paper, we propose an automatic photo tag expansion system for the community photo collections, such as Flickr. Our aim is to suggest relevant tags for a target photograph uploaded to the system by a user, by incorporating the visual and textual cues from other related photographs. As the first step, the system requires the user to add only a few initial tags for each uploaded photo. These initial tags are used to retrieve related photos including the same tags in their tag lists. The set of candidate tags collected from a large pool of photos is weighted according to the similarity of the target photo to the retrieved photo including the tag. Finally, the tags in the highest rankings are used to automatically expand the tags of the target photo. The experimental results on Flickr photos show that, the use of visual similarity of semantically relevant photos to recommend tags improves the quality of suggested tags compared to other techniques that use only text information.
6921 en WhoAmI - A Web2.0 Platform for Faceted Identity Management through Aggregation of Social Media In this paper, we describe methods and an implementation for the aggregation and visualization of personal social media. We consider it important to offer non-proprietary software that is capable to raise awareness for diverse aspects of mediated identity.nThis issue has become important especially over the course of the past five years, where the role of users changed more and more from consumers towards ``prosumers'', i.e. giving away facets of their daily life as a commodity.nIntelligent aggregation of such social media may lead to coarse descriptions of digital identities, if observed over long-enough time periods. Our tool offers such possibilities to users who are concerned about the way they deal with their life and its digital representation in the public.nThis paper also outlines a number of reasons why one should be concerned about this, which are confirmed by a small user study that we conducted.
6922 en K-Space prize posters 
6923 en Clustering of Visual Data using Ant-inspired Methods 
6925 en Eliminating the Back-Tracking Step in the Longest Common Subsequence (LCSS) Algorithm for Video Sequence Matching 
6926 en Research on Networks and Media in the Framework Programme: Results, Trends and Prospects 
6927 en PLATO for Information Mining in Satellite Imagery Satellite images are numerous and weakly exploited: it is urgent to develop an efficient and fast indexing/retrieval system to easy their access. Content-based image retrieval systems (CBIR) are known to provide an efficient framework. We thus propose to associate a CBIR approach with text-based queries to adapt to these big (12000$\times$12000 pixels) and semantically rich images. The presented system relies on a multimedia data mining system called PLATO able to adapt to any kind of multimedia data. Moreover state-of-the-art relevance feedback strategy is introduced to provide interactive learning and auto annotation.
6928 en Semantic-driven multimedia retrieval with the MPEG Query Format The MPEG Query Format (MPQF) is a new standard from the MPEG standardization committee which provides a standardized interface to multimedia document repositories. The purpose of this paper is describing the necessary modifications which will allow MPQF to manage metadata modelled with Semantic Web languages like RDF and OWL, and query constructs based on SPARQL. The suggested modifications include the definition of a new MPQF query type, and a generalization of the MPQF metadata processing model. As far as we know, this is the first work to apply the MPEG Query Format to semantic-driven search and retrieval of multimedia contents.
6929 en Enriching a Thesaurus to Improve Retrieval of Audiovisual Documents In many archives of audiovisual documents, annotation and retrieval are done using metadata from a structured vocabulary or a thesaurus. In practice, many of these thesauri have limited or no structure. The objective of this paper is to find out whether retrieval of audiovisual resources from a collection indexed with an in-house thesaurus can be improved by anchoring the thesaurus to an external, semantically richer thesaurus. We propose a method to enrich the structure of a thesaurus and we investigate its added value for retrieval purposes.nWe first anchor the thesaurus to an external resource, WordNet. From this anchoring we infer relations between pairs of terms in the thesaurus that were previously unrelated. We employ the enriched thesaurus in a retrieval experiment on a TRECVID 2007 data set. The results are promising: with simple techniques we are able to enrich a thesaurus in such a way that it adds to retrieval performance.
6930 en Validating the Detection of Everyday Concepts in Visual Lifelogs The Microsoft SenseCam is a small lightweight wearable camera used to passively capture photos and other sensor readings from a user's day-to-day activities. It can capture up to 3,000 images per day, equating to almost 1 million images per year. It is used to aid memory by creating a personal multimedia lifelog, or visual recording of the wearer's life. However the sheer volume of image data captured within a visual lifelog creates a number of challenges, particularly for locating relevant content. Within this work, we explore the applicability of semantic concept detection, a method often used within video retrieval, on the novel domain of visual lifelogs. A concept detector models the correspondence between low-level visual features and high-level semantic concepts (such as indoors, outdoors, people, buildings, etc.) using supervised machine learning. By doing so it determines the probability of a concept's presence. We apply detection of 27 everyday semantic concepts on a lifelog collection composed of 257,518 SenseCam images from 5 users. The results were then evaluated on a subset of 95,907 images, to determine the precision for detection of each semantic concept and to draw some interesting inferences on the lifestyles of those 5 users. We additionally present future applications of concept detection within the domain of lifelogging.
6931 en Using Fuzzy DLs to Enhance Semantic Image Analysis Research in image analysis has reached a point where detectors can be learned in a generic fashion for a significant number of conceptual entities. The obtained performance however exhibits versatile behaviour, reflecting implications over the training set selection, similarities in visual manifestations of distinct conceptual entities, and appearance variations of the conceptual entities. A factor partially accountable for these limitations relates to the fact that machine learning techniques realise the transition from visual features to conceptual entities based solely on information regarding perceptual features. Hence, a significant part of knowledge is missed. In this paper, we investigate the use of formal semantics in order to benefit from the logical associations between the conceptual entities, and thereby alleviate part of the challenges involved in extracting semantic descriptions. More specifically, a fuzzy DL based reasoning framework is proposed for the extraction of enhanced image descriptions based on an initial set of graded annotations, generated through generic image analysis techniques. Under the proposed reasoning framework, the initial descriptions are integrated at a semantic level, resolving inconsistencies emanating from conflicting descriptions. Furthermore, the descriptions are enriched by means of entailment, resulting in more complete image descriptions. Experimentation in the domain of outdoor images has shown very promising results, demonstrating the added value in terms of accuracy and completeness.
6932 en Labelling Image Regions Using Wavelet Features and Spatial Prototypes In this paper we present an approach for image region classification that combines low-level processing with high-level scene understanding. For the low-level training, predefined image concepts are statistically modeled using wavelet features extracted directly from image pixels. For classification, features of a given test region compared with these statistical models provide probabilistic evaluations for all possible image concepts. Maximizing these values themselves already leads to a classification result (label). However, in our paper they are used as an input for the high-level approach exploiting explicitly represented spatial arrangements of labels, so called spatial prototypes. We formalize the problem using Fuzzy Constraint Satisfaction Problems and Linear Programming. They provide a model with explicit knowledge that is suitable to aid the task of region labeling. Results of experiments performed for more than 6000 test image regions show that using the combination of low-level and high-level image analysis increases the labeling accuracy significantly.
6933 en Data by the people, for the people What can we learn from social media and community-contributed collections of information on the web? The most salient attribute of social media is the creation of an environment that promotes user contributions in the form of authoring, curation, discussion and re-use of content. This activity generates large volumes of data, including some types of data that were not previously available. Even more importantly, design decisions in these applications can directly influence the users' motivations to participate, and hugely affect the resultant data. I will discuss the cycle of social media, and argue that a 'holistic' approach to social media systems, which includes design of applications and user research, can advance data mining and information retrieval systems.n  Using Flickr as an example, I will describe a study in which we examine what motivates users to add tags and "geotags" to their photos. The new data enables extraction of meaningful (not to say "semantic") information from the Flickr collection. We use the extracted information, for example, to produce summaries and visualizations of the Flickr collection, making the repository more accessible and easier to search, browse and understand as it scales. In the process, the user input helps alleviate previously intractable problems in multimedia content analysis.
6934 en SAMT 2009 
6935 en The Sample Complexity of Learning the Kernel The success of kernel based learning algorithms depends upon the suitability of the kernel to the learning ntask. Ideally, the choice of a kernel should based on prior information of the learner about the task at hand. nHowever, in practice, kernel parameters are being tuned based on available training data. I will discuss the nsample complexity overhead associated with such ”learning the kernel” scenarios. I will address the setting nin which the training data for the kernel selection is target labeled examples, as well as settings in which this ntraining is based on di?erent types of data, such as unlabeled examples and examples labeled by a di?erent n(but related) tasks. Part of this work is joint with Nati Srebro. nn
6936 en Second Order Optimization of Kernel Parameters We investigate the use of second order optimization approaches for solving the multiple kernel learning (MKL) nproblem. We show that the hessian of the MKL can be computed e?ciently and this information can be used nto compute a better descent direction than the gradient (used in the state-of-the-art SimpleMKL algorithm). nWe then empirically show that our new approaches outperforms SimpleMKL in terms of computational ne?ciency. nn
6937 en Multi-Kernel Learning for Biology One of the primary tasks facing biologists today is to integrate the di?erent views of molecular biology that nare provided by various types of experimental data. In yeast, for example, for a given gene we typically know nthe protein it encodes, that protein’s similarity to other proteins, the mRNA expression levels associated with nthe given gene under hundreds of experimental conditions, the occurrences of known or inferred transcription nfactor binding sites in the upstream region of that gene, and the identities of many of the proteins that interact nwith the given gene’s protein product. Each of these distinct data types provides one view of the molecular nmachinery of the cell. nKernel methods allow us to represent these heterogeneous data types in a normal form, and to use kernel nalgebra to reason about more than one type of data simultaneously. Consequently, multi-kernel learningnmethods have been applied to a variety of biology applications. In this talk, I will describe several of these napplications, outline the lessons we have learned from applying multi-kernel learning methods to real data, nand suggest several avenues for future research in this area. n
6938 en Learning Sequence Kernels 
6939 en Learning with Multiple Similarity Functions 
6940 en Multi-Task Learning via Matrix Regularization 
6941 en Feature Selection - From Correlation to Causality Variable and feature selection have become the focus of much research in areas of application for which ndatasets with tens or hundreds of thousands of variables are available. These areas include text processing ofninternet documents, gene expression array analysis, and combinatorial chemistry. The ob jective of variable nselection is three-fold: improving the prediction performance of the predictors, providing faster and more ncost-e?ective predictors, and providing a better understanding of the underlying process that generated the ndata. This tutorial will cover a wide range of aspects of such problems: providing a better deﬁnition of nthe ob jective function, feature construction, feature ranking, multivariate feature selection, e?cient search nmethods, and feature validity assessment methods. Most feature selection methods do not attempt to nuncover causal relationships between feature and target and focus instead on making best predictions. We nwill examine situations in which the knowledge of causal relationships beneﬁts feature selection. Such beneﬁts nmay include: explaining relevance in terms of causal mechanisms, distinguishing between actual features and nexperimental artifacts, predicting the consequences of actions performed by external agents, and making npredictions in non-stationary environments. nn
6942 en Learning Bounds for Support Vector Machines with Learned Kernels 
6943 en Mixed Norm Kernels, Hyperkernels and Other Variants 
6944 en Non-sparse Multiple Kernel Learning 
6945 en Infinite Kernel Learning In this paper we build upon the Multiple Kernel Learning (MKL) framework. We rewrite the problem nin the standard MKL formulation which leads to a Semi-Inﬁnite Program. We devise a new algorithm to nsolve it (Inﬁnite Kernel Learning, IKL). The IKL algorithm is applicable to both the ﬁnite and inﬁnite case nand we ﬁnd it to be faster and more stable than SimpleMKL. Furthermore we present the ﬁrst large scale ncomparison of SVMs to MKL on a variety of benchmark datasets, also comparing IKL. The results show ntwo things: a) for many datasets there is no beneﬁt in using MKL/IKL instead of the SVM classiﬁer, thus nthe ﬂexibility of using more than one kernel seems to be of no use, b) on some datasets IKL yields massive nincreases in accuracy over SVM/MKL due to the possibility of using a largely increased kernel set. For those ncases parameter selection through Cross-Validation or MKL is not applicable. nn
6946 en Kernel Learning for Novelty Detection We consider kernel learning for one-class Support Vector Machines. We consider a mix of 2- and 1-norms nof the individual weight vector norms allowing control of the sparsity of the resulting kernel combination. nThe resulting optimisation can be solved e?ciently using a coordinate gradient method. We consider an napplication to automatically detecting the appropriate metric for a guided image search task. nn
6947 en Optimization in Machine Learning: Recent Developments and Current Challenges The use of optimization as a framework for formulating machine learning problems has become much more widespread in recent years. In some cases, the demands of the machine learning problems go beyond the scope of traditional optimization paradigms. While existing optimization formulations and algorithms serve as an good starting point for the solution strategies, important work must be carried out at the interface of optimization and machine learning to devise strategies that exploit the special features of the application and that perform well on very large data sets. This talk reviews recent developments from an optimization perspective, focusing on activity during the past three years, and looking in particular at problems where the machine learning application has motivated novel algorithms or analysis in the optimization domain. We also discuss some current challenges, highlighting several recent developments in optimization that may be useful in machine learning applications.nn
6948 en Online and Batch Learning Using Forward-Looking Subgradients 
6949 en Robustness and Regularization of Support Vector Machines We consider a robust classiﬁcation problem and show that standard regularized nSVM is a special case of our formulation, providing an explicit link between reg- nularization and robustness. At the same time, the physical connection of noise and nrobustness suggests the potential for a broad new family of robust classiﬁcation nalgorithms. Finally, we show that robustness is a fundamental property of classi- nﬁcation algorithms, by re-proving consistency of support vector machines using nonly robustness arguments (instead of VC dimension or stability).
6950 en Training a Binary Classifier with the Quantum Adiabatic Algorithm This paper describes how to make the problem of binary classiﬁcation amenable to nquantum computing. A formulation is employed in which the binary classiﬁer is nconstructed as a thresholded linear superposition of a set of weak classiﬁers. The nweights in the superposition are optimized in a learning process that strives to min- nimize the training error as well as the number of weak classiﬁers used. No efﬁcient nsolution to this problem is known. To bring it into a format that allows the applica- ntion of adiabatic quantum computing (AQC), we ﬁrst show that the bit-precision nwith which the weights need to be represented only grows logarithmically with the nratio of the number of training examples to the number of weak classiﬁers. This nallows to effectively formulate the training process as a binary optimization prob- nlem. Solving it with heuristic solvers such as tabu search, we ﬁnd that the resulting nclassiﬁer outperforms a widely used state-of-the-art method, AdaBoost, on a va- nriety of benchmark problems. Moreover, we discovered the interesting fact that nbit-constrained learning machines often exhibit lower generalization error rates. nChanging the loss function that measures the training error from 0-1 loss to least nsquares maps the training to quadratic unconstrained binary optimization. This ncorresponds to the format required by D-Wave’s implementation of AQC. Simu- nlations with heuristic solvers again yield results better than those obtained with nboosting approaches. Since the resulting quadratic binary program is NP-hard, nadditional gains can be expected from applying the actual quantum processor.
6951 en Polyhedral Approximations in Convex Optimization We propose a unifying framework for solution of convex programs by polyhedral approximation. It includes classical methods, such as cutting plane, Dantzig-Wolfe decomposition, bundle, and simplicial de- composition, but also includes refinements of these methods, as well as new methods that are well-suited for important large-scale types of problems, arising for example in network optimization. nn
6952 en Large-scale Machine Learning and Stochastic Algorithms The presentation stresses important differences between machine learning and conventional optimisation approaches and proposes some solutions. The first part discusses the the interaction of two kind of asympotic properties: those of the statistics and those of optimization algorithm. Unlikely optimization algorithm such as stochastic gradient show amazing performance for large-scale machine learning problems. The second part shows how the deeper causes of this performance suggests the theoretical possibility learn large-scale problems with a single pass over the data. Practical algorithms will be discussed: various second order stochastic gradients, averaging methods, dual methods with data reprocessing...
6953 en An Improved Branch-and-Bound Method for Maximum Monomial Agreement The nN P -hard maximum monomial agreement (MMA) problem consists of ﬁnd- ning a single logical conjunction that best ﬁts a weighted dataset of “positive” and n“negative” binary vectors. Computing classiﬁers using boosting methods involves na maximum agreement subproblem at each iteration, although such subproblems nare typically solved by heuristic methods. Here, we describe an exact branch nand bound method for maximum agreement over Boolean monomials, improv- ning on the earlier work of Goldberg and Shan [14]. In particular, we develop a ntighter upper bounding function and an improved branching procedure that ex- nploits knowledge of the bound and the dataset, while having a lower branching nfactor. Experimental results show that the new method is able to solve larger nproblem instances and runs faster within a linear programming boosting proce- ndure applied to medium-sized datasets from the UCI repository. nn
6954 en Tuning Optimizers for Time-Constrained Problems using Reinforcement Learning. Many popular optimization algorithms, like the Levenberg-Marquardt algorithm n(LMA), use heuristic-based “controllers” that modulate the behavior of the op- ntimizer during the optimization process. For example, in the LMA a damping nparameter ? is dynamically modiﬁed based on a set of rules that were developed nusing various heuristic arguments. Here we show that a modern reinforcement nlearning technique utilizing a very simple state space can dramatically improve nthe performance of general purpose optimizers, like the LMA, on problems where nthe number of function evaluations allowed is constrained by a budget. Results nare given on both classical non-linear optimization problems as well as a difﬁcult ncomputer vision task. Interestingly the controllers learned for a particular opti- nmization domain work well on other optimization domains. Thus, the controller nappeared to have extracted optimization rules that were not just domain speciﬁc nbut generalized across a range of optimization domains. nn
6956 en Welcome and program presentation, short overview over the posters Machine learning has traditionally been focused on prediction. Given observations that have been generatednby an unknown stochastic dependency, the goal is to infer a law that will be able to correctly predict futurenobservations generated by the same dependency. Statistics, in contrast, has traditionally focused on datanmodeling, i.e., on the estimation of a probability law that has generated the data. During recent years, thenboundaries between the two disciplines have become blurred and both communities have adopted methodsnfrom the other, however, it is probably fair to say that neither of them has yet fully embraced the ﬁeldnof causal modeling, i.e., the detection of causal structure underlying the data. Since the Eighties there hasnbeen a community of researchers, mostly from statistics and philosophy, who have developed methods aimingnat inferring causal relationships from observational data. While this community has remained relativelynsmall, it has recently been complemented by a number of researchers from machine learning. The goal ofnthis workshop is to discuss new approaches to causal discovery from empirical data, their applications andnmethods to evaluate their success. Emphasis will be put on the deﬁnition of objectives to be reached andnassessment methods to evaluate proposed solutions. The participants are encouraged to participate in ancompetition pot-luck in which datasets and problems will be exchanged and solutions proposed.
6957 en Causal Inference as Computational Learning 
6958 en Competition Results 
6959 en Benchmarks, wikis, and open-source causal discovery 
6960 en Causal Structure Search: Philosophical Foundations and Future Problems 
6961 en Distinguishing Causes from Effects using Nonlinear Acyclic Causal Models 
6962 en Causal models as conditional density models 
6963 en Analysis of the binary instrumental variable model 
6964 en Beware of the DAG! 
6965 en Introduction and overwiew We believe that the wide-spread adoption of open source software policies will have a tremendous ipactnon the ﬁeld of machine learning. The goal of this workshop is to further support the current dvelopmentsnin this area and give new impulses to it. Following the success of the inaugural NIPS-MLOSS workshopnheld at NIPS 2006, the Journal of Machine Learning Research (JMLR) has started a new track for machinenlearning open source software initiated by the workshop’s organizers. Many prominent machine learningnresearchers have co-authored a position paper advocating the need for open source software in machinenlearning. Furthermore, the workshop’s organizers have set up a community website mloss.org where peoplencan register their software projects, rate existing projects and initiate discussions about projects and relatedntopics. This website currently lists 156 such projects including many prominent projects in the area ofnmachine learning. The main goal of this workshop is to bring the main practitioners in the area of machinenlearning open source software together in order to initiate processes which will help to further improve thendevelopment of this area. In particular, we have to move beyond a mere collection of more or less unrelatednsoftware projects and provide a common foundation to stimulate cooperation and interoperability betweenndi?erent projects. An important step in this direction will be a common data exchange format such thatndi?erent methods can exchange their results more easily.
6966 en Octave GNU Octave is a high-level language, primarily intended for numerical computations. It provides a con-nvenient command line interface for solving linear and nonlinear problems numerically, and for performingnother numerical experiments using a language that is mostly compatible with Matlab. It may also be usednas a batch-oriented language.
6967 en Torch Torch provides a Matlab-like environment for state-of-the-art machine learning algorithms. It is easy to usenand very e?cient, thanks to a simple-yet-powerful fast scripting language (Lua), and a underlying C/C++nimplementation. Torch is easily extensible and has been shown to scale to very large applications.
6968 en Shark Shark is a C++ machine learning library. Tutorials and html documentation make Shark easy to learn.nThe installation of Shark is straightforward, it does not depend on any third party software and compilesnunder Linux, Solaris, MacOS, and Windows. Various example programs serve as starting points for ownnprojects. Shark provides methods for linear and nonlinear optimization, in particular evolutionary andngradient-based algorithms. It comes with di?erent types of artiﬁcial neural networks ranging from standard
6969 en Kernlab kernlab is an R package providing kernel-based machine learning functionality. It is designed to providentools for kernel algorithm development but also includes a range of popular machine learning methods fornclassiﬁcation, regression, clustering, novelty detection, quantile regression and dimensionality reduction.nAmong other algorithms included in the package are Support Vector Machines, Spectral Clustering, KernelnPCA, a QP solver and a range of kernels (Gaussian, Laplacian, string kernels etc.).
6970 en Machine Learning Py (mlpy) We introduce mlpy, a high-performance Python package for predictive modeling. It makes extensive use ofnNumPy to provide fast N-dimensional array manipulation and easy integration of C code. Mlpy provides highnlevel procedures that support, with few lines of code, the design of rich Data Analysis Protocols (DAPs) fornpredictive classiﬁcation and feature selection. Methods are available for feature weighting and ranking, datanresampling, error evaluation and experiment landscaping. The package includes tools to measure stabilitynin sets of ranked feature lists, of special interest in bioinformatics for functional genomics, for which largenscale experiments with up to 106 classiﬁers have been run on Linux clusters and on the Grid.
6971 en MDP – Modular toolkit for Data Processing Modular toolkit for Data Processing (MDP) is a Python data processing framework. From the user’s per-nspective, MDP is a collection of supervised and unsupervised learning algorithms and other data processingnunits that can be combined into data processing sequences and more complex feed-forward network archi-ntectures. From the scientiﬁc developer’s perspective, MDP is a modular framework, which can easily benexpanded. nnThe implementation of new algorithms is easy and intuitive. The new implemented units arenthen automatically integrated with the rest of the library. The base of available algorithms is steadily in-ncreasing and includes, to name but the most common, Principal Component Analysis (PCA and NIPALS),nseveral Independent Component Analysis algorithms (CuBICA, FastICA, TDSEP, and JADE), Slow FeaturenAnalysis, Gaussian Classiﬁers, Restricted Boltzmann Machine, and Locally Linear Embedding.
6972 en What is a good mloss project 
6973 en Matplotlib matplotlib is a python 2D plotting library which produces publication quality ﬁgures in a variety of hardcopynformats and interactive environments across platforms. matplotlib can be used in python scripts, the pythonnand ipython shell (ala matlab or mathematica), web application servers, and works with six graphical userninterface toolkits. matplotlib tries to make easy things easy and hard things possible. You can generatenplots, histograms, power spectra, bar charts, error charts, scatter plots, etc, with just a few lines of code.nFor the power user, you have full control of line styles, font properties, axes properties, etc, via an objectnoriented interface or via a handle graphics interface familiar to matlab users.
6974 en Disco Disco is an open-source implementation of the Map-Reduce framework for distributed computing. As thenoriginal framework, Disco supports parallel computations over large data sets on unreliable cluster of com-nputers. You don’t need a cluster to use Disco – a script is provided that installs Disco automatically to thenAmazon’s EC2 computing cloud where you get computing resources on demand basis.
6975 en Nieme Nieme is a machine learning library for large-scale classiﬁcation, regression and ranking. It relies on the framework of energy-based models which uniﬁes several learning algorithms ranging from simple perceptronsnto recent models such as the Pegasos support vector machine or L1-regularized maximum entropy models.nThis framework also uniﬁes batch and stochastic learning which are both seen as energy minimizationnproblems. Nieme can hence be used in a wide range of situations, but is particularly interesting for very-nlarge-scale learning tasks where both the examples and the features are processed incrementally. Being ablento deal with new incoming features at any time within the learning process is another key feature of thenNieme toolbox. Nieme is released under the GPL license. It is e?ciently implemented in C++ and worksnon Linux, MacOS and Windows. Interfaces are available for C++, Java and Python.
6976 en libDAI libDAI is a free and open source C++ library (licensed under GPL) that provides implementations ofnvarious (approximate) inference methods for discrete graphical models. libDAI supports arbitrary factorngraphs with discrete variables; this includes discrete Markov Random Fields and Bayesian Networks. Thenlibrary is targeted at researchers; to be able to use the library, a good understanding of graphical modelsnis needed. Currently, libDAI supports the following (approximate) inference methods: exact inference bynbrute force enumeration, exact inference by junction-tree methods, Mean Field, Loopy Belief Propagation,nTree Expectation Propagation, Generalized Belief Propagation, Double-loop GBP, and various variants ofnLoop Corrected Belief Propagation. Planned extensions are Gibbs sampling and IJGP, as well as variousnmethods for obtaining bounds on the partition sum and on marginals (Bound Propagation, Box Propagation,nTree-based Reparameterization).
6977 en BCPy2000 BCPy2000nJeremy Hill, Max-Planck-Institute for Biological Cybernetics, T¨ ubingen, GermanynBCPy2000 provides a platform for rapid, ﬂexible development of experimental Brain-Computer Interfacensystems based on the BCI2000.org project. From the developer’s point of view, the implementation isncarried out in Python, taking advantage of various high-level packages: VisionEgg for stimulus presentation,nNumPy and SciPy for signal processing and classiﬁcation, and IPython for interactive debugging. BCPy2000nimplements a lot of infrastructure allowing you to get new experiments up and running quickly. It alsoncontains a set of optional tools, which are still a work in progress but which are rapidly turning into ankind of “standard library” of object-oriented signal-processing and stimulus widgets. These features makenit a ﬂexible platform for developers of new NumPy/SciPy-based machine-learning algorithms in the ﬁeld ofnrealtime biosignal analysis.
6978 en Model Monitor Common practice in Machine Learning often implicitly assumes a stationary distribution, meaning that thendistribution of a particular feature does not change over time. In practice, however, this assumption is oftennviolated and real-world models have to be retrained as a result. It would be helpful, then, to be able tonanticipate and plan for changes in distribution in order to avoid this retraining. Model Monitor is a Javantoolkit that addresses this problem. It provides methods for detecting distribution shifts in data, comparingnthe performance of multiple classiﬁers under shifts in distribution, and evaluating the robustness of individualnclassiﬁers to distribution change. As such, it allows users to determine the best model (or models) for theirndata under a number of potential scenarios. Additionally, Model Monitor is fully integrated with the WEKAnmachine learning environment, so that a variety of commodity classiﬁers can be used if desired.
6979 en RL Glue and Codecs Glue RL-Glue is a protocol and software implementation for evaluating reinforcement learning algorithms. Ournsystem facilitates the comparison of alternative algorithms and can greatly accelerate research progress asnthe UCI database has accelerated progress in supervised machine learning. Creating a comparable bench-nmarking resource for reinforcement learning is challenging because of the temporal nature of reinforcementnlearning. Reinforcement learning agents interact with a dynamic process (the environment) which gener-nates observations and rewards. The observations and rewards received by the learning agent depend on the actions; training data cannot simply be stored in a ﬁle as they are in supervised learning. Instead, the rein-nforcement learning agent and environment must be interacting programs. RL-Glue agents and environmentsncan be written in Java, C/C++, Matlab, Python, and Lisp and can all run on one machine, or can connectnacross the Internet. In this seminar, we will introduce the design principles that helped shape RL-Gluenand demonstrate some of the interesting extensions that have been created by the reinforcement learningncommunity.
6980 en Experiment Databases for Machine Learning Experiment Databases for Machine Learning is a large public repository of machine learning experiments asnwell as a framework for producing similar databases for speciﬁc goals. This projects aims to bring the infor-nmation contained in many machine learning experiments together and organize it a way that allows everyonento investigate how learning algorithms have performed in previous studies. To share such information withnthe world, a common language is proposed, dubbed ExpML, capturing the basic structure of a large rangenof machine learning experiments while remaining open for future extensions. This language also enforcesnreproducibility by requiring links to the used datasets and algorithms and by storing all details of the ex-nperiment setup. All stored information can then be accessed by querying the database, creating a powerfulnway to collect and reorganize the data, thus warranting a very thorough examination of the stored results.nThe current publicly available database contains over 500,000 classiﬁcation and regression experiments, andnhas both an online interface, at http://expdb.cs.kuleuven.be, as well as a stand-alone explorer tool o?eringnvarious visualization techniques. This framework can also be integrated in machine learning toolboxes tonautomatically stream results to a global (or local) experiment database, or to download experiments thatnhave been run before.
6981 en Reproducible research 
6983 en Recent Advances in Learning Sparse Structured Input/Output Model: Models, Algorithms, and Applications 
6984 en Integrating Ontological Prior Knowledge into Relational Learning 
6985 en Relation-Prediction in Multi-Relational Domains using Matrix-Factorization 
6986 en Graphical Multi-Task Learning 
6987 en Joint Learning of Multiple Structured Output Prediction Tasks 
6988 en Learning to Predict Combinatorial Structures 
6989 en Joint Kernel Support Estimation for Structured Prediction 
6990 en Learning Optimal Subsets with Implicit User Preferences 
6991 en Learning Structural Support Vector Machines with Latent Variables 
6992 en Algebraic statistics for random graph models: Markov bases and their uses We use algebraic geometry to study a statistical model for the analysis of networks represented by graphs with directed edges due to Holland and Leinhardt, known as p1, which allows for differential attraction (popularity) and expansiveness, as well as an additional effect due to reciprocation. In particular, we attempt to derive Markov bases for p1 and to link these to the results on Markov bases for working with log-linear models for contingency tables. Because of the contingency table representation for p1 we expect some form of congruence. Markov bases and related algebraic geometry notions are useful for at least two statistical problems: (i) determining condition for the existence of maximum likelihood estimates, and (ii) using them to traverse conditional (given minimal sufficient statistics) sample spaces, and thus generating ``exact'' distributions useful for assessing goodness of fit. We outline some of these potential uses for the algebraic representation of p1.
6993 en Algebraic statistics and contingency tables In this talk I will give an overview of the role of algebraic statistics in the statistical analysis of contingency tables. I will survey major areas in which algebraic methods proved to be crucial and provided a fertile ground for novel research directions: computation of sharp integer bounds for cell entries, existence of maximum likelihood estimates, simulation from probability distributions on spaces of tables, Markov bases, high-dimensional sparse tables with structural zeros, log-linear model selection. I will give examples that illustrate this methodology and talk about open problems.n
6994 en Toric Modification on Mixture Models In the Bayes estimation, it was pointed out that resolution of singularity provides an algorithm to elucidate the generalization performance of learning machines. However, there is no effective procedure to find the resolution map. This presentation proposes a new method to find it based on the toric modification, using Newton diagram. By the proposed method, learning curves of several hierarchical models are clarified.
6995 en Learning Parameters in Discrete Naive Bayes Models by Computing Fibers of the Parametrization map Discrete Naive Bayes models are usually defined parametrically with a map from a parameter space to a probability distribution space. First, we present two families of algorithms that compute the set of parameters mapped to a given discrete Naive Bayes distribution satisfying certain technical assumptions. Using these results, we then present two families of parameter learning algorithms that operate by projecting the distribution of observed relative frequencies in a dataset onto the discrete Naive Bayes model considered. They have nice convergence properties, but their computational complexity grows very quickly with the number of hidden classes of the model.
6996 en Stationary Subspace Analysis Non-stationarities are an ubiquitous phenomenon in real-world data, yet they challenge standard Machine Learning methods: if training and test distributions differ we cannot, in principle, gen- eralise from the observed training sample to the test distribution. This affects both supervised and unsupervised learning algorithms. In a classification problem, for instance, we may infer spurious dependen- cies between data and label from the the training sample that are mere artefacts of the non-stationarities. Conversely, identifying the sources of non-stationary behaviour in order to better understand the analyzed system often lies at the heart of a scientific question. To this end, we propose a novel unsupervised paradigm: Stationary Subspace Analysis (SSA). SSA decomposes a multi-variate time-series into a stationary and a non-stationary subspace. We derive an efficient algorithm that hinges on an optimization procedure in the Special Orthogonal Group. By exploiting the Lie group structure of the optimization manifold, we can explicitly factor out the inherent symmetries of the problem and thereby reduce the number of parameters to the exact degrees of freedom. The practical utility of our approach is demonstrated in an application to Brain Computer-Interfacing (BCI).
6997 en Alternatives to the Discrete Fourier Transform It is well-known that the discrete Fourier transform (DFT) of a finite length discrete-time signal samples the discrete-time Fourier transform of the same signal at equidistant points on the unit circle. Hence, as the signal length goes to infinity, the DFT approaches the DTFT. Associated with the DFT are circular convolution and a periodic signal extension. In this paper we identify a large class of alternatives to the DFT using the theory of polynomial algebras. Each of these Fourier transforms approaches the DTFT just as the DFT does, but has its own signal extension and notion of convolution, which therefore are not periodic. Furthermore, these Fourier transforms have Vandermonde structure, which enables their computation via fast $O(n \log^2(n))$ algorithms.n
6998 en Graph Helmholtzian and rank learning The graph Helmholtzian is the graph theoretic analogue of the Helmholtz operator or vector Laplacian, in much the same way the graph Laplacian is the analogue of the Laplace operator or scalar Laplacian. We will see that a decomposition associated with the graph Helmholtzian provides a way to learn ranking information from incomplete, imbalanced, and cardinal score-based data. In this framework, an edge flow representing pairwise ranking is orthogonally resolved into a gradient flow (acyclic) that represents the L2-optimal global ranking and a divergence-free flow (cyclic) that quantifies the inconsistencies. If the latter is large, then the data does not admit a statistically meaningful global ranking. A further decomposition of the inconsistent component into a curl flow (locally cyclic) and a harmonic flow (locally acyclic) provides information on the validity of small- and large-scale comparisons of alternatives. This is joint work with Xiaoye Jiang, Yuan Yao, and Yinyu Ye.
6999 en Identity Management On Homogeneous spaces We consider the identity management problem, where the identities are classified into two classes, red and blue. The purpose here is to make predictions of the two class identities when confusions arise among identities. In this work, we propose a principle to maintain probability distributions over homogeneous space which provides a mechanism valid for taking into account of any desired degree of approximation. Markov models are used to formulate the two class identity management problem which tries to compactly summarize distributions on homogeneous spaces. Projecting down and lifting up information on different order of statistics can be achieved by using Radon transformations. The commutative property of Markov updating with Radon transform enable us to maintain exact information over different order of statistics. Thus, accurate classification predictions can be made based on the low order statistics we maintained. We evaluate the performance of our algorithms on a real camera network data and show effectiveness of our scheme.
7000 en Adaptive Fourier-Domain Inference on the Symmetric Group 
7001 en Consistent Structured Estimation for Weighted Bipartite Matching Given a weighted bipartite graph, the assignment problem consists of finding the heaviest perfect match. This is a classical problem in combinatorial optimization, which is solvable exactly and efficiently by standard methods such as the Hungarian algorithm, and is widely applicable in real-world scenarios. We give an exponential family model for the assignment problem. Edge weights are obtained from a suitable composition of edge features and a parameter vector, which is learned so as to maximize the likelihood of a sample consisting of training graphs and their labeled matches. The resulting consistent estimator contrasts with existing max-margin structured estimators, which are inconsistent for this problem.n
7002 en Beyond Search - Computational Intelligence for the Web - Introduction 
7003 en Collective Wisdom: Information Growth in Wikis and Blogs Wikis and blogs have become enormously successful media for collaborative information creation. Articles and posts accrue information through the asynchronous editing of users who arrive both seeking information and possibly able to contribute information. Most articles stabilize to high quality, trusted sources of information representing the collective wisdom of all the users who edited the article. We propose a model for information growth which relies on two main observations: (i) as an article's quality improves, it attracts visitors at a faster rate (a rich get richer phenomenon); and, simultaneously, (ii) the chances that a new visitor will improve the article drops (there is only so much that can be said about a particular topic). Our model is able to reproduce many features of the edit dynamics observed on Wikipedia and on blogs collected from LiveJournal; in particular, it captures the observed rise in the edit rate, followed by (1/t) decay.
7005 en Beyond the Semantic Web There are multiple sources of power available for forming and propelling automobiles; analogously, there are several sources of power for forming and propelling thoughts. Besides the neural ones you're most familiar with, and the Semantic Web ones that have received the lion's share of hype in recent years, there are some additional ones that we are tapping into with some success. These deep semantic representations and operations are able to produce useful and in cases even novel conclusions requiring induction, abduction, and analogy, as well as deductive reasoning. I will illustrate this with case examples from recent Cyc applications, including terrorism scenario generation for intelligence analysts and ad hoc clinical trial question answering for medical researchers.
7006 en Knowledge Representation and Reasoning - Discussion 
7007 en Scalable Collaborative Filtering Algorithms for Mining Social Networks Social networking sites such as Orkut, MySpace, Hi5, and Facebook attract billions of visits a day, surpassing the page views of Web Search. These social networking sites provide applications for individuals to establish communities, to upload and share documents/photos/videos, and to interact with other users. Take Orkut as an example. Orkut hosts millions of communities, with hundreds of communities created and tens of thousands of blogs/photos uploaded each hour. To assist users to find relevant information, it is essential to provide effective collaborative filtering tools to perform recommendations such as friend, community, and ads matching. In this talk, I will first describe both computational and storage challenges to traditional collaborative filtering algorithms brought by aforementioned information explosion. To deal with huge social graphs that expand continuously, an effective algorithm should be designed to 1) run on thousands of parallel machines for sharing storage and speeding up computation, 2) perform incremental retraining and updates for attaining online performance, and 3) fuse information of multiple sources for alleviating information sparseness. In the second part of the talk, I will present algorithms we recently developed including parallel Spectral Clustering [1], parallel PF-Growth [2], parallel combinational collaborative filtering [3], parallel LDA, parallel spectral clustering, and parallel Support Vector Machines [4].
7008 en Social Networks - Discussion 
7009 en Interactively Optimizing Information Systems as a Dueling Bandits Problem We present an online learning framework tailored towards real-time learning from observed user behavior in search engines and other information access systems. In particular, we only require pairwise comparisons which were shown to be reliably inferred from implicit feedback [4, 3]. We will present an algorithm with theoretical guarantees as well as simulation results.
7010 en Machine Learning - Discussion 
7011 en Online Search and Advertising, Future and Present Search engine companies are gathering treasure troves of user-generated data. It has already been shown that such data can be used to directly improve the user's online experience. I will discuss some ideas as to what online search and advertising might look like a few years hence, in light of the algorithms and data we have now. Moving from future to present, I will outline some recent work done by researchers in the Text Mining, Search and Navigation team at Microsoft Research; the work in TMSN touches many aspects of online search and advertising.
7012 en Machine Learning for the Web: A Unified View Machine learning and the Web are a technology and an application area made for each other. The Web provides machine learning with an ever-growing stream of challenging problems, and massive data to go with them: search ranking, hypertext classification, information extraction, collaborative filtering, link prediction, ad targeting, social network modeling, etc. Conversely, seemingly just about every conceivable machine learning technique has been applied to the Web. Can we make sense of this vast jungle of techniques and applications? Instead of attempting an (impossible) exhaustive survey, I will instead try to distill a unified view of the field from our experience to date. By using the language of Markov logic networks - which has most of the statistical models used on the Web as special cases - and the state-of-the-art learning and inference algorithms for it, we will be able to cover a lot of ground in a short time, understand the fundamental structure of the problems and solutions, and see how to combine them into larger systems.
7013 en Search Query Disambiguation from Short Sessions Web searches tend to be short and ambiguous. It is therefore not surprising that Web query disambiguation is an actively researched topic. However, most existing work relies on the existence of search engine log data in which each user's search activities are recorded over long periods of time. Such approaches may raise privacy concerns and may be difficult to implement for pragmatic reasons. In this work, we present an approach to Web query disambiguation that bases its predictions only on a short glimpse of user search activity, captured in a brief session of about 5--6 previous searches on average. Our method exploits the relations of the current search session in which the ambiguous query is issued to previous sessions in order to predict the user's intentions and is based on Markov logic. We present empirical results that demonstrate the effectiveness of our proposed approach on data collected form a commercial general-purpose search engine.
7014 en Machine Learning, Market Design, and Advertising Given the complexity of preferences in markets such as key word advertising it is hard to believe that the de facto standard, decentralized, local, greedy algorithmn(advertisers bid for clicks on keywords) is any where close to being optimal for any reasonable objective (welfare, profit, etc.). In this talk we consider the market design problem from a global perspective. We make connections between machine learning theory and market design theory, where machine learning design problems closely mirror game theoretic design problems. We reduce a general theoretical market design problem to a natural machine learning optimization problem. These theoretical results lead to a number of practical answers to advertising market design questions.
7015 en Internet Advertising and Optimal Auction Design This talk describes the optimal (revenue maximizing) auction for sponsored search advertising. We show that a search engine's optimal reserve price is independent of the number of bidders. Using simulations, we consider the changes that result from a search engine's choice of reserve price and from changes in the number of participating advertisers.
7016 en Learning optimally from self-interested data sources in on-line ad auctions In the analysis of current online ad auctions essential parameters such as click-through rates are often assumed to be known. The disregard of the uncertainty that is present in reality leads to several serious problems. In the talk we will highlight two: (i) there is no principled exploration of new ads, and (ii) there is no incentive for advertisers to only subscribe to well targeted key-words. In fact, there is an interesting opportunity for very poorly targeting advertisers to exploit this fact. We present a new auction that solves both problems. The key trick for this auction is that advertisers are not only requested to submit a bid, but also a belief over their own click-through rate.
7018 en Multiview Clustering via Canonical Correlation Analysis Clustering algorithms such as k-means perform poorly when the data is highdimensional. A number of efficient clustering algorithms developed in recent years address this problem by projecting the data into a lower-dimensional subspace, e.g. via principal components analysis (PCA) or random projections, before clustering. Such techniques typically require stringent requirements on the separation between the cluster means. Here we present ongoing work on projection-based clustering that addresses this using multiple views of the data. We use canonical correlation analysis (CCA)nto project the data in each view to a lower-dimensional subspace. Under the assumption that the correlated dimensions capture the information about the cluster identities, the separation conditions required for the algorithm to be successful are significantly weaker than those of prior results in the literature. We describe experiments on two domains, (a) speech audio and images of the speakers’ faces, and (b) text and links in Wikipedia articles. We discuss several issues that arise when clustering in these domains, in particular the existence of multiple possible “cluster variables” and of a hierarchical cluster structure.
7019 en Multi-View Dimensionality Reduction via Canonical Correlation Analysis We analyze the multi-view regression problem where we have two views (X1,X2) of the input data and a real target variable Y of interest. In a semi-supervised learning setting, we consider two separate assumptions (one based on redundancy and the other based on (de)correlation) and show how, under either assumption alone, dimensionality reduction (based on CCA) could reduce the labeled sample complexity. The basic semi-supervised algorithm is as follows: with unlabeledndata, perform CCA; with the labeled data, project the inputs onto a certain CCA subspace (i.e. perform dimensionality reduction) and then do least squares regression in this lower dimensional space. We show how, under either assumption, the number of labeled samples could be significantly reduced (in comparison to the single view setting) - in particular, we show how this dimensionality reduction only introduces little bias but could drastically reduce the variance. The two assumptions we consider are a redundancy assumption and an uncorrelated assumption. Under the redundancy assumption, we have that the best predictor from each view is roughly as good as the best predictor using both views. Under the uncorrelated assumption, we have that conditioned on Y the views X1nand X2 are uncorrelated. We show that under either of these assumptions, CCA is appropriate as a dimensionality reduction technique. We are also in the process of large scale experiments on word disambiguation (using theWikipedia, with the disambiguation pages as helping to provide labels).nThis work presents extensions of ideas in Ando and Zhang [2007] and Kakade and Foster [2007].
7020 en The Double-Barrelled LASSO (Sparse Canonical Correlation Analysis) We present a new method which solves a double-barelled LASSO in a convex least squares approach. In the presented method we focus on the scenario where one is interested in (or limited to) a primal (feature) representation for the first view while having a dual (kernel) representation for the second view. DB-LASSO minimises the number of features used in both the primal and dual projectionsnwhile minimising the error (maximising the correlation) between the two views.
7021 en Learning Shared and Separate Features of Two Related Data Sets using GPLVMs Dual source learning problems can be formulated as learning a joint representation of the datansources, where the shared information is represented in terms of a shared underlying process. However, there may be situations in which the shared information is not the only useful information,nand interesting aspects of the data are not common to both data sets. Some useful features withinnone data set may not be present in the other and vice versa; this complementary property motivatesnthe use of multiple data sources over single data sources which capture only one type of useful information. nnFor instance, having two eyes (and two streams of visual data) allows us to gain a 3-Dnimpression of the world. This ability of stereo vision combines both shared features and featuresnprivate to each data stream to form a coherent representation of the world; common shifted featuresncan be used in disparity estimation to infer depths of objects, while some features which may benseen in one view but not in the other, due to occlusions, can provide additional information aboutnthe scene. In this work, we present a probabilistic generative framework for analysing two sets of data, where the structure of each data set is represented in terms of a shared and private latent space. nnExplicitly modeling a private component for each data set avoids an oversimplified representation of the within-set variation such that the between-set variation can be modeled more accurately, as well as giving insight into potentially interesting features particular to a data set. Since two data sets may have a complex (possibly nonlinear) relationship, we use nonparametric Bayesian techniques - we define Gaussian process priors over the functions from latent to data spaces, such that each data set is modelled as a Gaussian Process Latent Variable Model (GPLVM) [1] where the dependency structure is captured in terms of shared and private kernels.
7022 en Multiview Fisher Discriminant Analysis CCA can be seen as a multiview extension of PCA, in which information from two sources is used for learning by finding a subspace in which the two views are most correlated. However PCA, and by extension CCA, does not use label information. Fisher Discriminant Analysis uses label information to find informative projections, which can be more informative in supervised learning settings. We show that FDA and its dual can both be formulated as generalized eigenproblems, enabling a kernel formulation. We derive a regularised two-view equivalent of Fisher Discriminant Analysis and its corresponding dual, both of which can also be formulated as generalized eigenproblems. We then show that these can be cast as equivalent disciplined convex optimisation problems, and subsequently extended to multiple views. We show experimental results on an EEG dataset and part of the PASCAL 2007 VOC challenge dataset.
7023 en Selective Multitask Learning by Coupling Common and Private Representations In this contribution we address the problem of selective transfer of knowledge in multitask learningnfor classification. We consider selective transfer an interesting framework since when tasks are notntruly closely related traditional multitask approaches become suboptimal. We study two multitask learning frameworks where we develop the aforementioned selective transfer paradigm. The two scenarios correspond to two ways of constructing the mapping from examples to output space. In the first naive scenario, the hypothesis is a direct mapping from the input space X onto the output space Y. In the second one, the overall hypothesis space is constructed in a more sophisticate way through a cascade of mappings into intermediate representation spaces. Examples of learning methods under the first scenario are Support Vector Machines (SVM) with linear kernel and Single Layer Perceptrons (SLP). Learning methods under the second type are Multi Layer Perceptrons (MLP) and non linear SVMs.
7024 en Regression Canonical Correlation Analysis In this paper we present Regression Canonical Correlation Analysis, an extension of Canonical Correlation Analysis, where one of the dimensions is fixed and demonstrate how it can be solved efficiently. We applied the extension to the task of query translation in the context of Cross-Lingual Information Retrieval.
7025 en Multiple kernel learning for multiple sources In this talk, I will consider the problem of learning a predictor from multiple sources of information, a situation common in many domains such as computer vision or bioinformatics. I will focus primarily on the multiple kernel learning framework, which amounts to consider one positive definite kernel for each source of information. Natural unanswered questions arise in this context, namely: Can one learn from infinitely many sources? Should one prefer closely related sources, or very different sources? Is it worth considering a large kernel-induced feature space as multiple sources?
7026 en GP-LVM for Data Consolidation Manymachine learning task are involvedwith the transfer of information fromone representation to a corresponding representation or tasks where several different observations represent the same underlying phenomenon. A classical algorithm for feature selection using information from multiple sources or representations is Canonical Correlation Analysis (CCA). In CCA the objective is to select features in each observation space that are maximally correlated compared to dimensionality reduction where the objective is to re-represent the data in a more efficient form. We suggest a dimensionality reduction technique that builds on CCA. By extending the latent space with two additional spaces, each specific to a partition of the data, the model is capable of representing the full variance of the data. In this paper we suggest a generative model for shared dimensionality reduction analogous to that of CCA.
7027 en Two-level infinite mixture for multi-domain data The combined, unsupervised analysis of coupled data sources is an open problem in machine learning.nA particularly important example from the biological domain is the analysis of mRNA and protein profiles derived from the same set of genes (either over time or under different conditions). Such analysis has the potential to provide a far more comprehensive picture of the mechanisms of transcription and translation than the individual analysis of the separate data sets. The problem is similar to that attacked with traditional Canonical Correlation Analysis (CCA) but in many application areas, the CCA assumptions are too restrictive. Probabilistic CCA [1] and kernel CCA [2] have both been recently proposed but the former is still limited to linear relationships and the latter compromises the interpretability in the original space. In this work, we preset a nonparametric model for coupled data that provides an interpretable description of the shared variability in the data (as well as that that isn’t shared) whilst being free of restrictive assumptions such as those found in CCA.nThe hierarchical model is built from two marginal mixtures (one for each representation - generalisation to three or more is straightforward). Each object will be assigned to one component in each marginal and the contingency table describing these joint assignments is assumed to have been generated by a mixture of tables with independent margins. This top-level mixture captures the sharednvariability whilst the marginal models are free to capture variation specific to the respective data sources. The number of components in all three mixtures is inferred from the data using a novel Dirichlet Process (DP) formulation.
7028 en Probabilistic Models for Data Combination in Recommender Systems We propose a method for jointly learning multiple related matrices, and show that, by sharing information between the two matrices, such an approach allows us to improve predictive performances for items where one of the matrices contains very sparse, or no, information. While the above justification has focused on recommender systems, the approach described is applicable to any two datasets that relate to a common set of items and can be represented in matrix form. Examples of such problems could include image data where each image is associated with a set of words (for example captioned or tagged images); sets of scientific papers that can be represented either using a bag-of-words representation or in terms of their citation links to and from other papers; corpora of documents that exist in two languages.
7029 en Discussion & Future Directions 
7030 en Semi-Supervised Learning and Learning via Similarity Functions: two key settings for Data-dependent Concept Spaces 
7031 en Theory of Matching Pursuit in Kernel Defined Feature Spaces We analyse matching pursuit for kernel principal components analysis (KPCA) by proving that the sparse subspace it produces is a sample compression scheme. We show that this bound is tighter than the KPCA bound of Shawe-Taylor et al and highly predictive of the size of the subspace needed to capture most of the variance in the data. We analyse a second matching pursuit algorithm called kernel matching pursuit (KMP) which does not correspond to a sample compression scheme. However, we give a novel bound that views the choice of subspace of the KMP algorithm as a compression scheme and hence provide a VC bound to upper bound its future loss. Finally we describe how the same bound can be applied to other matching pursuit related algorithms.n
7032 en Chromatic PAC-Bayes Bounds for Non-IID Data Pac-Bayes bounds are among the most accurate generalization bounds for classifiers learned with \iid data, and it is particularly so for margin classifiers. However, there are many practical cases where the training data show some dependencies and where the traditional \iid assumption does not apply. Stating generalization bound for such frameworks is therefore of the utmost interest. In this work, we propose the first, to the best of our knowledge, \pac-Bayes generalization bounds for classifiers trained on data exhibiting dependencies. The approach is based on the decomposition of a so-called dependency graph of the data in sets of independent data, through the tool of fractional covers. Our bounds are very general, since being able to find an upper bound on the chromatic number of the dependency graph is sufficient for it get new bounds for specific settings. We show how our results can be used to derive bounds for bipartite ranking and windowed prediction.
7033 en Sample Complexity for Multiresolution ICA 
7034 en Exploiting Cluster Structure to Predict The Labeling of a Graph The nearest neighbor and the perceptron algorithms are intuitively motivated by the aims to exploit the "cluster" and "linear separation" structure of the data to be classified, respectively. We develop a new online perceptron-like algorithm, Pounce, to exploit both types of structure. We refine the usual margin-based analysis of a perceptron-like algorithm to now additionally reflect the cluster-structure of the input space. We apply our methods to study the problem of predicting the labeling of a graph. We find that when both the quantity and extent of the clusters are small we may improve arbitrarily over a purely margin-based analysis.
7035 en Online Prediction on Large Diameter Graphs We continue our study of online prediction of the labelling of a graph. We show a fundamental limitation of Laplacian-based algorithms: if the graph has a large diameter then the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices, even when tackling simple problems. We overcome this drawback by means of an efficient algorithm which achieves a logarithmic mistake bound. It is based on the notion of a spine, a path graph which provides a linear embedding of the original graph. In practice, graphs may exhibit cluster structure; thus in the last part, we present a modified algorithm which achieves the “best of both worlds”: it performs well locally in the presencenof cluster structure, and globally on large diameter graphs.
7036 en Online Graph Prediction with Random Trees 
7037 en Representation of Prior Knowledge - from Bias to 'Meta-Bias' 
7038 en Generalization Bounds for Indefinite Kernel Machines 
7039 en From On-line Algorithms to Data-Dependent Generalization 
7040 en Study of Classification Algorithms using Moment Analysis 
7041 en The use of Unlabeled Data in Supervised Learning: the Manifold Dossier 
7042 en Transductive Learning and Computer Vision 
7043 en Learning Temporal Sequence of Biological Networks 
7044 en Switching Regulatory Models of Cellular Stress Response 
7045 en Detecting the Presence and Absence of Causal Relationships Between Expression of Yeast Genes with Very Few Samples 
7046 en KIRMES: Kernel-based Identification of Regulatory Modules in Euchromatic Sequences 
7047 en Approximate Substructure Matching for Biological Sequence Classification 
7048 en Predicting Binding Affinities of MHC Class II Epitopes Across Alleles 
7050 en Full Bayesian Survival Models for Analyzing Human Breast Tumors 
7051 en Probabilistic assignment of formulas to mass peaks in metabolomics experiments 
7052 en Learning “graph-mer” motifs that predict gene expression trajectories in development 
7053 en On the relationship between DNA periodicity and local chromatin structure 
7054 en The American Denial of Global Warming Polls show that between one-third and one-half of Americans still believe that there is "no solid" evidence of global warming, or that if warming is happening it can be attributed to natural variability. Others believe that scientists are still debating the point. nJoin scientist and renowned historian Naomi Oreskes as she describes her investigation into the reasons for such widespread mistrust and misunderstanding of scientific consensus and probes the history of organized campaigns designed to create public doubt and confusion about science.
7055 en In Antarctica: The Global Warning Antarctica's environment today is a microcosm of the world environment's future: as endangered creatures, such as the chinstrap penguins, humpback whales, and albatrosses, continue to face extinction, research scientists have concluded that this icy ecosystem serves as a final warning of impending environmental deterioration.nnIn Antarctica: The Global Warning, Sebastian Copeland's photographs have captured both the incredible beauty of the continent and the devastation that climate changes have wreaked on it. His data, photographs, and conclusions — along with contributions from Will Steger, David De Rothschild, Stephen Schneider, Zac Goldsmith, Mikhail Gorbachev, and Leonardo DiCaprio — bring to readers with insight and urgency the momentous reality of the not so distant future with insight and urgency.
7056 en Global Warming: Nation Under Siege The rapid depletion of fossil fuels and the rising sea level from the warming of the earth's atmosphere are converging to dramatically alter our future. nnEdward Mazria, founder of Architecture 2030, unveils a new study of sea level rise showing fly-over 3D images depicting potentially calamitous coastal and national impacts.
7057 en Conversations with History: Noam Chomsky On this edition of Conversations with History, UC Berkeley's Harry Kreisler is joined by linguist and political activist Noam Chomsky to discuss activism, anarchism and the role the United States plays in the world today.
7058 en The Time Paradox: The New Psychology of Time That Will Change Your Life Your every significant choice -- **every important decision you make** -- is determined by a force operating deep inside your mind: your perspective on time -- your internal, personal time zone. This is the most influential force in your life, yet you are virtually unaware of it. Once you become aware of your personal time zone, you can begin to see and manage your life in exciting new ways.nnIn The Time Paradox, Drs. Zimbardo and Boyd draw on thirty years of pioneering research to reveal, for the first time, how your individual time perspective shapes your life and is shaped by the world around you. Further, they demonstrate that your and every other individual's time zones interact to create national cultures, economics, and personal destinies.
7059 en Copyright laws in todays digital world 
7060 en Causal Directions in Noisy Environment 
7061 en When causality matters for prediction:Investigating the practical tradeoffs 
7062 en Lecture 1: Introduction 
7063 en Lecture 2: Biochemistry 1 
7064 en Lecture 3: Biochemistry 2 
7065 en Lecture 4: Biochemistry 3 
7066 en Lecture 5: Biochemistry 4 
7067 en Lecture 6: Genetics 1 
7068 en Lecture 7: Genetics 2 
7069 en Lecture 8: Genetics 3 
7070 en Lecture 9: Human Genetics 
7071 en Lecture 10: Molecular Biology 1 
7072 en Lecture 11: Molecular Biology 2 
7073 en Lecture 12: Molecular Biology 3 
7074 en Lecture 13: Gene Regulation 
7075 en Lecture 14: Protein Localization 
7076 en Lecture 15: Recombinant DNA 1 
7077 en Lecture 16: Recombinant DNA 2 
7078 en Lecture 17: Recombinant DNA 3 
7079 en Lecture 18: Recombinant DNA 4 
7080 en Lecture 19: Cell Cycle/Signaling 
7081 en Lecture 20: Cancer 
7082 en Lecture 21: Virology/Tumor Viruses 
7083 en Lecture 22: Immunology 1 
7084 en Lecture 23: Immunology 2 
7085 en Lecture 24: AIDS 
7086 en Lecture 25: Genomics 
7087 en Lecture 26: Nervous System 1 
7088 en Lecture 27: Nervous System 2 
7089 en Lecture 28: Nervous System 3 
7090 en Lecture 29: Stem Cells/Cloning 1 
7091 en Lecture 30: Stem Cells/Cloning 2 
7092 en Lecture 31: Molecular Medicine 1 
7093 en Lecture 32: Molecular Evolution 
7094 en Lecture 33: Molecular Medicine 2 
7095 en Lecture 34: Human Polymorphisms and Cancer Classification 
7096 en Lecture 35: Future of Biology 
7098 en ITER: Promises unkept? (1/2) 
7099 en Activized Learning: Transforming Passive to Active with Improved Label Complexity In active learning, a learning algorithm is given access to a large pool of unlabeled examples, and is allowed to request the labels of any particular examples in that pool, interactively. In empirically driven research, one of the most common techniques for designing new active learning algorithms is to use an existing passive learning algorithm as a subroutine, and actively construct a training set for that method by carefully choosing informative examples to label. The resulting active learning algorithms are thus able to inherit the tried-and-true learning bias of the underlying passive algorithm, while often requiring significantly fewer labels to achieve a given accuracy compared to random sampling.nnThis naturally raises the theoretical question of whether every passive learning algorithm can be "activized", or transformed into an active learning algorithm that uses a smaller number of labels to achieve a given accuracy. In this talk, I will address precisely this question. In particular, I will explain how to use any passive learning algorithm as a subroutine to construct an active learning algorithm that provably achieves a strictly superior asymptotic label complexity. Along the way, I will also describe many of the recent advances in the formal study of the potential benefits of active learning in general.
7100 en Probabilistic Decision-Making Under Model Uncertainty Partially Observable Markov Decision Processes offer a rich mathematical framework for decision-making under uncertainty. In recent years, a number of methods have been developed to optimize the choice of action, given a parametric model of the domain. In many applications, however, this model must be learned using a finite set of trajectories. When this data proves difficult or expensive to collect, it is often the case that the resulting model is poorly or imprecisely defined.nnIn this talk, I will present two recent results on the topic of decision-making under model uncertainty. In the first half, I will describe a method for estimating the bias and variance of the value function in terms of the statistics of the empirical transition and observation model. Such error terms can be used to meaningfully compare the value of different policies. In the second half, I will present a bayesian approach designed to simultaneously improve the model and select good actions. Performance of the two methods will be illustrated using problems drawn from the fields of robotics and medical treatment design.
7101 en Object Recognition and Segmentation by Association Many object recognition systems train a different classifier for each object category and use the sliding window approach to classify image regions. In this talk, we pose the object recognition problem as data association where a novel object is explained solely in terms of a small set of exemplar objects to which it is visually similar. We learn a different distance function for each exemplar such that the returned distances can be interpreted to detect the presence of an object. Our exemplars are represented as image regions and the learned distances capture the relative importance of shape, color, texture, and position features for that region. We use the distance functions to detect and segment objects in novel images by associating the bottom-up segments obtained from multiple image segmentations with the exemplar regions. We evaluate the detection and segmentation performance of our algorithm on real-world outdoor scenes from the LabelMe dataset and also show some qualitative image parsing results.
7102 en Local Minima Free Parameterized Appearance Models Parameterized Appearance Models (PAMs) (e.g. Eigen-tracking, Active Appearance Models, Morphable Models) are commonly used to model the appearance and shape variation of objects in images. While PAMs have numerous advantages relative to alternate approaches, they have at least two drawbacks. First, they are especially prone to local minima in the fitting process. Second, often few if any of the local minima of the cost function correspond to acceptable solutions. To solve these problems, this paper proposes a method to learn a cost function by explicitly optimizing that the local minima occur at and only at the places corresponding to the correct fitting parameters. To the best of our knowledge, this is the first paper to address the problem of learning a cost function to explicitly model local properties of the error surface to fit PAMs. Synthetic and real examples show improvement in alignment performance in comparison with traditional approaches.
7103 en Inference Complexity as Learning Bias Graphical models are usually learned without regard to the cost of doing inference with them. As a result, even if a good model is learned, it may perform poorly at prediction, because it requires approximate inference. We propose an alternative: learning models with a score function that directly penalizes the cost of inference. Specifically, we learn arithmetic circuits with a penalty on the number of edges in the circuit (in which the cost of inference is linear). Our algorithm is equivalent to learning a Bayesian network with context-specific independence by greedily splitting conditional distributions, at each step scoring the candidates by compiling the resulting network into an arithmetic circuit, and using its size as the penalty. We show how this can be done efficiently, without compiling a circuit from scratch for each candidate. Experiments on several real-world domains show that our algorithm is able to learn tractable models with very large treewidth, and yields more accurate predictions than a standard context-specific Bayesian network learner, in far less time. (Joint work with Daniel Lowd.)
7104 en Differentiable Sparse Coding Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a Laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the benefits of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efficiently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and find that online optimization of the parameters of the KL-regularized model can significantly improve prediction performance.
7105 en Partially Observed Maximum Entropy Discrimination Markov Networks Learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive, sometime unattainable fully annotated training data. While likelihood-based methods have been extensively explored, to our knowledge, learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem. In this paper, we present a partially observed Maximum Entropy Discrimination Markov Network (PoMEN) model that attempts to combine the advantages of Bayesian and margin based paradigms for learning Markov networks from partially labeled data. PoMEN leads to an averaging prediction rule that resembles a Bayes predictor that is more robust to overfitting, but is also built on the desirable discriminative laws resemble those of the M$^3$N. We develop an EM-style algorithm utilizing existing convex optimization algorithms for M$^3$N as a subroutine. We demonstrate competent performance of PoMEN over existing methods on a real-world web data extraction task.
7106 en Rare Category Detection for Spatial Data Given an unlabeled unbalanced data set, the goal of rare category detection is to discover examples from the minority classes with a few label requests. Rare category detection is an open challenge in machine learning, and it has a lot of applications, such as financial fraud detection, network intrusion detection, astronomy, spam image detection, etc. In this talk, I will introduce two methods for rare category detection with spatial data. The first one essentially performs local density differential sampling, and it requires the prior information about the data set as input. The second one is based on specially designed exponential families, and it is prior-free. Experimental results demonstrate the effectiveness of these methods on different real data sets.
7107 en Some Challenging Machine Learning Problems in Computational Biology: Time-Varying Networks Inference and Sparse Structured Input-Out Learning Recent advances in high-throughput technologies such as microarrays and genome-wide sequencing have led to an avalanche of new biological data that are dynamic, noisy, heterogeneous, and high-dimensional. They have raised unprecedented challenges in machine learning and high-dimensional statistical analysis; and their close relevance to human health and social welfare has often created unique demands on performance metric different from standard data mining or pattern recognition problems. In this talk, I will discuss two of such problems. First, I will present a new statistical formalism for modeling network evolution over time, and several new algorithms based on temporal extensions of the sparse graphical logistic regression, for parsimonious reverse-engineering the latent time varying networks. I will show some promising results on recovering the latent sequence of temporally rewiring gene networks over more than 4000 genes during the life cycle of Drosophila melanogaster from microarray time course, at a time resolution only limited by sample frequency. Second, I will present a family of sparse structured regression models in the context of uncovering true associations between linked genetic variations (inputs) in the genome and networks of human traits (outputs) in the phenome. If time allows, I will also present another class of new models known as the maximum entropy discrimination Markov networks, which address the same problem in the maximum margin paradigm, but using a entropic regularizer that lead to a distribution of structured prediction functions that are simultaneously primal and dual sparse (i.e., with few support vectors, and of low effective feature dimension).nnJoint work with Amr Ahmed, Seyoung Kim, Mladen Kolar, Le Song and Jun Zhu.
7108 en New methods for digital generation and postprocesing of true random numbers 
7109 en Lecture 1: Personal Media 
7110 en Lecture 2: Introducing the StarFestival curriculum 
7111 en Lecture 3: Next Big Thing: Video Internet 
7112 en Lecture 4: Media, Education, and Technology 
7113 en Lecture 5: Educational Uses of Technology 
7114 en Lecture 6: Media Literacy as a Strategy for Combatting Moral Panic 
7115 en Lecture 7: Educational Technology Initiatives in Business Education in the Sloan School of Management, MIT 
7116 en Lecture 8: Discussion of StarFestival 
7117 en Lecture 9: Ms. Maria D'Itria, Fifth grade, Harvard Kent School, Boston, USA 
7118 en Lecture 10: Ms. Mary Rudder, Kindergarten, Harvard Kent School, Boston, USA 
7230 en Welcome and Opening remarks Opening of OECD
7231 en Slovenia, Corporate Governance and OECD 
7232 en The Role of OECD in Global Corporate Governance 
7233 en Panel discussion 1 Panelists:nn* **Bojan Dremelj**, CEO Telekom Slovenia,n* **mag. ?iga Debeljak**, CEO Mercator,n* **Juan Carlos Fernandez Zara**, International Finance Corporation,n* **mag. Mirjana Dimc Perko**, CFO Gorenje,n* **dr. Marko Simoneti**, CEO Ljubljana Stock Exchange
7234 en OECD guidelines for State owned Enterprises & case of Norway 
7235 en Summary and Wrap up 
7236 en Panel discussion 2 Panelists:nn* **Ale? Hauc**, CEO Slovene Post,n* **mag. Borut Jamnik**, President of the Slovene Supervisory Board Association,n* **mag. Tadej Tufek**, CEO Adria Airways,n* **Peter Tev?**, Vicepresident of Association of Employers of Slovenia,n* **Marko Kry?anowski**, CEO Petrol
7241 en Semantic technologies: an introduction 
7242 en TAO, the way to semantic SOA: methodology and tools 
7243 en Vigitermes Project: a pharmaceutical knowledge base grounded on semantic technologies 
7244 en VideoLectures.net case study The task in the VideoLectures case study is to develop a software component that will aid the VideoLectures editors in categorizing recorded lectures (i.e. ontology population). This functionality is required due to the rapid growth of the number of hosted lectures as well as due to the fact that the categorization taxonomy is rather fine-grained (200 categories and growing). In addition to aiding the categorization of new lectures, the software will also be used for re-categorization and additional categorization of lectures already categorized.nnWe will show that we were successful in our task as the categorizer is highly accurate – it achieves accuracies that stretch 12–20% above the baseline – and highly robust in terms of missing data. The latter means that a lecture might be missing textual annotations (such as the description and slide titles) but is still categorized correctly. Furthermore, the categorizer has been successfully integrated into the VideoLectures Web site. Categorization suggestions (termed "quick links") are provided to the author in the categorization panel.
7245 en Linked Life Data for annotation of MEDLINE Semantic data-integration and search in the life science domain
7246 en Fundamental Constants in Physics and their Time Dependence In the Standard Model of Particle Physics we are dealing with 28 fundamental constants. In the experiments these constants can be measured, but theoretically they are not understood. I will discuss these constants, which are mostly mass parameters. Astrophysical measurements indicate that the finestructure constant is not a real constant, but depends on time. Grand unification then implies also a time variation of the QCD scale. Thus the masses of the atomic nuclei and the magnetic moments of the nuclei will depend on time. I proposed an experiment, which is currently done by Prof. Haensch in Munich and his group. The first results indicate a time dependence of the QCD scale. I will discuss the theoretical implications.
7250 en A Passion for Discovery The human side of doing theoretical physics is explored through stories about the interactions between physicists and about the effects world events can have on scientists' behavior, and even on their interests and style. These stories cluster nicely around certain bigger themes to create an overarching whole. This happens both on account of some interesting narrative structures intrinsic to the science of physics itself and on account of the way physics integrates into the general culture. The stories concern Einstein, Schrödinger, Pauli, Heisenberg, Stueckelberg, Jordan and Fock and also involve some mathematicians like Emmy Noether, Teichmüller and Bers and even the psychologist C.G. Jung.
7251 en The Historical Origins and Economic Logic of 'Open Science' Modern "big science" projects, such as the LHC experiments in physics that are being prepared to run at CERN, embody the distinctive ethos of cooperation and mechanisms of coordination among distributed groups of researchers that are characteristic of 'open science'. Much has been written about the institutions of open science, their supporting social norms, and their effectiveness in generating additions to the stock of reliable knowledge. But from where have these institutions and their supporting ethos come? How robust can we assume them to be in the face of the recent trends for universities and research institutes in some domains of science to seek to appropriate the benefits of new discoveries and inventions by asserting intellectual property claims? A search for the historical origins of the institutions of open science throws some new light on these issues, and the answers may offer some lessons for contemporary science and technology policy-making.
7253 en Hanbury Brown and Twiss and Other Atom-atom Correlations: Advances in Quantum Atom Optics Fifty years ago, two astronomers, R. Hanbury Brown and R. Q. Twiss, invented a new method to measure the angular diameter of stars, in spite of the atmospheric fluctuations. Their proposal prompted a hot debate among physicists : how might two particles (photons), emitted independently (at opposite extremities of a star) , behave in a correlated way when detected ? It was only after the development of R Glauber's full quantum analysis that the effect was understood as a two particle quantum interference effect. From a modern perspective, it can be viewed as an early example of the amazing properties of pairs of entangled particles. The effect has now been observed with bosonic and fermionic atoms, stressing its fully quantum character. After putting these experiments in a historical perspective, I will present recent results, and comment on their significance. I will also show how our single atom detection scheme has allowed us to demonstrate the creation of atom pairs by non linear mixing of matter waves. This result paves the way to experiments aiming at probing entanglement in atom pairs.
7254 en The genesis of WiFi and its applications In 1985 changes to US regulations caused a paradigm shift by permitting the use of radio spectrum for devices that did not need to have an end-user license. After a few years, products appeared on the market and a group developed a standard for broadband wireless communications among computers. The presentation will explain how the standard developed and was adopted by the Wi-Fi Alliance, as well as the global harmonization and expansion of the available radio spectrum to over half a GHz. nnThe success of Wi-Fi and user innovation and initiatives makes it a vehicle to bring broadband internet to rural areas both in developing as well as in developed countries. Vic Hayes is a Senior Research Fellow at the Delft University of Technology and is writing a book titled "The genesis of Wi-Fi and the road toward global success". nnHe holds a BSEE and joined NCR in the Netherlands in 1974. He co-established and chaired the IEEE 802.11 Standards Working Group for Wireless Local Area Networks and became known as the "Father of Wi-Fi". After chairing the WG, he successfully mobilized the computer industry to support the agenda item for 455 MHz of spectrum at co-primary allocation on the agenda of the World Radio Conference 2003. In October 2003, Vic retired from Agere Systems. nnFor his pioneering work on Wi-Fi, Vic is the recipient of the Innovation Award 2004 of "The Economist", the Dutch Vosko Trophy, 2 Wi-Fi Alliance Leadership Awards, The IEEE Standards Medallion, the IEEE Leadership Award, the IEEE Hans Karlsson Award and the IEEE Steinmetz Award.
7255 en Worrying About the LHC, a Lesson from Astrophysics? To worry about the LHC is a popular sport. I shall share my own worries, hopefully original, and do it via a parable (for this method, I can quote earlier authors). The parable concerns a topic in astrophysics (gamma-ray bursts) which happens to be a simple exercise --but quite an interesting one-- on elementary particle-physics and beam dynamics, topics not unrelated to the LHC. Though most of the talk will be dedicated to the physics and, in particular, to its recent developments, the allegory will allow me to detect what, I shall argue, may be dangerous 'viruses' invading science. I do not have the decisive antidotes, but I shall discuss some possible ones.
7256 en Energy, Sustainability and Development A huge increase in energy use is expected in the coming decades – see the IEA’s ‘business as usual’/reference scenario below. While developed countries could use less energy, a large increase is needed to lift billions out of poverty, including over 25% of the world’s population who still lack electricity. Meeting demand in an environmentally responsible manner will be a huge challenge. The World Bank estimates that coal pollution leads to 300,000 deaths in China each year, while smoke from cooking and heating with biomass kills 1.3 million world-wide – more than malaria. The IEA’s alternative scenario requires a smaller increase in energy use than the reference scenario and is also less carbon intensive, but it still implies that CO2 emissions will increase 30% by 2030 (compared to 55% in the reference scenario). Frighteningly, implementing the alternative scenario faces “formidable hurdles” according to the IEA, despite the fact that it would yield financial savings for consumers that far exceed the initial additional investment cost. I shall give an overview of the energy outlook and the portfolio of technological and economic measures that are need to meet the energy challenge and do better than the alternative scenario.
7257 en Fermilab Plan with a High Intensity Proton Source Fermilab, the US’s primary laboratory for particle physics, proposes a plan to maintain leadership for the laboratory and U.S. particle physics in the quest to discover the fundamental nature of the physical universe in the decades ahead. Discoveries of the physics of the Quantum Universe would come from powerful next generation particle accelerators. Fermilab’s Tevatron, currently the world’s most powerful particle accelerator, will shut down by the end of this decade after the LHC at CERN begins operations. At the LHC, U.S. physicists will join scientists from around the world in the exploration of the physics of the Terascale. To follow the LHC, physicists propose the International Linear Collider, a globally funded and operated accelerator to build on LHC results and illuminate Terascale science. Fermilab will work to host the proposed ILC in the U.S. as soon as possible, maintaining the nation’s historic leadership of frontier particle physics. Should events postpone the start of the ILC, Fermilab would build an intensity-frontier accelerator at one percent of the ILC’s length and combine it with existing accelerators to create Project X. Project X’s intense beams would give Fermilab’s scientific users a new way into the world of neutrinos and precision physics. With its ILC technology, Project X would spur U.S. industrialization and reduce costs of ILC components while advancing accelerator science for future applications in particle physics and beyond. In addition, Project X would drive forward the technology for still higher-energy accelerators of the future, such as a muon collider. Fermilab’s plan would maintain the nation’s leadership in particle physics, keeping the laboratory and U.S. particle physics on the pathway to discovery both at the Terascale with the ILC and beyond, and in the domain of neutrinos and precision physics at the intensity frontier.
7258 en "LHCb: Search for New Physics at a Hadron B factory" Many essential features of the quark sector of the Standard Model have first been probed by s- and b-quark decays in a indirect manner: notable examples include the SU(2) doublet structure, the existence of three generations, and the masses of the charm and top quarks. Recent progress made by the experiments at B factories and Tevatron in the study of b-hadron decays now put serious constraints in the characteristics of possible physics beyond the Standard Model, through measurements related to the Flavour Changing Neutral Current. When the LHC becomes operational, it will be the most powerful source of b-hadrons. The LHCb experiment will then be able to study such processes in the Bs system with a similar accuracy to that achieved by the B factories in the Bd system, allowing to improve further the stringent tests of the Standard Model. The experiment will also improve the measurements in the Bd system as well. It may observe a sign of new physics first and, in combination with the ATLAS and CMS results, will probe the flavour structure of the new physics. In this presentation, we review the evolution of the LHCb experiment in conjunction with the development of physics, the current status of the experiment and its prospects.
7259 en The Bill and Melinda Gates Foundation: pushing the boundaries of what is possible in public health - perspectives and challenges to the global community Introduction: In July 2008, Bill Gates will transition out of a day-to-day role at Microsoft, to spend more time on his philanthropic work with the Bill and Melinda Gates Foundation. As the world’s largest philanthropic organization, the Gates Foundation has set ambitious goals to tackle some of the world’s worst diseases. In this talk, Julie Jacobson will outline some of the objectives of the Gates Foundation and how it is impacting public health and challenging the global community. Speaker Bio: As Senior Programme Officer at the Bill and Melinda Gates Foundation, Julie Jacobson currently supports grants working toward the control of neglected tropical diseases and works with the development and implementation of new vaccines in the infectious disease group of Global Health. Previously Dr. Jacobson was Scientific Director of Immunization Solutions and Director of Japanese encephalitis (JE) project at PATH, an international non-profit organization. As director of the JE project, she managed a US$35 million grant to accelerate the control of JE in endemic countries by improving data on the distribution of JE, accelerating the development of an improved vaccine and diagnostic tests for JE, and helping countries integrate JE vaccine into immunization programs. In her role as scientific director she defined the direction and growth of immunization solutions work by increasing the availability of vaccines to the world’s most vulnerable populations. This ranged from work on clinical trials for specific vaccines to directly working with ministries of health and partners in decision-making on vaccine introduction and planning. Before that, Dr. Jacobson was responsible for prioritizing and designing field activities for PATH’s Children’s Vaccine Project in the areas including yellow fever and rotavirus. Prior to joining PATH, Dr. Jacobson worked at the U.S. Centers for Disease Control and Prevention as an Epidemic Intelligence Officer. In this capacity, she worked in disaster epidemiology and conducted needs assessments for disaster victims, evaluated national surveillance systems, and evaluated the health impact of earthquakes on displaced persons. Dr. Jacobson is a physician with training in clinical tropical medicine and applied epidemiology.
7260 en TOTEM, a different LHC experiment TOTEM will pursue a physics program (complementary to that of the other LHC detectors) spanning a wide range from total cross-section and elastic scattering measurements to the study of diffractive and forward phenomena. The TOTEM program will lead to a better understanding of the fundamental aspects of strong interactions. For the first time at hadron colliders, the very forward rapidity range, containing 90% of the energy flow and explored in high-energy cosmic ray experiments, is covered, allowing the search for unusual phenomena hinted at by cosmic ray experiments. The technical implementation of all TOTEM detectors is described. Silicon sensors housed in so-called Roman pots allow measurements of elastic and diffractive protons at distances as small as 1 mm from the beam centre. A scheme to tag events from Double-Pomeron-Exchange by diffractive protons on both sides transforms the LHC into an almost clean “gluon” collider, where the centre-of-mass energy is determined by the momentum losses of the forward protons, thus offering an interesting way to search for new particles. In a later stage, the combination of CMS and TOTEM will provide an unprecedented almost complete rapidity coverage, allowing a variety of new studies, including hard diffraction.
7261 en Summary Summary of Strings 2008, which was be held at CERN in the year during which the LHC started up. This conference, the largest and most important one on String Theory, runs annually and usually draws several hundred participants, mostly active researchers in the field. The main purpose of the conference is to review the latest developments for experts, but there were also be free talks for the general public.
7262 en Remodeling the Topological String, Perturbatively and Nonperturbatively Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7263 en Tadpole Cancellation in the Topological String Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7264 en Cosmological Unification of String Theories Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7265 en S-duality and Boundary Conditions in N=4 SYM Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7266 en Developments in BPS Wall-crossing Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7267 en Extremal Black Holes and AdS2/CFT1 Correspondence Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7268 en BPS Black Holes and Topological Strings: A Review Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7270 en 4-loop Perturbative Konishi from AdS5xS5 String Sigma Model Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7271 en General Gauge Mediation Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7272 en Holographic Gauge Mediation Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7274 en Holographic Recipes at Finite Density and Low Temperature Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7275 en Scattering Amplitudes via AdS/CFT Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7276 en Nonlinear Fluid Dynamics from Gravity Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7277 en Integrability of the AdS/CFT System Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7278 en The Search for New Physics at the LHC Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7279 en String Phenomenology on the Eve of the LHC Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7280 en F-theory, GUTs, and the Weak Scale Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7281 en A New Infinite Class of Anti-de Sitter Flux Vacua Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7282 en D-brane Instantons in Type II Orientifolds Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7283 en N=6 Chern Simons Matter Theories, M2 Branes and Supergravity Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7284 en Unveiling the Structure of Amplitudes in Gauge Theory and Gravity Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7285 en What is the Simplest Quantum Field Theory? Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7286 en Fermionic T-duality Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7287 en Dual Superconformal Symmetry of Scattering Amplitudes in N=4 SYM Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7288 en The Gauge-string Duality and QCD at Finite Temperature Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7289 en Who's afraid of blue, red, and green? Speaking about our populistically acting mass media environment and its influence on aspects of mediation in general the Austrian artist will present examples of his documentarynphotographic work and his interactive media projects within mass media systems.nThe meta-world of public representation where basic aspects of democracy or economy are transmitted by setting ideological standards seems to be irresistibly shifting over to un-balanced commercial contents or surveillance and propaganda, especially in moments of crisis, which does challenge the media systems (most important for the development of western democracies) in a principal way since its beginnings.nQuestions of media observation and of the public (media) space and its ownership and political implications will be linked to artistic concepts which try to research the “abstract”ntechnological aspects and strategies of a media society and which do suggest also that formal abstract work is not necessarily a contradiction to political aspects in artistic practices.
7290 en Who is speaking? ”The multitude of voices into which Regina Maria Moeller splits her artistic work corresponds to the various positions she adopts with regard to that which she confronts. Similar to the way in which overlapping circles form intersections that shift previously peripheral material into a (new) center, Moeller shifts attention and reshuffles the cards, so to speak: secondary characters become protagonists, coherencies become contents, opposites are robbed of their oppositions. Moeller uses the strategies inherent in her media, overwriting and adapting them for her own purposes, and shows that there are always a number of different versions underneath and alongside the official interpretation.”n(Matthias Herrmann in: Regina Moeller: embodiment – dress plot, exhibition catalogue Secession, Vienna 2004)nRegina Maria Moeller lives in Berlin. She studied art education, art history and history of middle ages at the Ludwig Maximilians University in Munich / Germany. She is the founding editor of the magazine regina and the creator of the label embodiment. Regina Moeller is visiting professor for the fall term 06 at MIT’s Visual Arts Program.nRegina Maria Moeller situates her works in the border realm of art, fashion, and comics. She takes various formats of contemporary cultural communication and reflects on their identity-forming, economic, and functional connotations.nFor example, in her magazine regina, with which she has reached a broad readership since the late 1990s, she adapts the language of women’s magazines. Through subtle shifts she under-mines the common constructions of female identity. Parallel to this, Regina Moeller has designed clothing, carpets, furniture, and inner decor under her own label embodiment since 1993.
7291 en Forty Years of High Energy String Collisions Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7292 en Projections and instrumentations With the aid of especially designed bodilynequipment, Wodiczko’s recent projectionsn“animate,” in real time, the facades of publicnedifices. Much smaller in their scale, thenwearable instrumentations share with thesenmonumental projections similar psychologicalnand political objectives. The aim of both projectsnis to inspire and assist the users-animators inndeveloping, perfecting, and “projecting” theirnvoice and gestures in public space.nBoth projections and instrumentations require anpreparatory video recording (and re-recording)nprocess that psychologically and politicallynengages participants and aids in constructingntheir testimonies. Through this process,nparticipants gradually recover and developnthe mastery and artistry of public speaking.nStep by step they recall, articulate and conveynoverwhelming life experiences. Armed withnnew psycho-cultural prosthetic equipment, andnempowered with the prestige and monumentalnscale of civic edifices, they become prominentnfearless speakers.nThe main objective of the use of media in thesenprojections and instrumentations is to create antransitional psychological space that encouragesna gradual passage from participants’ traumaticnsilence or traumatized speech toward a confidentnarticulate voice. In this way, their psycho-politicalndevelopment, to which Wodiczko’s projects seeknto contribute, may help participants to becomennot only frank transmitters of truth, but alsoncritical and passionate animators of public space.
7293 en Maximal Supersymmetry, Duality and Scattering Amplitudes Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7294 en Multiple Membrane Dynamics Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7295 en Game, risk, disaster “Are we here to play or to be serious? Andnin all seriousness what is of playing or gamesnfor that matter? ...For the past four years,nGustavo Artigas who is based in Mexico City,nbut coordinates the majority of his projectsnin other international cities, has developed anstunning, complex language surrounding thensocial tensions of group organization and thenconsequences and risks involved in game andndisaster situations. More specifically, Artigas’nwork engages a universe of limits. As for thengame, the limit between what is and what isnnot [a game] is more difficult to specify than itnseems at first glance. ...The disaster on the othernhand – in Artigas’ experience – is a limit confinednby fear and/or inevitability. Acting as Masternof Ceremonies, he has developed a three-ringncircus scheme juxtaposing games, performativengestures, and political critiques into episodicalnchapters.” – Jennifer Teets
7296 en Lagrangians for Multiple M2-branes Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7297 en Heterotic Standard Models Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7298 en Politechnics 
7299 en Superstring Amplitudes: Formal Progress and Implications for LHC Lecture held at Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7300 en Outlook Outlook for Strings 2008. On 18–23 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
7301 en Archive The MIT Museum has recently received the archives of Mary Otis Stevens (SBArch, 56), one of the most important female architects in the Northeast during the 1960s and 1970s.nnInitially a protégé of Walter Gropius, Stevens partnered with her husband Thomas McNulty (MArch 49) from 1956 to 1969. One of their most important projects was the 1965 Lincoln House, designed for themselves and their three small sons, in Lincoln MA.nnPerhaps the first house in the US to be built of glass and exposed concrete, it was an instant sensation in the international architectural press on the cover of Architectural Forum and Deutsche Bauzeitung and widely featured in the large-circulation press, as well. But it rocked the little town of Lincoln the same way the Gropius House had in the late 1930s. (Neighbors speculated since there were no windows facing the street, the occupants must have been nudists.)nnFor Lincoln House, Stevens and McNulty enlisted movement and hesitation as their basic design concepts. The originality of the house stemmed from the architects' rejection of preset notions of what a house was, freeing them to transfer their own ideas about movement and hesitation on the scale of the city to the scale of the house.nnTo maximize freedom and movement in the house, for instance, the architects eliminated most interior doors, fostering movement not only between the different areas of the interior, but also between the inside and the outside. “We wanted to make the house into a kind of miniature city”, says Stevens. “It was very urban. The idea was to bring people together, not isolate them in boxes on different floors. You had choices all the time.”nnApart from the unique space created by using urban concepts of movement in the setting of a single-family house, the Lincoln House was also one of the first with a curvilinear geometry, giving the building a sculptural quality. (Life called it a sculpture for living in.) The Lincoln House also served as a sundial: sited so that its longitudinal axis was exactly N-S, each cloudless day at noon a streak of light would shoot down the stairs and extend its length, as the afternoon wore on, along the pathway leading to the children's area.nnIn the 1960s, Stevens and McNulty founded iPress, Inc., which Stevens directed from 1968-1978, making a major contribution to architectural and urban theory with books focusing on the social context of architecture, such as The Ideal Communist City by Alexei Gutnov and Towards a Non-Oppressive Environment by Alexander Tzonis.nnIn 1970, she and McNulty published their own classic work, World of Variation, looking at the city as a social realm, identifying then-current problems and possible design solutions. They also collaborated with Gyorgy Kepes, founder of MIT’s Center for Advanced Studies, on the design of the City at Night exhibit at the 1968 Triennale, where they used moving lines of light, each line programmed with a different time sequence, to turn a 100-foot corridor into a place celebrating human movement.nnIn 1975, Stevens founded the Design Guild, a collaborative architectural practice focused on non-profit clients, historic preservation and adaptive re-use; the Guild disbanded in 1991. nnLast fall, the new museum collection was inventoried and catalogued by KatharinanMaria Tanzberger, a 2006 graduate of the University of Applied Arts Vienna, who was introduced to Stevens’ work by her professor, Liane Lefaivre. Intrigued by the notion of a house with no doors (Tanzberger was raised in just such a place) she was drawn to exploring Stevens’ work further, writing a research paper and eventually coming to MIT to work on the archive.nnThe collection consists of more than 300 drawings and diagrams, as well as many texts. Of particular interest is the large collection of the sketches over the years, sketches that differ from the architectural drawings of the prewar period in that they were not renderings of facades or elevations but rather diagrams charting the flow of movement.nThe Stevens collection is a companion to the McNulty papers held by the MIT Archives. McNulty was a member of MIT?s architecture faculty from 1949 to 1956, when he went into private practice with Stevens. For six years he also taught at the University of Petroleum and Minerals in Dhahran, Saudi Arabia, as professor of architecture.nThis article is based in part on “Living Outside the Box: Marty Otis Stevens and Thomas McNulty’s Lincoln House” by Professor Liane Lefaivre, Chair of Architectural History and Theory at the University of Applied Art in Vienna. It appeared in Harvard Design Magazine, Number 24, Spring/Summer 2006.
7302 en Rachel Harrison talks with Johanna Burton Since the early 1990s American artist Rachel Harrison has been developing a brand of unwieldy, unyielding sculpture, sometimes abject and sometimes abrasive in its refusal to give up meaning. Objects from the catch-all drawer -- a clown nose, a syringe, a framed photograph, a houseplant -- are deposited on built agglomerations of polystyrene and cement, giving individual works the pugnacious air of a bad joke, sometimes emphasized by a title (like 2006's Nice Rack). A dolly or stool or table or ladder, thickly encrusted or as good as new, lends most works a strong sense of autonomy, but never resolution.nnUsing elements fundamental to sculpture -- the way an object requires us to walk around it, the way we try to make sense out of two different things juxtaposed -- her works lead us towards one understanding and then makes us question it as we turn the corner. They leave you with your interpretive tools blunted, even as they hint at portraiture. Sometimes Harrison picks up a camera. In 2000, she took a series of photos of a window in Perth Amboy, NJ, where a vision of the Virgin Mary had appeared in the glass. Pilgrims tended to press a hand against the pane, as if the sense of touch were better equipped to pick up a trace of the event. These photographs, unexpectedly representing unfiltered human desire, were part of a maze-like installation of corrugated cardboard with objects.
7303 en Let's Put On a Puppet Show "Let's Put On a Puppet Show!" is an afternoon event revolving around puppetry scheduled for Friday April 6. The Center has invited John Bell, artist and co-founder of Great Small Works; Linda Norden, curator of the American Pavilion for the 2005 Venice Biennale and curator of Pierre Huyghe's 2004 "puppet opera" (featuring a puppet of Linda); Karen Zasloff, artist and shadow-puppeteer; and other special guests for a lecture. "Let's Put on a Puppet Show" brings together artists, curators, and puppeteers to explore the ways puppets and puppet theater have functioned within contemporary art and society.
7333 en Course Introduction //"3.53 is an elective. It is a technical elective in the graduate program here in the Department of Material Science and Engineering. That it is to say it is not a core requirement. So that poses some different challenges for the instructor. In a core class you have a captive audience. The students have to take the class because it satisfies some degree of requirement and so they are there whether they like it or not. In 3.53, the students who are taking 3.53 are taking it because they want to learn the material because presumably it is of relevance to their research. So you have the luxury of teaching a group that is self-selected..."//
7334 en Lecture 1: Vision Statement, Administrative Details - Introduction - Taxonomy of Chemical Species - Origins of Modern Chemistry //"Welcome to 3.091. My name is Donald Sadoway, and I'll be your lecturer this fall. We've got plenty of room here if you need to sit. You can sit up the steps and in the aisles. We'll get things sorted out in the next day or two. What I'd like to do today is to introduce myself, introduce the subject, introduce my plans. I have plans for you. I have plans for you called plans for learning because that's what it's all about.nPeople if you're coming in now, you can sit up here, or you even can sit on the floor up here. It's one big happy family. So, let me begin by telling you that 3.091 is the most important subject that you will take at MIT..."//
7335 en Lecture 2: Classification Schemes for the Elements - Mendeleyev and the Periodic Table - Atomic Structure //"You are allowed to use on the test the official version of the Periodic Table of the Elements which most of you should have by now. There was a shipment that came in partway through the day on Thursday.If you didn't get a Periodic Table, swing by the office. It is just down the hall from this lecture theater. And you bring the Periodic Table and the Table of Constants and a calculator and something to write with. That's four items. Show up, sit down, and the first question is does anybody have a pen? We may not have that many pens. So those are the four items. You do not use an aid sheet on the weekly test..."//
7336 en Lecture 3: Rutherford Model of the Atom, Bohr Model of Hydrogen //"It's time to get back to learning. A couple of announcements before we get started: tomorrow there will be a 10 minute quiz in recitation based upon the content of homework one. Obviously, it's not going to be a question identical to something you've done, but it's going to cover that subject matter.nn//So, please don't bring in your attorney if I didn't ask a question that's identical to one that you've worked. Second thing, we started talking about the periodic table, and I believe that it's a hallmark of any educated person in the 21st century who is technically literate to know the periodic table by heart."It's time to get back to learning. A couple of announcements before we get started: tomorrow there will be a 10 minute quiz in recitation based upon the content of homework one. Obviously, it's not going to be a question identical to something you've done, but it's going to cover that subject matter.."//
7337 en Lecture 4: Atomic Spectra of Hydrogen, Matter/Energy Interactions Involving Atomic Hydrogen //"You are required only to put the one or two-letter chemical abbreviation, the symbol. Some people ask me if I want you to know the molecular weights and so on, and I thought that was bit excessive.nnSo I think if you just know the symbols for the ones that are up there, that would be just dandy. Here is the lady's scarf, very hot, black. See, trim lines, blue and gold, and it has elements on it..."//
7338 en Lecture 5: The Shell Model (Bohr-Sommerfeld Model) and Multi-electron Atoms - Quantum Numbers: n, l, m, s //"The mnemonics contest deadline is a week from today at 5:00. Send it to me by e-mail if you want to win that hot tie or that hot scarf. It's come to my attention that people are moving freely about recitations without arranging through my office.//nn//And, some of the recitations sections are uncomfortably large, which defeats the purpose of the recitation in part. I mean, we want you to have access to the instructor and if you have 30 students there instead of 20, that dilutes your access..."//
7339 en Lecture 6: De Broglie, Heisenberg, and Schrödinger - The Aufbau Principle, Pauli Exclusion Principle, and Hund's Rules - Photoelectron Spectroscopy - Average Valence Electron Energy //"We saw that the Bohr model was able to correlate the observations of Angstrom, which had been formulated by Balmer. And, furthermore, the concept of energy level quantization was observed in mercury during the course of the Franck-Hertz experiment giving credence to the notion that quantization of energy levels is not the property of one-electron atoms alone but is something that is more broadly applicable.//nn//So Bohr was validated in very, very strong measure, but there were also some contrary data. And we saw those. First there was the observation by Michelson who back in the late 1880s had done very precise interferal metric measurements of the hydrogen lines and had observed that the 656 nanometer line associated with the transition of n equals 3 to n equals 2 was, in fact, a doublet..."//
7340 en Lecture 7: Octet Stability by Electron Transfer: Ionic Bonding - Properties of Ionic Compounds: Crystal Lattice Energy //"Chemical properties: so let's put something up as a hypothesis. Let's say, well, maybe it has to do with the energy that it takes to remove electrons. And, one other thing that I failed to point out, if you take a look at the energies associated with the outermost electrons, in this case, lithium, you see it's 0.5 megajoules per mole, and then what's the 6.26? That's associated with one, n equals one, inner shell.//nn//There is a huge difference between the energies in the outermost shell and the inner shells, which tells you that it's unlikely that any electrons except those in the outermost shell are going to be active..."//
7341 en Lecture 8: Born-Haber Cycle - Octet Stability by Electron Sharing: Covalent Bonding - Lewis Structures - Hybridization //"First thing, read the test. That comes as a big shock. A lot of people don't read the test. They just open it up, and they just start working. There's going to be, I don't know, four or five questions.//nn//Well, I just put them down in the order that came out of my head. I don't put them in any strategic order. I don't put hardest first, easiest first. I don't even think they are in chronological order by topic.//nn//I think it is just whatever. I said, well, let's do something on, I don't know, pick a topic. What are we asking you? I cannot remember. They are all made up and ready to go. But pick a topic. Pick something about the Periodic Table..."//
7342 en Lecture 9: Electronegativity, Partial Charge, Polar Bonds and Polar Molecules - Ionic Character of Covalent Bonds, Pauling's Calculation of Heteronuclear Bond Energies //"That is to say, guess the answer ahead of time, strip all the decimal places off. Just go with powers of ten, and then see what the answer should be. And, you will be within 25 or 30% every time. And, that's a good thing to know because if you come out with an answer that's somewhere near the diameter of the universe and it's supposed to be the diameter of an atom, then you will know that you probably made a mistake..."//
7343 en Lecture 10: LCAO MO, Energy Level Diagrams for H2, He2, Li2 - Hybridization, Double Bonds and Triple Bonds, Paramagnetism and Diamagetism //"This is 3.091 and we have standards here. 77% is class average and the standard deviation was 17%, so you can see where things lie. 50% is a pass. Congratulations, a lot of people put in the effort and there is some learning going on..."//
7344 en Lecture11: The Shapes of Molecules, Electron Domain Theory, Secondary Bonding //"And to go beyond that really requires some intensive quantum mechanics. So, we're going to stay with s and p-block. And we saw that if we had two atomic s-orbitals, when they blended to form the molecular orbital, it took on this sort of the ellipsoidal shape.//nn//We called that a sigma orbital. And, the characteristic of the sigma orbital is that you have continuous electron density from one nucleus to the other. We said no holidays; constant electronic density.//nn//We could then combine an s orbital and a p orbital, as for example in the case of hydrogen fluoride. In the case of s plus p, will end up with an orbital that looks something like this. And, it is also termed a sigma orbital because, again, if you look from one nucleus to the other, there is continuous electron density, no holidays...."//
7345 en Lecture 12: Metallic Bonding, Band Theory of Solids (Heitler and London), Band Gaps in Metals, Semiconductors, and Insulators, Absorption Edge of a Semiconductor //"It will be a three-hour extravaganza, a celebration of learning where you all come to show you what you have mastered. And it is going to be just a grand, grand time. And it will be over in the Johnson Athletic Center.//nn//I urge you to take a look at the final exam schedule and make your travel plans, as soon as you know what your last obligation is. Because things are going to get booked, and it is going to be harder and harder to get out of here, certainly to get out at a decent price.//nn//Last day we looked at Electron Domain Theory which allowed us to predict the shapes of molecules. And this is taken from the book. This is Table 4.3. And I have highlighted in orange. We worked through some of these molecules last day, the SF6, BrF5, ICl4-..."//
7346 en Lecture 13: Intrinsic and Extrinsic Semiconductors, Doping, Compound Semiconductors, Molten Semiconductors //"So, last day, we talked about secondary bonding, and we looked at various forms of secondary bonding. Here's the cartoons, dipole-dipole interactions, dipole induced, dipole in solutions, a little bit of a diversion, induced dipole, induced dipole, which was the London dispersion forces, or van der Waals bonds, and also hydrogen bonding.//nn//And, all of these helped us answer the question, what's the state of aggregation? The reason we want to know the state of aggregation is that this is solid-state chemistry. And we want to know when something is a solid.//nn//So, this helps us get to that conclusion. And, then we came to the point where we realized that three quarters of the periodic table wasn't covered by either ionic bonding, covalent bonding, or van der Waals bonding as a primary form of bonding..."//
7347 en Lecture 14: Introduction to the Solid State, the 7 Crystal Systems, the 14 Bravais Lattices //"Wireless Fantasy by Vladimir Ussachevsky. It's one of the first pieces of computer-generated music. It was done at Columbia University in 1960. It was commissioned by a group of fans of Lee de Forest, and it was in honor of de Forest's contribution to wireless broadcast.//nn//And one of the first broadcasts de Forest ever sent over the radio was the piece that you are hearing. It's Parsifal. It is from the Wagner Opera. And so what Ussachevsky has done in the piece is to process it to make it sound really distanct as though it is coming over a shortwave radio.//nn//And then he has Morse Code, and there is various Morse Code messages going through the piece. Wireless Fantasy, from 1960, done long-hand, by the way. It was really done with Morse Code in a mainframe computer and giant reel-to-reel decks..."//
7348 en Lecture 15: Properties of Cubic Crystals: Simple Cubic, Face-centered Cubic, Body-centered Cubic, Diamond Cubic - Crystal Coordinate Systems, Miller Indices //"I have nothing special planned today, just a run-of-the-mill lecture so you get a sense of what it's like to be sitting in one of these seats that you've paid so dearly for. I hope we can convince you that you've made a smart decision in putting your son or daughter on this campus.//nn//You will come to the quick conclusion it's not because of the facilities. Facilities here are nothing special. Classrooms, labs, and so on, it's the peer group. It's not the faculty. Faculty are OK, but I think it's the peer group..."//
7349 en Lecture 16: Characterization of Atomic Structure: The Generation of X-rays and Moseley's Law //"Let's see. A couple of announcements. Tomorrow there will be quiz six based on homework six just on the crystallography. There is a little bit at the end of homework six on x-rays. And we will just start to talk about x-rays today, so I think that we will leave that out.//nn//It will just be based on crystallography. And I have asked the recitation instructors to administer the test at the end of the period so you can have some chance to ask questions. Last day we talked about crystallography.//nn//We were introduced to the seven crystal systems and the 14 Bravais lattices which are shown up on the slide taken from the lecture notes. And we saw that if we wanted to describe the arrangement of atoms in a crystal, we could do so by combining the notion of the Bravais lattice of which there are 14 distinct types with a basis..."//
7350 en Lecture 17: X-ray Spectra, Bragg's Law //"Opera, that's a good one. That's good. It was opera, yes. What opera? No, it wasn't Wagner, but that's a good guess. It's certainly a reasonable style, way over the top. That was Maria Callas singing La Mamma Morta from Andrea Chenier, which was written in 1895, and premiered in the spring of 1896 exactly at the time when the world was going nuts over this mysterious form of radiation that can see inside the human body.//nn//So, I thought that was a good match. The thing's best in class. That's Maria Callas. This is opera for those of you who don't like opera. This, by the way, some of you may recognize if you saw the movie Philadelphia.//nn//This is the piece that's playing when the Tom Hanks character visits the loft, or excuse me, the Denzel Washington character visits the loft of the Tom Hanks character. And this is playing. It's a fantastic piece, way over the top..."//
7351 en Lecture 18: X-ray Diffraction of Crystals: Diffractometry, Debye-Scherrer, Laue - Crystal Symmetry //"Here I have plotted intensity versus wavelength. Long wavelength is low energy. Energy is increasing from right to left. And what we see is a family of nested curves each taking a different plate voltage.//nnLow voltage gives us low intensity. High voltage gives us high intensity. We see this whale-shaped background which we refer to as the continuous spectrum or "bremsstrahlung." And then once we reach a critical voltage beyond which we start to see this second set of lines here, which I have indicated in fuchsia.////nn//And this is the characteristic spectrum, and it is quantized. It is not continuous. We have discrete values of wavelength associated with energy transitions within the target. And those transitions are prompted by the ejection of inner shell electrons..."//
7352 en Lecture 19: Defects in Crystals: Point Defects, Line Defects, Interfacial Defects, Voids //"So, I'm just going to go quickly over the comments that I give before every test just to put everybody in the right frame of mind. The coverage is lecture 7-17. So, that will start with ionics, and go through the generation of x-rays, but not Bragg's law or anything like that.//nn//So, remember, you've got to bring five things with you: your periodic table, table of constants, something to write with, a calculator, and your aid sheet. And, you know, I'm going to make a bold suggestion.//nn//For some of you, why don't you write your recitation instructor's name on your aid sheet? Because I'm going to tell you what I'm going to do. If I don't have a recitation instructor's name on the exam, I'm not going to grade it..."//
7353 en Lecture 20: Amorphous Solids, Glass Formation, Inorganic Glasses: Silicates //"We have a high here of 95. We had a number of people in the 80s and 90s. We had a number of people that were in the 50s and 60s on the first test who moved to the right, and we had also some people that were up here who moved to the left.//nn//Far more moved to the left than moved to the right. I talked to the TAs and we put a fair bit of care into preparing the test that no one felt that it was atypically difficult. But somehow people didn't do as well.//nn//I have been getting emails from people asking about a makeup. Yeah, there is a third test on the 17th of November. We are not going to allow you to retry until you get a score that you like because there is no point in doing that...."//
7354 en Lecture 21: Engineered Glasses: Network Formers, Network Modifiers, Intermediates - Properties of Silicate Glasses - Metallic Glass //"So, that's the subject matter for tomorrow's ten minute test. And, I think that's all I have by way of introduction. Anybody recognize the music? Philip Glass, yes. What else would you play if you are teaching amorphous solids? You could play something like Glass Onion by The Beatles if you can stand the Beatles.//nn//But, I didn't want to do that. Oh yeah, there's people my generation, two categories: those who think the Beatles are great, and those who think they are just, ahem, and I'm in the second category.//nn//They bore me..."//
7355 en Lecture 22: Chemical Kinetics: The Rate Equation, Order of Reaction, Rate Laws for Zeroth, First, and Second Order Reactions - Temperature Dependence of Rate of Reaction //"The oxide ions go into the network and they break the oxygen bonds in a process we call scission. And then lastly in certain glasses we add a compound of a type called an intermediate. And these are actually network formers, but they form a different number of bonds than the parent network.//nn//Typically a larger number of bonds. And, in doing so, they create some free volume which enhances certain properties, sometimes the mechanical properties, sometimes the thermal properties. Specifically, thermal shock resistance is enhanced by the addition of network formers..."//
7356 en Lecture 23: Diffusion: Fick's First Law and Steady-state Diffusion, Dependence of the Diffusion Coefficient on Temperature and on Atomic Arrangement //"The specific chemical rate constant, k, is the proportionality. And, k can be influenced by temperature through this Arrhenius type relationship, which compares the exponential of the activation energy with the available thermal energy.//nn//And, we saw that we can integrate that rate equation, and put the data to the test. If we have a first-order reaction, we should get a semi-log dependence. And, I think I've got that shown here. Here's the data that I showed you last day.//nn//This is just a normal decrease in concentration with time, typical attenuation curve. If we put it to the first-order test, according to this relationship we should get a straight line with a slope of minus k..."//
7357 en Lecture 24: Fick's Second Law (FSL) and Transient-state Diffusion; Error Function Solutions to FSL //"One announcement. There will be Quiz 9, the weekly quiz tomorrow based on the content of Homework 9 which is, I don't remember what it is, but you know what it is. I made it up, but I am already thinking about the next glorious event.//nn//Now I remember. There is chemical kinetics and glasses. That's right, chemical kinetics and amorphous solids. That's right. Last day we started talking about diffusion which is solid state mass transport by random atomic motion.//nn//And we were drawn to the paper that is up on the display which was published in 1855 by Adolf Fick which gave us the law that bears his name describing how matter diffuses through matter. Fick was a very talented individual, and I want to draw attention to something else..."//
7358 en Lecture 25: Solutions: Solute, Solvent, Solution, Solubility Rules, Solubility Product //"So, we will start with the material beginning with Bragg's law. We didn't talk about diffraction on the last test. The coverage talked about generation of x-rays, but not their use in diffraction.//nn//So, starting with x-ray diffraction up through the end of last day's lecture. And, the reason I'm doing this is that tomorrow is a holiday. There are no academic exercises tomorrow. And so, I didn't think it made a good pedagogical sense to teach you something today and have no recitation on Thursday.//nn//You don't meet again with your recitation instructor until Tuesday and then Wednesday's the test. So, being mindful of that, I'm going to start talking about solutions today. None of that will be on the test..."//
7359 en Lecture 26: Acids and Bases: Arrhenius, Brønsted-Lowry, and Lewis Definitions, Acid Strength and pH //"And we came to know the common ion effect which in the presence of a common ion we observed repressed solubility. Today I want to talk about a subset, some specialized solution chemistry. And that takes us to acids and bases, which are pervasive.//nn//Perhaps you got up this morning and washed your hair with pH balanced shampoo and treated yourself to some orange juice with citric and ascorbic acid. Maybe you had the radio playing powered by zinc-alkaline batteries.//nn//I started my car thanks to a lead acid battery. And I had some appliances running, electricity-generated coal fired plant which was spewing sulfur dioxide generating acid rain. We are off to a good start..."//
7360 en Lecture 27: Organic Chemistry: Basic Concepts, Alkanes, Alkenes, Alkynes, Aromatics, Functional Groups, Alcohols and Ethers, Aldehydes and Ketones, Esters, Amines //"So, you should know what we are going to be examining you on, and the same comments that I made before: I want to give you feedback, let you know how you're doing, whether your study methods are effective or not.//nn//It's not an attempt to retest your admission to MIT, or anything like that. If you do your work, you should do very well. And if you have not done your work, you shouldn't do very well, and we'll be able to tell you so.//nn//So, just what we said in the past. Please take the time to read the whole exam, do the easy questions for you; the ones that you find easiest, do those first. But let's be honest, this isn't high school anymore..."//
7361 en Lecture 28: Organic Glasses - Polymers: Synthesis by Addition Polymerization and by Condensation Polymerization //"A lot of people improved. There were a lot of rags to riches stories, but there are still a fair number of people down here. And so my suggestion to you is that if you are down here and you want to -- Too much talking, people.//nn//If you want to start your weekend soon, I will help you by inviting you to leave. You don't have to stay here if you don't want to, but if you do choose to stay, please sit in silence. This is 3.091.//nn//If you are down here, get in and see us. Please talk to your recitation instructor, come on in and see me, and we will talk about strategies for studying, strategies for success. I want you to pass this class, but just doing the same thing over -- This is, I think, Einstein's definition of insanity, doing the same thing over and over again expecting a different outcome..."//
7362 en Lecture 29: Structure-property Relationships in Polymers, Crystalline Polymers //"So, last day we started talking about polymers, macromolecules. These are chemicals that have a very high molecular weight with a repeating chemical structure. I've given an example here of vinyl chloride as the individual molecule.//nn//And, by addition polymerization, we can break the double bond, have it propagate, and we end up with something which is the polyvinyl chloride. N numbers can be very large, and we can get molecular weights in the vicinity of 1,000,000 grams per mole.//nn//We further looked at tailoring properties by control of molecular architecture. We saw that we could look at bulk composition and form such things as copolymers, which are to polymers what alloys are to metals..."//
7363 en Lecture 30: Biochemistry: The Amino Acids, Peptides, and Proteins //"Today I want to talk about biochemistry. We are going to spend the next three lectures on biochemistry. This is the chemistry of living organisms. And I want to make several points by way of introduction.//nn//The first one is that living organisms are chemical systems. And they are governed by the same laws that apply to inanimate matter. We don't have a special chemistry. And, in fact, I came across this comment that was made by the Nobel Laureate Richard Feynman who said that if in some cataclysm all of scientific knowledge were to be destroyed and only one sentence passed onto the next generations of creatures, what statement would contain the most information in the fewest words? "I believe that it is the atomic hypothesis or the atomic fact or whatever you wish to call it, that all things are made of atoms..."//
7366 en Lecture 33: Phase Diagrams - Basic Definitions: Phase, Component, Equilibrium; One-component Phase Diagrams - Two-component Phase Diagrams: Complete Solid Solubility //"We are going to do the first of three lectures on the last topic. We're going to talk about phase diagrams starting today, Monday, and wrap it up on Wednesday. Phase diagrams is related to the question of stability and sustaining the solid state.//nn//And, we talked about the behavior of solids, and we've used solids in order to teach the rudiments of chemistry. But today, I want to talk about the conditions under which solids are stable. And, under what conditions do solids remain stable when do they become unstable? This is important in industry, for example..."//
7367 en Lecture 34: Two-component Phase Diagrams: Limited Solid Solubility - Lever Rule //"What I want to do today is finish the second installment of phase diagrams. Last day, we looked at unary phase diagrams. That is to say, single component. That meant all we had to do was worry about pressure versus temperature because everything is of simple composition.//nn//And I have up here on the display a phase diagram of water which we looked at last day. And I have redrawn it here for convenience. And I just wanted to draw your attention to several things. We are looking at how the stability of the different phases of water varies with pressure and temperature..."//
7368 en Lecture 35: Wrap-up: Closing Remarks about the Course - Student Course Evaluations //"You think you're happy? I'm happy. I'm very happy. What we will do, today is wrap up phase diagrams, make some comments about the final exam, and then we're going to get some people that aren't associated with class are going to come in, they will pass out paperwork and do the course evaluations.//nn//And, we'll get you out of here at 11:55. Before I go any further, draw your attention to another IAP subject. If you're looking for something to do during January, and this will be offered by myself and my postdoc, Patrick Trapa..."//
7382 en Advanced Topics in Programming Languages Series: Python Design Patterns 
7384 en USA/Korea Greeting 
7385 en USA/Korea Policy Trends 
7387 en USA/Korea Collaboration 
7388 en USA/Korea Ending 
7389 en Categorizing a Field – The Use of the Nanotechnology Label across Communities Labels are important to the emergence of organizational fields. The construction and use of labels enables communication and coordination across communities. This paper argues that new and existing communities’ uses of labels commence a categorization process central to the construction of meaning and definition of boundaries within organizational fields. Employing 25 ethnographic observations, 77 interviews and 12,774 articles from five different nanotechnology communities covering primarily the 21 year period from 1984 to 2005 I show the differentiated use of the nanotechnology label across communities. Scientists and entrepreneurs were not the creators and first adopters of the nanotechnology label, instead futurists, the government and venture capitalists played pivotal roles in promoting the nanotechnology label by supplying the field with resources and infusing the nanotechnology label with meaning. Theoretically this paper adds to our understanding of field emergence by reframing emergence as a categorization process.
7390 en Nanotechnology Collaboration, Information Transfer, and Field Structure Nanotechnology is a unique field encompassing many disciplines and specializations. Collaboration between firms is important the development of the field and collaboration across sub-fields may be particularly beneficial – stimulating innovation through the exchange of information. Based on a survey conducted on 242 Massachusetts nanotechnology firms, I explore the underlying factors that encourage and direct collaboration between firms. Firms are often embedded in several different networks of association – with university collaborators, industry-wide associations, informal networks of information transfer, and field identity. I consider whether these loose associations lead to more tangible types of cooperation between firms, and explore what factors push firms to create diverse ties across disciplines. Finally I consider how the resulting structure may impact firms and the industry.
7392 en Local Ecologies of Knowledge, National Systems of Innovation, and Nanotech Research in the Global South Science and technology studies tend to focus on the global South, and especially on Africa, mainly when “following the [Northern] scientist” southwards, when seeking sites of pathology and data collection, or as novice partners in European and American research projects. Yet the fields of bio-, nano- and information technologies are rapidly converging in many regions of the world, and research is proceeding apace in several key centers of erstwhile developing countries. This presentation will examine the features of South Africa’s complex local ecology of knowledge and its national system of innovation, suggesting how bio-pasts may be shaping nano-futures there in specific ways which may provide important insights to, and raise provocative questions for, the study of nano-tech in societies.
7393 en Bounding Nanotechnology: Deconstructing the Drexler-Smalley Debate Nanotechnology” has been touted by many as one of the most critical emerging technological fields today – offering promises of new treatments for cancer, new computing approaches, etc. However, determining what nanotechnology means, the nature of its benefits and its risks, whose research counts as nanotechnology, and who gets to speak on behalf of those who do nanotechnology – essentially, the process of drawing boundaries around nanotechnology as a field – has turned out to be a highly political process of constant negotiation with significant implications for funding, legislation, risk management and public support. In this presentation, I will focus on the construction of one of the most high-profile moments of controversy about nanotechnology’s possibilities: a debate between K. Eric Drexler and Richard Smalley, published as a “point-counterpoint feature” in Chemical & Engineering News. Drawing upon scholarship in science studies concerning boundary work as well as in organizational theory on the role of entrepreneurial actors in the emergence of institutions, we seek to broaden analysis of the debate to include important institutional and organizational contexts – particularly the role of science journalism and editorial decision-making.nnSarah Kaplan is Assistant Professor of Management at the Wharton School, University of Pennsylvania. She is also is co-author of the bestselling business book, Creative Destruction (Currency/Doubleday, 2001) which looks at the challenges of performance in dynamic markets. She received her Ph.D. from MIT’s Sloan School of Management. Her research investigates the role of interpretive processes in shaping technology evolution and firm response to technical change in the biotechnology, telecommunications, personal digital assistant and, most recently, nanotechnology fields. Prior to her academic career, she was a management consultant for nearly a decade with McKinsey & Company where she put innovation tools and techniques to work in a broad range of companies in many industries such as medical devices, retailing, steel, pharmaceuticals and consumer packaged goods.n----
7394 en Laboratory Engagements: Risky Discourse and Research Decisions This presentation describes semi-structured interactions between social scientists and nanoscale researchers in and around the laboratory that are based on an approach termed midstream modulation. Contrary to initial skepticism on the part of nanoscale scientists, the interactions came to be seen as valuable both from the standpoint of responsible innovation and also in terms of the research process itself.nnErik Fisher is Assistant Research Professor, Center for Nanotechnology in Society and Consortium for Science, Policy, and Outcomes. Fisher joins CNS-ASU from the University of Colorado's Center for Science and Technology Policy Research. In the summer of 2006, a CNS-ASU fellowship allowed him to spend several weeks in the Netherlands, comparing international studies of nanotechnology in society. His Ph.D research included both macro- and micro-level analysis of the integration of societal considerations into R&D. He conducted ethnographic-style research in a nanoscale engineering laboratory to investigate the possibility and utility of implementing key provisions of federal nanotechnology legislation. Fisher's research has been published in Science and Public Policy, Technology in Society, and Philosophy Today. He holds a graduate certificate in Science and Technology Policy, an MA in Classics, and a BA in Philosophy and Mathematics
7395 en A Mirror of Social Development: Industry decisions regarding new technologies This study examines how organizations manage risks presented by new technologies and the impact the institutional environment has on this governance process. Micro and nanotechnology research and development (R&D) presents an important case in which to study the influence of institutional environments on organizations. Our survey addresses the perspectives and actions firms take to address the potential importance of environmental health and safety, public perceptions and regulation in their R&D decisions. Recent public reports have raised concern about a seeming lack of attention to potential governance and regulatory concerns within the industry. Our preliminary survey data collected from Massachusetts based firms supports the view that companies' R&D decisions concerning nanotechnology are not usually influenced by environmental/ occupational health and safety concerns, or consumer and general public perceptions. Companies are sensitive to federal and state regulations although these are a minor concern for companies surveyed.
7396 en Advancing the Science of Science and Innovation Policy: Current Approach and Next Step The U.S. scientific enterprise is often cited as being a critical factor contributing to the creation of high wage jobs and firm competitiveness. Yet our scientific understanding of the functioning of the scientific enterprise is extremely limited, which in turns limits our understanding of the consequences of different policy interventions. The Science of Science & Innovation Policy program at NSF was established in order to a better empirical and theoretical basis for science and innovation policy. The presentation will provide an overview of the current state of the program, the research, and a discussion of the future research agenda.
7397 en Inventor Mobility and Knowledge Transmission in Nanotechnology Using U.S. patent records in nanotechnology, we study the relationship between inventor mobility among firms and knowledge diffusion. We find evidence consistent with a story that, in one important nanotechnology subfield, when inventors move among firms they spread knowledge. In particular, we find that if we consider any two patents, A and B, where A and B are in the ``Chemicals, misc.'' subclass, A and B are assigned to different firms and where A is granted after B, patent A is more likely to cite patent B if the patent A firm employs an inventor who earlier worked for the patent B firm.
7398 en Nano Social Science: An Emerging Specialization Our group at Georgia Tech (Alan L. Porter, Philip Shapiro and Jan Youtie) continues to analyze an expanding compilation of nanoscience and nanoengineering (“nano”) research literature from three databases, as well as nanopatents. We have recently updated our Science Citation Index nano search through mid-year 2008; it contains 508,000 article abstracts dating from 1991. We have now compiled counterpart nano searches in the Social Science Citation Index, Arts & Humanities Citation Index, and Scopus – totaling 307 articles. We profile this small, but rapidly growing, literature to explore the development of social sciences addressing nanotechnology. We find evidence that social scientists are increasingly drawing on their own body of literature, whereas in earlier years they drew relatively more heavily upon the nano science and engineering literatures. Bibliometric analyses identify a contingent of some 60 authors whose work is heavily cited by the nano social science literature. We parse them into eight dimensions and explore the literature’s changing emphases. Our results support the hypothesis that social studies of nano are coalescing into a research network in their own right.
7399 en Manufacturing Issues and the National Nanomanufacturing Network Without manufacturing, there are no products. Nanomanufacturing involves a broad range of issues that are addressed with best practices developed from previous manufacturing experiences as well as new approaches. The National Nanomanufacturing Network (NNN) is an open network for collaboration and information exchange among the nanomanufacturing research, development and education community. This network is an alliance of academic, government and industry partners that cooperate to advance nanomanufacturing strength in the U.S. The NNN conducts strategic workshops and facilitates other cooperative activities to build effective and responsible communities of practice in nanomanufacturing. The NNN hosts InterNano, the open source information clearinghouse, to provide vital information to nanomanufacturing community.nnMark Tuominen is P rofessor of Physics and Co-Director of the Center for Hierarchical Manufacturing (CHM) and MassNanoTech, University of Massachusetts Amherst. CHM is a research and education center for the development of efficient process platforms and versatile tools for the two and three dimensional integration of components and systems across multiple length scales. Its mission includes collaboration and cyberinfrastructure activities through a National Nanomanufacturing Network (internano.org) and a digital library-based nanomanufacturing clearinghouse. MassNanoTech is a campus wide initiative for nanoscale science and engineering. With more than 50 faculty researchers, the Initiative provides a single point of contact for academic and industrial collaborations. Tuominen's personal research interests are around nanostructures from self-assembling block copolymer templates and nanoscale device physics.
7400 en Question and Answers for Pannel 1 
7401 en Question and Answers for Pannel 2 
7402 en Question and Answers for Pannel 3 
7403 en Welcomes & Introductions 
7417 en IP in R&D Projects at CERN The speaker will review and illustrate with examples the different types of IP aspects occurring in R&D; projects at CERN and the current initiatives aiming at properly handling IP in these projects.
7418 en Intellectual Property Issues in Practice, Protection, Use and Dissemination of Project Results Based on practical cases, the speaker will first show the good practices regarding IP issues and framework, and then address IP Rights and their application in FP7 projects.
7419 en Patenting at CERN The speaker will present why & when CERN takes patents highlighting motivations and benefits of the patent system for CERN.
7420 en Information from Patents: Applications in Commercialising Scientific and Industrial Research and Development The speaker will address the role of patent information on the basis of case studies covering pure research, applied research, innovation and development.n
7421 en Organization and Content of the Seminar Presentation of the Intellectual Property seminar.
7422 en Introduction to IP in research General overview on the role of IP in research at CERN.
7435 en Equality in Internet Access: Broadband in Massachusetts A public Town Hall Forum with MA Department of Telecommunications and Cable Commissioner, Sharon Gillett was held Wednesday, April 30, 2008 at the University of Massachusetts Amherst.nnThe Town Hall Forum was designed to provide an opportunity for researchers, citizens, advocates, and students to discuss MA Governor Deval Patrick's broadband initiative and related policy issues concerning telecommunications and cable.nnThe Forum was sponsored by the Science, Technology and Society Initiative at the Center for Public Policy and Administration and the National Center for Digital Government.
7438 en Human Rights and Global Issues of our Time 
7440 en Human Rights and Global Issues of our Time 
7441 en Human Rights and Global Issues of our Time 
7442 en Implementing a common framework on business and human rights Professor John Ruggie, UN Special Representative on Business and Human RightnIrene Khan, Secretary General, Amnesty InternationalnModerated by: John Morrison, Programme Director, Business Leaders Initiative on Human Rightsn(BLIHR)
7443 en Human Rights and Global Issues of our Time 
7444 en Implementing a common framework on business and human rights 
7445 en Questions, Reactions from the audience 
7446 en Global Network Initiative Public Forum 
7447 en ‘Looking back’ – what have we learned from the past 10 years 
7448 en ‘Looking back’ – what have we learned from the past 10 years 
7451 en ‘Looking back’ – what have we learned from the past 10 years 
7452 en ‘Looking forward’ – what are some of the challenges of the next 10 years 
7453 en Global human rights at the age of 60. The key lessons of the Universal declaration 
7454 en European Union: An Area of Globalization 
7455 en International Day of Democracy 
7467 en ‘Looking forward’ – what are some of the challenges of the next 10 years 
7468 en ‘Looking forward’ – what are some of the challenges of the next 10 years 
7469 en ‘Looking forward’ – what are some of the challenges of the next 10 years 
7471 en Questions from the audience 
7477 en The responsibility to respect human rights, due diligence and avoiding complicity 
7480 en The value of a human rights-aware approach By business to key global challenges 
7481 en The value of a human rights-aware approach By business to key global challenges 
7482 en The value of a human rights-aware approach By business to key global challenges 
7483 en The value of a human rights-aware approach By business to key global challenges 
7484 en The value of a human rights-aware approach By business to key global challenges 
7490 en Questions from the audience 
7491 en Compiling a monolingual dictionary for native speakers 
7492 en Lexicographer's notes related to a corpus-based dictionary 
7529 en Lecture 1: Introduction: A layered view of digital communication 
7530 en Lecture 2: Discrete source encoding 
7531 en Lecture 3: Memory-less sources, prefix free codes, and entropy 
7533 en Lecture 4: Entropy and asymptotic equipartition property 
7534 en Lecture 5: Markov sources and Lempel-Ziv universal codes 
7535 en Lecture 6: Quantization 
7536 en Lecture 7: High rate quantizers and waveform encoding 
7537 en Lecture 8: Measure, fourier series, and fourier transforms 
7538 en Lecture 9: Discrete-time fourier transforms and sampling theorem 
7539 en Lecture 10: Degrees of freedom, orthonormal expansions, and aliasing 
7540 en Lecture 11: Signal space, projection theorem, and modulation 
7541 en Lecture 12: Nyquist theory, pulse amplitude modulation (PAM), quadrature amplitude modulation (QAM), and frequency translation 
7542 en Lecture 13: Random processes 
7543 en Lecture 14: Jointly Gaussian random vectors and processes and white Gaussian noise (WGN) 
7544 en Graphical models This course covers the basics of Probabilistic Graphical Models, including the basic theory of Bayesian Networks and Markov Random Fields, as well as inference and learning algorithms and applications.
7545 en Reinforcement learning This course covers the theory and application of reinforcement learning: the task of learning to make optimal sequential decisions when given a delayed reward signal. Topics will include planning in known and unknown environments and will place equal emphasis on theoretical results and practical implementation issues in the context of various applications.
7546 en Document Analysis We will consider various problems in document analysis (named entity recognition, natural language parsing, information retrieval), and look at various probabilistic graphical models and algorithms for addressing the problem. This will not be an extensive coverage of information extraction or natural language processing, but rather a look at some of the theory, methods and practice of particular cases.
7547 en Group Theory in Machine Learning This course covers diverse aspects of the role played by symmetry in pattern analysis and machine learning. It is designed to provide background knowledge using examples and to touch current research topics without over emphasizing formalizations and technical descriptions.
7548 en Learning Theory This course highlights some relationships between surrogate losses, scoring rules, f-divergences, Bregman divergences, statistical information and ROC curves and their implications for applications such as divergence estimation.
7549 en Introduction to logic 
7550 en Computer vision A pseudo-boolean function is a function from the space B^n of boolean (0-1) vector to the real numbers. They occur naturally in problems in computer vision related to segmentation where every pixel in an image should be labelled 0 or 1 to minimize a certain cost function. Although the minimization of such functions in in general NP hard, many techniques have been develloped to minimize certain classes of such functions. This is the topic of pseudo-boolean optimization, which will be the subject of this talk. Useful methods include graph-cuts algorithms, message passing and linear programming relaxation. The extension to functions with a finite label set will also be considered.
7551 en Game Theory & Clustering The course will provide an overview of recent work on pairwise data clustering which has lead to establish intriguing connections between unsupervised learning and (evolutionary) game theory. The framework is centered around the notion of a "dominant set," a novel graph-theoretic concept which generalizes that of a maximal clique to edge-weighted graphs. Algorithms inspired from evolutionary game theory, and applications in computer vision and pattern recognition will be discussed.
7552 en Unsupervised learning The first part of his tutorial will discuss un-supervised, semi-supervised and partially-supervised learning. Convex relaxations will be presented for un-supervised and semi-supervised training of support vector machines, max-margin Markov networks, log-linear models and Bayesian networks. The concept of partially-supervised training will then be introduced, with convex relaxations developed for training multi-layer perceptrons and deep networks. Relationships of these methods to classical training algorithms (EM, Viterbi-EM, and self-supervised training) will be discussed. Limitations of convex relaxations will also be considered. The tutorial will then present methods for scaling up such training algorithms. Finally, some simple approximation bounds will be introduced, along with a rudimentary generalization theory for self-supervised training.
7553 en Data Mining The ability to distinguish, differentiate and contrast between different datasets is a key objective in data mining. Such an ability can assist domain experts to understand their data, and can help in building classification models. This presentation will introduce the principal techniques for contrasting datasets. It will also focus on some important real world application areas that illustrate how mining contrasts is advantageous.
7554 en Lecture 15: Linear functionals and filtering of random processes 
7555 en Fundamentals Of Metalogic This course provides an introduction to the metatheory of elementary logic. Following a "refresher" on the basics of notation and the use of classical logic as a representation language, we concentrate on the twin notions of models and proof. An axiomatic system of first order logic is introduced and proved complete for the standard semantics, and then we give an overview of the basic concepts of proof theory and of formal set theory. The material in this course is presupposed by other courses in the Summer School, which is why it is presented first.
7556 en Computability And Incompleteness In these lectures we cover the following topics: Computability and Recursive Functions, Proof that exactly the partial recursive functions are computable, Gödel’s Incompleteness Theorems, Löb's Theorem.These very deep and very powerful results in metalogic from the 1930s were unexpected. They arose in a context in which it was expected that a finitary proof of consistency of arithmetic would shortly be forthcoming. This followed the proposal by the mathematician David Hilbert (1862-1943) for the complete axiomatisation and formalisation of all mathematical knowledge and proofs. Although committed to formal methods, many of Hilbert’s proofs were existential in nature, which ran counter to the finitistic, constructivist methods of mathematics. To deal with this criticism, Hilbert proposed that the formal methods program should establish that all of the “Ideal” existential arguments could in principle be replaced by “Real” constructive arguments, by showing some sort of conservation result. However, the incompleteness results showed that this ‘program’ could not be carried out in a simple way.
7557 en Introduction to Modal Logic We cover the syntax, Kripke semantics, correspondence theory and tableaux-style proof theory of propositional modal and temporal logics. These logics have important applications in a diverse range of fields incuding Artificial Intelligence, Theoretical Computer Science and Hybrid Systems.
7558 en Lecture 16: Review; introduction to detection 
7559 en Overview of Automated Reasoning Course Description:In many applications, we expect computers to reason logically. We might naively expect this to be what computers are good at, but in fact they find it extremely difficult. In this overview course, we look briefly at several varieties of mechanical reasoning. The first is automated deduction, whereby conclusions are derived from assumptions purely by following an algorithm, without user intervention. Automated deduction procedures are parametrized by the logic they are capable of reasoning with. We distinguish between propositional logic and first-order logic. Development and application of propositional logic procedures, also called SAT solvers, received considerable attention in the last ten years, e.g., for solving constraint satisfaction problems, applications in hardware design, verification, and planning and scheduling. Regarding automated deduction in first-order logic, we discuss applications, standard deductive procedures such as resolution, and basic concepts, such as unification. We also examine the dual problem of theorem proving, viz., generating models of a given theory, which has applications to finding counterexamples for non-theorems. A third important area covered in the course is dealing with interactive theorem proving. Interactive theorem proving requires certain amount of instructions from the user to tell the proving program (the theorem prover) how to proceed with a proof. Such interaction is required usually because of the use of higher-order logics, whose expressive formalisms allow natural modeling of complex systems, such as operating system or various protocols. A recent trend in the development of interactive proving is to improve its automation, by combining the power of automatic provers.
7560 en Overview of Automated Reasoning Course Description:In many applications, we expect computers to reason logically. We might naively expect this to be what computers are good at, but in fact they find it extremely difficult. In this overview course, we look briefly at several varieties of mechanical reasoning. The first is automated deduction, whereby conclusions are derived from assumptions purely by following an algorithm, without user intervention. Automated deduction procedures are parametrized by the logic they are capable of reasoning with. We distinguish between propositional logic and first-order logic. Development and application of propositional logic procedures, also called SAT solvers, received considerable attention in the last ten years, e.g., for solving constraint satisfaction problems, applications in hardware design, verification, and planning and scheduling. Regarding automated deduction in first-order logic, we discuss applications, standard deductive procedures such as resolution, and basic concepts, such as unification. We also examine the dual problem of theorem proving, viz., generating models of a given theory, which has applications to finding counterexamples for non-theorems. A third important area covered in the course is dealing with interactive theorem proving. Interactive theorem proving requires certain amount of instructions from the user to tell the proving program (the theorem prover) how to proceed with a proof. Such interaction is required usually because of the use of higher-order logics, whose expressive formalisms allow natural modeling of complex systems, such as operating system or various protocols. A recent trend in the development of interactive proving is to improve its automation, by combining the power of automatic provers.
7561 en Introduction To Statistical Machine Learning This course provides a brief overview of the methods and practice of statistical machine learning, which is concerned with the development of algorithms and techniques that learn from observed data by constructing stochastic models that can be used for making predictions and decisions. The idea of the course is to (a) give a mini-introduction and background to logicians interested in the AI courses, and (b) to summarize the core concepts covered by the machine learning courses during this week.
7562 en Logic, Automata & Games This course provides the students with fundamental notions of temporal logic, mu-calculus, two-player infinite games, alternating tree automata, and with their relationship to answer the model-checking and satisfiability problems.
7563 en Dynamical Logic Dynamic Logic was developed in the late 1970s by David Harel building on previous work by V.R.Pratt. It is a modal logic and as any modal logic it allows to reason about the truth of statements in different states (or worlds). It extends classical modal logic however by taking explicitly into account the transitions from one state to the next state(s). Descriptions of actions (events, or programs) are part of the syntax of Dynamic Logic. This course will start with an introduction into the logical theory of propositional and first-order dynamic logic with respect to a very fine grained notion of actions. At the next level a Dynamic Logic for an abstract programing language will be considered. We will end by presenting a Dynamic Logic for a real programming language and show how this can be used in software verification including a demo of a working system.
7564 en Lecture 17: Detection for random vectors and processes 
7565 en Lecture 18: Theorem of irrelevance, M-ary detection, and coding 
7566 en Lecture 19: Baseband detection and complex Gaussian processes 
7567 en Lecture 20: Introduction of wireless communication 
7568 en Lecture 21: Doppler spread, time spread, coherence time, and coherence frequency 
7569 en Lecture 22: Discrete-time baseband models for wireless channels 
7570 en Lecture 23: Detection for flat rayleigh fading and incoherent channels, and rake receivers 
7571 en Lecture 24: Case study — code division multiple access (CDMA) 
7572 en Official welcomes / Pozdravni nagovori 
7575 en The Centre is Everywhere: Religion and the Public Space 
7576 en Europe as a Trope in Contemporary Muslim Thought 
7577 en Maria’s Flag: Religion and Public Space in Europe 
7578 en Turkey, EU and the Consolidation of Democracy in Turkey 
7579 en Turkish “State Islam”– an Example of The Relationship Between Religion and State 
7580 en Accession of Turkey to the European Union: Analysis of Historical, Political and Economical Processes 
7581 en The “Responsibility to Protect” as a Common Approach by the EU and Turkey 
7582 en Does Globalization Threaten Democracy? 
7583 en Globalized Governance, the Democratic Deficit, and the ansnationalization of the Public Sphere 
7584 en Characteristic “Defects” of Pluralistic Media Systems in Eastern Europe 
7585 en Somaesthetics and Democracy: From Theory to Practice – Considering Advertising of The Equality Based in Public Perception of The Bodily Differences 
7586 en The New “Oriental Despotism”: Imagery of Democratic Europe through its Others 
7587 en The Missing Link: Flaws in EU and U.S. Democracy Promotion Strategies 
7588 en Solidarity and Social Hope: The European Case 
7589 en Have the ‘Eastward Enlargements’ of the European Union Further Extended and Deepened Democratic Scrutiny, Accountability, Deliberation, Participation and Control in the New Member States? 
7590 en Transition and Europeanisation in Central and Eastern Europe – National or (Post)National? 
7591 en Democracy as a Concept of Political Responsibility of the People 
7592 en Democratic Antinomies: Freedom and Security, Liberty and Equality, Individual and Collective Claims 
7593 en Interpersonal relationships as a basis of ethics 
7596 en Justification, Legitimacy, and the Authority of the European Polity 
7598 en The Contingent Encounter of Equality and Liberty in Contemporary Theories of Postdemocracy 
7599 en European Citizenship: The Roma Issue 
7600 en Gender as a (still) significant qualifier of the labour market structure. Gender differentiation and segregation of some key professions; The Example of the Republic of Slovenia. 
7601 en Measuring Social Capital in Multicultural Communities 
7602 en Balkan Wars, The Past and the Future of Europe: Historical Perspectives in Western and Eastern European Historiographies 
7642 en Lecture 1: State of a system, 0th law, equation of state 
7643 en Lecture 2: Work, heat, first law 
7644 en Lecture 3: Internal energy, expansion work 
7645 en Lecture 4: Enthalpy 
7647 en Lecture 5: Adiabatic changes 
7649 en Lecture 6: Thermochemistry 
7650 en Lecture 7: Calorimetry 
7652 en Lecture 8: Second law 
7655 en Lecture 9: Entropy and the Clausius inequality 
7657 en Lecture 10: Entropy and irreversibility 
7658 en Lecture 11: Fundamental equation, absolute S, third law 
7660 en Lecture 12: Criteria for spontaneous change 
7662 en Lecture 13: Gibbs free energy 
7663 en Lecture 14: Multicomponent systems, chemical potential 
7665 en Lecture 15: Chemical equilibrium 
7666 en Lecture 16: Temperature, pressure and Kp 
7667 en Lecture 17: Equilibrium: application to drug design 
7668 en Lecture 18: Phase equilibria — one component 
7669 en Lecture 19: Clausius-Clapeyron equation 
7670 en Lecture 20: Phase equilibria — two components 
7671 en Lecture 21: Ideal solutions 
7672 en Lecture 22: Non-ideal solutions 
7673 en Lecture 23: Colligative properties 
7674 en Lecture 24: Introduction to statistical mechanics 
7675 en Lecture 25: Partition function (q) — large N limit 
7676 en Lecture 26: Partition function (Q) — many particles 
7677 en Lecture 27: Statistical mechanics and discrete energy levels 
7678 en Lecture 28: Model systems 
7679 en Lecture 29: Applications: chemical and phase equilibria 
7680 en Lecture 30: Introduction to reaction kinetics 
7681 en Lecture 31: Complex reactions and mechanisms 
7682 en Lecture 32: Steady-state and equilibrium approximations 
7683 en Lecture 33: Chain reactions 
7684 en Lecture 34: Temperature dependence, Ea, catalysis 
7685 en Lecture 35: Enzyme catalysis 
7686 en Lecture 36: Autocatalysis and oscillators 
7775 en Online Social Networks: Modeling and Mining Online social networks have become major and driving phenomena on the web. In this talk we will address key modeling and algorithmic questions related to large online social networks. From the modeling perspective, we raise the question of whether there is a generative model for network evolution. The availability of time-stamped data makes it possible to study this question at an extremely fine granularity. We exhibit a simple, natural model that leads to synthetic networks with properties similar to the online ones. From an algorithmic viewpoint, we focus on data mining challenges posed by the magnitude of data in these networks. In particular, we examine topics related to influence and correlation in user activities and compressibility of such networks.
7776 en Information Theoretic Comparison of Stochastic Graph Models: Some Experiments The Modularity-Q measure of community structure is known to falsely ascribe community structure to random graphs, at least when it is naively applied. Although Q is motivated by a simple kind of comparison of stochastic graph models, it has been suggested that a more careful comparison in an information-theoretic framework might avoid problems like this one. Most earlier papers exploring this idea have ignored the issue of skewed degree distributions and have only done experiments on a few small graphs. By means of a large-scale experiment on over 100 large complex networks, we have found that modeling the degree distribution is essential. Once this is done, the resulting information-theoretic clustering measure does indeed avoid Q’s bad property of seeing cluster structure in random graphs.
7777 en Approximating the Number of Network Motifs World Wide Web, the Internet, coupled biological and chemical systems, neural networks, and social interacting species, are only a few examples of systems composed by a large number of highly interconnected dynamical units. These networks contain characteristic patterns, termed network motifs, which occur far more often than in randomized networks with the same degree sequence. Several algorithms have been suggested for counting or detecting the number of induced or non-induced occurrences of network motifs in the form of trees and bounded treewidth subgraphs of size O(log(n)), and of size at most 7 for some motifs.nIn addition, counting the number of motifs a node is part of was recently suggested as a method to classify nodes in the network. The promise is that the distribution of motifs a node participate in is an indication of its function in the network. Therefore, counting the number of network motifs a node is part of provides a major challenge. However, no such practical algorithm exists.nWe present several algorithms with time complexity O((e^2k) * k * n * |E| * log((1/delta)/epsilon^2)) that, for the first time, approximate for every vertex the number of non-induced occurrences of the motif the vertex is part of, for k-length cycles, k-length cycles with a chord, and (k???1)-length paths, where k?=?O(log(n)), and for all motifs of size of at most four. In addition, we show algorithms that approximate the total number of non-induced occurrences of these network motifs, when no efficient algorithm exists. Some of our algorithms use the color coding technique.n
7778 en Finding dense subgraphs with size bounds 
7779 en The giant component in a random subgraph of a given graph 
7780 en Quantifying the impact of information aggregation on complex networks: a temporal perspective 
7781 en A Policy Perspective on Query Log Privacy-Enhancing Techniques As popular search engines face the sometimes conflicting interests of protecting privacy while retaining query logs for a variety of uses, numerous technical measures have been suggested to both enhance privacy and preserve at least a portion of the utility of query logs. This article seeks to assess seven of these techniques against three sets of criteria: (1) how well the technique protects privacy, (2) how well the technique preserves the utility of the query logs, and (3) how well the technique might be implemented as a user control. A user control is defined as a mechanism that allows individual Internet users to choose to have the technique applied to their own query logs.
7782 en Survey and evaluation of query intent detection methods 
7783 en Analysis of Long Queries in a Large Scale Search Log 
7784 en Comparative Analysis of Clicks and Judgments for IR Evaluation 
7785 en Query Suggestions Using Query-Flow Graphs 
7786 en Intentional Query Suggestion: Making User Goals More Explicit During Search 
7787 en Search Shortcuts Using Click-Through Data from the 2006 RFP Dataset 
7845 en Adopt a rag doll and save a child For the sixth consecutive year now, UNICEF Slovenia has been successfully implementing the project Rag doll, which under the title 'Adopt a rag doll and save a child' ensures the vaccination of children against six infectious diseases (diphtheria, measles, whooping cough, infantile paralysis, tuberculosis, and tetanus) in the developing countries. Namely, every day 2 million children around the world die from the diseases that could be prevented.nThe project Rag doll has spread throughout the entire country with extreme success among the small and big, the young and old, men and women makers of rag dolls. The rag doll is a toy known to all cultures of the world and in this project by UNICEF it symbolizes a child from a developing country who needs our help. With the purchase or adoption of a rag doll you enable the vaccination of one child against six infectious diseases.nThe manufacturing of rag dolls is based on the voluntary work of older volunteers, pupils of primary and secondary schools, and children and their teachers in kindergartens. The purchase of a rag doll forms a bond between those who helped a child with their contribution, and those who enabled this with their voluntary work. For this purpose, every rag doll has an identification card with her or his characteristics, and a greeting card, with which a buyer can inform the manufacturer about the doll's new home. Thus the doll is not bought but, rather, adopted to her new family.nn**Facts and figures:**nn* more than 30 million children around the world are not vaccinated;n* due to the diseases that could be prevented by vaccination, die:n** over 2 million children every year;n** six children every minute;n** one child every minute solely due to measles;n* while in the 80s only every fifth child was vaccinated, today the number has risen to 80 % of the world's children.nn**What is UNICEF doing?**n* UNICEF is the main supplier of vaccines to the developing countries;n* UNICEF vaccinates more than 40 % of all the children around the world.nn**20 EUR:**n* diphtheria;n* measles;n* whooping cough;n* infantile paralysis;n* tuberculosis;n* tetanus.nnMake your own unique rag doll and save a childnIt is not difficult to make a doll, it can be done by anyone, a child or an adult, all you need is a little imagination and effort.
7850 en The Secret History of Silicon Valley How Stanford the CIA/NSA Built the Valley We Know TodaynnHow much does an average Googler know about the history of the placenhe works in? Silicon Valley.nCome and test your knowledge. I have seen this talk and I assure you -neven seasoned Silicon Valleynveterans will find this story interesting. Silicon Valley entrepreneurnSteve Blank will talk about hownWorld War II set the stage for the creation and explosive growth ofnSilicon Valley, and the role ofnFrederick Terman and Stanford in working with government agenciesn(including the CIA and thenNational Security Agency) to set up companies in this area thatnsparked the creation of hundredsnof other enterprises.
8086 en Machine Learning, Pattern Recognition, Cross-modal analysis and fusion 
8087 en Multimedia Information Indexing and Retrieval 
8088 en Semantic Web and Multimedia 
8089 en Multimedia Ontologies for Reasoning and Analysis 
8090 en Multimedia Signal Processing 
8091 en Flexible Interfaces for Semantically Annotated Multimedia 
8092 en Calais - the media and the semantic web 
8093 en Watch this social computing space 
8094 en Concerns about Privacy & Innovation in ICT and Media Industries 
8095 en Taling about Networked Journalism, Search, Privacy, Filtering, Personalisation 
8096 en Semantics and 3D media 
8097 en Social Networks and Multimedia Semantics 
8195 en Authors@Google: Slavoj Zizek The Authors@Google program was pleased to welcome Slavoj ?i?ek to Google's New York office to discuss his latest book, "Violence".
8198 en New materials - where chemistry and materials sciences meet / Novi materiali - kjer se sre?ata kemija in znanost o materialih New materials are the basis for new technologies. To overcome the limits of traditional materials, new materials concepts are needed. An example is inorganic-organic hybrid materials. Combination of inorganic and organic building blocks on a molecular scale results in properties between that of purely organic (polymers, molecular materials etc.) and purely inorganic materials (glass, ceramics, etc.). In order to obtain the desired “tailor-made” property profiles, the reactivity and functionality of the molecular precursors must be carefully tuned. This offers the opportunity to design chemical strategies for the synthesis of materials with pre-defined properties.n---- nNovi materiali so osnova novih tehnologij. Da bi presegli meje tradicionalnih materialov, so potrebni konceptualno novi pristopi k sintezi. Primer le-teh so anorgansko- organski hibridni materiali. Kombinacija anorganskih in organskih gradnikov na molekularni ravni vodi do lastnosti, ki so med lastnostmi ?istih organskih (polimeri, molekularni materiali itd.) in ?istih anorganskih materialov (steklo, keramika, itd.). nnDa bi dosegli ?elene lastnosti novega materiala, je potrebno skrbno izbrati oz. prilagoditi reaktivnost in funkcionalnost molekulskih snovi, iz katerih izhajamo. To pa daje mo?nosti na?rtovanja kemijskih strategij za sintezo materialov z vnaprej dolo?enimi lastnostmi.
8200 en Service-oriented architectures 
8201 en Service-oriented architectures: Case study 
8202 en Web service technologies 
8203 en Ontology Engineering 
8204 en Semantics, Semantic Web 
8205 en Semantic web Languages 
8206 en Web Service Modeling Ontology (WSMO) 
8207 en Web service modeling language (WSML) 
8208 en Semantic execution environments 
8209 en Engineering SWS with WSMT 
8210 en Business Process Management: The semantics of "B" in BPM 
8211 en Web services retrieval 
8212 en Towards verifying compliance in Semantic Web service compositions 
8213 en Web services in geospatial semantic web 
8241 en Non-classical Logic Non-classical logics are used to characterize phenomena with which classical logic has difficulty or to represent alternative views of reasoning. Relevant logic, for example, rejects the rule of classical logic that allows us to add new premises to an already valid inference to produce another valid inference. Relevant logic, as its name suggests, demands that all the premises of a valid argument be actually relevant to the derivation of the conclusion. In contrast, a weaker form of relevant logic – linear logic – is not supposed to represent an alternative view of valid inference, but rather describe relationships between different sorts of entities than classical (or relevant logic). Traditionally, logics are thought to represent relationships between propositions but linear logic represents relationships between resources and actions. The resulting logic is quite different from traditional logics and is interestingly related to the other logics that we will study such as relevant logic. A variant of linear logic, that we will also examine, is used to study the flow of information between agents.
8242 en Intelligent Agents An agent is an entity that receives percepts from the environment in which it is operating and applies actions to the environment in order to achieve its goals. The notion of an agent provides a unifying conceptual framework for current research in artificial intelligence.nnIn these three lectures, I will introduce the basic ideas of agents, describe some agent architectures, and comment briefly on relevant philosophical and historical issues.
8243 en Search and Games Search is a major direction in current AI research and a powerful solving technology in a wide range of real-life problems. This course focuses on single-agent search techniques. Pathfinding in games is used as an application domain.
8244 en Artificial Intelligence Planning The course presents the most important approaches to state space traversal used in planning, including techniques based on propositional satisfiability testing, heuristic state-space search, and logic-based data structures like binary decision diagrams. The main applications of these techniques in classical planning and in more complex forms of planning is discussed.
8245 en Knowledge Representation and Reasoning Research in knowledge representation and reasoning has a long history in artificial intelligence and logic-based approaches have played a major part in the fields development. In this course we will survey logic-based in KRR from non-monotonic logics though to description logics and the semantic web.
8246 en Universal Artificial Intelligence The dream of creating artificial devices that reach or outperform human intelligence is many centuries old. In this course I will present an elegant parameter-free theory of an optimal reinforcement learning agent embedded in an arbitrary unknown environment that possesses essentially all aspects of rational intelligence. The theory reduces all conceptual AI problems to pure computational questions.
8247 en Weighted Graphs and Disconnected Components: Patterns and a Generator The vast majority of earlier work has focused on graphs which are both connected (typically by ignoring all but the giant connected component), and unweighted. Here we study numerous, real, weighted graphs, and report surprising discoveries on the way in which new nodes join and form links in a social network. The motivating questions were the following: How do connected components in a graph form and change over time? What happens after new nodes join a network– how common are repeated edges? We study nu- merous diverse, real graphs (citation networks, networks in social media, internet traffic, and others); and make the following contributions: nn* we observe that the non-giant connected components seem to stabilize in size, n* we observe the weights on the edges follow several power laws with surprising exponents, and (c) we propose an intuitive, generative model for graph growth that obeys observed patterns.nnJoint work with Leman Akoglu and Christos Faloutsos.
8248 en Large Scale Scene Matching for Graphics and Vision 
8249 en Efficient Parallel Learning of Linear Dynamical Systems on SMPs 
8250 en Prequential Statistics 
8251 en Machine Learning Applications / Challenges in Natural Language Parsing 
8252 en Graph Identification 
8253 en Lattice dynamics studied by inelastic neutron scattering and numerical modelling Neutron scattering and lattice dynamics have a long history, in particular with experimental data being used to refine inter-atomic force constants. Nowadays there is still a strong synergy between experiment and simulation, with the difference that ab initio computational methods are often now used to analyse inelastic scattering data from complex systems. In addition, abinitio methods give access to electronic and magnetic structure, the latter being directly probed by neutron scattering techniques.n
8368 en Opening - LICSB '09 
8369 en Genotyping Structural Variation 
8370 en Detecting Evolutionary Inter-Gene Heterogeneity in Borrelia burgdorferi Borrelia burgdorferi is one of the bacterial species responsible for the most prevalent vector-borne disease in the temperate zone of the northern hemisphere, Lyme borreliosis [1]. Phylogenetic analyses of B. burgdorferi are now based on a concatenation of several housekeeping genes that are assumed to evolve according to one evolutionary pattern. nnThis is a strong assumption and, when untrue, inferences are a compromise between different phylogenetic signals., We have designed a Bayesian mixture model under a missing data formulation to automatically recover the evolutionary pattern of each site in a DNA alignment. Evolutionary consistency among a set of genes can be argued whenever most of the sites are allocated to the same evolutionary class. nnOnly in this case will a concatenation of genes produce valid inferences., In this study we demonstrate consistency in the evolution of eight housekeeping genes and evolutionary inconsistency between these housekeeping genes and the gene encoding the immunodominant outer surface protein C. Our method is a suitable indicator of evolutionary agreement or disagreement when employing large-scale gene concatenations, not only in B. burgdorferi, but for any phylogenetic analysis.,nn [1] Margos, G. et al., 2008. Proceedings of the National Academy of Sciences of the USA, 105(25): 8730 - 8735.
8371 en Role of Gene Mutations Predicted from a Computational Model of the Cochlea of the Inner Ear The mutations in the GJB2 gene encoding for the connexin 26 (Cx26) protein are the most common source of nonsyndromic forms of deafness. Cx26 is a building block of gap junctions (GJ), establishing electrical intercellular connectivity between cells in distinct cochlear compartments. Cochlear circulation of ions such as potassium (K+) and metabolites such as IP3 is essential for normal hearing: animal models of the Cx26 deficiency in the organ of Corti (one of the compartments) seem to suggest the death of sensory cells (outer and inner hair cells, OHC and IHC, respectively) due to failed K+ homeostasis as the underlying problem. However, this mechanism may not be the only one. In search for alternative mechanisms we have used a large scale three-dimensional model of mechano-electrical transduction of sound in the cochlea (Mistrik et al., 2009). Indeed, a careful analysis revealed that reduced GJ conductivity in the organ of Corti would decrease the receptor potential across the OHC basolateral membrane. As the OHC electromotility is crucial for sound amplification granting the cochlear sensitivity and frequency selectivity we conclude that the reduction of the OHC somatic electromotility could represent an additional pathological mechanism in the Cx26 related forms of deafness.
8372 en Gaussian process regression bootstrapping Both mechanistic and empirical modelling techniques are employed in systems biology. The former construct models whose structure explicitly describes components of the biological system under investigation, while the latter make predictions on the strength of patterns in the data. Although empirical models such as Gaussian process regression (GPR) do not directly help us to elucidate the processes that generated a given data set, they can nevertheless form part of a strategy for testing and investigating hypotheses and mechanistic models. , In our work, we exploit the predictive power of GPR in order to generate plausible simulated data sets from experimentally obtained time-course data. This amounts to a parametric bootstrap (in which the parametric model is a multivariate normal) that implicitly takes into account the time-dependence in the data. Having obtained bootstrap samples, we fit mechanistic models to both the original and simulated data. The variability amongst these fitted models reveals the sensitivity of the fit to uncertainty in the data. We use this approach to investigate the effects of data uncertainty upon parameter estimates in a model of a signalling pathway and upon gene network inference.
8373 en Estimation of Multiple Transcription Factor Activities using ODEs and Gaussian Processes Recently, ordinary differential equations (ODEs) have been used to infer the concentration of a single transcription factor (TF) protein from time series expression data of a set of target genes. For instance, this has been applied to uncover the concentration of the p53 protein; see Barenco et al. (2006). In the present work, we propose a framework to estimate multiple TFs from a set of observed gene expressions that are co-regulated by these TFs. We assume that the connectivity network (that describes which TFs regulate each of the genes) is partially and probabilistically observed. For example, such side information can be available through a technique such as Chromatine Immunoprecipitation (ChIP). The objective of inference is to estimate the structure of the sub-network, the concentration of the transcription factor proteins continuously in time as well as to infer the type of regulation in each network link (i.e. activation, repression or non-regulation). This multiple-TF framework uses Gaussian process priors to model the unobserved TF activities continuously in time, as considered in Lawrence, et al. (2007) for the single-TF case. The ODE model of transcriptional regulation using multiple TFs is based on the following linear differential equation, dy_j(t)/dt = B_j+ S_j*g(f_1(t),...,f_R (t);w_j)? D_jy_j(t), where y_j(t) denotes the gene expression of jth gene at time t, (B_j,S_j,D_j) are the kinetic parameters of the equation, each f_r(t) is a TF concentration function, w_j are the connectivity weights between the gene and the TFs and g is a sigmoid (e.g. Michaelis-Menten) type of function. Given a set of observations of the gene expression at discrete time points, the parameters {B_j,S_j,w_j, D_j} and the protein concentration functions {f_r(t)} are estimated by using a full Bayesian methodology that employs a Markov chain Monte Carlo algorithm. Gaussian process priors are placed on the functions {f_r(t)}, while the connectivity weights {w_j} are given sparse priors so that the side prior information about the network connectivity is taken into account. The whole framework is currently applied to sub-networks in yeast cell-cycle gene expression data in Spellman et al. (1998) and Orlando et al. (2008) by using the connectivity ChiP information provided in Lee et al. (2002). This is a joint work with Magnus Rattray and Neil Lawrence.
8374 en Definition of Valid Proteomic Biomarkers: Bayesian Solutions to a Currently Unmet Challenge Clinical proteomics is suffering from high hopes generated by reports on apparent biomarkers, most of which could not be later substantiated via validation. This has brought into focus the need for improved methods of finding a panel of clearly defined biomarkers. To examine this problem, urinary proteome data was collected from healthy adult males and females, and analysed to find biomarkers that differentiated between genders. We believe that models that incorporate sparsity in terms of variables are desirable for biomarker selection, as proteomics data typically contains a huge number of variables (peptides) and few samples making the selection process potentially unstable. nnThis suggested the application of the two-level hierarchical Bayesian probit regression model that Bae and Mallick (2004) proposed for variable selection, which used three different priors for the variance of the regression coefficients (inverse Gamma, exponential and Jeffreys) to incorporate different levels of sparsity in their model. We have also developed an alternative method for biomarker selection that combines model based clustering and sparse binary classification. nnBy averaging the features within the clusters obtained from model based clustering, we deﬁne “superfeatures” and use them to build a sparse probit regression model, thereby selecting clusters of similarly behaving peptides, aiding interpretation.
8375 en Time-varying genetic network inference using informative priors 
8376 en Moment closure and block updating for parameter inference in stochastic biological models This talk will tackle one of the key problems in the new science of systems biology: inference for the rate parameters underlying complex stochastic kinetic biochemical network models, using partial, discrete and noisy time-course measurements of the system state. Although inference for exact stochastic models is possible, it is computationally intensive for relatively small networks, We explore the Bayesian estimation of stochastic kinetic rate parameters using approximate models, based on moment closure analysis of the underlying stochastic process. By assuming a Gaussian distribution and using moment-closure estimates of the first two moments, we can greatly increase the speed of parameter inference. The parameter space can be efficiently explored by embedding this approximation into an MCMC procedure. We impute the missing species using a bridge updating scheme where each proposed move is a bridge of length m. We investigate how the choice of m affects the efficiency of the sampling in a auto-regulatory gene network.
8377 en Bayesian model selection: mechanistic models of Erk MAP kinase phosphorylation dynamics ABC SMC is a Bayesian parameter inference algorithm which is based on efficient simulation of mechanistic models. We have adapted it for model selection by defining it on an extended parameter space (M, \theta). Model selection ABC SMC algorithm chooses the best model for the system given the set of available models, balancing the fit to the data and the complexity of the model. , Here we apply it to the phosporylation dynamics of Erk MAP kinase. It has been demonstrated that in vitro phosphorylation and dephosphorylation of MAPK occur though a distributive mechanism (Burack 1997, Ferrell 1997, Zhao 2001). Recently, novel experimental techniques based on automated high-throughput immunostaining and image processing have allowed for collection of data based on population of individual cells in vivo (Ozaki et al., in preparation). We are going to examine four different hypotheses , 1) distributive phosphorylation and dephosphorylation , 2) processive phosphorylation and dephosphorylation , 3) distributive phosphorylation, processive dephosphorylation , 4) processive phosphorylation, distributive dephosphorylation , modeled by kinetic ODE models and employ Bayesian model selection tool based on ABC SMC algorithm (Toni et al., 2009) to determine the most likely mechanisms of phosphorylation and dephosphorylation occuring in Erk signaling pathway in vivo.
8378 en Exploring experimental designs for network inference using perturbations and a Bayesian sequential learning strategy Modern approaches to systems biology call for a tightly coupled iterative cycle of computational modelling and independent experimental validation of model predictions. A Bayesian formulation to model inference should be exceptionably amenable to this type of experimental paradigm. Given prior knowledge that has been encoded into a model, we can train the model on data from experiment A. The result is a posterior distribution over, say, gene regulatory networks which can act as a prior for the next model, trained on data from experiment B. The Bayesian model at each stage can be seen as a distillation of the experimental data obtained up to that point, and since it is a probabilistic model it can be used as an expert prior for the model trained on the next data set. A Bayesian sequential learning strategy can therefore be employed, instead of waiting for all the data to be collected before training the first model. We explore this paradigm using simulated data from a realistic in silico model network and experimental microarray time series data sets studying stress responses in Arabidopsis and E. coli.
8379 en Hybrid Inference for Stochastic Kinetic Models 
8380 en Bayesian Hypotheses Testing in Raman Spectroscopy Surface enhanced resonance Raman spectroscopy (SERRS) can be used to detect a wide range of biochemical species by employing a speciﬁc set of nanoparticle probes. New data obtained using this technology will signiﬁcantly improve our abilities to understand biological systems by enabling high throughput measurements of protein concentrations. Analysis of spectra produced by SERRS is often done manually, and a solid statistical approach to interpreting such results is very important to draw valid conclusions. We model data obtained using SERRS using Gaussian Processes. This modelling approach enables computing marginal likelihoods over di?erent covariance functions of GPs, and therefore consistent hypotheses testing can be performed. We investigate several important problems in analytical biochemistry: n• Whether the spectroscopic response of analytes changes in time, or the observed variations can be explained by measurement errors.n• Is it possible to measure the di?erences in concentrations of an analyte given practical variability of the measurement.n• What are the most informative frequency bands to measure the concentration of a given protein with high conﬁdence. We, additionally, develop a calibration procedure based on GP regression of the spectroscopic data using Markov Chain Monte Carlo to marginalise over the hyper-parameters of the covariance function.
8381 en Inference in a probabilistic model of dynamic DNA Microsatellites are simple sequence repeats present in both coding and non-coding regions of the genome. DNA instability at some microsatellites is the underlying genetic defect in a number of human diseases including myotonic dystrophy type 1 (DM1). New quantitative data, collected by single molecule analysis of repeat length in blood cells from 145 DM1 patients reveals the extent and nature of the genetic variation within and between patients (Morales, PhD thesis, 2006). This dataset of thousands of de novo mutations provides a unique opportunity to examine the underlying mechanism of mutation, which is thought to be a universal biological process that is simply amplified in the disease case. We are developing discrete mathematical models and stochastic simulation techniques that capture key features of the mutation mechanism underlying repeat length evolution. We derive analytical expressions for the length distribution of an adapted birth and death process and employ Bayesian techniques to calibrate our models against the biological data and test model hypotheses. Our work aims to improve prognostic information for patients, as well as providing a deeper understanding of the underlying biological process. In particular we will provide evidence that a previous model (Kaplan et al., 2007) can be improved by introducing a non-zero contraction rate.
8382 en Beyond Molecular Biology – Applying Gene Regulation Network Inference Methods in Ecology Reconstructing gene regulation networks from gene expression data is an important task in molecular biology for which various network inference methods have been developed. In ecology, species interaction networks serve a similar purpose, in that they show how different species relate to each other. We have investigated the possibility of applying the methods that were developed for gene regulation networks to reconstruct species interaction networks from species abundance data. We used a Lotka-Volterra style simulation model to produce synthetic data based on species interaction networks, and then tried to reconstruct the original network from this data using Bayesian networks, LASSO (Least Absolute Shrinkage and Selection Operator) and SBR (Sparse Bayesian Regression). We also developed extensions to these methods for dealing with the problem of spatial autocorrelation. Our experiments showed that we can retrieve many species interactions, while keeping the false positive rate low. We compared the different methods, and found that LASSO and Bayesian networks perform best.
8383 en Temporal Development and Collapse of an Arctic Plant-Pollinator Network Topology and linkage rules of plant-pollinator networks have received much attention lately. One aspect that is difficult to study is the temporal dynamics of the network as it requires observation of how the network changes over time. Here we study an Arctic plant-pollinator network in two consecutive years using mathematical models and describe the temporal dynamics (daily assembly and disassembly of links) by simple statistical distributions. Among other things, we demonstrate that the dynamics is strikingly similar in both years despite a strong turnover in the composition of the pollinator community and the day-to-day development of the network poorly correlates with (available) weather parameters.
8385 en Mentoring and Persistence among Lower-Income First Generation College Students in STEM Increasing the diversity of the STEM workforce has been an issue of national concern for decades. African American and Latino students, from working class families, are significantly underrepresented in science and technical fields, and this is especially the case for female students within computer science and engineering. Over half of first generation, lower-income Latinos and African American students use two-year colleges, or trade colleges, as an entry point to the four-year degree, but so few actually complete these pathways. Thus, research is warranted to better understand the experiences of ethnically diverse working class women and men within these complex pathways. My research, guided broadly by an ecological perspective that highlights the importance of macro-economic factors and multiple contexts (e.g., home, school, and work), has focused on the mentoring experienced by lower-income students as they strive to “get on track” and persist toward a four-year STEM degree. Drawing upon longitudinal survey and interview data with high school students, trade college students, community college, and university students, I have investigated how particular functions of mentoring are associated with STEM persistence. I will describe examples of essential instrumental functions of mentoring and productive mentoring constellations, articulate a need for greater organizational infrastructures for mentoring, and point to implications for designing mentoring interventions, governmental aid for students pursuing higher education, and transfer program designs that link shorter-term certificate and degree programs to four-year degree programs.
8411 en The state of art of policies for skid resistance 
8412 en Low noise road surfaces. current practices in Europe 
8413 en General discussion on the potential value of harmonised policies 
8414 en Report-back on the results of group discussions 
8415 en Conclusions of the 2nd Tyrosafe Workshop 
8428 en Lecture 1 - The Motivation & Applications of Machine The Motivation & Applications of Machine Learning, The Logistics of the Class, The Definition of Machine Learning, The Overview of Supervised Learning, The Overview of Learning Theory, The Overview of Unsupervised Learning, The Overview of Reinforcement Learningn
8439 en Sparse Exponential Weighting and Langevin Monte-Carlo The performance of statistical estimators in several scenarios, such as nadaptive nonparametric estimation, aggregation of estimators and estima- ntion under the sparsity constraint can be assessed in terms of sparsity oracle ninequalities (SOI) for the prediction risk. One of the challenges is to ﬁnd nestimators that attain the sharpest SOI under minimal assumptions on the ndictionary. Methods of estimation adapted to the sparsity scenario like the nLasso, the Dantzig selector or their modiﬁcations, can be easily realized for nvery large dimensions of the problem but their performance is conditioned nby severe restrictions on the dictionary. Such methods fail when the ele- nments of the dictionary are not approximately non-correlated. This is some- nwhat unsatisfactory, since it is known that the BIC method enjoys better nSOI without any assumption on the dictionary. However, the BIC method is nNP-hard. This talk will focus on Sparse Exponential Weighting, a new tech- nnique of sparse recovery aiming to realize a compromise between theoretical noptimality and computational e?ciency. The method is based on aggrega- ntion with exponential weights using a heavy-tailed sparsity favoring prior. nThe theoretical performance of Sparse Exponential Weighting in terms of nSOI is comparable with that of the BIC and is even better in some aspects. nNo assumption on the dictionary is needed. At the same time, we show that nthe method is computationally feasible for relatively large dimensions of the nproblem. We prove that Langevin Monte-Carlo (LMC) algorithms can be nsuccessfully used for computing Sparse Exponential Weighting estimators. nNumerical experiments conﬁrm fast convergence properties of the LMC and ndemonstrate nice performance of the resulting estimators. This is a joint nwork with Arnak Dalalyan.
8440 en hase transitions phenomenon in Compressed Sensing Compressed Sensing reconstruction algorithms typically exhibit a zeroth-order phase transition phenomenon for large problem sizes, where there is a domain of problem sizes for which successful recovery occurs with overwhelming probability, and there is a domain of problem sizes for which recovery failure occurs with overwhelming probability.nnThe mathematics underlying this phenomenon will be outlined for $\ell1$ regularization and non-negative feasibility point regions. Both instances employ a large deviation analysis of the associated geometric probability event.nnThese results give precise if and only if conditions on the number of samples needed in Compressed Sensing applications.nnLower bounds on the phase transitions implied by the Restricted Isometry Property for Gaussian random matrices will also be presented for the following algorithms: $\ell^q$-regularization for $q\in (0,1]$, CoSaMP, Subspace Pursuit, and Iterated Hard Thresholding.
8441 en Large Precision Matrix Estimation for Time Series Data with Latent Factor Model Estimating a large precision (inverse covariance) matrix is di?cult due to nthe curse of dimensionality. The sample covariance matrix is notoriously bad nfor estimating the covariance matrix when the dimension p of the multivariate nvector is comparable or even larger than the number of time points n observed. nIt is singular and hence cannot be inverted for the precision matrix. nWe use the factor model and procedure proposed by Pan and Yao (2008) nfor multivariate time series data to carry out dimension reduction when p n≈ n nor even p > n. A version of the unknown factors and the corresponding factor nloadings matrix are obtained. We show that when each factor is shared by nO(p) cross-sectional data points, the estimated factor loadings matrix, as well nas the estimated precision matrix for the original data, converge weakly in nL2 -norm to the true ones at a rate independent of p. This striking result ndemonstrates clearly when the “curse” is cancelled out by the “blessings” in ndimensionality. It is particularly useful in portfolio allocation in ﬁnance when nthe number of stocks p is large. Convergence rate in L2 norm for the precision nmatrix is directly related to the goodness of the estimated optimal portfolio, nwhich converges weakly to the true one in the average squared L2 norm at a nrate also independent of p as a result. nWe also show that the method cannot estimate the covariance matrix nbetter than the sample covariance matrix, which coincides with the result in nFan et al. (2008) when factors are known. Simulations demonstrate a variety nof e?ects to the estimators when assumptions are not met. A set of real stock nmarket data is analysed. n
8442 en Fast methods for sparse recovery: alternatives to L1 Finding sparse solutions to underdetermined inverse problems is a fundamental challenge encountered in a wide range of signal processing applications, from signal acquisition to source separation. Recent theoretical advances in our understanding of this problem have further increased interest in their application to various domains. In many areas, such as for example medical imaging or geophysical data acquisition, it is necessary to find sparse solutions to very large underdetermined inverse problems. Fast methods therefore have to be developed. In this talk, we will present two classes of fast algorithm that are competitive with the more classical L1 minimization (Basis Pursuit). The first techniques is based upon a greedy selection approach. However in each iteration, several new elements are selected. The selected coefficients are then updated using a conjugate update direction. This is an extension of the previously suggested Gradient Pursuit framework to allow an even greedier selection strategy. It also has the unique property of allowing a smooth trade-off between recovery performance and computational complexity. The second technique that we discuss is an extremely simple strategy called Iterative Hard thresholding (IHT). Despite its simplicity it can be shown that: it gives near-optimal performance guarantees; it is robust to observation noise; it has low bounded computational complexity per iteration and only requires a fixed number of iterations depending on the signal to noise ratio of the signal. Unfortunately a niaive application of IHT yields empirical performance substantially below that of other state of the art recovery algorithms. We therefore discuss have the algorithm can be modified to obtain performance that is comparable with state of the art while retaining its strong theoretical properties.
8443 en Poster Spotlights 1 **A Comparison of Inference Methods for Sparse Factor Analysis Models**\\nOliver Stegle, Kevin Sharp, Magnus Rattray, John Winnnn**Generalization Bounds for Learning the Kernel: Rademacher Chaos Complexity**\\nYiming Ying, Colin Campbellnn**Learning Non-Sparse Kernel Mixtures**\\nMarius Kloft, Ulf Brefeld, Soren Sonnenburg, Alexander Zien, Pavel Laskov, Klaus-Robert Mullernn**l1 regularization path for functional features**\\nManuel Loth, Philippe Preuxnn**Lq-regularised sparse classifiers: A PAC-Bayes analysis**\\nAta Kabannn**Robust Regression and Lasso**\\nHuan Xu, Constantine Caramanis, Shie Mannornn**Selection in Functional ANOVA Models with Non-uniform Data**\\nMarco Signoretto, Kristiaan Pelckmans, Johan A.K. Suykensnn**Sparse and Interpretable Principal Components**\\nDoyo Gragn, Nickolay T. Trendafilovnn**Sparse multiscale spatial models of fMRI on irregular graphs**\\nHarrison L., Woods W., Green G.nn**Stagewise Polytope Faces Pursuit for Recovery of Sparse Representations**\\nMark D. Plumbley, Marco Bevilacquann**Subspectral Algorithms for Sparse Learning**\\nBaback Moghaddam, Yair Weiss, Shai Avidannn**Variable Selection and Sparsity Recovery via L1/2 Penalty**\\nZongben Xu, Hai Zhang, Yao Wang, Xiangyu Chang
8444 en Multi-Task Learning via Matrix Regularization We present a method for learning representations shared across multiple tasks. Multi-task learning has become increasingly important recently in applications such as collaborative filtering, object detection, integration of databases, signal processing etc. Our method addresses the problem of learning a low-dimensional subspace on which task regression vectors lie. This non-convex problem can be relaxed as a trace (nuclear) norm regularization problem, which we solve with an alternating minimization algorithm. This algorithmic scheme can be shown to always converge to an optimal solution. Moreover, the method can easily be extended in order to use nonlinear feature maps as inputs via reproducing kernels. This is a consequence of optimality conditions known as representer theorems, for which we show a necessary and sufficient condition. Finally, we consider matrix regularization with more general spectral functions, such as the Schatten Lp norms, instead of the trace norm. We show that our algorithm and results apply in these cases as well.
8445 en Algorithmic Strategies for Non-convex Optimization in Sparse Learning We consider optimization formulations with non-convex regularization that are natural for learning sparse linear models. There are two approaches to this problem: 1. Heuristic methods such as gradient descent that only find a local minimum; a drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. 2. Convex relaxation such as L1-regularization that solves the problem under some conditions, but often leads to sub-optimal sparsity in reality.nnThis talk tries to remedy the above gap between theory and practice by presenting two algorithms for direct optimization of non-convex objectives in sparse learning, for which performance guarantees can be established. The first method is a multi-stage convex relaxation scheme that iteratively refines the L1-regularization. The second approach is a greedy procedure for solving non-convex sparse regularization. We show that under appropriate conditions, both procedures lead to desirable local solutions of sparse non-convex formulations that are superior to the global solution of L1-regularization.
8446 en High-Dimensional Non-Linear Variable Selection through Hierarchical Kernel Learning We consider the problem of high-dimensional non-linear variable selection for supervised learning. Our approach is based on performing linear selection among exponentially many well-defined groups of features or positive definite kernels, that characterize non-linear interactions between the original variables. To select efficiently from these many kernels, we use the natural hierarchical structure of the kernels to extend the multiple kernel learning framework to kernels that can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a graph-adapted sparsity-inducing norm, in polynomial time in the number of selected kernels. Moreover, we study the consistency of variable selection in high-dimensional settings, showing that under certain assumptions, our regularization framework allows a number of irrelevant variables which is sub-exponential in the number of observations.
8447 en Matching Pursuit Kernel Fisher Discriminant Analysis We consider the problem of high-dimensional non-linear variable selection for supervised learning. Our approach is based on performing linear selection among exponentially many well-defined groups of features or positive definite kernels, that characterize non-linear interactions between the original variables. To select efficiently from these many kernels, we use the natural hierarchical structure of the kernels to extend the multiple kernel learning framework to kernels that can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a graph-adapted sparsity-inducing norm, in polynomial time in the number of selected kernels. Moreover, we study the consistency of variable selection in high-dimensional settings, showing that under certain assumptions, our regularization framework allows a number of irrelevant variables which is sub-exponential in the number of observations.
8448 en Some results for the adaptive Lasso We consider the high-dimensional linear regression model, with n observations, and p > > n variables. The adaptive Lasso uses least squares loss with a weighted l_1-penalty, where the weights are proportional to the inverse of an initial estimator of the coefficients. We e.g. show for the case that the initial estimator is obtained from the standard Lasso, then, under the restricted eigenvalue condition as given in Bickel et al. (2007), with large probability, there will be no false positives, and adaptive Lasso will detect all coefficients larger than a certain value c_n, provided the number of coefficients smaller than c_n is small. If we assume a (perhaps) stronger version of the restricted eigenvalue condition, the adaptive Lasso will in fact detect even smaller coefficients. In the limiting case, under the irrepresentable condition (Zhao and Yu (2006)), coefficients of order at least (log (p)/n)^{1/2} will be detected. These results can be obtained from an oracle inequality for Lasso with general weights. We will present the results in an non-asymptotic form.
8450 en Latent Variable Sparse Bayesian Models A variety of practical approaches have recently been introduced for performing estimation and inference using linear models with sparse priors on the unknown coefficients, a process that can have wide-ranging implications in diverse areas such as model selection and compressive sensing. While not always derived or marketed as such, many of these methods can be viewed as arising from Bayesian models capitalizing on latent structure, expressible via hyperparameters, inherent in sparse distributions. Here we focus on four such strategies: (i) standard MAP estimation, (ii) hyperparameter MAP estimation, also called evidence maximization or empirical Bayes, (iii) variational Bayes using a factorial posterior, and (iv) local variational approximation using convex lower bounding. All of these approaches can be used to compute tractable posterior approximations to the underlying full distribution; however, the exact nature of these approximations is frequently unclear and so it is a challenging task to determine which strategy and sparse prior are appropriate. Rather than justifying such selections using the credibility of the full Bayesian model as is sometimes done, we base evaluations on the actual underlying cost functions that emerge from each method. To this end we discuss a common, unifying objective function that encompasses all of the above and then assess its properties with respect to representative applications such as finding maximally sparse (i.e., minimal L0 quasi-norm) representations. This objective function can be expressed in either coefficient space or hyperparameter space, a duality that facilitates direct comparisons between seemingly disparate approaches and naturally leads to theoretical insights and useful optimization strategies such as reweighted L1 and L2 minimization. This perspective also suggests extensions of the sparse linear model, including alternative likelihood functions (e.g., for classification) and more general sparse priors applicable to covariance component estimation, group selection, and the incorporation of explicit coefficient constraints (e.g., non-negativity). Several examples related to neuroimaging and compressive sensing will be considered.
8451 en Poster Spotlights 2 **A Space of Feature Selection Criteria based on Multivariate Mutual Information**\\ nGavin Brownnn**Alternatives to modelling sparsity in gene expression networks**\\nN.J. Burroughs, M.A. Juarez, E.R.Morrisseynn**Average-case theory for sparse Bayesian PCA in p > n regime**\\nMagnus Rattraynn**Inferring genetic regulation using Sparse Bayesian Regression with Penalised Splines**\\nNigel Burroughs, Miguel Juraez, Ed Morrisseynn**Learning Graphical Model Structure with Bayesian Sparse Linear Factor Models**\\nRicardo Henao, Ole Winthernn**Learning Sparse Classifiers with Applications to Biomedical Research**\\nColin Campbell, Yiming Yingnn**l1 Regularisation with Conditionally Positive Definite Kernels**\\nChristian Walder, Jalal Fadili, Stephane Canunn**Model Selection with Penalised Likelihood In a Multi-Dimensional Contingency Table**\\nSusana Conde, Gilbert MacKenzie, Peter Eggernn**Sparse Algorithms are not Stable: A No-free-lunch Theorem**\\nHuan Xu, Constantine Caramanis, Shie Mannor nn**Sparsity in Adaptive Control**\\ nPhilippe Preux, Sertan Girginnn**Speech signal modelling with sparse autoregression**\\ nMahesan Niranjannn**State Extraction by Dimensionality Reduction**\\ nWendelin Boehmer, Steffen Grunewalder, Klaus Obermayer**\\
8452 en Sparsity in online multitask/multiview learning Multitask and multiview learning is an interesting case for the study of interacting learning systems. In this talk we describe results in the multitask/multiview domain where co-regularization is achieved by imposing sparsity constraints of geometrical and spectral nature in an online learning framework.nnJoint work with Giovanni Cavallanti and Claudio Gentile.
8453 en Learning with Many Reproducing Kernel Hilbert Spaces In this talk, we consider the problem of learning a target function that belongs to the linear span of a large number of reproducing kernel Hilbert spaces. Such a problem arises naturally in many practice situations with the ANOVA, the additive model and multiple kernel learning as the most well known and important examples. We investigate approaches based on l1-type complexity regularization and the nonnegative garrote respectively. We show that the computation of both procedures can be done efficiently and the nonnegative garrote could be more favorable at times. We also study their theoretical properties from both variable selection and estimation perspective. We establish several probabilistic inequalities providing bounds on the excess risk and L2-error that depend on the sparsity of the problem. Part of the talk is based on joint work with Vladimir Koltchinskii.
8454 en Distilled Sensing: Active sensing for sparse recovery The study and use of sparse representations in data-rich applications has garnered signicantnattention in the signal processing, statistics, and machine learning communities. In the presentnwork we describe a novel sensing procedure called Distilled Sensing (DS), which is a sequential andnadaptive approach for recovering sparse signals in noise.nPassive sensing approaches, currently the most widespread data collection methods, involve non-nadaptive data collection procedures that are completely specied before any data is observed. Inncontrast, DS collects data in a sequential and adaptive manner. Often such procedures are knownnas active sensing or sequential experimental design, and allow the use of data observed in earliernstages to guide the collection of future data. The added nexibility of active sensing, together with ansparsity assumption, has the potential to enable extremely effcient and accurate inference.
8455 en Testing and estimation in a sparse normal means model, with connections to shape restricted inference Donoho and Jin (2004), following work of Ingster (1999), studied the problem of testing nfor any signal in a sparse normal means model and showed that there is a “detection boundary” nabove which the signal can be detected and below which no test has any power. They showed that nTukey’s “higher criticism” statistic achieves the detection boundary. I will introduce a new family nof test statistics based on phi-divergences indexed by s n? [?1, 2] which all achieve the Donoho-Jin- nIngster detection boundary. I will also review recent work on estimating the proportion of non-zero nmeans and make some connections to shape-constrained estimation.
8456 en Robotics and Mechatronics - From Space to Surgery and the Virtual World After briefly emphasizing the importance of mechatronics for our future societies, the talk briefly comments the development and evolvement of industrial robots over the past 20 years. It emphasizes the importance of mechatronic concepts and sensory feed-back for more precision and autonomy in the future. The progress and perspectives in space robotics are addressed next. Space technology is characterized as major driver for a new generation of power-saving ultralightweight arms and articulated hands - an important prerequisite for the emerging field of mobile production assistants and service robotics. nnThe technological potentials are demonstrated by DLR`s space robot experiments and the newest light weight arm and four-finger hand generation, which are fully joint-torque-controlled and thus are provided with programmable cartesian impedance - a feature which allows for new programming techniques and "human-friendly" operational modes. One of the most challenging application fields for these new technologies is surgical robotics; its state of the art and perspectives are briefly outlined.nnnHowever mechatronics is of crucial importance for artificial organs and prostheses, too. Finally the talk points out the importance of intelligent mobility - be it the development of rovers and crawlers on mars and moon, or the robotic electric cars of the future and the flying robots, which are capable of modeling the world photorealistically in 3D.
8459 en Why study insulators? Superconductors are sexier and semiconductors produce a billion $ per year in devices. So why should scientists study insulating materials? Firstly, most magnets are insulators; and second, all ferroelectrics (which switch charge in an applied voltage) are insulators. So phenomena that involve magnetism or ferroelectricity (piezoelectricity and pyroelectricity) are generally insulating. In the past few years the study of insulating materials has taken two new directions: The study of nano-devices, including sensors, actuators, and transducers. The first thing one discovers is that if you make an insulating material thin enough, it conducts quite well. And what are the conduction mechanisms: Poole-Frenkel, Schottky, Fowler-Nordheim tunneling, space-chage-limited? The second thing one finds is that ferroelectrics and ferromagnets have domains -- and the smaller the object, the smaller its domains. We have developed a theory of nano-domains that works in all magnets and ferroelectrics from 2 nm in size to 2 mm -- six orders of magnitude -- with no adjustable parameters. We also find domains that are round instead of rectangular and fractal instead of integer in dimension. n   Finally, I will talk about materials that are simultaneously magnetic and ferroelectric. Gilbert showed in 1600 that electrostatics and magnetism are unrelated -- but that isn't quite true. If time permits I will show some ferroelectric memories, including the ones in the SONY Playstation.
8465 en Lecture 2 - An Application of Supervised Learning - Autonomous Deriving An Application of Supervised Learning - Autonomous Deriving, ALVINN, Linear Regression, Gradient Descent, Batch Gradient Descent, Stochastic Gradient Descent (Incremental Descent), Matrix Derivative Notation for Deriving Normal Equations, Derivation of Normal Equations
8466 en Lecture 3 - The Concept of Underfitting and Overfitting The Concept of Underfitting and Overfitting, The Concept of Parametric Algorithms and Non-parametric Algorithms, Locally Weighted Regression, The Probabilistic Interpretation of Linear Regression, The motivation of Logistic Regression, Logistic Regression, Perceptron
8467 en Lecture 4 - Newton's Method Newton's Method, Exponential Family, Bernoulli Example, Gaussian Example, General Linear Models (GLMs), Multinomial Example, Softmax Regression
8473 en The World Wide Web and the Wealth of Nations: Does IT Matter? 
8496 en Lecture 5 - Discriminative Algorithms Discriminative Algorithms, Generative Algorithms, Gaussian Discriminant Analysis (GDA), GDA and Logistic Regression, Naive Bayes, Laplace Smoothing
8497 en Lecture 6 - Multinomial Event Model Multinomial Event Model, Non-linear Classifiers, Neural Network, Applications of Neural Network, Intuitions about Support Vector Machine (SVM), Notation for SVM, Functional and Geometric Margins
8498 en On Belief and Otherness Slavoj Zizek speaking about belief, the other, others radical otherness, respect for otherness, resistance, hatred, intolerance towards wisdom, totalitarian regimes, displacement, multitude and diversity, just action, fighting fascism, preserving humanity by killing the enemy, Alain Badiou, Judith Butler, including references to movies like Unbreakable with Bruce Willis and Shrek. Public open lecture for the students of the European Graduate School EGS, Media and Communication Studies department program, Saas-Fee, Switzerland, Europe, 2006, Slavoj Zizek.
8500 en Lecture 7 - Optimal Margin Classifier Optimal Margin Classifier, Lagrange Duality, Karush-Kuhn-Tucker (KKT) Conditions, SVM Dual, The Concept of Kernels
8501 en Lecture 8 - Kernels, Mercer's Theorem... Kernels, Mercer's Theorem, Non-linear Decision Boundaries and Soft Margin SVM, Coordinate Ascent Algorithm, The Sequential Minimization Optimization (SMO) Algorithm, Applications of SVM
8502 en Lecture 9 - Bias/variance Tradeoff Bias/variance Tradeoff, Empirical Risk Minimization (ERM), The Union Bound, Hoeffding Inequality, Uniform Convergence - The Case of Finite H, Sample Complexity Bound, Error Bound, Uniform Convergence Theorem & Corollary
8503 en Lecture 10 - Uniform Convergence - The Case of Infinite H Uniform Convergence - The Case of Infinite H, The Concept of 'Shatter' and VC Dimension, SVM Example, Model Selection, Cross Validation, Feature Selection
8504 en Lecture 11 - Bayesian Statistics and Regularization Bayesian Statistics and Regularization, Online Learning, Advice for Applying Machine Learning Algorithms, Debugging/fixing Learning Algorithms, Diagnostics for Bias & Variance, Optimization Algorithm Diagnostics, Diagnostic Example - Autonomous Helicopter, Error Analysis, Getting Started on a Learning Problemn
8505 en Lecture 12 - The Concept of Unsupervised Learning The Concept of Unsupervised Learning, K-means Clustering Algorithm, K-means Algorithm, Mixtures of Gaussians and the EM Algorithm, Jensen's Inequality, The EM Algorithm, Summaryn
8506 en Lecture 13 - Mixture of Gaussian Mixture of Gaussian, Mixture of Naive Bayes - Text clustering (EM Application), Factor Analysis, Restrictions on a Covariance Matrix, The Factor Analysis Model, EM for Factor Analysis
8507 en Lecture 14 - The Factor Analysis Model The Factor Analysis Model,0 EM for Factor Analysis, Principal Component Analysis (PCA), PCA as a Dimensionality Reduction Algorithm, Applications of PCA, Face Recognition by Using PCA
8508 en Lecture 15 - Latent Semantic Indexing (LSI) Latent Semantic Indexing (LSI), Singular Value Decomposition (SVD) Implementation, Independent Component Analysis (ICA), The Application of ICA, Cumulative Distribution Function (CDF), ICA Algorithm, The Applications of ICAn
8509 en Lecture 16 - Applications of Reinforcement Learning Applications of Reinforcement Learning, Markov Decision Process (MDP), Defining Value & Policy Functions, Value Function, Optimal Value Function, Value Iteration, Policy Iteration
8510 en Lecture 17 - Generalization to Continuous States Generalization to Continuous States, Discretization & Curse of Dimensionality, Models/Simulators, Fitted Value Iteration, Finding Optimal Policy
8511 en Lecture 18 - State-action Rewards State-action Rewards, Finite Horizon MDPs, The Concept of Dynamical Systems, Examples of Dynamical Models, Linear Quadratic Regulation (LQR), Linearizing a Non-Linear Model, Computing Rewards, Riccati Equation
8512 en Lecture 19 - Advice for Applying Machine Learning Advice for Applying Machine Learning, Debugging Reinforcement Learning (RL) Algorithm, Linear Quadratic Regularization (LQR), Differential Dynamic Programming (DDP), Kalman Filter & Linear Quadratic Gaussian (LQG), Predict/update Steps of Kalman Filter, Linear Quadratic Gaussian (LQG)
8513 en Lecture 20 - Partially Observable MDPs (POMDPs) Partially Observable MDPs (POMDPs), Policy Search, Reinforce Algorithm, Pegasus Algorithm, Pegasus Policy Search, Applications of Reinforcement Learning
8520 en Umetni?ka pokraina 
8524 en Enterprise Collaboration Services 
8525 en TCC plenary session 
8526 en SP3 & Semantic Web Services 
8527 en COIN Service Platform with hands-on 
8528 en EURIDICE IP presentation 
8529 en Enterprise Interoperability Services 
8530 en Web 20th Anniversary Panel As WWW2009 coincides with the 20th Anniversary of the Web conference organisers would like to include this panel to commemorate it. The initial idea is to provide a rich overview of the past 20 years and as well as forecast into the future. nnChair: Wendy Hall n
8531 en Reflecting on the last 20 years and looking forward to the next 20 
8533 en The Continuing Metamorphosis of the Web The invention of HTML and HTTP catalyzed a path of enormous innovation that was hard to foresee in the early 1990’s. The Web’s continuing metamorphosis has led to fantastically increased capabilities and economic value. It has catalyzed the creation of distributed systems orders of magnitude larger than any previously built, new programming and distribution models for computer applications, great advances in the fields of information retrieval, entirely new domains for theoretical computer science, and more. This greatly enhanced web is changing the entire environment and enabling some early research promises to become a reality for most Internet users. In this presentation, I will discuss such examples, and in particular, what happens when speech, image processing, human language translation, and mobility are woven into all we do. I will also extrapolate from some current research and advanced web technologies to paint a picture of the web five-to-ten years out. This should have implications for the computer science community, as well as the vast community that is leveraging the web for ever greater goals.
8534 en The Emergence of Web Science Since the term was coined in 2005, Web Science has provided a rallying call for researchers who are interested in the social and organisational behaviour engendered by the Web as about the underpinning technology. Web Science is inherently inter-disciplinary. Web Science research aims to provide the means to better model the Web’s structure, describe the principles that have fuelled its phenomenal growth, and discover how online human interactions are driven by and can change social conventions. Research is needed to reveal the principles that will ensure that the network continues to grow productively. Research is required to implement principles that can settle complex issues such as privacy protection and intellectual property rights. Of course, we cannot predict what this nascent discipline might reveal. But Web science has already generated powerful insights, how the Web is structured, how resilient it is, how ideas travel through the tens of millions of blogs, how we might include information in Web content so that its accuracy and origin is more transparent. At the micro scale, the Web is an infrastructure of artificial languages and protocols; it is a piece of engineering. However, it is fundamentally about the interaction of human beings creating, linking and consuming information. It is this interaction that we also need to research and understand. It is this interaction that generates the Web's behavior as emergent properties at the macro scale. These macro properties are often surprising and require analytic methods to understand them. The Web’s use is part of a wider system of human interaction – the Web has had profound effects on society, with each emerging wave creating both new challenges and new opportunities available to wider sectors of the population than ever before.nnThe Web Science Research Initiative (WSRI) was launched in November 2006 by Tim Berners-Lee, Wendy Hall, Nigel Shadbolt and Daniel Weitzner. At WWW2007 in Banff, WSRI sponsored a reception to present the ideas behind Web Science to the WWW community. At WWW2008, WSRI sponsored a workshop entitled “Understanding Web Evolution: A Prerequisite for Web Science” chaired by Dave De Roure. It attracted a lot of excellent papers – see http://webscience.org/events/www2008/ - and was one of the largest workshops at the conference. In March 2009, WSRI is running its first Web Science conference in Athens, WebSci’09 – see www.websci09.org - with the aim of bringing computer scientists and social scientists together to discuss this important topic.nnThe aim of this panel is to bring this debate to the heart of the WWW community at WWW2009 in Madrid.
8535 en Latent Space Domain Transfer between High Dimensional Overlapping Distributions Transferring knowledge from one domain to another is challenging due to a number of reasons. Since both conditional and marginal distribution of the training data and test data are non-identical, model trained in one domain, when directly applied to a diffeerent domain, is usually low in accuracy. For many applications with large feature sets, such as text document, sequence data, medical data, image data of different resolutions, etc. two domains usually do not contain exactly the same features, thus introducing large numbers of "missing values" when considered over the union of features from both domains. In other words, its marginal distributions are at most overlapping. In the same time, these problems are usually high dimensional, such as, several thousands of features. Thus, the combination of high dimensionality and missing values make the relationship in conditional probabilities between two domains hard to measure and model. To address these challenges, we propose a framework that first brings the marginal distributions of two domains closer by "filling up" those missing values of disjoint features. Afterwards, it looks for those comparable sub-structures in the "latent-space" as mapped from the expanded feature vector, where both marginal and conditional distribution are similar. With these sub-structures in latent space, the proposed approach then find common concepts that are transferable across domains with high probability. During prediction, unlabeled instances are treated as "queries", the mostly related labeled instances from out-domain are retrieved, and the classifcation is made by weighted voting using retrievd out-domain examples. We formally show that importing feature values across domains and latent semantic index can jointly make the distributions of two related domains easier to measure than in original feature space, the nearest neighbor method employed to retrieve related out domain examples is bounded in error when predicting in-domain examples.
8536 en Large Scale Online Bayesian Recommendations We present a probabilistic model for generating personalised recommendations of items to users of a web service. The system makes use of content information in the form of user and item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user. Users and items are represented by feature vectors which are mapped into a low-dimensional `trait space' in which similarity is measured in terms of inner products. The model can be trained from different types of feedback in order to learn user-item preferences. Here we present three alternatives: direct observation of an absolute rating each user gives to some items, observation of a binary preference (like/ don't like) and observation of a set of ordinal ratings on a user-specific scale. Efficient inference is achieved by approximate message passing involving a combination of Expectation Propagation (EP) and Variational Message Passing. We also include a dynamics model which allows an items popularity, a user's taste or a user's personal rating scale to drift over time. By using Assumed-Density Filtering (ADF) for training, the model requires only a single pass through the training data. This is an on-line learning algorithm capable of incrementally taking account of new data so the system can immediately reflect the latest user preferences. We evaluate the performance of the algorithm on the MovieLens and Netflix data sets consisting of ~1,000,000 and ~100,000,000 ratings respectively. This demonstrates that training the model using the on-line ADF approach yields state-of-the-art performance with the option of improving performance further if computational resources are available by performing multiple EP passes over the training data.
8537 en Learning Consensus Opinion: Mining Data from a Labeling Game In this paper, we consider the challenge of how to identify the consensus opinion of a set of users as to how the results for a query should be ranked. Once consensus rankings are identified for a set of queries, these rankings can serve for both evaluation and training of retrieval and learning systems. We present a novel approach to collecting user preferences over image-search results: we use a collaborative game in which players are rewarded for agreeing on which image result is best for a query. Our approach is distinct from other labeling games because we are able to elicit directly the preferences of interest with respect to image queries extracted from query logs. As a source of relevance judgments, this data provides a useful complement to click data. Furthermore, it is free of positional biases and does not carry the risk of frustrating users with non-relevant results associated with proposed mechanisms for debiasing clicks. We describe data collected over 35 days from a deployed version of this game that amounts to about 19 million expressed preferences between pairs. Finally, we present several approaches to modeling this data in order to extract the consensus rankings from the preferences and better sort the search results for targeted queries.
8538 en Rated Aspect Summarization of Short Comments Web 2.0 technologies have enabled more and more people to freely comment on different kinds of entities (e.g. sellers, products, services). The large scale of information poses the need and challenge of automatic summarization. In many cases, each of the user generated short comments comes with an overall rating. In this paper, we study the problem of generating a ``rated aspect summarization'' of short comments, which is a decomposed view of the overall ratings for the major aspects so that a user could gain different perspectives towards the target entity. We formally define the problem and decompose the solution into three steps. We demonstrate the effectiveness of our methods by using eBay sellers' feedback comments. We also quantitatively evaluate each step of our methods and study how human agree on such summarization task. The proposed methods are quite general and can be used to generate rated aspect summary given any collection of short comments each associated with an overall rating.
8540 en DBpedia - A Linked Data Hub and Data Source for Web Applications and Enterprises The DBpedia project provides Linked Data identifiers for currently 2.6 million things and serves a large knowledge base of structured information. DBpedia developed into the central interlinking hub for the Linking Open Data project, its URIs are used within named entity recognition services such as OpenCalais and annotation services such as Faviki, and the BBC started using DBpedia as their central semantic backbone. DBpedia's structured data serves as background information in the process interlinking datasets and provides a rich source of information for application developers. Beside making the DBpedia knowledge base available as linked data and RDF dumps, we offer a Lookup Service which can be used by applications to discover URIs for identifying concepts, and a SPARQL endpoint that can be retrieve data from the DBpedia knowledge base to be used in applications. This talk will give an introduction to DBpedia for web developers and an overview of DBpedia's development over the last year. We will demonstrate how DBpedia URIs are used for document annotation and how Web applications can via DBpedia facilitate Wikipedia as a source of structured knowledge.
8541 en A new tool to improve the filtering options in advanced searching We have developed a software application that analyzes in detail a text in English and labels the text with linguistic attributes plus additional information in a fully automatic way. With this new texts, a search engine is able to index the information in a way that provides new filtering possibilities for advanced searches.
8542 en Web infrastructure for the 21st Century The Web success in leading the information technology revolution has relied on an enormous computing infrastructure. In recent years, both cloud computing as well as social networks are putting even more burden in such infrastructure. The cloud computing paradigm is creating a massive shift in computing – from PC-based applications to cloud-based applications. Cloud computing frees users from having to remember where the data resides, gives users access to information anywhere, and provides fast services through essentially infinite online computing. Social networks, on the other hand, emerge the social aspects of the Web where the social interactions put demands on Web applications, and in turn further demands in the Web's infrastructure. nThe Internet, which was mostly developed for interactive applications between humans and computers, has struggled to handle the necessities of a Web designed around content. For instance, as Web content moves from one place to another, Web pointers are broken and so do search ranking algorithms. Similarly, content is often not where it should be when you need it and routers waste capacity copying the same content millions of times. As a result, we have seen the emergence of Internet systems that were not planned for from the beginning, e.g. Web Caching, Content Distribution Networks, or P2P networks.nIn this talk I will discuss the challenges that the Web is posing in today's Internet infrastructure, and argue about various solutions to cope with them. In particular, I will argue how to re-think the Internet to do networking at the content/information layer, and the underlying architectural system design principles needed to guide the efficient engineering of new Web infrastructure services.
8543 en Mining the Web 2.0 for Better Search There are several semantic sources that can be found in the Web that are either explicit, e.g. Wikipedia, or implicit, e.g. derived from Web usage data. Most of them are related to user generated content (UGC) or what is called today the Web 2.0. In this talk we show several applications of mining the wisdom of crowds behind UGC to improve search. We will show live demos to find relations in the Wikipedia or to improve image search as well as our current research in the topic. Our final goal is to produce a virtuous data feedback circuit to leverage the Web itself.
8544 en Mining the Web to Facilitate Fast and Accurate Approximate Match Tasks relying on recognizing entities have recently received significant attention in the literature. Many such tasks assume the existence of reference entity tables. In this paper, we consider the problem of determining whether a candidate string approximately matches with a reference entity. This problem is important for extracting named entities such as products or locations from a reference entity table, or matching entity entries across heterogenous sources. Prior approaches have relied on string-based similarity which only compare a candidate string and an entity it matches with. In this paper, we observe that considering such evidence across multiple documents significantly improves the accuracy of matching. We develop efficient techniques which exploit web search engines to facilitate approximate matching in the context of our proposed similarity functions. In an extensive experimental evaluation, we demonstrate the accuracy and efficiency of our techniques.
8545 en SmartMiner: A New Framework for Mining Large Scale Web Usage Data In this paper, we propose a novel framework called Smart- Miner for web usage mining problem which uses link information for producing accurate user sessions and frequent navigation patterns. Unlike the simple session concepts in the time and navigation based approaches, where sessions are sequences of web pages requested from the server or viewed in the browser, in Smart-Miner sessions are set of paths traversed in the web graph that corresponds to users' navigations among web pages. We have modeled session reconstruction as a new graph problem and utilized a new algorithm, Smart-SRA, to solve this problem efficiently. For the pattern discovery phase, we have developed an efficient version of the Apriori-All technique which uses the structure of web graph to increase the performance. From the experiments that we have performed on both real and simulated data, we have observed that Smart-Miner produces at least 30%more accurate web usage patterns than other approaches including previous session construction methods. We have also studied the effect of having the referrer information in the web server logs to show that different versions of Smart-SRA produce similar results. Another novel work is that we have implemented distributed version of the Smart Miner framework by employing Map-Reduce paradigm which enables processing huge size web server logs belonging to multiple web sites. To the best of our knowledge this paper is the first attempt to propose such large scale framework forweb usage mining problem. We conclude that we can efficiently process terabytes of web server logs belonging to multiple web sites by employing our scalable framework.
8546 en Releasing Search Queries and Clicks Privately The question of how to publish an anonymized search log was brought to the forefront by a well-intentioned, but privacy-unaware AOL search log release. Since then a series of ad-hoc techniques have been proposed in the literature, though none are known to be provably private. In this paper, we take a major step towards a solution: we show how queries, clicks and their associated perturbed counts can be published in a manner that rigorously preserves privacy. Our algorithm is decidedly simple to state, but non-trivial to analyze. On the opposite side of privacy is the question of whether the data we can safely publish is of any use. Our findings offer a glimmer of hope: we demonstrate that a non-negligible fraction of queries and clicks can indeed be safely published via a collection of experiments on a real search log. In addition, we select an application, keyword generation, and show that the keyword suggestions generated from the perturbed data resemble those generated from the original data.
8547 en Large Scale Integration of Senses for the Semantic Web Nowadays, the increasing amount of semantic data available on the Web leads to a new stage in the potential of Semantic Web applications. However, it also introduces new issues due to the heterogeneity of the available semantic resources. One of the most remarkable is redundancy, that is, the excess of different semantic descriptions, coming from different sources, to describe the same intended meaning.nIn this paper, we propose a technique to perform a large scale integration of senses (expressed as ontology terms), in order to cluster the most similar ones, when indexing large amounts of online semantic information. It can dramatically reduce the redundancy problem on the current Semantic Web. In order to make this objective feasible, we have studied the adaptability and scalability of our previous work on sense integration, to be translated to the much larger scenario of the Semantic Web. Our evaluation shows a good behaviour of these techniques when used in large scale experiments, then making feasible the proposed approach.
8548 en Triplify - Light-weight Linked Data Publication from Relational Databases We present Triplify - a simplistic but effective approach to publish linked data from relational databases. Triplify is based on mapping HTTP-URI requests onto relational database queries. Triplify transforms the resulting relations into RDF statements and publishes the data on the Web in various RDF serializations, in particular as Linked Data. The rationale for developing Triplify is that the largest part of information on the Web is already stored in structured form, often as data contained in relational databases but published by Web applications merely as HTML mixing structure, layout and content. In order to reveal the pure structured information behind the current Web we implemented Triplify as a light-weight software component, which can be easily integrated and deployed with the numerous widely installed Web applications. Our approach includes a method for publishing update logs to enable incremental crawling of linked data sources. Triplify is complemented by a library of configurations for common relational schemata and a REST-enabled datasource registry. Triplify configurations are provided containing mappings for many popular Web applications, including Wordpress, Drupal, Joomla, Gallery, and phpBB. We show that despite its light-weight architecture Triplify is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project.
8549 en SOFIE: Self-Organizing Flexible Information Extraction This paper presents SOFIE, a system that can extend an existing ontology by new facts. SOFIE provides a integrative framework, in which information extraction, word disambiguation and semantic reasoning all become part of one unifying model. SOFIE processes text or Web sources and finds meaningful patterns. It maps the words in the pattern to entities in the ontology. It hypothesizes on the meaning of the pattern, and checks the semantic plausibility of the hypothesis with the existing ontology. Then the new fact is added to the ontology, avoiding inconsistency with the existing facts. The logical model that connects existing facts, new hypotheses, extraction patterns, and consistency constraints is represented as a set of propositional clauses. We use an approximation algorithm for the Weighted MAX SAT problem to compute the most plausible subset of hypotheses. Thereby, the SOFIE framework integrates the paradigms of pattern matching, entity disambiguation, and ontological reasoning into one unified model, and enables the automated growth of large ontologies. Experiments, using the YAGO ontology as existing knowledge and various text and Web corpora as input sources, show that our method yields very good precision around 90 percent or higher.
8550 en Emergent Semantics of Social Tagging Social bookmarking systems are becoming increasingly important data sources for bootstrapping and maintaining Semantic Web applications. Their emergent information structures have become known as folksonomies. A key question for harvesting semantics from these systems is how to extend and adapt traditional notions of similarity to folksonomies, and which measures are best suited for applications such as community detection, navigation support, semantic search, user profiling and ontology learning. Here we build an evaluation framework to compare various general folksonomy-based similarity measures, which are derived from several established information-theoretic, statistical, and practical measures. Our framework deals generally and symmetrically with users, tags, and resources. For evaluation purposes we focus on similarity between tags and between resources and consider different methods to aggregate annotations across users. After comparing the ability of several tag similarity measures to predict user-created tag relations, we provide an external grounding by user-validated semantic proxies based on WordNet and the Open Directory Project. We also investigate the issue of scalability. We find that mutual information with distributional micro-aggregation across users yields the highest accuracy, but is not scalable; per-user projection with collaborative aggregation provides the best scalable approach via incremental computations. The results are consistent across resource and tag similarity.
8551 en Measuring the Similarity between Implicit Semantic Relations from the Web Measuring the similarity between semantic relations that hold among entities is an important and necessary step in various Web related tasks such as relation extraction, information retrieval and analogy detection. For example, consider the case in which a person knows a pair of entities (e.g. Google, YouTube), between which a particular relation holds (e.g. acquisition). The person is interested in retrieving other_such pairs with similar relations (e.g. Microsoft, Powerset). Existing keyword-based search engines cannot be applied directly in this case because, in keyword-based search, the goal is to retrieve documents that are relevant to the words used in a query -- not necessarily to the relations implied by a pair of words. We propose a relational similarity measure, using a Web search engine, to compute the similarity between semantic relations implied by two pairs of words. Our method has three components: representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns, clustering the extracted lexical patterns to identify the different patterns that express a particular semantic relation, and measuring the similarity between semantic relations using a metric learning approach. We evaluate the proposed method in two tasks: classifying semantic relations between named entities, and solving word-analogy questions. The proposed method outperforms all baselines in a relation classification task with a statistically significant average precision score of 0.74. Moreover, it reduces the time take by Latent Relational Analysis to process 374 word-analogy questions from 9 days to less than 6 hours, with a SAT score of 51%.
8552 en Extracting Key Terms From Noisy and Multitheme Documents We present a novel method for key term extraction from text documents. In our method, document is modeled as a graph of semantic relationships between terms of that document. We exploit the following remarkable feature of the graph: the terms related to the main topics of the document tend to bunch up into densely interconnected subgraphs or communities, while non- important terms fall into weakly intercon-nected communities, or even become isolated vertices. We apply graph community detection techniques to partition the graph into thematically cohesive groups of terms. We introduce a criterion function to select groups that contain key terms discarding groups with unimportant terms. To weight terms and determine semantic relatedness between them we exploit information extracted from Wikipedia. Using such an approach gives us the following two advantages. First, it allows effectively processing multi-theme documents. Second, it is good at filtering out noise information in the document, such as, for example, navigational bars or headers in web pages. Evaluations of the method show that it outperforms existing methods producing key terms with higher precision and recall. Additional experiments on web pages prove that our method appears to be substantially more e ective on noisy and multi- theme documents than existing methods.
8553 en WWW2009 - Closing Ceremony 
8554 en TP1 - Leveraging Complex Knowledge 
8560 en TechCrunch - Dealing with the Media TechCrunch was founded on June 11, 2005, as a weblog dedicated to obsessively profiling and reviewing new Internet products and companies. In addition to covering new companies, we profile existing companies that are making an impact (commercial and/or cultural) on the new web space.nnTechCrunch has now grown into a network of technology focused sites offering a wide range of content and new media.
8561 en 1st Pre-Kick off NGO meeting From the email:nnHere we would seek to synchronize with other similar initiatives on thenworld scale in terms of goals, strategies and standards, to setup meansnfor content sharing and to exchange opinions, lessons learned and thusnenforcing the sustainability and integrability of video lectures to thenglobal open education initiatives.nnAs you already know, we are making first steps in establishing a formalnnon-profit organisation (with the support of European Commission in thenframe of EU funded project Pascal2) and we would like to use thisnopportunity to invite interested parties to become founding members ofnan NGO.nnThe meeting agenda:nn*Welcome - Mitja Jermoln*Introduction – John Shawe Taylorn*MIT OpenCourseWare presentation – Cecilia d'Oliveiran*Opencast presentation – Mara Hancockn*OpenCourseWare Consortium - Meena Hwang (Skype)n*OpenCourseWare Consortium presentation – Larry Coopermann*VideoLectures.Net technology view - Peter Ke?en*Universität Osnabrück ETH Zürich - Markus Ketterl (Skype)n*University of Cambridge / Global Grid for Learning / Steeple presentation - Bjoern Hasslern*ETH Zürich presentation - Olaf A. Schulten*Vision, Ideas, Finances presentation – Mitja Jermol
8577 en Introduction to Semantic Search 2009 In recent years we have witnessed tremendous interest and substantial economic exploitation of search technologies, both at web and enterprise scale. However, the representation of user queries and resource content in existing search appliances is still almost exclusively achieved by simple syntax?based descriptions of the resource content and the information need such as in the predominant keyword-centric paradigm (i.e. keyword queries matched against bag?of?words document representation).nnOn the other hand, recent advances in the field of semantic technologies have resulted in tools and standards that allow for the articulation of domain knowledge in a formal manner at a high level of expressivity. At the same time, semantic repositories and reasoning engines have only now advanced to a state where querying and processing of this knowledge can scale to realistic IR scenarios.nnIn parallel to these developments, in the past years we have also seen the emergence of important results in adapting ideas from IR to the problem of search in RDF/OWL data, folksonomies, microformat collections or semantically tagged natural text. Common to these scenarios is that the search is focused not on a document collection, but on metadata (which may be possibly linked to or embedded in textual information). Search and ranking in metadata stores is another key topic addressed by the workshop.nnAs such, semantic technologies are now in a state to provide significant contributions to IR problems.nIn this context, several challenges arise for Semantic Search systems. These include, among others:nn  * How can semantic technologies be exploited to capture the information need of the user?n  * How can the information need of the user be translated to expressive formal queries without enforcing the user to be capable of handling the difficult query syntax?n  * How can expressive resource descriptions be extracted (acquired) from documents (users)?n  * How can expressive resource descriptions be stored and queried efficiently on a large scale?n  * How can vague information needs and incomplete resource descriptions be handled?n  * How can semantic search systems be evaluated and compared with standard IR systems?n
8578 en Correlator: things we did, things we should do, and things we don't know how to Correlator (http://sandbox.yahoo.com/Correlator) is a demo showcasing work developed at Yahoo! Research Barcelona in the areas of information extraction, retrieval and visualization. I will use this and other Yahoo! demos during my talk to discuss some of the technologies used, to evaluate its strengths and weaknesses, and to pinpoint some of the research problems which I find most interesting in this area.
8579 en Investigating the Demand Side of Semantic Search through Query Log Analysis In this paper, we propose a method to create aggregated representations of the information needs of Web users when searching for particular types of objects. We suggest this method as a way to investigate the gap between what Web search users are expecting to ¯nd and the kind of information that is provided by Semantic Web datasets formatted according to a particular ontology. We evaluate our method qualitatively by measuring its power as a query completion mechanism. Last, we perform a qualitative evaluation comparing the information Web users search for with the information available in Dbpedia, the structured data representation of Wikipedia.
8580 en Semantic Search for Enterprise 2.0 In this paper, we describe how Enterprise 2.0 can benefit from lightweight semantics for information integration, enabling a better and easier search process for the end-users.
8581 en Improving search results with lightweight semantic search: discussion paper The goal of each search service is to yield the most relevant results on a given query. Traditional full-text search is not enough and many approaches to improve search rankings are adopted. In this paper we propose a method of combined search query scoring computation leveraging lightweight semantics represented by metadata related to searchable content. It extends state-of-the-art approaches at both indexing and searching stage. We discuss two approaches of so-called concept scoring computation in order to capture different properties of available metadata.
8582 en Wanderlust: Extracting Semantic Relations from Natural Language Text Using Dependency Grammar Patterns A great share of applications in modern information technology can benet from large coverage, machine accessible knowledge bases. However, the bigger part of todays knowledge is provided in the form of unstructured data, mostly plain text. As an initial step to exploit such data, we presentnWanderlust, an algorithm that automatically extracts semantic relations from natural language text. The procedure uses deep linguistic patterns that are dened over the dependency grammar of sentences. Due to its linguistic nature, the method performs in an unsupervised fashion and is notnrestricted to any specic type of semantic relation. The applicability of the proposed approach is examined in a case study, in which it is put to the task of generating a semantic wiki from the English Wikipedia corpus. We present an exhaustive discussion about the insights obtained from thisnparticular case study including considerations about the generality of the approach.
8583 en Towards ECSSE: live Web of Data search and integration We illustrate the works toward implementing an Entity Centric Semantic Search Engine (ECSSE). ECSSE leverages the Sindice Semantic Web Index to find and combine together semantically structured data published on the web. With respect to previous Semantic Web Data integrators, ECSSE, uses an holistic approach in which large scale semantic web indexing, logic reasoning, data aggregation heuristics, ad hoc ontology consolidation, external services and user interaction all play together to create rich entity descriptions and live, embeddable data mash ups.
8584 en Story Link Detection with Entity Resolution News archives present a vast base of cultural and social knowledge. However, their size is also the cause for difficult navigation through the sequence of articles, belonging to a certain topic thread. In the ideal scenario, one could navigate over the whole sequence of articles, where every article would link to other relevant articles, discussing the same event. Continuing progress in entity resolution and extraction has enabled the possibility to apply semantic background knowledge to the task of story link detection (SLD), adding additional information to existing article text and annotations. In this paper, we propose a method of extracted entity resolution to measure its effect on performance the task of topic link detection. We developed a system which extracts additional entities from article text and links them to entities from our background knowledge base. Current experiments of this ongoing work show that although entity resolution via text similarity outperforms using plain text in the case of story link detection, it only achieves SLD performance comparable to human annotations in some cases.
8585 en Retrieval and Ranking of Semantic Entities for Enterprise Knowledge Management Tasks We describe a task-sensitive approach to retrieval and ranking of semantic entities, using the domain information available in an enterprise. Our approach utilizes noisy named-entity tagging and document classification, on top of an enterprise search engine, to provide input to a novel ranking metric for each entity retrieved for a task. Retrieval is query-centric, where the user query is the target topic (e.g., a technology needed for a proposal). Named entities are then extracted from the retrieved documents, and ranked according to their similarity to the target topic. We evaluate our approach by comparing to a baseline retrieval and ranking technique that is based on entity occurrence rates, and show encouraging results.
8586 en P2P Concept Search: Some Preliminary Results Concept Search extends syntactic search, i.e., search based on the computation of string similarity between words, with semantic search, i.e., search based on the computation of semantic relations between complex concepts. It allows us to deal with ambiguity of natural language. P2P Concept Search extends Concept Search by allowing distributed semantic search over structured P2P network. The key idea is to exploit distributed background knowledge and indices.
8587 en Question Answering Based on Semantic Graphs In this paper we present a question answering system supported by semantic graphs. Aside from providing answers to natural language questions, the system offers explanations for these answers via a visual representation of documents, their associated list of facts described by subject – verb – object triplets, and their summaries. The triplets, automatically extracted from the Penn Treebank parse tree obtained for each sentence in the document collection, can be searched, and we have implemented a question answering system to serve as a natural language interface to this search. The vocabulary of questions is general because it is not limited to a specific domain, however the questions's grammatical structure is restricted to a predetermined template because our system can understand only a limited number of question types. The answers are retrieved from the set of facts, and they are supported by sentences and their corresponding document. The document overview, comprising the semantic representation of the document generated in the form of a semantic graph, the list of facts it contains and its automatically derived summary, offers an explanation to each answer. The extracted triplets are further refined by assigning the corresponding co referenced named entity, by resolving pronominal anaphors, as well as attaching the associated WordNet synset. The semantic graph belonging to the document is developed based on the enhanced triplets while the document summary is automatically generated from the semantic description of the document and the extracted facts.
8588 en Relevance Feedback Between Hypertext and Semantic Search Relevance feedback is one method for creating a ‘virtuous cycle’ - as put by Baeza-Yates - between semantics and search. Previous approaches to search have generally considered the SemanticnWeb and hypertext Web search to be entirely disparate, indexing and searching over different domains. While relevance feedback have traditionally improved information retrieval performance, relevance feedback is normally used to improve rankings of a single data-set. Our novel approach is to use relevance feedback from hypertext Web search to improve the retrieval of Semantic Web data.nWe also inspect whether relevance feedback from Semantic Web data can improve hypertext Web search results. In both cases, an evaluation based on certain kinds of informational queries (abstractnconcepts, people, and places) selected from a query log and human judges show that relevance feedback works: relevance feedback from hypertext Web search can improve the retrieval of Semantic Web data, and vice versa. We evaluate our work over a wide range of algorithms, and show it improves baseline performance on these queries for deployed systems as well, such as the semantic Search engine FALCON-S and the commercial Web search engine Yahoo!nsearch.
8589 en Managing Collaboration Projects using Semantic Email 
8590 en Using TREC for cross-comparison between classic IR and ontology-based search models at a Web scale The construction of standard datasets and benchmarks to evaluate ontology-based search approaches and to compare then against baseline IR models is a major open problem in the semantic technologiesncommunity. In this paper we propose a novel evaluation benchmark for ontology-based IR models based on an adaptation of the well-known Cranfield paradigm (Cleverdon, 1967) traditionally used by the IR community. The proposed benchmark comprises: 1) a text document collection, 2) a set of queries and their corresponding document relevance judgments and 3) a set of ontologies and Knowledge Bases covering the query topics. The document collection and the set of queries and judgments are taken from one of the most widely used datasets in the IR community,nthe TREC Web track. As a use case example we apply the proposed benchmark to compare a real ontology-based search model (Fernandez, et al., 2008) against the best IR systems of TREC 9 and TREC 2001 competitions. A deep analysis of the strengths and weaknesses of this benchmark and a discussion of how it can be used to evaluate other ontology-based search systems is also included at the end of the paper.
8591 en Searching and ranking in RDF documents and social networks As semantic web based applications are gaining popularity,nvery large RDF documents are becoming common. SPARQLnis the de-facto standard in querying RDF data and researchnon efficient implementations of SPARQL interfaces for verynlarge RDF graphs has attracted a great deal of interest innthe recent years. However, in large datasets, the user facesnthe problem that the result set for her queries can be large.nIn this situation there is no clear for the user, from wherento start looking at the results, since all of them are equallynvalid. Moreover, given the result of a SPARQL query, thenonly possible order is lexicographical which doesn’t help thenuser to distinguish which of the returned values should shenlook first. In this sense, it would be desirable to have a notionnof “relevance” of nodes. A related problem is that ofnanalyzing social network data. Most social network analysisnconcentrates heavily on finding social groups and findingnthe importance of individuals in a social network. However,nthis work generally considers the social network as a graphnwith a single type of connection, edges representing the existencenof social communication or friendship for example.nThere are not many methods developed for social networksnwith many different types of semantic connections. As anresult, there is very little work on querying of semanticallynrich social network data.
8592 en Discussion and Closing 
8593 en Query Log Mining Web Search Engines have stored in their logs information about users since they started to operate. This information often serves many purposes. The primary focus of this tutorial is to introduce to the discipline of query mining by showing its foundations and by analyzing the basic algorithms and techniques that could be used to extract and to exploit useful knowledge from this (potentially) infinite source of information. Furthermore, participants to this tutorial will be given a unified view on the literature on query log analysis.
8597 en 1. Galactic Arms Race (GAR): Automatic Content Generation In a Multiplayer Online Video Game This video showcases a new AI algorithm called cgNEAT thatnautomatically generates content in video games. To demonstrate thisnnew technique, we created a near-commercial-quality game callednGalactic Arms Race (GAR) in which the weapons systems are entirelyninvented by the game itself. The cgNEAT method, which is short forncontent-generating NeuroEvolution of Augmenting Topologies, evolvesnnew weapons (which are controlled by artificial neural networks) bynvarying the most popular weapons of the past. In this way, annevolutionary process causes the algorithm to explore the space ofnweapons as the game is played, producing a never-ending supply ofnnovel and functional content. The aim is to show that AI can bensophisticated enough to produce some of the content in games withoutnthe need for artists or programmers, by observing what players likednin the past. The video presents a montage of actual gameplay thatndemonstrates the surprising variety of compelling weapons invented bynthe game itself. It also explicates the underlying AI technology morenthrough action than through words. For more information on GAR, whichnwill be released this Spring, please visit: http://gar.eecs.ucf.edu.n
8598 en 2. EDGE: Enhanced Device for Geospatial Exploration This video is a presentation of a robotics simulation project. Microsoft Robotics Project was used for the development of this project. The video shows the robot/agent in the simulation environment, looking for hostages, takes pictures of the hostage and locates their positions. The video shows how the robot scans the area and avoids walls using its laser. The video shows the screens of the software with a nice rock song! and a brief description of the methods and algorithms used by the robot.
8599 en 3. Learning Kinematic Models of Articulated Objects Robots operating in home environments must be able to interact with articulated objects such as doors or drawers. Ideally, robots are able to autonomously infer articulation models by observation. In this video, we briefly present an approach for learning kinematic models by inferring the connectivity of rigid parts and the articulation models for the corresponding links. Our method uses a mixture of parameterized and parameter-free (Gaussian process) representations and finds low-dimensional manifolds that provide the best explanation of the given observations. Our approach has been implemented and evaluated using real data obtained in various realistic home environment settings. Corresponding paper: http://www.informatik.uni-freiburg.de/~sturm/media/sturm09ijcai.pdf
8600 en 4. The Autonomous City Explorer This video presents the Autonomous City Explorer (ACE) project. Its goal was to create a robot capable of navigating in an unknown urban environments without the use of prior map knowledge or GPS data. The robot had to find its way solely by interacting with pedestrians and building a topological representation of its surroundings. This video outlines the necessary ingredients for successful low-level navigation on sidewalks, vision processing in cluttered outdoor environments, and information retrieval from pedestrians. More information can be found at [[http://www.ace-robot.de]].
8601 en 5. Applying Case-Based Reasoning to Texas Hold'em Poker This video summarizes the use of Case-Based Reasoning principles to the area of of a Texas Hold'em poker. Introductions to CBR and Texas Hold'em are included as well as a description about two poker-bots which we have developed that make use of CBR to play poker. Results and conclusions of our research are briefly presented. For more information you can visit our website: [[http://www.cs.auckland.ac.nz/research/gameai]].
8603 en 7. Motion Synthesis and Control Learning for (Un)Knotting Deformable Linear Objects This research deals with motion planning and control for Deformable Linear Objects (DLOs). It is still a complex task to get a robot manipulate a rope or cloth. To realise this vision of getting a robot to handle dexterous objects, we have taken the simplest object i.e. a DLO for our study purpose. The operations performed on the DLO are knot-(un)tying. The DLO is thus parameterised as a Knot and we make the DLO (un)tied into various knot types. The mathematical branch of Knot Theory is extensively used here to realise the Knots. The configuration space that the Knot can move is computed by the Knot Energy. We use the Minimum Distance Knot energy here. With this, we create a hierarchical graph structure with nodes corresponding to optimal knot configurations obtained by optimising this Knot energy functional. Thus by navigating this graph, we are able to (un)tie various knots. The study looks into 3 simple and 2 complex knot types. Motion control while (un)tying is also brought about using the SARSA(Ã?Â») [Reinforcement Learning] algorithm. The motion planner is resilient to perturbations as well. Thus by devising Knot Energy together with SARSA(Ã?Â»), we have built a multi-scale, reactive knot (un)tying motion planner. Results show that our method is incrediblynfaster than normal Probabilistic and feedback control methods. For more, please refer to my MSc thesis titled 'Multi-scale, Reactive Motion Planning with Deformable Linear Objects' at [[http://www.inf.ed.ac.uk/publications/thesis/online/IM080596.pdf]] or for the complete version including Motion control at [[http://www.mediafire.com/file/dozd2zm3mam/MSc]] thesis - SARSA and Knot energy version.pdf.
8604 en 8. AIspace: Tools for Learning Artificial Intelligence This video summarizes our work on a set of interactive algorithm visualization tools for teaching and learning AI. The tools cover many of the topics that would be in an intro AI course. They are freely available online at [[http://aispace.org]]. They were developed at the Laboratory for Computational Intelligence at the University of British Columbia.
8605 en 9. The Dinochrome Brigade This video provides a short introduction to our technology for robot localization, and then shows how this technology enables various robotic tasks. Brief demonstrations are shown for moving robot formations, uniform coverage of a region, chain formations, and collaborative pulling. The video is meant to highlight our accomplishments, but is not a tutorial. More information can be found at: [[http://www.cs.uwyo.edu/~wspears/maxelbot]] and [[http://www.cs.uwyo.edu/~wspears/pubs.html]].
8607 en 10. Multi-Camera People Tracking Presented by a Humanoid A humanoid gives a talk, like a human presenter, about a real-time vision system using multiple cameras for people tracking in a room. The humanoid demonstrates a few perception/control capabilities including visually guided hand/arm control. It briefly describes the algorithm used and shows some people tracking results including each person's trajectory and walking direction, and reports the number of people in the room. The vision system with two quad-core 3.0 Ghz CPUs runs in real-time. In the end of the video, a game application based on person tracking is presented.n
8608 en 11. Copycat Hand for All "Copycat Hand for All" is a robot system that imitates the human motions, by visually estimating the human hand and arm postures at a high speed and with high accuracy. The system uses only one high-speed camera and note PC. But once you stand in front of the robot and move your hand and arm freely, it reproduces your behavior without time delay. At the first stage, our system uses coarse screening by the proportional information on the hand images which roughly correspond to forearm rotation, bending of the thumb or four fingers. And then, at the second stage, it performs a detailed search for the selected candidates. The estimation error is less than one degree in the joint angle, and the processing time is 80 fps or more. We are now enhancing flexibility for those having thick/thin, long/short, or deeply hooking fingers, through using different typical hand CG images featuring different bone length and thickness and joint movable ranges in order to permit the system to respond accurately to people of all ages and both sexes, including foreign people. For more information, please see our lab's page at [[http://www.kz.tsukuba.ac.jp/~hoshino/AI/copycat.pdf]].n
8610 en 13. Toward Interactive Learning of Container and Non-Container Objects This video highlights our work toward creating robots that interactively learn about container and non-container objects. It demonstrates that a robot can distinguish between them by dropping a block in the area above the object and observing the co-movement patterns while pushing the object. Also, the video illustrates that the robot can learn a perceptual model of containers to identify novel containers in the environment. Check out the lab's webpage atn[[http://www.ece.iastate.edu/~alexs/lab/]] or the first author's webpage at [[http://www.ece.iastate.edu/~shaneg/research.html]] for more information.
8611 en 14. A Plan-Based Machinima Creation System This video describes our plan-based machinima creation system and concludes with an example film created with it.n
8612 en 15. How to Cook as Perfect Love Story This movie is about the Taaable project ([[http://taaable.fr]]), a Web-based CBR Cooking system (also a participant to the Computer Cooking Contest). The movie illustrates on a real situation how a CBR system can help a human to solve a cooking problem and how it learns in case of failure. The main topics addressed by the movie are: case-based reasoning and interactive learning.
8613 en 16. CogSketch: Open-Domain Sketch Understanding for Research and Education This video describes CogSketch, a sketch understanding system being developed for research and educational purposes. CogSketch automatically generates symbolic representations of sketches. These representations can be used as the input to reasoning systems. The video demonstrates two educational applications of CogSketch: Worksheets, which can provide students with automatic feedback by comparing their sketches to teacher sketches and identifying differences; and the Design Buddy, which uses qualitative mechanics to analyze engineering design sketches.
8614 en 17. Bodies In Motion: Dynamic Motion Capture We explore the use of full-body 3D physical simulation for human kinematic tracking from monocular and multi-view video sequences within the Bayesian filtering framework. Towards greater physical plausibility, we consider a human's motion to be generated by a "feedback control loop", where Newtonian physics approximates the rigid-body motion dynamics of the human and the environment through the application and integration of forces. The result is more faithful modeling of human-environment interactions, such as ground contacts, resulting from collisions and the human's motor control. For more information, please see: [[http://robotics.cs.brown.edu/projects/dynamical_tracking/]].
8615 en 18. Situated Interaction This video provides an overview of the Situated Interaction project at Microsoft Research, an effort towards developing open-world interactive systems that can embed interaction and computation deeply into the natural flow of daily tasks, activities and collaborations. The video highlights project goals and reviews a prototype platform which weaves together several component technologies, including learning and inference about activities and goals, speech recognition and synthesis, vision, conversational scene analysis, and multiparty dialog management, to support fluid interactions with multiple users in an open-world context. The video illustrates several challenges and competencies with an applicationnrunning on the platform, named Receptionist, that handles problems in the domain of building receptionists. More information about the project and related research are available at [[http://research.microsoft.com/en-us/um/people/dbohus/research_situated_interaction.html]].
8618 en 21. Reinforcement Learning by Example A brief introduction to reinforcement learning, using the task of bartending as an example.n
8619 en 22. Real Live Robot Learning We created a reinforcement-learning demo -- a simple robot navigation task -- and took it to the public to teach them about AI and robotics. The video shows the system adapting in real time to various modifications to the robot's design and provides a very gentle introduction to the idea of model-based reinforcement learning.
8620 en 23. Using Entropy to Distinguish Shape versus Text in Hand-Drawn Diagrams Most sketch recognition systems are accurate in recognizing either text or shape (graphic) ink strokes,but not both. Distinguishing between shape and text strokes is, therefore, a critical task in recognizing hand-drawn digital ink diagrams that contain text labels and annotations. We have found the entropy rate to be an accurate criterion of classification. We found that the entropy rate is significantly higher for text strokes compared to shape strokes and can serve as a distinguishing factor between the two. The paper has been accepted for publication in the 2009 IJCAI proceedings. Thisnvideo shows our system in action. Please visit our lab here -> [[http://srlweb.cs.tamu.edu/srlng/home]].
8621 en 24. News at Seven: The Future of the Future This video summarizes News at Seven, an automated news system. The system is able to find relevant text, process that text, and supplement it with images, video, and blogger responses. The final output of the system is an online Flash presentation that uses animated avatars with generated speech and is modeled after traditional nightly news broadcast. Current segments include a Movie Review, an Entertainment Segment, and a Louis Black style rant segment. More information about News at Seven can be found at [[http://newsatseven.com]]. News at Seven is also currently live at [[http://www.zap2it.com/news/zap-news-at-7,0,6717570.htmlstory]].
8622 en 25. Penso BCI System This video reveals how Brain Computer Interfaces work in basic terms by demonstrating a BCI system called Penso. By providing a less formal nutshell view of the basic principles of a BCI system in a language accessible to a general audience, this 2-minute video has sparked interest and enthusiasm in prospective postgraduate students in the BCI area. For more information, please see our team's page at [[http://www.weg.ee.usyd.edu.au/projects/penso]].
8623 en 26. Write. Reflect. Polish. This video introduces a writing support too called "Glosser" that leverages language processing and text analysis techniques to analyze and provide feedback to students as they write an essay. Although the video was designed to welcome engineering students to a tool they would be using in their first year, it also ignited student interest in natural language processing and related AI technologies, by showing how certain technologies can be applied in the real world. For more information, please see our team's page at [[http://www.weg.ee.usyd.edu.au/projects/glosser]].
8625 en 27. Robots to the Rescue: Mixed-initiative human-robot teaming for disaster response The humanoid robot Nexi and a team of robot helicopters are deployed in response to a simulated fire aboard a Navy ship. Working together with a remote human operator, the robots search for survivors and guide them to safety. A mixed-initiative tasking system allows the human operator to specify team goals via a tasking interface, and also allows goals to be generated by the robots as a result of local observations and interactions with victims. The robots autonomously handle the details of navigation and task execution, and communicate their location, task status, and important observations to the operator via the interface. The interface also allows the robots to ask for the operator's help with difficult recognition problems, such as confirming the location of a victim from a partial identification.
8626 en 28. Improving Offensive Performance Through Opponent Modeling Although in theory opponent modeling can be useful in any adversarial domain, in practice it is both difficult to do accurately and to use effectively to improve game play. In this video, we present an approach for online opponent modeling and illustrate how it can be used to improve offensive performance in the Rush 2008 football game. In football, team behaviors have an observable spatio-temporal structure, defined by the relative physical positions of team members over time; we demonstrate that this structure can be exploited to recognize football plays at a very early stage of the play using a supervised learning method. Based on the teams' play history, our system evaluates the competitive advantage of executing a play switch based on the potential of other plays to increase the yardage gained and the similarity of the candidate plays to the current play. In this video, we investigate two types of play switches: 1) whole team and 2) subgroup. Both types of play switches improve offensive performance, but modifying the behavior of only a key subgroup of offensive players yields greater improvements in yardage gained.
8627 en 31. Playbook: a new approach to tasking interfaces Playbook is a system developed at Smart Information Flow Technologies that allows the delegation of complex tasks from humans to multiple unmanned systems. Through this method, an operator can declare high level instructions that are understandable by unmanned aerial vehicles, allowing them to automatically calculate most efficient means to completing their goal, dramatically simplifying the operator's workload. More information at www.sift.info.
8629 en 33. Conversational Virtual Role Players This video showcases a technology called Virtual Role Players (VRP) for implementing conversational virtual humans that engage in spoken dialog, and exhibit culturally appropriate behavior. It is designed for use as a plug-in to training simulation systems. The version displayed is integrated with the Virtual Battlespace 2 (VBS2) mission rehearsal environment.
8630 en 34. The Stanford Autonomous Helicopter Stanford's Autonomous Helicopter project pushes the limits of autonomous flight control by teaching a computer to fly a competition-class remote controlled (RC) helicopter through a range of aerobatic stunts. Our apprenticeship learning approach learns to fly the helicopter by observing human demonstrations and is capable of a wide variety of expert maneuvers. In many cases, it can even exceed the performance of the human expert from which it learned. http://heli.stanford.edu.
8631 en 35. The Intelligent "Dynamic" Workbook for Learning Written East Asian Languages Our video summarizes one of our recent projects regarding sketch recognition for the domain of written East Asian languages such as Chinese and Japanese. Our research focuses on developing computer-assisted language instruction (CALI) systems which provide human instructor-level assessment of students' written East Asian writings, both for visual structure and written technique. That is, we strive to provide an intelligent "dynamic" workbook for supplementing current East Asian language programs, in order to alleviate the difficulties of learning written East Asian.
8632 en 36. Little Robot Goes Missing In this video we demonstrate how robots are able to find their location on a map. The video is based on the project work of 4th year students of the University of Alberta.
8633 en 37. Robotic Secrets Revealed, Episode 001 Using Three-Cup Shuffle Magic Trick as a backdrop, we present a system which performs head, body pose, and fiducial-based object tracking all in 3D as well as hand and head gesture recognition and production. The sensor born information and the capabilities are tightly integrated in a psychologically plausible manner within an embodied version of the ACT-R cognitive architecture, ACT-R/E. The Mobile-Dextrous-Social (MDS) Robot interprets and uses gestures, including deictic and symbolic hand gestures as well as head nods and shakes. The overall approach provides us with a powerful, integrated system facilitating many different avenues of research in human-robot interaction.
8634 en 29. WiiGesture WiiGesture is a gesture recognition program for actions that use accelerometer data. It uses artificial intelligence to classify gestures using a wiimote from a few examples of each gesture. This was a project for a Machine Learning class. Many algorithms were tried, like LCSS, Bagged Trees, SVMs, and Fast Fourier Transforms and the video highlights the one we found most useful (Cross Correlation). This was a really fun project that has real world applications in the video game industry, and I hope it encourages students to consider studying Artificial Intelligence.
8635 en 30. Real-Life Reinforcement Learning A lot of the research in the field of reinforcement learning has been focused/tested on simulation domains. Some of the characteristics of real-life domains, such as the shape of their noise function or imperfect perception that results in violation of Markovianness are typically neglected in the simulation. In this video, we promote performing reinforcement learning research on real problems to make sure algorithms are more robust to these design imperfections.
8650 en 39. Casey's Quest: Transfer Learning for Adversarial Environments A dramatization of Transfer Learning research through the journey of an 8-bit football player. Based on research conducted by David Aha, Matthew Molineaux, and Gita Sukthankar for ICCBR'09. More information is available at [[http://www.knexusresearch.com/projects/rush]].
8652 en Welcome statements given by the representatives 
8654 en Regions Drive the New Industrial Revolution 
8655 en European Research and Technology Organisations and the Challenges of Sustainability and Competitiveness 
8656 en Promoting Applied Research, Development and Innovation - Croatian Perspectives 
8657 en Regional Cooperation for Sustainable Knowledge Societies in SEE 
8658 en Presentation and the Role of the “Technology and Business Innovation Center for Mariculture !” of the University of Dubrovnik in the Croatian Aquaculture Industry 
8659 en Contribution on Intellectual Property Promotion and Technology Innovation Management in South East Europe 
8660 en Governance and Benchmarking of RTOs 
8661 en STP project: Business Plan and Inovation Strategy Development in a Public Research Institute 
8662 en Human Capital Building in R&D - the Key to Enhanced Economic Competitiveness of SEE Economies 
8663 en Interregional Innovation Policy in South East Europe: Past Experiences and Prospects for the Future 
8664 en Sustainability of Cross-Border RTD and Innovation Cooperation Projects and Networks 
8665 en Technology Parks and Technology Transfer - Making Full Use of Cross-Border Opportunities 
8666 en National innovation system- BICROs view 
8667 en Business perspectives 
8668 en Instruments of Promoting Research Cooperation in South Eastern Europe 
8669 en The Global Crisis: An Opportunity for New RTD Strategies in Environmental and Energy Policies 
8670 en Collaboration in Developement of Solar Cells – Materials and Device 
8671 en YEAR: Initiating the R&D co-operation of Young Researchers through Training and Networking 
8673 en What happened in the first decade of the 21st century Zizek’s visit is organised by the Department for Social Critique, a group of social scholars who publish an Albanian-language magazine called “Critique and Society.”nnAgon Hamza, co-founder of the Department for Social Critique, rated Zizek as one of the “most important living philosophers”. “I believe his lectures will spark a debate and will lead to a different perspective of our existing socio-political life,” Hamza told Balkan Insight.nnZizek, who is widely regarded as a fiery and colourful scholar never reluctant to make controversial remarks, is seen as a Marxist philosopher and is one of the “most wanted” contemporary lecturers.nnDescribed by other scholars as the “most formidably brilliant” recent theorist to have emerged from Europe, Zizek’s work is infamously idiosyncratic. His work includes striking dialectical reversals of received common sense, and sheds an alternative view on social and political events.
8674 en Ideology between Symptom and Fetish Zizek’s visit is organised by the Department for Social Critique, a group of social scholars who publish an Albanian-language magazine called “Critique and Society.”nnAgon Hamza, co-founder of the Department for Social Critique, rated Zizek as one of the “most important living philosophers”. “I believe his lectures will spark a debate and will lead to a different perspective of our existing socio-political life,” Hamza told Balkan Insight.nnZizek, who is widely regarded as a fiery and colourful scholar never reluctant to make controversial remarks, is seen as a Marxist philosopher and is one of the “most wanted” contemporary lecturers.nnDescribed by other scholars as the “most formidably brilliant” recent theorist to have emerged from Europe, Zizek’s work is infamously idiosyncratic. His work includes striking dialectical reversals of received common sense, and sheds an alternative view on social and political events.
8676 en Opening of the conference 
8677 en FP7 project PETAMEDIA 
8678 en Multimedia Search in the perspective of Future Internet Services 
8679 en FP6 project PHAROS 
8680 en Chorus in the landscape of European effort in MMSE 
8681 en FP6 project VITALAS 
8682 en Introduction of Conference Objectives 
8683 en FP7 project LIVINGKNOWLEDGE 
8684 en Prospective in MMSE Research 
8685 en The EU AMI and AMIDA projects: Recognition and Understanding of Meetings and Lectures 
8686 en Towards Web-scale content search: the SAPIR approach 
8687 en THESEUS – Advanced Services for MMSE 
8688 en Chorus Findings 
8689 en A review of first preliminary results from Quaero 
8690 en Search for Mobile 2.0 
8691 en Chorus Roadmap and recommendations 
8692 en Industrial trends for Internet search 
8693 en The Future of Web Search 
8694 en Introduction, and brief overview of CHORUS findings and recommendation regarding user-centric SE design 
8695 en Enterprise search trends and challenges 
8696 en CHORUS studies outcome on use-case typology dimensions 
8697 en MOVIMOS - a scalable, distributed multimedia search engine for mobile applications 
8698 en VideoCLEF: evaluation of moving image retrieval 
8699 en "Chorus Vision: Outcome of the Think-Tank" 
8700 en User scenarios and user requirements from media professionals 
8701 en Chorus: Status and Challenges of MMSE Technology 
8702 en ImageCLEF LS-VCDT: Evaluation of multilabel image annotation incorporating domain knowledge and concept subjectivity 
8703 en "3D Scene Structure Analysis for Semantic Annotation and Retrieval of Unedited Video" 
8704 en Closure 
8705 en ERC project SECO 
8706 en Panel closing session: Disruptive Technologies and Services in the near Future 
8823 en Introduction of Best Poster 
8824 en Emergence of Cooperation in Agricultural Production 
8825 en Large Events on the Stock Market: A Study of High Resolution Data 
8826 en How to Quantify the Influence of Correlations on Investment Diversification When assets are correlated, benefits of investment diversification are reduced. To measure the influence of correlations on investment performance, a new quantity - the effective portfolio size - is proposed and investigated in both artificial and real situations. We show that in most cases, the effective portfolio size is much smaller than the actual number of assets in the portfolio and that it lowers even further during financial crises.
8827 en Assessing the Critical Factors that Determine the Availability of Wood Fuel in Switzerland with an Agent Based Model 
8828 en Best Poster Award 
8858 en Welcome and introduction to HeavyRoute09 
8859 en Welcome and introduction to HeavyRoute09 
8860 en Welcome and introduction to HeavyRoute09 
8861 en Welcome and introduction to HeavyRoute09 
8862 en Presentations of project results - HeavyRoute09 
8863 en Vehicle/infrastructure interaction models 
8865 en The Heavyroute cost database 
8866 en Panel discussion on the requirements on the models and data used for the HR applications in order to have a system that can be used European-wide 
8867 en Pre-trip routing application 
8868 en Driver support/warning - black spots application 
8869 en Driver support/warning - roll-over application 
8870 en Bridge managment application 
8871 en Driving simulator and field tests 
8872 en Traffic simulations and CBA 
8873 en Panel discussion on the applications developed by HeavyRoute 
8874 en Results from field and simulator tests 
8875 en Conclusions: Heavyroute 09 Brussels 
8878 en Poster session promo clip 
8885 en Opening of ICE 2008 
8886 en About this years ICE 2008 
8887 en Welcome to ICE 2008 
8888 en Web 2.0 and Collaborative Working Environments: What can we learn? Web 2.0 applications are becoming more and more widespread, not only for leisure and entertainment, but also for business purposes. As developers of collaborative work environments, we need to ask ourselves, if we consider this wave of new ideas and applications as a hazard that sweeps us away or if we can learn to surf this wave. In my presentation I’ll first distill Web 2.0 applications and technologies to identify the basic concepts. Then I’ll discuss how these concepts can be used for the design and development of cooperative work applications.
8889 en Value Proposition for Enterprise Interoperability in Manufacturing Value Chains Manufacturing, generating wealth and jobs by fully exploiting knowledge and resources, is the fundamental enabler and sustainer of Europe’s Competitive and Sustainable Development. Manufacturing in Europe provides presently 41,5 % of the added-value (over €1,535 million) and 30,4 % of the employment (34 million people), with each job at the factory floor generating two other jobs in services. Manufacturing in Europe is presently under threat and in crucial and urgent need to add value and decrease costs by embedding design and technology, as to compensate for the fierce competition from the emerging economies. The need to keep manufacturing operations and jobs in Europe calls for industry transformation, as to ensure strong cost reductions, increased flexibility and smaller response times while keeping high standards in product quality with increasing complex novel products. But the ability to manage increased product complexity, by spreading subcontracting and outsourcing, cannot be achieved without building collaborative networks over the complete supply and value chain. Enterprise interoperability is thus a stringent requirement in highly competitive manufacturing value chains. And it is also an emergent one if we want to be able to design and deploy a competitively sustainable European Production System, through the use of disruptive technological processes enabled by digital production.
8892 en Surface plasmons meet organic optoelectronics Surface plasmons are optical modes at the interface of a metal and a dielectric which are important in a variety of fields as nanooptics, surface enhanced spectroscopy, metamaterials and photonic devices. Recent years have brought substantial progress in controlling surface plasmons with micro- and nanostructures forming waveguides, mirrors, splitters or resonators. To complement this toolbox of passive plasmonic elements by dynamic or active devices, organic semiconductor devices have proven efficient. On one hand,norganic light emitting diodes can be applied for direct surface plasmon excitation. On the other hand, surface plasmon detection can be based on integrated organic diodes.
8893 en Tracking primary photoinduced events in biomolecules with few-optical-cycle light pulses Many light-induced processes in organic molecules, such as energy relaxation, energy/charge transfer and conformational changes, occur on ultrafast timescales, ranging from 10-14 to 10-12 s. The speed of such elementary processes is intimately linked to their efficiency, making ultrafast optical spectroscopy an invaluable tool for their investigation. Pump-probe spectroscopy requires both short pulses, in order to observe fast dynamics, and broad frequency tunability, to excite a system on resonance and probe optical transitions occurring at different frequencies. Optical parametric amplifiers (OPAs) are ideal tools for such experiments, because they provide frequency tunability and support broad gain bandwidths, enabling the generation of very short pulses. In this talk I will describe a state of the art system, based on two synchronized OPAs, providing sub-20-fs temporal resolution over a very broad spectral range, from 400 nm to 1.5 mm. After reviewing the pulse generation techniques and the system performance, I will present selected examples of applications, such as: energy transfer in photosynthetic light harvesting complexes, electronic and vibrational dynamics in carbon nanotubes, isomerization of rhodopsin.
8913 en The contributions of Presence and Ergonomics to the Digital Factory Presence and Ergonomics are key elements in the development of virtual and augmented reality (VR/AR) systems and environments. This paper describes the contributions of these human factors to the digital factory within the DiFac (FP6-2005-IST-5-035079) European Commission funded research project. For Presence, the main contribution is the construction and validation of a new questionnaire called Flow for Presence Questionnaire. This paper presents the concept of Presence, describes the development of the questionnaire and discusses the initial findings. For ergonomics the focus is on placing end-users at the core of the development process. Industrial partners have been involved from the beginning of DiFac to ensure that the final solution meets their requirements. This approach should also increase their acceptance of the final solutions by increasing their sense of ownership. However, in recognition of the constraints on end-users time, the development process has also included expert evaluations of the technologies.
8914 en Approach and development of an innovative tool for integration of immersive devices in virtual manufacturing environments: Immersive Integrator 
8930 en Pozdravni nagovor 
8931 en The Norwegian experience The Norwegian Ambassador, Ms. Guro Vikør, spoke about the UN resolution 1325 on Women, Peace and Security and the Action Plan based on the resolution 1325 before Prof. Dr. Jelu?i? talked about Slovenian efforts regarding peace building and gender equality. She promised the audience that the Republic of Slovenia will adopt the Action Plan 1325 into their politics. She was very impressed over what Norway and Sweden in specific have accomplished both regarding gender equality in their home countries and peace building where it has been needed.
8932 en Teoreti?na in prakti?na izhodi??a s podro?ja izgradnje miru, medkulturnega dialoga in enakosti spolov 
8933 en Theoretical and practical views for Peace Building, Interreligious Dialogue and Gender Equality The former prime minister of Norway, Mr. Kjell Magne Bondevik, who also is the founder and president of the Oslo Center for Peace and Human Rights, elaborated about what and how Norway have and can do regarding peace building and gender equality. Norway with other Scandinavian countries have been the first to take gender equality seriously into politics and they have adopted the Action Plan 1325.
8934 en Woman's institution as woman's role in peacemaking and peacebuilding on the world 
8935 en Interreligious dialogue in Building Peace 
8936 en Obstacles to Gender Equality Emerita Prof. Dr. Maca Jogan held a short presentation of some of her research regarding obstacles to gender equality and the Swedish Ambassador Ms. Inger Ultvedt shared with the attendants the Swedish experience in adopting the Action Plan 1325. The female lieutenant Tonja Delopst ended the round table perfectly by telling about her own experience of being one of the soldiers in the ISAF operations in Afghanistan.
8937 en Swedish Experience in Adopting Action Plan 1325 [UN S/RES 1325 (2000) on Women, Peace and Security] 
8938 en Slovenian Experience in Peacebuilding Operations 
8939 en Female Soldiers in the Field 
8940 en Personal experience from ISAF 
8942 en Closing and slovenian plans 
8968 en Opening and setting the scene During the workshop, co-innovation and approaches for managing and fostering innovation will be revisited and discussed both from a individual, firm and ?inter-firm? perspective. Critical reviews of the new paradigm from a theoretical perspective will be complemented by discussion of empirical findings and case studies. Interactive discussions will provide insights into key challenges and barriers of co-creation and co-innovation. Participants are invited to exchange experiences during the discussion and to define future research.
8970 en Key dimensions of renewable capabilities 
8971 en Attitudes and behaviours fostering creative deliverables Creative products such as books, films, music and design objects are an increasingly significant sector of modern advanced economies. The processes required to develop these new products are often complex, as they require creative individuals to work with businesses (that are often seeking a commercial return on any investment). Traditionally, research has focussed on the processes and review points that could improve the likelihood of success of new creative products. This paper argues that an alternative theoretical perspective may benefit participants and researchers understand the process of innovation: Value exchange theory. This theory considers equivalent exchange to underpin (voluntary) transactional interactions between parties. Data collected from participants involved in four creative innovations (two books and two pieces of public art) are presented. This data suggests transactions can be reciprocated, however that there are also a number of significant non-reciprocated transactions. The paper concludes that researchers should consider the possibility of the gift if they are to understand the process of innovation in the creative industries.
8972 en Open innovation – Opening towards open innovation Open innovation is claimed to be the new breed of innovation requiring enterprises to look beyond the boundaries of their organisation and to use external and internal actors and knowledge to successfully create value. In related business management literature, shifts from closed to open models are argued to be triggered by new technological, economic and social trends so that many organisations (at least partially) virtualise, their members being distributed across different locations and embedded in various socio-economic and cultural contexts. Complementing that line of argument, the current paper undertakes socio-systems-theoretical revisits to key institutional premises assumed in concepts above organisation, knowledge, collaboration, complexity etc. - to examine, explore and restate in which sense 'openness' may be feasible in organisational realm and with what respects 'open innovation' may be supported, accordingly. It is argued that, rather than requiring to tear down organisational walls 'outwards' (which in principle is not possible), 'open innovation' enables, and is enabled by, differentiating organisations 'inwards' to better turn 'irritations' feared into chances recognised to leverage upon.
8973 en The networked SME – What is the role of openness in superior innovation management? 
8974 en Cooperation coaching: A novel Approach to empower SMEs to innovate 
8975 en 2020 Vision: SME Challenges for Collaborative Innovation for Product Service Organisation 
8976 en Different EU initiatives and Projects for SMEs in clusters to target Innovation 
8977 en Towards Adaptive Interenterprise Systems Interenterprise operability appears to be of high complexity with regard to technology, organisation, and application. It poses many challenges to supporting ICT solutions, frequently resumed under the term interoperability. A number of application issues have been pointed out in the Enterprise Interoperability Research Roadmap of the European Commission. The optimal organisational fit of ICT systems becomes a basic issue, as enterprise interoperability is always linked with business processes. And technological issues, in the end, put forward the question of software engineering, i.e. how to address the specifics of interenterprise processes by appropriate systems engineering approaches. This paper reflects on these issues and suggests to combine the concept of component based software engineering and the software product line approach, as a means to facilitate the engineering of adaptive interenterprise systems. We argue that such combination offers the potential of supporting interenterprise operability at a reasonable level of complexity, while keeping resulting systems adaptive and evolutionary.
8978 en Enterprise Interoperability: Towards a Framework for Enterprise Applications Deployment in Emerging Economy SMEs Emerging economies are considered to provide the future growth opportunities for the ICT industry. With regard to enterprise applications, the envisaged growth will essentially be driven by SMEs as they represent the largest proportion of the untapped market. In order to capture this future growth market, suppliers of enterprise applications systems have to consider deployment strategies that address long tail demand patterns as the comparatively small scale of emerging economy SMEs present fundamental discrepancy from industrialised economy cases. This paper presents a theoretical analysis of the fundamentals of this discrepancy in order to establish a basis for a preliminary framework for deploying enterprise applications in a manner that accommodates long tail demand patterns. The proposed framework seeks to overcome the reliance of commercially successful deployment strategies such as COTS ERP on formalised business process models.
8979 en The Semantic Enterprise - Bringing Meaning to Business Processes This lecture presents an answer on the demand for more interoperability of data, systems and thus organisations. PROCESSUS as a particular project of the German national funded high-tech-initiative THESEUS has the objective to create an IT-based corporate system that will allow companies to compare products, solutions and details of business associates, as well as locating the complex and sometimes obscure specialist information needed by employees whose work involves high-density knowledge bases. The research teams are also aiming to develop a basic semantic platform that will integrate a companys internal planning of resources with management of the digital content of agile business processes. One specific scenario taken from the domain of mechanical engineering will demonstrate the requirements of intra- and intererenterprise communication and the envisaged solution.
8980 en Ambient Intelligence Technologies for Industrial Working Environments in Manufacturing SMEs 
8981 en Collaborative Environment for Virtual Collaborative Networks of ELV Recycling SMEs 
8982 en Intelligent Networked Devices for Enabling Proactive Collaboration with Customers 
8983 en Network-centric Middleware supporting dynamic Web Service Deployment on heterogeneous Embedded Systems 
8993 en Scaffolding innovations –Implications of regional innovation barriers for platform-based innovation management improvement This lecture presents different typical SME innovation profiles found within a sample of 87 Central Swiss companies. For each profile the main barriers of innovation are depicted. Subsequently, an innovation management platform is depicted, which is available since 2007. Bringing profiles and the characteristics of the platform together, some crucial implications for innovation coaches will be pointed out. This paper bases on results of the qualitative research project Innovation intense at Lucerne School of Business. The objective of the project was to integrate, complement and expand insights that have been created by several previous research projects of the Lucerne School of Business on innovation management in Central Swiss SMEs.
8994 en Experiences in virtual Enterprise Networking in Switzerland 
8995 en New challenges in Manufacturing 
9040 en Prihodnost medijev - okrogla miza 
9105 en Multi-Objective Design Exploration (MODE) - Visualization and Mapping of Design Space Multi-Objective Design Exploration (MODE) and its application are presented. MODE reveals the structure of the design space from the trade-off information and visualizes it as a panorama for Decision Maker. Self-Organizing Map is incorporated into MODE as a visual data mining tool for the design space. The resulting MODE was applied to the multi-disciplinary wing design problem and revealed the design sweet spot and the detailed trade-off information about aerodynamic and structural performance successfully.
9106 en Self-Organizing Maps to Enhance Local Performance of Multi Objective Opimization This work focuses on a new approach to enhance the results obtained by a multi objective ptimization, especially tailored for computationally hard CAE models. The methodology combines Self-Organising Maps and Response Surfaces with evolutionary multi-objective optimization algorithms. The challenge is to improve the results spending only few extra computations.n  First, an optimization with an evolutionary algorithm explores the wide range of the possible solutions. Then, Self-Organising Maps has been used to detect local correlations, this way circumscribing the scope of the search in the design parameter space. After the definition of new bounds for the input variables, a new Design of Experiment was performed on the CFD model, and then interpolated with Response Surface Modeling techniques. Then, a virtual optimization has been applied to these meta-models, and the most interesting virtual designs were validated by means of new CFD simulations.n  Beyond the improvements with regard to the initial design, the methodology guaranteed a reduction of the computational resources needed to obtain such results, compared to a full direct optimization. Self-Organizing Maps allowed for selecting the most promising area for the objectives and gained new insights to the relations between input and output variables.
9107 en Learning and Prediction - A Survey This survey considers the design of methods for learning mathematical models from data. The contemporary toolbox of machine learning consists of a wide set of techniques, essentially reducing to a few formal arguments. The aim of this presentation is then to give insight in some of them, and to argue how they could be implemented successfully. When doing so, we will touch topics as risk-based modeling, probabilistic inference, convex optimization and kernel-based learning amongst others. I will exemplify two application areas, namely (A) identification of dynamic systems, and (B) modeling and prediction of reliability data.
9108 en Knowledge Extraction from Aerodynamic Design Data and its Application to 3D Turbine Blade Geometries Applying numerical optimisation methods in the field of aerodynamic design optimisation normally leads to a huge amount of heterogeneous design data. While often only the most promising results are investigated and used to drive further optimisations, general methods for investigating the entire design data set are rare. It is our target to extract comprehensive knowledge from the design data concerning the interrelation between the shape and the performance of the design. The extracted knowledge is prepared in a way that it is usable for guiding further computational as well as manual design and optimisation processes.n  For the design of complex aerodynamic shapes it is common to use different kinds of representations, what makes it difficult or even impossible to analyse the entire design data set. We suggest the transformation of the design data into discrete unstructured surface meshes and hence result in a homogeneous parametrisation of the designs. This makes it possible to analyse the design data independent of the representation used during the design and optimization process.n  On the basis of discrete unstructured surface meshes we propose a displacement measure in order to analyse local differences between designs1. The measure provides information on the amount and the direction of surface modifications. We recently introduced a framework2 that uses the displacement data in conjunction with statistical methods and techniques from machine learning to provide meaningful knowledge from the dataset at hand. The framework comprises a number of approaches for the displacement analysis, sensitivity analysis, dimensionality reduction and rule extraction.n  In order to demonstrate the feasibility of the suggested framework, we applied the proposed methods to a data set of a ultra-low aspect ratio transonic turbine stator blade of a small Honda turbofan engine that resulted from a computational optimisation run3. Decision trees have been formulated to generate a set of design rules which refer to a pre-defined blade design.n  The results have been verified by means of modifying the turbine blade geometry using direct manipulation of free form deformation (DMFFD)4 techniques. The performance of the deformed blade design has been calculated by running computational fluid dynamic (CFD) simulations. It is shown that the suggested framework provides reasonable results which can directly be transformed into design modifications in order to guide the design process.
9109 en Surrogate Assisted Optimization Methods: Recent Developments and Challenges There is an ever increasing trend in the use of more and more accurate and often more computationally expensive analyses tools in early stages of design. Optimization using such computationally expensive analyses tools demand the use of surrogate assisted methods, where a surrogate or an approximation is used in lieu of the expensive analysis to contain the computational time within affordable limits. The performance of such methods is known to be largely dependent on the choice of the underlying optimization algorithm, the surrogate model, the training and surrogate model management schemes.n  This presentation will introduce a surrogate assisted optimization framework developed by the MDO Group at UNSW@ADFA which alleviates some of the common problems associated with the current approaches. In the proposed approach, surrogates of multiple types (MLP, RBF, Kriging and RSM) coexist within the optimization framework at all times and the surrogate with the least prediction error (based on neighborhood RMSE) is used to approximate the objective and the constraint functions individually. An external archive of all solutions evaluated via actual analysis is maintained to train the surrogates, while a surrogate validity check is performed prior to its use to avoid misguiding the search (in the event of poor approximation or attempts to approximate in unexplored regions). The underlying optimization algorithm is a population based, elitist evolutionary algorithm which explicitly maintains marginally infeasible solutions for a faster rate of convergence. Apart from standard recombination schemes used in any evolutionary algorithm, a memetic recombination operator is embedded to further improve the rate of convergence. A number of examples will be presented to illustrate the performance of the proposed schemes.n  Finally, the presentation will list areas of further development and present some preliminary results of constrained many objective optimization and spatial approximation schemes that are currently being developed by the group.n
9111 en Surrogate-Based Optimization at ONERA: Some Recent Examples With the development of computational resources and the increasing complexity of industrial needs, stohastic optimization strategies, and especially those based on evolutionary concepts, have had a growing success among the community in the recent years. Indeed, they offer global-focusing minimization opportunities to potentially complex problems (e.g. with multiple minima over non-connected search spaces of (dis)continuous state variables) without relying on the computation of the objective function´s gradient, and as such remain almost completely independent on the physical nature of the problem to be treated (analyzer and optimizer are two distinct and autonomous processes to be interfaced with one another, hence the usual "black-box˝ denomination).n  Yet, the search of an optimum based on the entire range of possible solutions (where, formally, nothing guarantees the absolute global character of the result) is usually penalized by its important computational cost, which can increase exponentially with the complexity of the problem (Belmann´s curse of dimensionality) and become rapidly prohibitive (especially when one deals with precise aerodynamic evaluations on large configurations). This remark partly explains the popularity of surrogate-based optimization procedures, where the expensive analyzer is replaced by a low-fidelity but cheap model on which the optimizer browses.n  ONERA has been active on the field of surrogate optimization for some time now, with topics ranging from surrogate modeling itself (RBF/ANN, (Co)Kriging, high order RSM...) to efficient coupling and optimization (sampling methods, refinement criteria, on- or off-line implementation...), for a wide variety of applications (single or multi-objective performances of multidisciplinary problems). It is the aim of this presentation to review some of the results obtained throughout some of the most recent projects, such as the optimization of flow control parameters for novel high-lift design.
9112 en Multi-Objective Design Exploration (MODE) - Aerodynamic Applications at Tohoku University With the development of CFD techniques and design optimization methods, aerodynamic optimizations have been widely used in various aircraft designs. Particularly, the recent progress in surrogate-based aerodynamic optimization techniques enables efficient search over the wide range of design space with high-fidelity time-consuming CFD. The benefit of surrogate-based modeling is not only the reduction of number of expensive CFD but also the spread of sampling points that cover the whole design space. This enables to conduct various statistical analyses for data-mining purpose that helps to understand design problems in detail for further improvement. In this presentation, recent applications of Multi-Objective Design Exploration to new aircraft designs will be presented at the workshop.
9114 en On the Use of Supervised Learning Techniques to Speed up the Design of Aeronautics Components A crucial issue in the design of complex systems is the evaluation of a large number of potential alternative designs. A too expensive evaluation procedure can consequently slow down the search for good configurations mainly in the case of high dimensional parameter spaces. The talk will discuss the use of machine learning techniques for speeding up the evaluation and the exploration of large design spaces. In particular, two supervised learning techniques, feedforward neural networks and lazy learning, are assessed and compared in the task of accelerating the design of a heat-pipe, a cooling ndevice commonly used in aeronautics and electronics.
9115 en Identification of Eigenmodes in Vibration Data Vibration is the response of a system to an internal or external stimulus causing it to oscillate. Vibration causes dynamic stress if the system is excited at the same frequency as the so called Eigenmodes and this can damage the system [2]. Thus, the identication of Eigenmodes in vibration data is an important issue in the aerospace industry, e.g. jet engines need to be certied before going into service and any dangerous vibration has to be detected. This data is usually analyzed manually, since this a time consuming process, machine learning can be applied in order to support engineers in their work. The vibration data is usually visualised as 2D images (campbell plots) and the Eigenmodes are displayed as lines.n  We introduce an iterative algorithm using background knowledge for the identication of Eigenmodes. Our algorithms extends the original Hough Transform [3, 1], an image processing algorithm used for detection of lines and other parametrisable shapes. Finally we show in our evaluation that our approach for identifying Eigenmodes, applied on a data set provided by a major European jet engine manufacturer, outperforms the prediction of the Finite Element Model and is competitive to the base model using lab measurements.
9116 en Differentiable and Quasi-Differentiable Methods for Optimal Shape Design in Aerospace Optimal shape design can be approached either as an unknown boundary problems as done for most problems of fluid dynamics or as an unknown domain problem as done in structural mechanics for topological optimization. We shall present both methods together with some applications in aerospace. Problems are discretized by the finite element method; differentiable optimization is used when possible and pseudo differentiable methods for topological optimization. n  Shape optimization is usually computer intensive and parallel computing is a necessity. While evolutionary methods have an edge, gradient methods can be parallelized by domain decomposition just as well. n  But sensitivity evaluation is too computer intensive and problematic when black-box solvers are used. Data learning and surrogated models can be applied to provide low-fidelity models for the state. These can be used in gradient free, quasi-differentiable or differentiable minimization methods. Then incomplete sensitivity can be used to upgrade data learning at zero cost beyond what available with just the functional. This extra information also gives insights on robustness of the design and allows to discriminate between Pareto points. It also enables the user to have ideas on the impact of uncertainties in independent parameters which are not design parameter. This ensemble leads to a design method, may be less efficient for academic problems, but more robust and reliable in realistic situations with uncertainties on all parameters.
9117 en Surrogate-based Constrained Multi-Objective Optimization Aerospace design is synonymous with the use of long running and computationally intensive simulations, which are employed in the search for optimal designs in the presence of multiple, competing objectives and constraints. The difficulty of this search is often exacerbated by numerical `noise' and inaccuracies in simulation data and the frailties of complex simulations, that is they often fail to return a result. Surrogate-based optimization methods can be employed to solve, mitigate, or circumvent problems associated with such searches. This presentation gives an overview of constrained multi-objective optimization using Gaussian process based surrogates, with an emphasis on dealing with real-world problems.
9122 en Opening Ceremony Artificial intelligence continues to be one of the most vibrant, challenging, and forward-looking areas of computer science. These proceedings collect the papers accepted for presentation at the Twenty-first International JointnConference on Artificial Intelligence (IJCAI-09) held in Pasadena, California, USA from July 11–17, 2009. This collection represents some of the most exciting research taking place in AI today, continuing the tradition of excellence established by IJCAI at its inception 40 years ago in 1969.n  The theme of this year’s conference is "the interdisciplinary reach of artificial intelligence." Apart from its forward-looking nature, AI has always been outward-looking, and this theme allows us to explore the broad impact of AI on science, engineering, medicine, social sciences, arts and humanities through invited talks, workshops, tutorials, and other events dedicated to this theme. Many of the papers in this volume are explicitly interdisciplinary, and still more contribute to or draw from othernintellectual disciplines indirectly.
9123 en Computer Mediated Transactions These days nearly every economic transaction involves a computer in some form or other. What does this mean for economics? I argue that the ubiquity of computers enables new and more efficient contractual forms, better alignment of incentives, more sophisticated data extraction and analysis, creates an environment for controlled experimentation, and allows for personalization and customization. I review some of the long and rich history of these phenomena and describe some of their implications for current and future practices.
9124 en Scaling AI Through Multi-Agent Organizations Scaling remains one of the grand challenges for AI. Lesser has been using organizational control to build multiagent systems with hundreds to thousands of intelligent agents. This approach can also be used to structure complex AI systems with extensive and heterogeneous knowledge. Organizational control is a multi-level approach in which organizational goals, roles, and responsibilities are dynamically developed,ndistributed, and maintained to serve as guidelines for making detailed operational control decisions by the individual agents. Lesser will illustrate the use of organizational control in three distributed application areas:n(1) an adaptive sensor and interpretation vehicle-tracking network, (2) a peer-to-peer information search and retrieval system, and (3) a self-improving task allocation system. He will highlight the importantnbalance between externally-directed and self-directed agent activities in uncertain and dynamic environments. Then he will present the continuing research challenges, including how to automate the design of an organization and evolve it as conditions change, create an organizationally situated agent, and evaluate and predict an organization’s performance.
9125 en Video Competition Award Ceremony As a sequel to the successful AAAI-07 and AAAI-08 AI Video Competitions, IJCAI now solicits submissions for the 3rd annual video competition! Its goal is to show the World how much fun AI is by documenting exciting artificial intelligence advances in research, education, and application. The rules are simple: Compose a short video about an exciting AI project, and narrate it so that it is accessible to a broad online audience. Accepted videos will be screened in the IJCAI-09 registration area during the conference. On the evening of 14 July 2009 (7:00pm-7:30pm) at IJCAI-09, immediately after and in a room adjacent to the Computers and Thought Lectures, we will hold a red-carpet awards ceremony to celebrate the nominees of the best video awards. The developers of the winners will be formally presented with trophies in a ceremony that resembles the Oscars. We strongly encourage student participation. So: go ahead and make a cool online video about your AI project, and get a ton of attention!nnCheck also the previous editions of the AI Video Competition:n[[http://videolectures.net/aaai07/|AAAI 2007]], [[http://videolectures.net/aaai08/|AAAI 2008]], [[http://videolectures.net/ijcai09_video_competition/|IJCAI 2009]]
9126 en IJCAI 2009 Industry Day Panel This is a one-day session scheduled for Friday, July 17, 2009, intended to be a forum for industry to "tell their AI story." The content is entirely up to the industry participants. The forum can be utilized for networking, recruiting, marketing, or even just bragging. All conference registrants have access to the Industry Day event. The day will be structured around a panel and a series of industry presentations.
9127 en How Optimized Environmental Sensing Helps Address Information Overload on the Web In this talk, we tackle a fundamental problem that arises when using sensors to monitor the ecological condition of rivers and lakes, the network of pipes that bring water to our taps, or the activities of an elderly individual when sitting on a chair: Where should we place the sensors in order to make effective and robust predictions? Such sensing problems are typically NP-hard, and in the past, heuristics without theoretical guarantees about the solution quality have often been used. In this talk, we present algorithms which efficiently find provably near-optimal solutions to large, complex sensing problems. Our algorithms are based on the key insight that many important sensing problems exhibit submodularity, an intuitive diminishing returns property: Adding a sensor helps more the fewer sensors we have placed so far. In addition to identifying most informative locations for placing sensors, our algorithms can handle settings, where sensor nodes need to be able to reliably communicate over lossy links, where mobile robots are used for collecting data or where solutions need to be robust against adversaries and sensor failures. We present results applying our algorithms to several real-world sensing tasks, including environmental monitoring using robotic sensors, activity recognition using a built sensing chair, and a sensor placement competition. We conclude with drawing an interesting connection between sensor placement for water monitoring and addressing the challenges of information overload on the web. As examples of this connection, we address the problem of selecting blogs to read in order to learn about the biggest stories discussed on the web, and personalizing content to turn down the noise in the blogosphere.
9128 en STAIR: The STanford Artificial Intelligence Robot Project This talk will describe the STAIR home assistant robot project, and the satellite projects that led to key STAIR components such as (1) robotic grasping of previously unknown objects, (2) depth perception from a single still image, (3) practical object recognition using multimodal sensors, and (4) a software architecture for integrative AI. Since its birth in 1956, the AI dream has been to build systems that exhibit broad-spectrum competence and intelligence. STAIR revisits this dream, and seeks to integrate onto a single robot platform tools drawn from all areas of AI including learning, vision, navigation, manipulation, planning, and speech and NLP. This is in distinct contrast to, and also represents an attempt to reverse, the 30 year old trend of working on fragmented AI sub-fields. STAIR’s goal is a useful home assistant robot, and over the long term, we envision a single robot that can perform tasks such as tidying up a room, using a dishwasher, fetching and delivering items, and preparing meals. In this talk, Ng will describe our progress on having the STAIR robot fetch items from around the office, and on having STAIR take inventory of office items. Specifically, he’ll describe learning to grasp previously unseen objects (including unloading items from a dishwasher); probabilistic multi-resolution maps, which enable the robot to open or use doors; and a robotic foveal plus peripheral vision system for object recognition and tracking. Ng will also outline some of the main technical ideas - such as learning 3-D reconstructions from a single still image, and reinforcement learning algorithms for robotic control - that played keynroles in enabling these STAIR components.
9129 en IJCAI-09 Computers and Thought Award The Computers and Thought Award is presented at IJCAI conferences to outstanding young scientists in artificial intelligence. The award was established with royalties received from the book, Computers and Thought, edited by Edward Feigenbaum and Julian Feldman; it is currently supported by income from IJCAI funds. Past recipients of this honor have been: Terry Winograd (1971), Patrick Winston (1973), Chuck Rieger (1975), Douglas Lenat (1977), David Marr (1979), Gerald Sussman (1981), Tom Mitchell (1983), Hector Levesque (1985), Johan de Kleer (1987), Henry Kautz (1989), Rodney Brooks (1991), Martha Pollack (1991), Hiroaki Kitano (1993), Sarit Kraus (1995), Stuart Russell (1995), Leslie Kaelbling (1997), Nicholas Jennings (1999), Daphne Koller (2001), Tuomas Sandholm (2003), and Peter Stone (2007). nnThere are two winners of the 2009 IJCAI Computers and Thought Award: Carlos Guestrin, Assistant Professor in the Machine Learning Department and the Computer Science Department of Carnegie Mellon University, and Andrew Ng, Assistant Professor in the Computer Science Department of Stanford University. Professor Guestrin is recognized for significant contributions to machine learning, probabilistic reasoning, and intelligent distributed sensor networks. Professor Ng is recognized for fundamental contributions to the application of machine learning to robot perception and control, for leadership in constructing robots that perform unscripted tasks in real environments, and for major contributions to machine learning.
9130 en Embodied Language Games for Autonomous Robots Artificial Intelligence methods and techniques have reached a high level of sophistication so that we can tackle difficult outstanding problems in science. In this talk, I will show how the question of the origins of language can be approached this way. This question has puzzled evolutionary biologists since Darwin and is still considered to be unsolved. I will outline a theory of language evolution by linguistic selection and then report a number of concrete experiments with humanoid robots that attempt to work out and validate this theory. The experiments all center around the notion of a language game, which is a routinized situated interaction that involves some form of language. Robots use linguistic strategies to evolve a communication system to deal with a particular class of language games. I will discuss examples of this and also address the question how new strategies can arise and how the robots can autonomously decide which strategies they will collectively use to bootstrap their language.
9131 en Intelligent Tutoring Systems: New Challenges and Directions Can we devise educational systems that provide individualized instruction tailored to the needs of the individual learners, as many good teachers do? Intelligent Tutoring Systems is the interdisciplinary field that investigates this question by integrating research in Artificial Intelligence, Cognitive Science and Education. Successful intelligent tutoring systems have been deployed to support traditional problem solving activities by tailoring the instruction to the student's domain knowledge. In this talk, I will present a variety of projects that illustrate our efforts to extend the scope of intelligent tutors to both support novel forms of pedagogical interactions (e.g., example-based and exploration-based learning) and adapt to student's traits beyond knowledge (e.g., student's meta-cognitive abilities and affective states). I will discuss the challenges of this research, the results that we have achieved so far and future opportunities.
9132 en Machine Learning in Ecosystem Informatics and Sustainability Ecosystem Informatics brings together mathematical and computational tools to address scientific and policy challenges in the ecosystem sciences. These challenges include novel sensors for collecting data, algorithms for automated data cleaning, learning methods for building statistical models from data and for fitting mechanistic models to data, and algorithms for designing optimal policies for biosphere management. This talk will describe recent work on the first two of these---new devices for automated arthropod population counting and linear Gaussian DBNs for automated cleaning of sensor network data. It will also give examples of open problems along the whole spectrum from sensors to policies.
9133 en From Low-level Sensors to High-level Intelligence: Activity Recognition Links the Knowledge Food Chain Sensors provide computer systems with a window to the outside world. Activity recognition "sees" what is in the window to predict the locations, trajectories, actions, goals and plans of humans and objects. Building an activity recognition system requires a full range of interaction from statistical inference on lower level sensor data to symbolic AI at higher levels, where prediction results and acquired knowledge are passed up each level to form a knowledge food chain. In this talk, I will give an overview of activity recognition and explore its relation to other fields, including planning and knowledge acquisition, machine learning and Web search. I will also describe its applications in assistive technologies, security monitoring and mobile commerce.nn
9274 en Opening Session 
9280 en Opening Session 
9281 en The Wealth of Places 
9282 en Open Innovation and Living Labs 
9283 en A Culture of Creativity: The Arnhem-Nijmegen Region 
9284 en Creative Partnerships: Ten Years On 
9285 en Inspiration, Creativity, Innovation: Fostering Entrepreneurial Creativity 
9286 en Motivation, Fun, Innovation: A Successful Competition for Ideas 
9287 en Schools of the Future? The SKUB Project: Reinventing 12 Schools in the City of Gentofte 
9288 en First Panel Session on Innovation and Education 
9289 en Interview 
9290 en Interview 
9291 en Interview 
9292 en Interview 
9293 en Interview 
9294 en Listening to Stakeholders 
9295 en “Big Dipper”: A Moving Science Laboratory 
9296 en VideoLectures.net 
9297 en Social Computing Enabling Creative and Innovative Learning: An Analysis of Practices 
9298 en Second Panel Session on Innovation and Education 
9299 en Innovation and Employment 
9301 en Innovation and Wellbeing 
9302 en Innovation and Education 
9303 en Closing Speech 
9334 en Computational Knowledge, Science and Wolfram|Alpha Wolfram will describe the concepts, technology and science that underlie Wolfram|Alpha—an ambitious project to make as much knowledge as possible computable. Stephen Wolfram is the founder and CEO of Wolfram Research, the creator of Mathematica, the author of A New Kind of Science and now the creator of Wolfram|Alpha.n----n**//Disclaimer:// Videolectures.Net emphasises that the quality of this video was notably improved,nbecause of low light and sound quality conditions provided in the lecture auditorium.**n----
9335 en BioPlanner: A Plan Adaptation Approach for the Discovery of Biological Pathways across Species 
9336 en Human Computation This talk is about harnessing human brainpower to solve problems that computers cannot. Although computers have advanced dramatically over the last 50 years, they still do not possess basic conceptual intelligence or perceptual capabilities that most humans take for granted. By leveraging human abilities in a novel way, I want to solve large-scale computational problems and collect data to teach computers basic human talents. To this end, I treat human brains as processors in a distributed system, each performing a small part of a massive computation.nnLuis von Ahn works in the Computer Science Department at Carnegie Mellon University. He is the recipient of a MacArthur Fellowship and a Microsoft New Faculty Fellowship. He has been named one of the 50 Best Minds in Science by Discover Magazine, one of the "Brilliant 10" of 2006 by Popular Science Magazine, one of the 50 most influential people in technology by Silicon.com, and one of the Top Innovators in the Arts and Sciences by Smithsonian Magazine. His research interests include encouraging people to do work for free, as well as catching and thwarting cheaters in online environments.
9338 en Welcome and Introduction The SemanticWeb initiative has brought forward the idea thatnthe web may become a space not only for publishing and interlinkingndocuments (through HTML hyperlinks), but alsonfor publishing and interlinking knowledge bases (e.g. in thenform of RDF graphs) in an open and fully decentralized environment.nThis is how Tim Berners-Lee expressed this idea inna note from 1998:nThe Semantic Web is what we will get if we performnthe same globalization process to KnowledgenRepresentation that the Web initially did to Hypertext1nEven though models and languages to achieve this goalnhave been taken from long-standing research in AI, it is importantnto remark that the priorities are different. While traditionallynthe focus has been on theories to support sound andncomplete reasoning, web-oriented KR primarily aims at dealingnwith issues of web-wide information interoperability andnintegration. With respect to this, perhaps the most central issuesnis Principle of Global Identifiers: ”global naming leadsnto global network effects” (see Architecture of the WorldnWideWeb, Volume One, 2004, at http://www.w3.org/nTR/2004/REC-webarch-20041215/). In other words,nif a resource (where a resource may range from concretento abstract objects, from particulars to universals) is globallynidentified through a uniform identifier in any knowledgenrepository exposed on the web (e.g. in an RDF store), thennany knowledge about it would be much easier to gather andnintegrate, distributed reasoning becomes practically possible,nand knowledge-based navigation across interlinked knowledgensources can be enabled. As it happened for the webnof documents, the overall value of such an open and distributednnetwork of interlinked knowledge sources would benimmensely bigger than the sum of the value of the components.nTechnically, URIs (Uniform Resource Identifiers, seenhttp://www.w3.org/Addressing/) are used to identify entitiesnon the Semantic Web, but how to achieve shared URI understandingnand reuse is object of research. This central role of identity and reference for a web-scale KR poses new challengesnto traditional KR, and many researchers have suggestednthat the concept of URI may deeply affect the notionsnof language (e.g. the semantics of using the ”same” URI inndifferent models), reference (e.g. rigid vs. non rigid designation),ninterpretation (e.g. the meaning of ”links” acrossnknowlkedge bases) & reasoning (e.g. distributed reasoningnacross theories) in traditional logic-based KR in AI.nThe goal of the workshop on Identity and Reference innweb-based Knowledge Representation workshop, which innits past editions was mainly restricted to the Web and SemanticnWeb communities (see past editions at WWW20062,nWWW20073 and ESWC20084), is to open the debate on thenimpact and the challenges that web-oriented KR poses tonsome of the core concepts of traditional AI.nThese working notes collect the papers which have beennselected for presentation at the workshop, which was held innconjunction with IJCAI-09 at Pasadena (CA) in July 2009.nThe papers provide different perspectives on the issue of identitynand reference, and are also an illustration of the relevancenof the problem and on the diversity of views which exist onnit.nWe’d like to thank all the authors and the participants forntheir contribution to the success of the workshop.
9339 en Is the Web a Web of Documents or Things? How we design and structure our information in thenWeb is essentially influenced by our philosophicalnviewpoints on what the Web is. In this paper, wencompared two fundamentally different positions:none takes the Web to be a web of documents andnthe other a web of things. By using Fred Dretske’snsemantic information theory, we discussed why wenshould favor the second model over the formernthrough our formulation of two information triads.nThe first one is the Knowledge-Information-Datan(KID) triad that allows us to clearly define thesenconcepts within a communicative praxis. The secondnone is the Symbol-Information-Referent (SIR)ntriad that allows us to clearly define and connect allnkinds of information systems, regardless they arenthe man-made or naturally occurring ones.
9346 en Welcome Statment Given by the Program Co-Chairs 
9347 en How Do Infants Bootstrap into Spoken Language?: Models and Challenges Human infants learn spontaneously and effortlessly the language(s) spoken in their environments, despite the extraordinary complexity of the task. Here, I will present an overview of the early phases of language acquisition and focus on one area where a modeling approach is currently being conducted using tools of signal processing and automatic speech recognition: the unsupervized acquisition of phonetic categories. During their first year of life, infants construct a detailed representation of the phonemes of their native language and lose the ability to distinguish nonnative phonemic contrasts. Unsupervised statistical clustering is not sufficient; it does not converge on the inventory of phonemes, but rather on contextual allophonic units or subunits. I present an information-theoretic algorithm that groups together allophonic variants based on three sources of information that Can be acquired independently: the statistical distribution of their contexts, the phonetic plausibility of the grouping, and the existence of lexical minimal pairs. This algorithm is tested on several natural speech corpora. We find that these three sources of information are probably not language specific. What is presumably unique to language is the way in which they are combined to optimize the emergence of linguistic categories.nnEmmanuel Dupoux is the director of the Laboratoire de Sciences Cognitives et Psycholinguistique in Paris. He conducts research on the early phases of language and social acquisition in human infants, using a mix of behavioral and brain-imaging techniques as well as computational modeling. He teaches at the Ecole des Hautes Etudes en Sciences sociales where he has set up an interdisciplinary graduate program in Cognitive Science.
9348 en Drifting Games, Boosting and Online Learning 
9349 en Can Learning Kernels Help Performance? Kernel methods combined with large-margin learning algorithms such as SVMs have been used successfully to tackle a variety of learning tasks since their introduction in the early 90s. However, in the standard framework of these methods, the choice of an appropriate kernel is left to the user and a poor selection may lead to sub-optimal performance. Instead, sample points can be used to select a kernel function suitable for the task out of a family of kernels fixed by the user. While this is an appealing idea supported by some recent theoretical guarantees, in experiments, it has proven surprisingly difficult to consistently and significantly outperform simple fixed combination schemes of kernels. This talk will survey different methods and algorithms for learning kernels and will present novel results that tend to suggest that significant performance improvements can be obtained with a large number of kernels. (Includes joint work with Mehryar Mohri and Afshin Rostamizadeh.)
9350 en Awards Session 
9351 en Solution Stability in Linear Programming Relaxations: Graph Partitioning and Unsupervised Learning We propose a new method to quantify the solutionnstability of a large class of combinatorialnoptimization problems arising in machinenlearning. As practical example we apply thenmethod to correlation clustering, clusteringnaggregation, modularity clustering, and relativenperformance signicance clustering. Ournmethod is extensively motivated by the ideanof linear programming relaxations. We proventhat when a relaxation is used to solve thenoriginal clustering problem, then the solutionnstability calculated by our method is conservative,nthat is, it never overestimates the solutionnstability of the true, unrelaxed problem.nWe also demonstrate how our methodncan be used to compute the entire path ofnoptimal solutions as the optimization problemnis increasingly perturbed. Experimentally,nour method is shown to perform wellnon a number of benchmark problems.
9352 en A Scalable Framework for Discovering Coherent Co-clusters in Noisy Data Clustering problems often involve datasetsnwhere only a part of the data is relevant tonthe problem, e.g., in microarray data anal-nysis only a subset of the genes show cohe-nsive expressions within a subset of the con-nditions/features. The existence of a largennumber of non-informative data points andnfeatures makes it challenging to hunt for co-nherent and meaningful clusters from suchndatasets. Additionally, since clusters couldnexist in different subspaces of the featurenspace, a co-clustering algorithm that simul-ntaneously clusters objects and features is of-nten more suitable as compared to one thatnis restricted to traditional “one-sided” clus-ntering. We propose Robust Overlapping Co-nClustering (ROCC), a scalable and very ver-nsatile framework that addresses the problemnof efficiently mining dense, arbitrarily posi-ntioned, possibly overlapping co-clusters fromnlarge, noisy datasets. ROCC has several de-nsirable properties that make it extremely wellnsuited to a number of real life applications.n1
9353 en Multi-View Clustering via Canonical Correlation Analysis Clustering data in high dimensions is believednto be a hard problem in general. Annumber of efficient clustering algorithms developednin recent years address this problemnby projecting the data into a lower dimensionalnsubspace, e.g. via PrincipalnComponents Analysis (PCA) or random projections,nbefore clustering. Here, we considernconstructing such projections using multiplenviews of the data, via Canonical CorrelationnAnalysis (CCA).nUnder the assumption that the views are uncorrelatedngiven the cluster label, we shownthat the separation conditions required fornthe algorithm to be successful are significantly weaker than prior results in the literature.nWe provide results for mixturesnof Gaussians and mixtures of log concavendistributions. We also provide empiricalnsupport from audio-visual speaker clusteringn(where we desire the clusters to correspond tonspeaker ID) and from hierarchical Wikipediandocument clustering (where one view is thenwords in the document and the other is thenlink structure).
9354 en Spectral Clustering Based on the Graph p-Laplacian We present a generalized version of spectralnclustering using the graph p-Laplacian,na nonlinear generalization of the standardngraph Laplacian. We show that the secondneigenvector of the graph p-Laplacian interpolatesnbetween a relaxation of the normalizednand the Cheeger cut. Moreover, wenprove that in the limit as p ! 1 the cutnfound by thresholding the second eigenvectornof the graph p-Laplacian converges to thenoptimal Cheeger cut. Furthermore, we providenan efficient numerical scheme to computenthe second eigenvector of the graph p-nLaplacian. The experiments show that thenclustering found by p-spectral clustering is atnleast as good as normal spectral clustering,nbut often leads to significantly better results.
9355 en Nearest Neighbors in High-Dimensional Data: The Emergence and Influence of Hubs High dimensionality can pose severe difficulties,nwidely recognized as different aspects ofnthe curse of dimensionality. In this paper wenstudy a new aspect of the curse pertaining tonthe distribution of k-occurrences, i.e., the numbernof times a point appears among the k nearestnneighbors of other points in a data set. We shownthat, as dimensionality increases, this distributionnbecomes considerably skewed and hub pointsnemerge (points with very high k-occurrences).nWe examine the origin of this phenomenon,nshowing that it is an inherent property of highdimensionalnvector space, and explore its influencenon applications based on measuring distancesnin vector spaces, notably classification,nclustering, and information retrieval.
9356 en Fitting a Graph to Vector Data We introduce a measure of how well a combinatorialngraph ts a collection of vectors.nThe optimal graphs under this measure maynbe computed by solving convex quadraticnprograms and have many interesting properties.nFor vectors in d dimensional space, thengraphs always have average degree at mostn2(d+1), and for vectors in 2 dimensions theynare always planar. We compute these graphsnfor many standard data sets and show thatnthey can be used to obtain good solutions tonclassification, regression and clustering problems.
9357 en The Adaptive k-Meteorologists Problem and Its Application to Structure Learning and Feature Selection in Reinforcement Learning The purpose of this paper is three-fold. First, we formalize and study a problem of learning probabilistic concepts in the recently proposed KWIK framework. We give details of an algorithm, known as the Adaptive k-Meteorologists Algorithm, analyze its sample complexity upper bound, and give a matching lower bound. Second, this algorithm is used to create a new reinforcement learning algorithm for factoredstate problems that enjoys significant improvement over the previous state-of-the-art algorithm. Finally, we apply the Adaptive k-Meteorologists Algorithm to remove a limiting assumption in an existing reinforcement-learning algorithm. The effectiveness of our approaches are demonstrated empirically in a couple benchmark domains as well as a robotics navigation problem.
9358 en Optimistic Initialization and Greediness Lead to Polynomial Time Learning in Factored MDPs In this paper we propose an algorithm for polynomial-time reinforcement learning in factored Markov decision processes (FMDPs). The factored optimistic initial model (FOIM) algorithm, maintains an empirical model of the FMDP in a conventional way, and always follows a greedy policy with respect to its model. The only trick of the algorithm is that the model is initialized optimistically. We prove that with suitable initialization (i) FOIM converges to the fixed point of approximate value iteration (AVI); (ii) the number of steps when the agent makes non-near-optimal decisions (with respect to the solution of AVI) is polynomial in all relevant quantities; (iii) the per-step costs of the algorithm are also polynomial. To our best knowledge, FOIM is the first algorithm with these properties.
9359 en Dynamic Analysis of Multiagent Q-learning with e-greedy Exploration The development of mechanisms to understand and model the expected behaviour of multiagent learners is becoming increasingly important as the area rapidly find application in a variety of domains. In this paper we present a framework to model the behaviour of Q-learning agents using the e-greedy exploration mechanism. For this, we analyse a continuous-time version of the Q-learning update rule and study how the presence of other agents and the e-greedy mechanism affect it. We then model the problem as a system of difference equations which is used to theoretically analyse the expected behaviour of the agents. The applicability of the framework is tested through experiments in typical games selected from the literature.
9360 en Hoeffding and Bernstein Races for Selecting Policies in Evolutionary Direct Policy Search Uncertainty arises in reinforcement learning from various sources, and therefore it is necessary to consider statistics based on several roll-outs for evaluating behavioral policies. We add an adaptive uncertainty handling based on Hoeffding and empirical Bernstein races to the CMA-ES, a variable metric evolution strategy proposed for direct policy search. The uncertainty handling adjusts individually the number of episodes considered for the evaluation of a policy. The performance estimation is kept just accurate enough for a sufficiently good ranking of candidate policies, which is in turn sufficient for the CMA-ES to find better solutions. This increases the learning speed as well as the robustness of the algorithm.
9361 en A Simpler Unified Analysis of Budget Perceptrons The kernel Perceptron is an appealing online learning algorithm that has a drawback: whenever it makes an error it must increase its support set, which slows training and testing if the number of errors is large. The Forgetron and the Randomized Budget Perceptron algorithms overcome this problem by restricting the number of support vectors the Perceptron is allowed to have. These algorithms have regret bounds whose proofs are dissimilar. In this paper we propose a unified analysis of both of these algorithms by observing that the way in which they remove support vectors can be seen as types of $L_2$-regularization. By casting these algorithms as instances of online convex optimization problems and applying a variant of Zinkevich's theorem for noisy and incorrect gradient, we can bound the regret of these algorithms more easily than before. Our bounds are similar to the existing ones, but the proofs are less technical.
9362 en Efficient Learning Algorithms for Changing Environments We study online learning in an oblivious changing environment. The standard measure of regret bounds the difference between the cost of the online learner and the best decision in hindsight. Hence, regret minimizing algorithms tend to converge to the static best optimum, clearly a suboptimal behavior in changing environments. On the other hand, various metrics proposed to strengthen regret and allow for more dynamic algorithms produce inefficient algorithms.nnWe propose a different performance metric which strengthens the standard metric of regret and measures performance with respect to a changing comparator. We then describe a series of data-streaming-based reductions which transform algorithms for minimizing (standard) regret into adaptive algorithms albeit incurring only poly-logarithmic computational overhead.nnUsing this reduction, we obtain efficient low adaptive-regret algorithms for the problem of online convex optimization. This can be applied to various learning scenarios, i.e. online portfolio selection, for which we describe experimental results showing the advantage of adaptivity.
9363 en Online Learning by Ellipsoid Method In this work, we extend the ellipsoid method, which was originally designed for convex optimization, for online learning. The key idea is to approximate by an ellipsoid the classification hypotheses that are consistent with all the training examples received so far. This is in contrast to most online learning algorithms where only a single classifier is maintained at each iteration. Efficient algorithms are presented for updating both the centroid and the positive definite matrix of ellipsoid given a misclassified example. In addition to the classical ellipsoid method, an improved version for online learning is also presented. Mistake bounds for both ellipsoid methods are derived. Evaluation with the USPS dataset and three UCI data-sets shows encouraging results when comparing the proposed online learning algorithm to two state-of-the-art online learners.n
9364 en Learning Prediction Suffix Trees with Winnow Prediction suffix trees (PSTs) are a popular tool for modeling sequences and have been successfully applied in many domains such as compression and language modeling. In this work we adapt the well studied Winnow algorithm to the task of learning PSTs. The proposed algorithm automatically grows the tree, so that it provably remains competitive with any fixed PST determined in hindsight. At the same time we prove that the depth of the tree grows only logarithmically with the number of mistakes made by the algorithm. Finally, we empirically demonstrate its effectiveness in two different tasks.
9365 en Identifying Suspicious URLs: An Application of Large-Scale Online Learning This paper explores online learning approaches for detecting malicious Web sites (those involved in criminal scams) using lexical and host-based features of the associated URLs. We show that this application is particularly appropriate for online algorithms as the size of the training data is larger than can be efficiently processed in batch and because the distribution of features that typify malicious URLs is changing continuously. Using a real-time system we developed for gathering URL features, combined with a real-time source of labeled URLs from a large Web mail provider, we demonstrate that recently-developed online algorithms can be as accurate as batch techniques, achieving classification accuracies up to 99% over a balanced data set.n
9366 en Ranking with Ordered Weighted Pairwise Classiﬁcation In ranking with the pairwise classiﬁcation approach, the loss associated to a predicted ranked list is the mean of the pairwise classiﬁca- ntion losses. This loss is inadequate for tasks such as information retrieval where we prefer ranked lists with high precision on the top of the nlist. We propose to optimize a larger class of loss functions for ranking, based on an ordered weighted average (OWA) (Yager, 88) of the nclassiﬁcation losses. Convex OWA aggregation operators range from the max to the mean depending on their weights, and can be used to nfocus on the top ranked elements as they give more weight to the largest losses. When aggregating hinge losses, the optimization problem nis similar to the SVM for interdependent output spaces. Moreover, we show that an OWA aggregation of margin-based classiﬁcation losses nhas good generalization properties. Experiments on the Letor 3.0 benchmark dataset for information retrieval validate our approach.
9367 en BoltzRank: Learning to Maximize Expected Ranking Gain Ranking a set of retrieved documents according to their relevance to a query is a popular problem in information retrieval. Methods nthat learn ranking functions are difﬁcult to optimize, as ranking performance is typically judged by metrics that are not smooth. In this paper nwe propose a new listwise approach to learning to rank. Our method creates a conditional probability distribution over rankings assigned nto documents for a given query, which allows for gradient ascent optimization of the expected value of some performance measure. The nrank probabilities take the form of a Boltzmann distribution, based on an energy function that depends on a scoring function composed of nindividual and pairwise potentials. Including pairwise potentials is a novel contribution, allowing the model to encode regularities in the nrelative scores of documents; existing models assign scores at test time based only on individual documents, with no pairwise constraints nbetween documents. Experimental results on the LETOR3.0 data sets show that our method out-performs existing learning approaches to nranking.
9368 en Decision Tree and Instance-Based Learning for Label Ranking The label ranking problem consists of learning a model that maps instances to total orders over a ﬁnite set of predeﬁned labels. This npaper introduces new methods for label ranking that complement and improve upon existing approaches. More speciﬁcally, we propose nextensions of two methods that have been used extensively for classiﬁcation and regression so far, namely instance-based learning and ndecision tree induction. The unifying element of the two methods is a procedure for locally estimating predictive probability models for nlabel rankings.
9369 en Ranking Interesting Subgroups Subgroup discovery is the task of identifying the top k patterns in a database with most signiﬁcant deviation in the distribution of a ntarget attribute Y . Subgroup discovery is a popular approach for identifying interesting patterns in data, because it effectively combines nstatistical signiﬁcance with an understandable representation of patterns as a logical formula. However, it is often a problem that some nsubgroups, even if they are statistically highly signiﬁcant, are not interesting to the user for some reason. In this paper, we present an napproach based on the work on ranking Support Vector Machines that ranks subgroups with respect to the user’s concept of interestingness, and ﬁnds subgroups that are interesting to the user. It will be shown that this approach can signiﬁcantly increase the quality of the subgroups.
9370 en Generalization Analysis of Listwise Learning-to-Rank Algorithms This paper presents a theoretical framework for ranking, and demonstrates how to perform generalization analysis of listwise ranking nalgorithms using the framework. Many learning-to-rank algorithms have been proposed in recent years. Among them, the listwise approach nhas shown higher empirical ranking performance when compared to the other approaches. However, there is no theoretical study on the nlistwise approach as far as we know. In this paper, we propose a theoretical framework for ranking, which can naturally describe various nlistwise learning- to-rank algorithms. With this framework, we prove a theorem which gives a generalization bound of a listwise ranking nalgorithm, on the basis of Rademacher Average of the class of compound functions. The compound functions take listwise loss functions nas outer functions and ranking models as inner functions. We then compute the Rademacher Averages for existing listwise algorithms of nListMLE, ListNet, and RankCosine. We also discuss the tightness of the bounds in different situations with regard to the list length and ntransformation function.
9371 en Structure Preserving Embedding Structure Preserving Embedding (SPE) isnan algorithm for embedding graphs in Euclideannspace such that the embedding is lowdimensionalnand preserves the global topologicalnproperties of the input graph. Topologynis preserved if a connectivity algorithm, suchnas k-nearest neighbors, can easily recover thenedges of the input graph from only the coordinatesnof the nodes after embedding. SPEnis formulated as a semidefinite program thatnlearns a low-rank kernel matrix constrainednby a set of linear inequalities which capturesnthe connectivity structure of the input graph.nTraditional graph embedding algorithms donnot preserve structure according to our definition,nand thus the resulting visualizationsncan be misleading or less informative. SPEnprovides significant improvements in termsnof visualization and lossless compression ofngraphs, outperforming popular methods suchnas spectral embedding and Laplacian eigenmaps.nWe find that many classical graphsnand networks can be properly embedded usingnonly a few dimensions. Furthermore,nintroducing structure preserving constraintsninto dimensionality reduction algorithms producesnmore accurate representations of highdimensionalndata.
9372 en Graph Construction and b-Matching for Semi-Supervised Learning Graph based semi-supervised learning (SSL)nmethods play an increasingly important rolenin practical machine learning systems. Ancrucial step in graph based SSL methodsnis the conversion of data into a weightedngraph. However, most of the SSL literaturenfocuses on developing label inference algorithmsnwithout extensively studying thengraph building method and its effect on performance.nThis article provides an empiricalnstudy of leading semi-supervised methodsnunder a wide range of graph constructionnalgorithms. These SSL inference algorithmsninclude the Local and Global Consistencyn(LGC) method, the Gaussian RandomnField (GRF) method, the Graph Transductionnvia Alternating Minimization (GTAM)nmethod as well as other techniques. Severalnapproaches for graph construction, sparsificationnand weighting are explored includingnthe popular k-nearest neighbors methodn(kNN) and the b-matching method. As opposednto the greedily constructed kNN graph,nthe b-matched graph ensures each node in thengraph has the same number of edges and producesna balanced or regular graph. Experimentalnresults on both artificial data and realnbenchmark datasets indicate that b-matchingnproduces more robust graphs and thereforenprovides significantly better prediction accuracynwithout any significant change in computationntime.
9373 en Partial Order Embedding with Multiple Kernels We consider the problem of embedding arbitrarynobjects (e.g., images, audio, documents) into Euclideannspace subject to a partial order over pairwisendistances. Partial order constraints arise naturallynwhen modeling human perception of similarity.nOur partial order framework enables thenuse of graph-theoretic tools to more efficientlynproduce the embedding, and exploit global structurenwithin the constraint set.nWe present an embedding algorithm based onnsemidefinite programming, which can be parameterizednby multiple kernels to yield a unifiednspace from heterogeneous features.
9374 en Probabilistic Dyadic Data Analysis with Local and Global Consistency Dyadic data arises in many real world applicationsnsuch as social network analysis and informationnretrieval. In order to discover the underlyingnor hidden structure in the dyadic data, manyntopic modeling techniques were proposed. Thentypical algorithms include Probabilistic LatentnSemantic Analysis (PLSA) and Latent DirichletnAllocation (LDA). The probability density functionsnobtained by both of these two algorithmsnare supported on the Euclidean space. However,nmany previous studies have shown naturally occurringndata may reside on or close to an underlyingnsubmanifold. We introduce a probabilisticnframework for modeling both the topical and geometricalnstructure of the dyadic data that explicitlyntakes into account the local manifold structure.nSpecifically, the local manifold structure isnmodeled by a graph. The graph Laplacian, analogousnto the Laplace-Beltrami operator on manifolds,nis applied to smooth the probability densitynfunctions. As a result, the obtained probabilisticndistributions are concentrated around the datanmanifold. Experimental results on real data setsndemonstrate the effectiveness of the proposed approach.
9375 en Non-Linear Matrix Factorization with Gaussian Processes A popular approach to collaborative filtering is matrix factorization. In this paper we consider the "probabilistic matrix factorization" and by taking a latent variable model perspective we show its equivalence to Bayesian PCA. This inspires us to consider probabilistic PCA and its non-linear extension, the Gaussian process latent variable model (GP-LVM) as an approach for probabilistic non-linear matrix factorization. We apply approach to benchmark movie recommender data sets. The results show better than previous state-of-the-art performance.
9376 en Analytic Moment-Based Gaussian Process Filtering We propose an analytic moment-based filter for nonlinear stochastic dynamical systems modeled by Gaussian processes. Exact expressions for the expected value and the covariance matrix are provided for both the prediction and the filter step, where an additional Gaussian assumption is exploited in the latter case. The new filter does not require further approximations. In particular, it avoids sample approximations. We compare the filter to a variety of available Gaussian filters, such as the EKF, the UKF, and the GP-UKF recently proposed by Ko et al. (2007).
9377 en Function Factorization Using Warped Gaussian Processes We introduce a new approach to non-linear regression called function factorization, that is suitable for problems where an output variable can reasonably be modeled by a number of multiplicative interaction terms between non-linear functions of the inputs. The idea is to approximate a complicated function on a high-dimensional space by the sum of products of simpler functions on lower-dimensional subspaces. Function factorization can be seen as a generalization of matrix and tensor factorization methods, in which the data are approximated by the sum of outer products of vectors. We present a non-parametric Bayesian approach to function factorization where the priors over the factorizing functions are warped Gaussian processes, and we do inference using Hamiltonian Markov chain Monte Carlo. We demonstrate the superior predictive performance of the method on a food science data set compared to Gaussian process regression and tensor factorization using PARAFAC and GEMANOVA models.
9378 en Tractable Nonparametric Bayesian Inference in Poisson Processes with Gaussian Process Intensities The inhomogeneous Poisson process is a point process that has varying intensity across its domain (usually time or space). For nonparametric Bayesian modeling, the Gaussian process is a useful way to place a prior distribution on this intensity. The combination of an Poisson process and GP is known as a Gaussian Cox process, or doubly-stochastic Poisson process. Likelihood-based inference in these models requires an intractable integral over an infinite-dimensional random function. In this paper we present the first approach to Gaussian Cox processes in which it is possible to perform inference without introducing approximations or finite-dimensional proxy distributions. We call our method the Sigmoidal Gaussian Cox Process, which uses a generative model for Poisson data to enable tractable inference via Markov chain Monte Carlo. We compare our methods to competing methods on synthetic data and also apply it to several real-world data sets.
9379 en Large-Scale Collaborative Prediction Using a Nonparametric Random Effects Model A nonparametric model is introduced that allows multiple related regression tasks to take inputs from a common data space. Traditional transfer learning models can be inappropriate if the dependence among the outputs cannot be fully resolved by known input-specific and task-specific predictors. The proposed model treats such output responses as conditionally independent, given known predictors and appropriate unobserved random effects. The model is nonparametric in the sense that the dimensionality of random effects is not specified a priori but is instead determined from data. An approach to estimating the model is presented uses an EM algorithm that is efficient on a very large scale collaborative prediction problem. The obtained prediction accuracy is competitive with state-of-the-art results.
9380 en Hilbert Space Embeddings of Conditional Distributions with Applications to Dynamical Systems In this paper, we extend the Hilbert space embedding approach to handle conditional distributions. This leads us to a nonparametric method for modeling dynamical systems, and allows us to update the belief state of a dynamical system by maintaining a conditional embedding. Our method is very general in terms of both the domains and the types of distributions that it can handle, and we demonstrate the effectiveness of our method in various dynamical systems. We expect that Hilbert space embedding of {\em conditional} distributions will have wide applications beyond modeling dynamical systems.
9381 en Learning Nonlinear Dynamic Models We present a novel approach for learning nonlinear dynamic models, which leads to a new set of tools capable of solving problems that are otherwise difficult. We provide theory showing this new approach is consistent for models with long range structure, and apply the approach to motion capture and high-dimensional video data, yielding results superior to standard alternatives.
9383 en Learning Linear Dynamical Systems without Sequence Information Virtually all methods of learning dynamic systems from data start from the same basic assumption: that the learning algorithm will be provided with a sequence, or trajectory, of data generated from the dynamic system. In this paper we consider the case where the data is not sequenced. The learning algorithm is presented a set of data points from the system's operation but with no temporal ordering. The data are simply drawn as individual disconnected points.nnWhile making this assumption may seem absurd at first glance, we observe that many scientific modeling tasks have exactly this property. In this paper we restrict our attention to learning linear, discrete time models. We propose several algorithms for learning these models based on optimizing approximate likelihood functions and test the methods on several synthetic data sets.
9384 en Dynamic Mixed Membership Block Model for Evolving Networks In a dynamic social or biological environment, the interactions between the underlying actors can undergo large and systematic changes. Each actor in the networks can assume multiple related roles and their affiliation to each role as determined by the dynamic links will also exhibit rich temporal phenomenon. We propose a state space mixed membership stochastic blockmodel which captures the dependency between these multiple correlated roles, and enables us to track the mixed membership of each actor in the latent space across time. We derived efficient approximate learning and inference algorithms for our model, and applied the learned models to analyze an email network in Enron Corp., and a rewiring gene interaction network of yeast collected during its full cell cycle. In both cases, our model reveals interesting patterns of the dynamic roles of the actors.
9385 en Gradient Descent with Sparsiﬁcation: An Iterative Algorithm for Sparse Recovery with Restricted Isometry Property In this paper, we present an algorithm for ﬁnding an s-sparse vector x that minimizes the square-errorn?y ? ?x?n2nwhere ? satisﬁes thenrestricted isometry property (RIP). Our algorithm, called GraDeS (Gradient Descent with Sparsiﬁcation) starts from an arbitrary s-sparsenx and iteratively updates it as: x? Hsx + 1? · ?t (y ? ?x)where ? > 1 is a constant and Hs sets all but largest s coordinates innabsolute value to zero.nWe show that GraDeS, in constant number of iterations, computes the correct s-sparse solution to the system y = ?x where ? satisﬁesnthe condition that the isometric constant ?2s < 1/3. This is the most general condition for which, near-linear time algorithm is known. Inncomparison, the best condition under which any polynomial-time algorithm is known, is ?2s < √2n? 1. An important contribution of thenpaper is to analyze how the hard-thresholding function Hs acts w.r.t. the potentialn?y ? ?x?n2n. A special case of GraDeS, correspondingnto ? = 1, called Iterative Hard Thresholding (IHT), was previously shown to converge when ?3s < 1/√32.nOur Matlab implementation of GraDeS out-performs previously proposed algorithms like Subspace Pursuit, StOMP, OMP, and Lassonby an order of magnitude. Curiously, our experiments also uncovered several cases where L1-regularized regression (Lasso) fails butnGraDeS ﬁnds the correct solution.
9386 en Learning Dictionaries of Stable Autoregressive Models for Audio Scene Analysis In this paper, we explore an application of basis pursuit to audio scene analysis. The goal of our work is to detect when certain nsounds are present in a mixed audio signal. We focus on the regime where out of a large number of possible sources, a small but unknown nnumber combine and overlap to yield the observed signal. To infer which sounds are present, we decompose the observed signal as a linear ncombination of a small number of active sources. We cast the inference as a regularized form of linear regression whose sparse solutions nyield decompositions with few active sources. We characterize the acoustic variability of individual sources by autoregressive models of ntheir time domain waveforms. When we do not have prior knowledge of the individual sources, the coefﬁcients of these autoregressive nmodels must be learned from audio examples. We analyze the dynamical stability of these models and show how to estimate stable models nby substituting a simple convex optimization for a difﬁcult eigenvalue problem. We demonstrate our approach by learning dictionaries of nmusical notes and using these dictionaries to analyze polyphonic recordings of piano, cello, and violin.
9387 en Online Dictionary Learning for Sparse Coding Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, nneuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to speciﬁc ndata, an approach that has recently proven to be very effective for signal reconstruction and classiﬁcation in the audio and image processing ndomains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which nscales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with nnatural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and nlarge datasets.
9388 en Learning Non-Redundant Codebooks for Classifying Complex Objects Codebook-based representations are widely employed in the classiﬁcation of complex objects such as images and documents. Most nprevious codebook-based methods construct a single codebook via clustering that maps a bag of low-level features into a ﬁxed-length nhistogram that describes the distribution of these features. This paper describes a simple yet effective framework for learning multiple nnon-redundant codebooks that produces surprisingly good results. In this framework, each codebook is learned in sequence to extract ndiscriminative information that was not captured by preceding codebooks and their corresponding classiﬁers. We apply this framework nto two application domains: visual object categorization and document classiﬁcation. Experiments on large classiﬁcation tasks show nsubstantial improvements in performance compared to a single codebook or codebooks learned in a bagging style.
9389 en Prototype Vector Machine for Large Scale Semi-Supervised Learning Practical data analysis and mining rarely falls exactly into the supervised learning scenario. Rather, the growing amount of unlabelled ndata from various scientiﬁc domains poses a big challenge to large-scale semi-supervised learning (SSL). We note that the computational nintensiveness of graph-based SSL arises largely from the manifold or graph regularization, which may in turn lead to large models that nare difﬁcult to handle. To alleviate this, we proposed the prototype vector machine (PVM), a highly scalable, graph-based algorithm for nlarge-scale SSL. Our key innovation is the use of “prototypes vectors” for efﬁcient approximation on both the graph-based regularizer nand the model representation. The choice of prototypes are grounded upon two important criterion: they not only perform effective low- nrank approximation on the kernel matrix, but also span a model suffering the minimum information loss compared with the complete nmodel. These criterion lead to consistent prototype selection scheme, allowing us to design a uniﬁed algorithm (PVM) that demonstrates nencouraging performance while at the same time possessing appealing scaling properties (empirically linear with sample size).
9390 en Multi-Assignment Clustering for Boolean Data Conventional clustering methods typically assume that each data item belongs to a single cluster. This assumption does not hold in general. In order to overcome this limitation, we propose a generative method for clustering vectorial data, where each object can be assigned to multiple clusters. Using a deterministic annealing scheme, our method decomposes the observed data into the contributions of individual clusters and infers their parameters.\\ Experiments on synthetic Boolean data show that our method achieves higher accuracy in the source parameter estimation and superior cluster stability compared to state-of-the-art approaches. We also apply our method to an important problem in computer security known as role mining. Experiments on real-world access control data show performance gains in generalization to new employees against other multi-assignment methods. In challenging situations with high noise levels, our approach maintains its good performance, while alternative state-of-the-art techniques lack robustness.
9391 en K-Means in Space: A Radiation Sensitivity Evaluation Spacecraft are increasingly making use of onboard data analysis to inform additional data collection and prioritization decisions. However, many spacecraft operate in high-radiation environments in which the reliability of data-intensive computation is not known. This paper presents the first study of radiation sensitivity for k-means clustering. Our key findings are that 1) k-means data structures differ in sensitivity, and sensitivity is not determined by the amount of memory exposed, 2) no special radiation protection is needed below a data-set-dependent radiation threshold, enabling the use of faster, smaller, and cheaper onboard memory in some cases, and 3) subsampling improves radiation tolerance slightly, but the use of kd-trees unfortunately reduces tolerance. Our conclusions can be used to tailor k-means for future use in high-radiation environments.
9392 en Information Theoretic Measures for Clusterings Comparison: Is a Correction for Chance Necessary? Information theoretic based measures form a fundamental class of similarity measures for comparing clusterings, beside the class of pair-counting based and set-matching based measures. In this paper, we discuss the necessity of correction for chance for information theoretic based measures for clusterings comparison. We observe that the baseline for such measures, i.e. average value under random partitioning of a data set, does not take on a constant value, and tends to have larger variation when the ratio between the number of data points and the number of clusters is small. This effect is similar in some other non-information theoretic based measures such as the well-known Rand Index. Assuming a hypergeometric model of randomness, we derive the analytical formula for the expected mutual information value between a pair of clusterings, and then propose the adjusted version for several popular information theoretic based measures. Some examples are given to demonstrate the need and usefulness of the adjusted measures.
9393 en Fast Evolutionary Maximum Margin Clustering The maximum margin clustering approach is a recently proposed extension of the concept of support vector machines to the clustering problem. Briefly stated, it aims at finding an optimal partition of the data into two classes such that the margin induced by a subsequent application of a support vector machine is maximal. We propose a method based on stochastic search to address this hard optimization problem. While a direct implementation would be infeasible for large data sets, we present an efficient computational shortcut for assessing the ``quality'' of intermediate solutions. Experimental results show that our approach outperforms existing methods in terms of clustering accuracy.
9394 en Discriminative k-Metrics The $k$ $q$-flats algorithm is a generalization of the popular $k$-means algorithm where $q$ dimensional best fit affine sets replace centroids as the cluster prototypes. In this work, a modification of the $k$ $q$-flats framework for pattern classification is introduced. The basic idea is to replace the original reconstruction only energy, which is optimized to obtain the $k$ affine spaces, by a new energy that incorporates discriminative terms. This way, the actual classification task is introduced as part of the design and optimization. The presentation of the proposed framework is complemented with experimental results, showing that the method is computationally very efficient and gives excellent results on standard supervised learning benchmarks.
9395 en Orbit-Product Representation and Correction of Gaussian Belief Propagation We present a new view of Gaussian beliefnpropagation (GaBP) based on a representa-ntion of the determinant as a product over or-nbits of a graph. We show that the GaBPndeterminant estimate captures totally back-ntracking orbits of the graph and consider hownto correct this estimate. We show that thenmissing orbits may be grouped into equiva-nlence classes corresponding to backtracklessnorbits and the contribution of each equiv-nalence class is easily determined from thenGaBP solution. Furthermore, we demon-nstrate that this multiplicative correction fac-ntor can be interpreted as the determinant of anbacktrackless adjacency matrix of the graphnwith edge weights based on GaBP. Finally,nan efficient method is proposed to computena truncated correction factor including allnbacktrackless orbits up to a specified length.
9396 en Convex Variational Bayesian Inference for Large Scale Generalized Linear Models We show how variational Bayesian inference can be implemented for very large generalized linear models. Our relaxation is proven to be a convex problem for any log-concave model. We provide a generic double loop algorithm for solving this relaxation on models with arbitrary super-Gaussian potentials. By iteratively decoupling the criterion, most of the work can be done by solving large linear systems, rendering our algorithm orders of magnitude faster than previously proposed solvers for the same problem. We evaluate our method on problems of Bayesian active learning for large binary classification models, and show how to address settings with many candidates and sequential inclusion steps.
9397 en Archipelago: Nonparametric Bayesian Semi-Supervised Learning Semi-supervised learning (SSL), is classificationnwhere additional unlabeled data can benused to improve accuracy. Generative approachesnare appealing in this situation, asna model of the data’s probability density cannassist in identifying clusters. NonparametricnBayesian methods, while ideal in theory duento their principled motivations, have been difficultnto apply to SSL in practice. We presentna nonparametric Bayesian method that usesnGaussian processes for the generative model,navoiding many of the problems associatednwith Dirichlet process mixture models. Ournmodel is fully generative and we take advantagenof recent advances in Markov chainnMonte Carlo algorithms to provide a practicalninference method. Our method comparesnfavorably to competing approaches on syntheticnand real-world multi-class data.
9398 en The Bayesian Group-Lasso for Analyzing Contingency Tables Group-Lasso estimators, useful in many applications,nsuffer from lack of meaningful variance estimatesnfor regression coefficients. To overcomensuch problems, we propose a full Bayesian treatmentnof the Group-Lasso, extending the standardnBayesian Lasso, using hierarchical expansion.nThe method is then applied to Poisson modelsnfor contingency tables using a highly efficientnMCMC algorithm. The simulated experimentsnvalidate the performance of this method on artificialndatasets with known ground-truth. Whennapplied to a breast cancer dataset, the methodndemonstrates the capability of identifying the differencesnin interactions patterns of marker proteinsnbetween different patient groups.
9399 en Split Variational Inference We propose a deterministic method to eval-nuate the integral of a positive function basednon soft-binning functions that smoothly cutnthe integral into smaller integrals that areneasier to approximate. In combination withnmean-field approximations for each individ-nual sub-part this leads to a tractable algo-nrithm that alternates between the optimiza-ntion of the bins and the approximation of thenlocal integrals. We introduce suitable choicesnfor the binning functions such that a stan-ndard mean field approximation can be ex-ntended to a split mean field approximationnwithout the need for extra derivations. Thenmethod can be seen as a revival of the ideasnunderlying the mixture mean field approach.nThe latter can be obtained as a special casenby taking soft-max functions for the binning.
9400 en Proto-Predictive Representation of States with Simple Recurrent Temporal-Difference Networks We propose a new neural network architecture, called Simple Recurrent Temporal-Difference Networks (SR-TDNs), that learns to predict future observations in partially observable environments. SR-TDNs incorporate the structure of simple recurrent neural networks (SRNs) into temporal-difference (TD) networks to use proto-predictive representation of states. Although they deviate from the principle of predictive representations to ground state representations on observations, they follow the same learning strategy as TD networks, i.e., applying TD-learning to general predictions. Simulation experiments revealed that SR-TDNs can correctly represent states with incomplete set of core tests (question networks), and consequently, SR-TDNs have better on-line learning capacity than TD networks in various environments.
9401 en Regularization and Feature Selection in Least Squares Temporal-Difference Learning We consider the task of reinforcement learning with linear value function approximation. Temporal difference algorithms, and in particular the Least-Squares Temporal Difference (LSTD) algorithm, provide a method for learning the parameters of the value function, but when the number of features is large this algorithm can over-fit to the data and is computationally expensive. In this paper, we propose a regularization framework for the LSTD algorithm that overcomes these difficulties. In particular, we focus on the case of l1 regularization, which is robust to irrelevant features and also serves as a method for feature selection. Although the l1 regularized LSTD solution cannot be expressed as a convex optimization problem, we present an algorithm similar to the Least Angle Regression (LARS) algorithm that can efficiently compute the optimal solution. Finally, we demonstrate the performance of the algorithm experimentally.
9402 en Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation Sutton, Szepesvari and Maei (2009) recently introduced the first temporal-difference learning algorithm compatible with both linear function approximation and off-policy training, and whose complexity scales only linearly in the size of the function approximator. Although their gradient temporal difference (GTD) algorithm converges reliably, it can be very slow compared to conventional linear TD (on on-policy problems where TD is convergent), calling into question its practical utility. In this paper we introduce two new related algorithms with better convergence rates. The first algorithm, GTD2, is derived and proved convergent just as GTD was, but uses a different objective function and converges significantly faster (but still not as fast as conventional TD). The second new algorithm, linear TD with gradient correction, or TDC, uses the same update rule as conventional TD except for an additional term which is initially zero. In our experiments on small test problems and in a Computer Go application with a million features, the learning rate of this algorithm was comparable to that of conventional TD. This algorithm appears to extend linear TD to off-policy learning with no penalty in performance while only doubling computational requirements.
9403 en Kernelized Value Function Approximation for Reinforcement Learning A recent surge in research in kernelized approaches to reinforcement learning has sought to bring the benefits of kernelized machine learning techniques to reinforcement learning. Kernelized reinforcement learning techniques are fairly new and different authors have approached the topic with different assumptions and goals. Neither a unifying view nor an understanding of the pros and cons of different approaches has yet emerged. In this paper, we offer a unifying view of the different approaches to kernelized value function approximation for reinforcement learning. We show that, except for different approaches to regularization, Kernelized LSTD (KLSTD) is equivalent to a model based approach that uses kernelized regression to find an approximate reward and transition model, and that Gaussian Process Temporal Difference learning (GPTD) returns a mean value function that is equivalent to these other approaches. We also demonstrate the relationship between our model based approach and the earlier Gaussian Processes in Reinforcement Learning (GPRL). Finally, we decompose the Bellman error into the sum of transition error and reward error terms, and demonstrate through experiments that this decomposition can be helpful in choosing regularization parameters.
9404 en Constraint Relaxation in Approximate Linear Programs Approximate linear programming (ALP) is a reinforcement learning technique with nice theoretical properties, but it often performs poorly in practice. We identify some reasons for the poor quality of ALP solutions in problems where the approximation induces virtual loops. We then introduce two methods for improving solution quality. One method rolls out selected constraints of the ALP, guided by the dual information. The second method is a relaxation of the ALP, based on external penalty methods. The latter method is applicable in domains in which rolling out constraints is impractical. Both approaches show promising empirical results for simple benchmark problems as well as for a more realistic blood inventory management problem.
9405 en Evaluation Methods for Topic Models A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact ncomputation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including nthe harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods nare unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and nefﬁcient.
9406 en Accounting for Burstiness in Topic Models Many different topic models have been used successfully for a variety of applications. However, even state-of-the-art topic models nsuffer from the important ﬂaw that they do not capture the tendency of words to appear in bursts; it is a fundamental property of language nthat if a word is used once in a document, it is more likely to be used again. We introduce a topic model that uses Dirichlet compound nmultinomial (DCM) distributions to model this burstiness phenomenon. On both text and non-text datasets, the new model achieves better nheld-out likelihood than standard latent Dirichlet allocation (LDA). It is straightforward to incorporate the DCM extension into topic models nthat are more complex than LDA.
9407 en Topic-Link LDA: Joint Models of Topic and Author Community Given a large-scale linked document collection, such as a collection of blog posts or a research literature archive, there are two nfundamental problems that have generated a lot of interest in the research community. One is to identify a set of high-level topics covered nby the documents in the collection; the other is to uncover and analyze the social network of the authors of the documents. So far these nproblems have been viewed as separate problems and considered independently from each other. In this paper we argue that these two nproblems are in fact inter-dependent and should be addressed together. We develop a Bayesian hierarchical approach that performs topic nmodeling and author community discovery in one uniﬁed framework. The effectiveness of our model is demonstrated on two blog data sets nin different domains and one research paper citation data from CiteSeer.
9408 en MedLDA: Maximum Margin Supervised Topic Models for Regression and Classiﬁcation Supervised topic models utilize document’s side information for discovering predictive low dimensional representations of documents; nand existing models apply likelihood-based estimation. In this paper, we present a max-margin supervised topic model for both continuous nand categorical response variables. Our approach, the maximum entropy discrimination latent Dirichlet allocation (MedLDA), utilizes the nmax-margin principle to train supervised topic models and estimate predictive topic representations that are arguably more suitable for nprediction. We develop efﬁcient variational methods for posterior inference and demonstrate qualitatively and quantitatively the advantages nof MedLDA over likelihood-based topic models on movie review and 20 Newsgroups data sets.
9409 en Independent Factor Topic Models Topic models such as Latent Dirichlet Allocation (LDA) and Correlated Topic Model (CTM) have recently emerged as powerful nstatistical tools for text document modeling. In this paper, we improve upon CTM and propose Independent Factor Topic Models (IFTM) nwhich use linear latent variable models to uncover the hidden sources of correlation between topics. There are 2 main contributions of this nwork. First, by using a sparse source prior model, we can directly visualize sparse patterns of topic correlations. Secondly, the conditional nindependence assumption implied in the use of latent source variables allows the objective function to factorize, leading to a fast Newton- nRalphson based variational inference algorithm. Experimental results on synthetic and real data show that IFTM runs on average 3-5 times nfaster than CTM, while giving competitive performance as measured by perplexity and log-likelihood of held-out data.
9410 en Semi-Supervised Learning Using Label Mean Semi-Supervised Support Vector Machines (S3VMs) typically directly estimate the label assignments for the unlabeled instances. This is often inefficient even with recent advances in the efficient training of the (supervised) SVM. In this paper, we show that S3VMs, with knowledge of the means of the class labels of the unlabeled data, is closely related to the supervised SVM with known labels on all the unlabeled data. This motivates us to first estimate the label means of the unlabeled data. Two versions of the meanS3VM, which work by maximizing the margin between the label means, are proposed. The first one is based on multiple kernel learning, while the second one is based on alternating optimization. Experiments show that both of the proposed algorithms achieve highly competitive and sometimes even the best performance as compared to the state-of-the-art semi-supervised learners. Moreover, they are more efficient than existing S3VMs.
9411 en Partially Supervised Feature Selection with Regularized Linear Models This paper addresses feature selection techniques for classification of high dimensional data, such as those produced by microarray experiments. Some prior knowledge may be available in this context to bias the selection towards some dimensions (genes) a priori assumed to be more relevant. We propose a feature selection method making use of this partial supervision. It extends previous works on embedded feature selection with linear models including regularization to enforce sparsity. A practical approximation of this technique reduces to standard SVM learning with iterative rescaling of the inputs. The scaling factors depend here on the prior knowledge but the final selection may depart from it. Practical results on several microarray data sets show the benefits of the proposed approach in terms of the stability of the selected gene lists with improved classification performances.
9412 en Optimal Reverse Prediction: A Unified Perspective on Supervised, Unsupervised and Semi-Supervised Learning raining principles for unsupervised learning are often derived from motivations that appear to be independent of supervised learning, causing a proliferation of semisupervised training methods. In this paper we present a simple unification of several supervised and unsupervised training principles through the concept of optimal reverse prediction: predict the inputs from the target labels, optimizing both over model parameters and any missing labels. In particular, we show how supervised least squares, principal components analysis, k-means clustering and normalized graph-cut clustering can all be expressed as instances of the same training principle, differing only in constraints made on the target labels. Natural forms of semi-supervised regression and classification are then automatically derived, yielding semi-supervised learning algorithms for regression and classification that, surprisingly, are novel and refine the state of the art. These algorithms can all be combined with standard regularizers and made non-linear via kernels.
9413 en Supervised Learning from Multiple Experts: Whom to Trust When Everyone Lies a Bit We describe a probabilistic approach for supervised learning when we have multiple experts/annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method clearly beats the commonly used majority voting baseline.
9414 en Good Learners for Evil Teachers We consider a supervised machine learning scenario where labels are provided by a heterogeneous set of teachers, some of which are mediocre, incompetent, or perhaps even malicious. We present an algorithm, built on the SVM framework, that explicitly attempts to cope with low-quality and malicious teachers by decreasing their influence on the learning process. Our algorithm does not receive any prior information on the teachers, nor does it resort to repeated labeling (where each example is labeled by multiple teachers). We provide a theoretical analysis of our algorithm and demonstrate its merits empirically. Finally, we present a second algorithm with promising empirical results but without a formal analysis.
9415 en Stochastic Methods for L1 Regularized Loss Minimization We describe and analyze two stochastic methods for $\ell_1$ regularized loss minimization problems, such as the Lasso. The first method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature/example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.
9416 en Blockwise Coordinate Descent Procedures for the Multi-Task Lasso with Applications to Neural Semantic Basis Discovery We develop a cyclical blockwise coordinate descent algorithm for the multi-task Lasso that efficiently solves problems with thousands of features and tasks. The main result shows that a closed-form Winsorization operator can be obtained for the sup-norm penalized least squares regression. This allows the algorithm to find solutions to very large-scale problems far more efficiently than existing methods. This result complements the pioneering work of Friedman, et al. (2007) for the single-task Lasso. As a case study, we use the multi-task Lasso as a variable selector to discover a semantic basis for predicting human neural activation. The learned solution outperforms the standard basis for this task on the majority of test participants, while requiring far fewer assumptions about cognitive neuroscience. We demonstrate how this learned basis can yield insights into how the brain represents the meanings of words.
9417 en An Efficient Projection for L1 Infinity Regularization In recent years the L1,Infinity norm has been proposed for joint regularization. In essence, this type of regularization aims at extending the L1 framework for learning sparse models to a setting where the goal is to learn a set of jointly sparse models. In this paper we derive a simple and effective projected gradient method for optimization of L1,Infinity regularized problems. The main challenge in developing such a method resides on being able to compute efficient projections to the L1,Infinity ball. We present an algorithm that works in O(n log n) time and O(n) memory where n is the number of parameters. We test our algorithm in a multi-task image annotation problem. Our results show that L1,Infinity leads to better performance than both L2 and L1 regularization and that it is is effective in discovering jointly sparse solutions.
9418 en An Accelerated Gradient Method for Trace Norm Minimization We consider the minimization of a smooth loss function regularized by the trace norm of the matrix variable. Such formulation finds applications in many machine learning tasks including multi-task learning, matrix classification, and matrix completion. The standard semidefinite programming formulation for this problem is computationally expensive. In addition, due to the non-smoothness nature of the trace norm, the optimal first-order black-box method for solving such class of problems converges as O(1/sqrt(k)), where k is the iteration counter. In this paper, we exploit the special structure of the trace norm, based on which we propose an extended gradient algorithm that converges as O(1/k). We further propose an accelerated gradient algorithm, which achieves the optimal convergence rate of O(1/k^2) for smooth problems. Experiments on multi-task learning problems demonstrate the efficiency of the proposed algorithms.
9419 en Group Lasso with Overlaps and Graph Lasso We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of covariates defined a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theoretical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.
9420 en Uniting "a priori" and "a posteriori" knowledge: A research framework The ability to perform machine classification is ancritical component of an intelligent system. Wenpropose to unite the logical, a priori approachnto this problem with the empirical, a posteriorinapproach. We describe in particular how the anpriori knowledge encoded in Cyc can be mergednwith technology for probabilistic inference usingnMarkov logic networks. We describe two problemndomains – the Whodunit Problem and noun phrasenunderstanding – and show that Cyc’s commonsensenknowledge can be fruitfully combined with probabilisticnreasoning.
9421 en Denotation as a Two-Step Mapping in Semantic Web Architecture In RDF, URIs are used to denote resources -- thingsnin the universe of discourse. According to RDFnsemantics, an interpretation defines the mappingnfrom a URI to a resource. Many interpretations maynbe consistent with a given RDF graph, and RDFnsemantics does not specify how to select a suitableninterpretation from among the possible candidates.nIn other writings the author has advocated that innsemantic web architecture, such denotation shouldnbe viewed as a two-step mapping: from the URI to anset of core assertions specified in a URI declaration,nand thence to the resource. The reason for this viewnis that it permits a consistent resource identity to benassociated with a URI: the constraints expressed innthe URI declaration represent a common identity fornthat URI. This paper shows how this view ofndenotation corresponds to established RDFnsemantics.
9422 en From unstructured to linked data: Entity extraction and disambiguation by collective similarity maximization In this paper, we describe a pipeline of methods fornidentifying and resolving entities from unstructuredndata using a semi-structured background knowledgendatabase. For this purpose, we employ namednentity extraction, co-reference resolution and investigatenperformance of disambiguation using collectivenmaximization of inter-entity similarity, comparednto using only pair-wise disambiguation. Wenexplore possibilities of using DBpedia and Yago asnbackground knowledge databases with the goal ofnannotating unstructured text documents with globalnentity references.
9423 en The URI Lifecycle in Semantic Web Architecture Abstract. Various parties are typically involved innthe creation and use of a URI, including the URInowner, an RDF statement author, and a consumer ofnthat RDF statement. What principles should thesenparties follow, to ensure that a consistent resourcenidentity is established and (to the extent possible)nmaintained throughout that URI's lifetime? Thisnpaper proposes a set of roles and responsibilities fornestablishing and determining a URI's resourcenidentity through its lifecycle.
9424 en Identity and Reference for the Global Giant Graph In this paper we address the issue of how data onnthe Global Giant Graph (GGG) can be used to answernglobal queries. We start with a formal modelnfor the GGG, and then we use it to provide a formalnspecification of three very general modes fornanswering a query on the GGG, called bounded,nnavigational and direct access mode respectively.nIn the final discussion, we connect our model to recentndiscussions on URI reference and identity innthe Semantic Web community.
9425 en Sig.ma: Entity-centric search on the Web 
9426 en Final discussion and wrap-up 
9427 en Web datasets integration with RDF-AI With the recent publication of large quantitiesnof RDF data, the Semantic Web now allowsnconcrete applications to be developed. Multiplendatasets are effectively published accordingnto the linked-data principles. Integrating thesendatasets through interlink or fusion is needednin order to assure interoperability between thenresources composing them. There is thus angrowing need for tools providing datasets management.nWe present in this paper RDF-AI, anframework and a tool for managing the integrationnof RDF datasets. The framework includesnfive modules for pre-processing, matching, fusing,ninterlinking and post-processing datasets.nThe framework inplementation results in a toolnproviding RDF datasets integration functionalitiesnin a linked-data context. Evaluation ofnRDF-AI on existing datasets shows promisingnresults towards a Semantic Web aware datasetsnintegration tool.
9428 en Text Mining and Link Analysis The tutorial on Text Mining and Link Analysis for Web Data will focus on two main analytical approaches when analyzing web data: text mining and link analysis for the purpose of analyzing web documents and their linkage. First, the tutorial will cover some basic steps and problems when dealing with the textual and network (graph) data showing what is possible to achieve without very sophisticated technology. The idea of this first part is to present the nature of unstructured and semi-structured data. Next, in the second part, more sophisticated methods for solving more difficult and challenging problems will be shown. In the last part, some of the current open research issues will be presented and some practical pointers on the available tolls for solving previously mentioned problems will be provided.
9429 en Gaussian processes for Bayesian Filtering 
9430 en Learning vehicular dynamics models with application to helicopter modeling and control 
9431 en Optimized Information Gatheringin Robotics and Sensor Networks 
9432 en Adaptation and Self-Supervision in Mobile Robots Poster Spotlight Presentations 
9433 en The Role of Function Approximation for both Regression and Classifiction in Robotics 
9434 en Going forward with Probablistic Local Learning Approaches 
9435 en Function Approximation for Imitation Learning in Humanoid Robots 
9436 en Reinforcement Learning by Reward-Weighted Regression 
9437 en Panel Discussion 
9438 en Bayesian Clustering for Email Campaign Detection We discuss the problem of clustering elements according to the sources that have generated them. For elements that are characterized nby independent binary attributes, a closed-form Bayesian solution exists. We derive a solution for the case of dependent attributes that is nbased on a transformation of the instances into a space of independent feature functions. We derive an optimization problem that produces a nmapping into a space of independent binary feature vectors; the features can reﬂect arbitrary dependencies in the input space. This problem nsetting is motivated by the application of spam ﬁltering for email service providers. Spam traps deliver a real-time stream of messages nknown to be spam. If elements of the same campaign can be recognized reliably, entire spam and phishing campaigns can be contained. nWe present a case study that evaluates Bayesian clustering for this application.
9439 en A Novel Lexicalized HMM-Based Learning Framework for Web Opinion Mining Merchants selling products on the Web often ask their customers to share their opinions and hands-on experiences on products they have npurchased. As e-commerce is becoming more and more popular, the number of customer reviews a product receives grows rapidly. This nmakes it difﬁcult for a potential customer to read them to make an informed decision on whether to purchase the product. In this research, nwe aim to mine customer reviews of a product and extract highly speciﬁc product related entities on which reviewers express their opinions. nOpinion expressions and sentences are also identiﬁed and opinion orientations for each recognized product entity are classiﬁed as positive nor negative. Different from previous approaches that have mostly relied on natural language processing techniques or statistic information, nwe propose a novel machine learning framework using lexicalized HMMs. The approach naturally integrates linguistic features, such as npart-of-speech and surrounding contextual clues of words into automatic learning. The experimental results demonstrate the effectiveness nof the proposed approach in web opinion mining and extraction from product reviews.
9440 en Learning Spectral Graph Transformations for Link Prediction We present a uniﬁed framework for learning link prediction and edge weight prediction functions in large networks, based on the ntransformation of a graph’s algebraic spectrum. Our approach generalizes several graph kernels and dimensionality reduction methods and nprovides a method to estimate their parameters efﬁciently. We show how the parameters of these prediction functions can be learned by nreducing the problem to a one-dimensional regression problem whose runtime only depends on the method’s reduced rank and that can nbe inspected visually. We derive variants that apply to undirected, weighted, unweighted, unipartite and bipartite graphs. We evaluate our nmethod experimentally using examples from social networks, collaborative ﬁltering, trust networks, citation networks, authorship graphs nand hyperlink networks.
9441 en Interactively Optimizing Information Retrieval Systems as a Dueling Bandits Problem We present an online learning framework tailored towards real-time learning from observed user behavior in search engines and other information retrieval systems. In particular, we only require pairwise comparisons, which were shown to be reliably inferred from implicit feedback. We will present an algorithm with theoretical guarantees as well as simulation results.
9442 en Transfer Learning for Collaborative Filtering via a Rating-Matrix Generative Model Cross-domain collaborative ﬁltering solves the sparsity problem by transferring rating knowledge across multiple domains. In this paper, we propose a rating-matrix generative model (RMGM) for effective cross-domain collaborative ﬁltering. We ﬁrst show that the relatedness across multiple rating matrices can be established by ﬁnding a shared implicit cluster-level rating matrix, which is next extended nto a cluster-level rating model. Consequently, a rating matrix of any related task can be viewed as drawing a set of users and items from a user-item joint mixture model as well as drawing the corresponding ratings from the cluster-level rating model. The combination of these two models gives the RMGM, which can be used to ﬁll the missing ratings for both existing and new users. A major advantage of RMGM is that it can share the knowledge by pooling the rating data from multiple tasks even when the users and items of these tasks do not overlap. nWe evaluate the RMGM empirically on three real-world collaborative ﬁltering data sets to show that RMGM can outperform the individual models trained separately.
9443 en Curriculum Learning Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculumnlearning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neuralnnetworks), we explore curriculum learning in various set-ups. The experiments show that signicant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation methodn(a general strategy for global optimization of non-convex functions).
9444 en Herding Dynamical Weights to Learn A new "herding" algorithm is proposed which directly converts observed moments into a sequence of pseudo-samples. The pseudo-samples respect the moment constraints and may be used to estimate (unobserved) quantities of interest. The procedure allows us to sidestep the usual approach of first learning a joint model (which is intractable) and then sampling from that model (which can easily get stuck in a local mode). Moreover, the algorithm is fully deterministic, avoiding random number generation) and does not need expensive operations such as exponentiation.
9445 en Sequential Bayesian Prediction in the Presence of Changepoints We introduce a new sequential algorithmnfor making robust predictions in the presence of changepoints. Unlike previous approaches, which focus on the problem of detecting and locating changepoints, our algorithm focuses on the problem of making predictions even when such changes might be present. We introduce nonstationary co-variance functions to be used in Gaussian process prediction that model such changes, then proceed to demonstrate how to effectively manage the hyperparameters associated with those covariance functions. By using Bayesian quadrature, we can integrate out the hyperparameters, allowing us to calculate the marginal predictive distribution.nFurthermore, if desired, the posterior distribution over putative changepoint locations can be calculated as a natural byproduct of our prediction algorithm.
9446 en Model-Free Reinforcement Learning as Mixture Learning We cast model-free reinforcement learning as the problem of maximizing the likelihood of a probabilistic mixture model via sampling, addressing both the innite and nite horizon cases. We describe a Stochastic ApproximationnEM algorithm for likelihood maximizationnthat, in the tabular case, is equivalentnto a non-bootstrapping optimistic policy iteration algorithm like Sarsa(1) that can be applied both in MDPs and POMDPs. On the theoretical side, by relating the proposednstochastic EM algorithm to the family of optimistic policy iteration algorithms, we provide new tools that permit the design and analysis of algorithms in that family. On thenpractical side, preliminary experiments on a POMDP problem demonstrated encouraging results.
9447 en Active Learning for Directed Exploration of Complex Systems Physics-based simulation codes are widely used in science and engineering to model complex systems that would be infeasible to study otherwise. Such codes provide the highest-fidelity representation of system behavior, but are often so slow to run that insightninto the system is limited. For example,nconducting an exhaustive sweep over and-dimensional input parameter space with k-steps along each dimension requires kd simulation trials (translating into kd CPU-days for one of our current simulations). An alternative is directed exploration in which the next simulation trials are cleverly chosen at each step. Given the results of previous trials, supervised learning techniques (SVM, KDE, GP) are applied to build up simplified predictive models of system behavior.nThese models are then used within an active learning framework to identify the most valuable trials to run next. Several active learning strategies are examined including a recently-proposed information-theoretic approach.nPerformance is evaluated on a set of thirteen synthetic oracles, which serve as surrogates for the more expensive simulations and enable the experiments to be replicated by other researchers.
9448 en Bayesian Inference for Plackett-Luce Ranking Models This paper gives an efficient Bayesian method for inferring the parameters of a Plackett-Luce ranking model. Such models are parameterised distributions over rankings of a nite set of objects, and have typically been studied and applied within the psychometric, sociometric and econometric literature. The inference scheme is an application of Power EP (expectation propagation). The scheme is robust and can be readily applied to large scale data sets. The inference algorithm extends tonvariations of the basic Plackett-Luce model, including partial rankings. We show a number of advantages of the EP approach over the traditional maximum likelihood method. We apply the method to aggregate rankings of NASCAR racing drivers over the 2002 season, and also to rankings of movie genres.
9449 en Incorporating Domain Knowledge into Topic Modeling via Dirichlet Forest Priors Users of topic modeling methods often have knowledge about the composition of words that should have high or low probability in various topics. We incorporate such domain knowledge using a novel Dirichlet Forest prior in a Latent Dirichlet Allocation framework.nThe prior is a mixture of Dirichlet tree distributions with special structures. We present its construction, and inference via collapsed Gibbs sampling. Experiments on synthetic and real datasets demonstrate our model’s ability to follow and generalize beyond userspecified domain knowledge.
9450 en Nonparametric Factor Analysis with Beta Process Priors We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BPFA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on thenunderlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allowsnfor analytical posterior calculation andnstraightforward inference. We derive a variational Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.
9451 en Accelerated Gibbs Sampling for the Indian Buffet Process We often seek to identify co-occurring hidden features in a set of observations. The Indian Buffet Process (IBP) provides a nonparametric prior on the features present in each observation, but current inference techniques for the IBP often scale poorly. The collapsed Gibbs sampler for the IBP has a running time cubic in the number of observations, and the uncollapsed Gibbs sampler, while linear, is often slow to mix. We presentna new linear-time collapsed Gibbs sampler for conjugate likelihood models and demonstrate its efficacy on large real-world datasets.
9452 en A Stochastic Memoizer for Sequence Data We propose an unbounded-depth, hierarchical, Bayesian nonparametric model for discrete sequence data. This model can be estimated from a single training sequence, yet shares statistical strength between subsequentnsymbol predictive distributions in suchna way that predictive performance generalizes well. The model builds on a specific parameterization of an unbounded-depth hierarchical Pitman-Yor process. We introduce analytic marginalization steps (using coagulationnoperators) to reduce this model to onenthat can be represented in time and space linear in the length of the training sequence. We show how to perform inference in such a model without truncation approximation and introduce fragmentation operators necessary to do predictive inference. We demonstrate the sequence memoizer by using it as a language model, achieving state-of-the-art results.
9453 en Binary Action Search for Learning Continuous-Action Control Policies Reinforcement Learning methods for controlling stochastic processes typically assume a small and discrete action space. While continuous action spaces are quite common in real-world problems, the most common approach still employed in practice is coarse discretization of the action space. This paper presents a novel method, called Binary Action Search, for realizing continuous-action policies by searching efficiently the entire action range through increment and decrement modifications to the values of the action variables according to an internal binary policy defined over an augmented state space. The proposed approach essentially approximates any continuous action space to arbitrary resolution and can be combined with any discrete-action reinforcement learning algorithm for learning continuous-action policies. Binary Action Search eliminates the restrictive modification steps of Adaptive Action Modification and requires no temporal action locality in the domain. Our approach is coupled with two well-known reinforcement learning algorithms (Least-Squares Policy Iteration and Fitted Q-Iteration) and its use and properties are thoroughly investigated and demonstrated on the continuous state-action Inverted Pendulum, Double Integrator, and Car on the Hill domains.
9454 en Predictive Representations for Policy Gradient in POMDPs We consider the problem of estimating the policy gradient in Partially Observable Markov Decision Processes (POMDPs) with a special class of policies that are based on Predictive State Representations (PSRs). We compare PSR policies to Finite-State Controllers (FSCs), which are considered as a standard model for policy gradient methods in POMDPs. We present a general actor-critic algorithm for learning both FSCs and PSR policies. The critic part computes a value function that has as variables the parameters of the policy. These latter parameters are gradually updated to maximize the value function. We show that the value function is polynomial for both FSCs and PSR policies, with a potentially smaller degree in the case of PSR policies. Therefore, the value function of a PSR policy can have less local optima than the equivalent FSC, and consequently, the gradient algorithm is more likely to converge to a global optimal solution.
9455 en Stochastic Search Using the Natural Gradient To optimize unknown `fitness' functions, we introduce Natural Search, a novel stochastic search method that constitutes a principled alternative to standard evolutionary methods. It maintains a multinormal distribution on the set of solution candidates. The Natural Gradient is used to update the distribution's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information matrix whereas previous methods had to use approximations. Other novel aspects of our method include optimal fitness baselines and importance mixing, a procedure adjusting batches with minimal numbers of fitness evaluations. The algorithm yields competitive results on a number of benchmarks.
9456 en Approximate Inference for Planning in Stochastic Relational Worlds Relational world models that can be learned from experience in stochastic domains have received significant attention recently. However, efficient planning using these models remains a major issue. We propose to convert learned noisy probabilistic relational rules into a structured dynamic Bayesian network representation. Predicting the effects of action sequences using approximate inference allows for planning in complex worlds. We evaluate the effectiveness of our approach for online planning in a 3D simulated blocksworld with an articulated manipulator and realistic physics. Empirical results show that our method can solve problems where existing methods fail.
9457 en Discovering Options from Example Trajectories We present a novel technique for automated problem decomposition to address the problem of scalability in Reinforcement Learning. Our technique makes use of a set of near-optimal trajectories to discover {\it options} and incorporates them into the learning process, dramatically reducing the time it takes to solve the underlying problem. We run a series of experiments in two different domains and show that our method offers up to 30 fold speedup over the baseline.
9458 en Nonparametric Estimation of the Precision-Recall Curve The Precision-Recall (PR) curve is a widely used visual tool to evaluate the performance of scoring functions in regards to their capacities to discriminate between two populations. The purpose of this paper is to examine both theoretical and practical issues related to the statistical estimation of PR curves based on classification data. Consistency and asymptotic normality of the empirical counterpart of the PR curve in sup norm are rigorously established. Eventually, the issue of building confidence bands in the PR space is considered and a specific resampling procedure based on a smoothed and truncated version of the empirical distribution of the data is promoted. Arguments of theoretical and computational nature are presented to explain why such a bootstrap is preferable to a "naive" bootstrap in this setup.
9459 en Surrogate Regret Bounds for Proper Losses We present tight surrogate regret bounds for the class of proper (i.e., Fisher consistent) losses. The bounds generalise the margin-based bounds due to Bartlett et al. (2006). The proof uses Taylor's theorem and leads to new representations for loss and regret and a simple proof of the integral representation of proper losses. We also present a different formulation of a duality result of Bregman divergences which leads to a demonstration of the convexity of composite losses using canonical link functions.
9460 en Robust Bounds for Classification via Selective Sampling We introduce a new algorithm for binary classification in the selective sampling protocol. Our algorithm uses Regularized Least Squares (RLS) as base classifier, and for this reason it can be efficiently run in any RKHS. Unlike previous margin-based semi-supervised algorithms, our sampling condition hinges on a simultaneous upper bound on bias and variance of the RLS estimate under a simple linear label noise model. This fact allows us to prove performance bounds that hold for an arbitrary sequence of instances. In particular, we show that our sampling strategy approximates the margin of the Bayes optimal classifier to any desired accuracy $\ve$ by asking $\widetilde{\scO}\bigl(d/\ve^2\bigr)$ queries (in the RKHS case $d$ is replaced by a suitable spectral quantity). While these are the standard rates in the fully supervised i.i.d.\ case, the best previously known result in our harder setting was $\widetilde{\scO}\bigl(d^3/\ve^4\bigr)$. Preliminary experiments show that some of our algorithms also exhibit a good practical performance.
9461 en PAC-Bayesian Learning of Linear Classifiers We present a general PAC-Bayes theorem from which all known PAC-Bayes bounds are simply obtained as particular cases. We also propose different learning algorithms for finding linear classifiers that minimize these PAC-Bayes risk bounds. These learning algorithms are generally competitive with both AdaBoost and the SVM.
9462 en Piecewise-Stationary Bandit Problems with Side Observations We consider a sequential decision problem where the rewards are generated by a piecewise-stationary distribution. However, the different reward distributions are unknown and may change at unknown instants. Our approach uses a limited number of side observations on past rewards, but does not require prior knowledge of the frequency of changes. In spite of the adversarial nature of the reward process, we provide an algorithm whose regret, with respect to the baseline with perfect knowledge of the distributions and the changes, is O(k \log(T), where k is the number of changes up to time T. This is in contrast to the case where side observations are not available, and where the regret is at least Omega(sqrt{T}).
9463 en Bandit-Based Optimization on Graphs with Application to Library Performance Tuning The problem of choosing fast implementations for a class of recursive algorithms such as the fast Fourier transforms can be formulated nas an optimization problem over the language generated by a suitably deﬁned grammar. We propose a novel algorithm that solves this nproblem by reducing it to maximizing an objective function over the sinks of a directed acyclic graph. This algorithm valuates nodes using nMonte-Carlo and grows a subgraph in the most promising directions by considering local maximum k-armed bandits. When used inside nan adaptive linear transform library, it cuts down the search time by an order of magnitude compared to the existing algorithm. In some ncases, the performance of the implementations found is also increased by up to 10% which is of considerable practical importance since it nconsequently improves the performance of all applications using the library.
9464 en Robust Feature Extraction via Information Theoretic Learning In this paper, we present a robust feature extraction framework based on information-theoretic learning. Its formulated objective aims nat dual targets, motivated by the Renyi’s quadratic entropy of the features and the Renyi’s cross entropy between features and class labels, nrespectively. This objective function reaps the advantages in robustness from both redescending M-estimator and manifold regularization, nand can be efﬁciently optimized via half-quadratic optimization in an iterative manner. In addition, the popular algorithms LPP, SRDA and nLapRLS for feature extraction are all justiﬁed to be the special cases within this framework. Extensive comparison experiments on several nreal-world data sets, with contaminated features or labels, well validate the encouraging gain in algorithmic robustness from this proposed nframework.
9465 en Block-Wise Construction of Acyclic Relational Features with Monotone Irreducibility and Relevancy Properties We describe an algorithm for constructing a set of acyclic conjunctive relational features by combining smaller conjunctive blocks. nUnlike traditional level-wise approaches which preserve the monotonicity of frequency, our block-wise approach preserves a form of nmonotonicity of the irreducibility and relevancy feature properties, which are important in propositionalization employed in the context nof classiﬁcation learning. With pruning based on these properties, our block-wise approach efﬁciently scales to features including tens of nﬁrst-order literals, far beyond the reach of state-of-the art propositionalization or inductive logic programming systems.
9466 en Rule Learning with Monotonicity Constraints In the ordinal classiﬁcation with monotonicity constraints, it is assumed that the class label should increase with increasing values on nthe attributes. In this paper we aim at formalizing the approach to learning with monotonicity constraints from statistical point of view, nwhich results in the algorithm for learning rule ensembles. The algorithm ﬁrst ”monotonizes” the data using a nonparametric classiﬁcation nprocedure and then generates rule ensemble consistent with the training set. The procedure is justiﬁed by a theoretical analysis and veriﬁed nin a computational experiment.
9467 en Grammatical Inference as a Principal Component Analysis Problem One of the main problems in probabilistic grammatical inference consists in inferring a stochastic language, i.e. a probability distribu- ntion, in some class of probabilistic models, from a sample of words independently drawn according to a ﬁxed unknown target distribution p. nHere we consider the class of rational stochastic languages composed of stochastic languages that can be computed by muliplicity automata, nwhich can be viewed as a generalization of probabilistic automata. Rational stochastic languages p have a useful algebraic characterization: nall the mappings up:v-¿p(uv) lie in a ﬁnite dimensional vector subspace Vp of the vector space R(E) composed of all real-valued functions ndeﬁned over E. Hence, a ﬁrst step in the grammatial inference process can consist in identifying the subspace Vp. In this paper, we study nthe possibility of using principal component analysis to achieve this task. We provide an inference algorithm which computes an estimate nof the target distribution. We prove sometheoreticalpropertiesofthisalgorithmandweprovideresultsfromnumericalsimulationsthatconﬁrm the relevance of our approach.
9468 en Polyhedral Outer Approximations with Application to Natural Language Parsing Recent approaches to learning structured predictors often require approximate inference for tractability; yet its effects on the learned model are unclear. Meanwhile, most learning algorithms act as if computational cost was constant within the model class. This paper sheds some light on the first issue by establishing risk bounds for max-margin learning with LP relaxed inference, and addresses the second issue by proposing a new paradigm that attempts to penalize "time-consuming" hypotheses. Our analysis relies on a geometric characterization of the outer polyhedra associated with the LP relaxation. We then apply these techniques to the problem of dependency parsing, for which a concise LP formulation is provided that handles non-local output features. A significant improvement is shown over arc-factored models.
9469 en On Primal and Dual Sparsity of Markov Networks Sparsity is a desirable property in high dimensional learning. The $\ell_1$-norm regularization can lead to primal sparsity, while max-margin methods achieve dual sparsity; but achieving both in a single structured prediction model remains difficult. This paper presents an $\ell_1$-norm max-margin Markov network ($\ell_1$-M$^3$N), which enjoys both primal and dual sparsity, and analyzes its connections to the Laplace max-margin Markov network (LapM$^3$N), which inherits the dual sparsity of max-margin models but is pseudo-primal sparse. We show that $\ell_1$-M$^3$N is an extreme case of LapM$^3$N when the regularization constant is infinity. We also show an equivalence between $\ell_1$-M$^3$N and an adaptive M$^3$N, from which we develop a robust EM-style algorithm for $\ell_1$-M$^3$N. We demonstrate the advantages of the simultaneously (pseudo-) primal and dual sparse models over the ones which enjoy either primal or dual sparsity on both synthetic and real data sets.
9470 en Learning Structural SVMs with Latent Variables We present a large-margin formulation and algorithm for structured output prediction that allows the use of latent variables. The paper identifies a particular formulation that covers a large range of application problems, while showing that the resulting optimization problem can generally be addressed using Concave-Convex Programming. The generality and performance of the approach is demonstrated on a motif-finding application, noun-phrase coreference resolution, and optimizing precision at k in information retrieval.
9471 en An Efficient Sparse Metric Learning in High-Dimensional Space via L1-Penalized Log-Determinant Regularization This paper proposes an efficient sparse metric learning algorithm in high dimensional space via an $\ell_1$-penalized log-determinant regularization. Compare to the most existing distance metric learning algorithms, the proposed algorithm exploits the sparsity nature underlying the intrinsic high dimensional feature space. This sparsity prior of learning distance metric serves to regularize the complexity of the distance model especially in the ``less example number $p$ and high dimension $d$" setting. Theoretically, by analogy to the covariance estimation problem, we find the proposed distance learning algorithm has a consistent result at rate $\mathcal O\left(\sqrt{\left( {m^2 \log d} \right)/n}\right)$ to the target distance matrix with at most $m$ nonzeros per row. Moreover, from the implementation perspective, this $\ell_1$-penalized log-determinant formulation can be efficiently optimized in a block coordinate descent fashion which is much faster than the standard semi-definite programming which has been widely adopted in many other advanced distance learning algorithms. We compare this algorithm with other state-of-the-art ones on various datasets and competitive results are obtained.
9472 en Learning Instance Specific Distances Using Metric Propagation In many real-world applications, such as image retrieval, it would be natural to measure the distances from one instance to others using \textit{instance specific distance} which captures the distinctions from the perspective of the concerned instance. However, there is no complete framework for learning instance specific distances since existing methods are incapable of learning such distances for test instance and unlabeled data. In this paper, we propose the ISD method to address this issue. The key of ISD is \textit{metric propagation}, that is, propagating and adapting metrics of individual labeled examples to individual unlabeled instances. We formulate the problem into a convex optimization framework and derive efficient solutions. Experiments show that ISD can effectively learn instance specific distances for labeled as well as unlabeled instances. The metric propagation scheme can also be used in other scenarios.
9473 en Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations There has been much interest in unsupervisednlearning of hierarchical generative modelsnsuch as deep belief networks. Scalingnsuch models to full-sized, high-dimensionalnimages remains a difficult problem. To addressnthis problem, we present the convolutional deep belief network, a hierarchical generativenmodel which scales to realistic imagensizes. This model is translation-invariant andnsupports efficient bottom-up and top-downnprobabilistic inference. Key to our approachnis probabilistic max-pooling, a novel techniquenwhich shrinks the representations of highernlayers in a probabilistically sound way. Ournexperiments show that the algorithm learnsnuseful high-level visual features, such as objectnparts, from unlabeled images of objectsnand natural scenes. We demonstrate excellentnperformance on several visual recognitionntasks and show that our model can performnhierarchical (bottom-up and top-down)ninference over full-sized images.
9474 en Using Fast Weights to Improve Persistent Contrastive Divergence The most commonly used learning algorithmnfor restricted Boltzmann machines is contrastive divergence which starts a Markovnchain at a data point and runs the chainnfor only a few iterations to get a cheap, lownvariance estimate of the sufficient statisticsnunder the model. Tieleman (2008) showednthat better learning can be achieved by estimating the model’s statistics using a smallnset of persistent ”fantasy particles” that arennot reinitialized to data points after eachnweight update. With sufficiently small weightnupdates, the fantasy particles represent thenequilibrium distribution accurately but to explain why the method works with much largernweight updates it is necessary to consider theninteraction between the weight updates andnthe Markov chain. We show that the weightnupdates force the Markov chain to mix fast,nand using this insight we develop an evennfaster mixing chain that uses an auxiliary setnof ”fast weights” to implement a temporarynoverlay on the energy landscape. The fastnweights learn rapidly but also decay rapidlynand do not contribute to the normal energynlandscape that defines the model.
9475 en Large-Scale Deep Unsupervised Learning Using Graphics Processors The promise of unsupervised learning methodsnlies in their potential to use vast amountsnof unlabeled data to learn complex, highlynnonlinear models with millions of free parameters.nWe consider two well-known unsupervisednlearning models, deep belief networksn(DBNs) and sparse coding, that have recentlynbeen applied to a flurry of machine learningnapplications (Hinton & Salakhutdinov, 2006;nRaina et al., 2007). Unfortunately, currentnlearning algorithms for both models are toonslow for large-scale applications, forcing researchersnto focus on smaller-scale models, ornto use fewer training examples.nIn this paper, we suggest massively parallelnmethods to help resolve these problems.nWe argue that modern graphics processorsnfar surpass the computational capabilities ofnmulticore CPUs, and have the potential tonrevolutionize the applicability of deep unsupervisednlearning methods. We develop generalnprinciples for massively parallelizing unsupervisednlearning tasks using graphics processors.nWe show that these principles cannbe applied to successfully scaling up learningnalgorithms for both DBNs and sparse coding.nOur implementation of DBN learning is up ton70 times faster than a dual-core CPU implementationnfor large models. For example, wenare able to reduce the time required to learn anfour-layer DBN with 100 million free parametersnfrom several weeks to around a singlenday. For sparse coding, we develop a simple,ninherently parallel algorithm, that leads to an5 to 15-fold speedup over previous methods.
9476 en Factored Conditional Restricted Boltzmann Machines for Modeling Motion Style The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factorize the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from O(N^3) to O(N^2). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly between them.
9477 en Deep Learning from Temporal Coherence in Video This work proposes a learning method forndeep architectures that takes advantage ofnsequential data, in particular from the temporalncoherence that naturally exists in unlabelednvideo recordings. That is, two successivenframes are likely to contain the samenobject or objects. This coherence is used asna supervisory signal over the unlabeled data,nand is used to improve the performance on ansupervised task of interest. We demonstratenthe effectiveness of this method on some poseninvariant object and face recognition tasks.
9478 en Robot Trajectory Optimization Using Approximate Inference The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot.
9479 en Trajectory Prediction: Learning to Map Situations to Robot Trajectories Trajectory planning and optimization is a fundamental problem in articulated robotics. Algorithms used typically for this problem compute optimal trajectories from scratch in a new situation. In effect, extensive data is accumulated containing situations together with the respective optimized trajectories - but this data is in practice hardly exploited. The aim of this paper is to learn from this data. Given a new situation we want to predict a suitable trajectory which only needs minor refinement by a conventional optimizer. Our approach has two essential ingredients. First, to generalize from previous situations to new ones we need an appropriate situation descriptor - we propose a sparse feature selection approach to find such well-generalizing features of situations. Second, the transfer of previously optimized trajectories to a new situation should not be made in joint angle space - we propose a more efficient task space transfer of old trajectories to new situations. Experiments on trajectory optimization for a simulated humanoid reaching problem show that we can predict reasonable motion prototypes in new situations for which the refinement is much faster than an optimization from scratch.
9480 en Learning Complex Motions by Sequencing Simpler Motion Templates Abstraction of complex, longer motor tasks into simpler elemental movements enables humans and animals to exhibit motor skills which have not yet been matched by robots. Humans intuitively decompose complex motions into smaller, simpler segments. For example when describing simple movements like drawing a triangle with a pen, we can easily name the basic steps of this movement.nnSurprisingly, such abstractions have rarely been used in artificial motor skill learning algorithms. These algorithms typically choose a new action (such as a torque or a force) at a very fast time-scale. As a result, both policy and temporal credit assignment problem become unnecessarily complex - often beyond the reach of current machine learning methods.nnWe introduce a new framework for temporal abstractions in reinforcement learning (RL), i.e. RL with motion templates. We present a new algorithm for this framework which can learn high-quality policies by making only few abstract decisions.
9481 en Learning When to Stop Thinking and Do Something! An anytime algorithm is capable of returning a response to the given task at essentially any time; typically the quality of the response improves as the time increases. Here, we consider the challenge of learning when we should terminate such algorithms on each of a sequence of iid tasks, to optimize the expected average reward per unit time. We provide an algorithm for answering this question. We combine the global optimizer Cross Entropy method and the local gradient ascent, and theoretically investigate how far the estimated gradient is from the true gradient. We empirically demonstrate the applicability of the proposed algorithm on a toy problem, as well as on a real-world face detection task.
9482 en Monte-Carlo Simulation Balancing In this paper we introduce the first algorithms for efficiently learning a simulation policy for Monte-Carlo search. Our main idea is to optimise the balance of a simulation policy, so that an accurate spread of simulation outcomes is maintained, rather than optimising the direct strength of the simulation policy. We develop two algorithms for balancing a simulation policy by gradient descent. The first algorithm optimises the balance of complete simulations, using a policy gradient algorithm; whereas the second algorithm optimises the balance over every two steps of simulation. We compare our algorithms to reinforcement learning and supervised learning algorithms for maximising the strength of the simulation policy. We test each algorithm in the domain of 5x5 Computer Go, using a softmax policy that is parameterised by weights for a hundred simple patterns. When used in a simple Monte-Carlo search, the policies learnt by simulation balancing achieved significantly better performance, with half the mean squared error of a uniform random policy, and equal overall performance to a sophisticated Go engine.
9483 en Boosting Products of Base Classiﬁers In this paper we show how to boost products of simple base learners. Similarly to trees, we call the base learner as a subroutine but nin an iterative rather than recursive fashion. The main advantage of the proposed method is its simplicity and computational efﬁciency. On nbenchmark datasets, our boosted products of decision stumps clearly outperform boosted trees, and on the MNIST dataset the algorithm nachieves the second best result among no-domain-knowledge algorithms after deep belief nets. As a second contribution, we present an nimproved base learner for nominal features and show that boosting the product of two of these new subset indicator base learners solves nthe maximum margin matrix factorization problem used to formalize the collaborative ﬁltering task. On a small benchmark dataset, we get nexperimental results comparable to the semi-deﬁnite-programming-based solution but at a much lower computational cost.
9484 en ABC-Boost: Adaptive Base Class Boost for Multi-Class Classiﬁcation We propose ABC-Boost (Adaptive Base Class Boost) for multi-class classiﬁcation and present ABC-MART, an implementation of nABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has been popular in certain industry applications (e.g., nWeb search). For binary classiﬁcation, ABC-MART recovers MART. For multi-class classiﬁcation, ABC-MART improves MART, as nevaluated on several public data sets.
9485 en Compositional Noisy-Logical Learning We describe a new method for learning the conditional probability distribution of a binary-valued variable from labelled training nexamples. Our proposed Compositional Noisy-Logical Learning (CNLL) approach learns a noisy-logical distribution in a compositional nmanner. CNLL is an alternative to the well-known AdaBoost algorithm which performs coordinate descent on an alternative error measure. nWe describe two CNLL algorithms and test their performance compared to AdaBoost on two types of problem: (i) noisy-logical data (such nas noisy exclusive-or), and (ii) four standard datasets from the UCI repository. Our results show that we outperform AdaBoost while using nsigniﬁcantly fewer weak classiﬁers, thereby giving a more transparent classiﬁer suitable for knowledge extraction.
9486 en Boosting with Structural Sparsity We derive generalizations of AdaBoost and related gradient-based coordinate descent methods that incorporate sparsity-promoting npenalties for the norm of the predictor that is being learned. The end result is a family of coordinate descent algorithms that integrate nforward feature induction and back-pruning through regularization and give an automatic stopping criterion for feature induction. We study npenalties based on the ?1 , ?2 , and ? n∞ norms of the predictor and introduce mixed-norm penalties that build upon the initial penalties. nThe mixed-norm regularizers facilitate structural sparsity in parameter space, which is a useful property in multiclass prediction and other nrelated tasks. We report empirical results that demonstrate the power of our approach in building accurate and structurally sparse models.
9487 en Learning with Structured Sparsity This paper investigates a new learning formulation called structured sparsity, which is a natural extension of the standard sparsity nconcept in statistical learning and compressive sensing. By allowing arbitrary structures on the feature set,this concept generalizes the ngroup sparsity idea. A general theory is developed for learning with structured sparsity, based on the notion of coding complexity associated nwith the structure. Moreover, a structured greedy algorithm is proposed to efﬁciently solve the structured sparsity problem. Experiments ndemonstrate the advantage of structured sparsity over standard sparsity.
9517 en Acquisition and Understanding of Process Knowledge Using Problem Solving Methods At the defense of his PhD thesis (Madrid, April 23rd 2009), Jose Manuel talks about process knowledge and how it is possible to enable users without any kind of IT skills to i) model processes and ii) analyze the provenance of process executions, without the intervention of software or knowledge engineers. Jose Manuel proposes the utilization of Problem Solving Methods (PSMs) as key enablers for the accomplishment of such objectives and demonstrates the solutions developed, evaluated in the contexts of Project Halo and the Provenance Challenge, respectively. Jose Manuel concludes the talk with a process-centric overview on the challenges raised by the new web-driven computing paradigm, where large amounts of data are contributed and exploited by users on the web, requiring scalable, non-monotonic reasoning techniques as well as stimulating collaboration while preserving trust.nJose Manuel’s advisors Oscar Corcho (Universidad Politécnica de Madrid) and Richard Benjamins (Telefónica) were present at the defense, whose PhD committee comprised the following members: Manuel Hermenegildo (Universidad Politécnica de Madrid), Asunción Gómez-Pérez (Universidad Politécnica de Madrid), Mark Greaves (Vulcan Inc.), Luc Moreau (University of Southampton), and Bert Bredeweg (University of Amsterdam).n
9518 en Reductions in Machine Learning Machine learning reductions are about reusing solutions to simple, core problems in order to solve more complex problems. A basic difficulty in applying machine learning in practice is that we often need to solve problems that don't quite match the problems solved by standard machine learning algorithms. Reductions are techniques that transform such practical problems into core machine learning problems. These can then be solved using any existing learning algorithm whose solution can, in turn, be used to solve the original problem.nThe material that we plan to cover is both algorithmic and analytic. We will discuss existing and new algorithms, along with the methodology for analyzing and creating new reductions. We will also discuss common design flaws in folklore reductions. In our experience, this approach is an effective tool for designing empirically successful, automated solutions to learning problems.
9519 en Convergence of Natural Dynamics to Eqilibria Recently, a lot of e?ort has been devoted to analyzing response dynamics in various games. Questionsnabout the dynamics themselves and their convergence properties attracted a great deal of attention. Thisnincludes, for example, questions like “How long do uncoordinated agents need to reach an equilibrium?”nand “Do uncoordinated agents quickly reach a state with low social cost?”. An important aspect innstudying such dynamics is the learning model employed by self-interested agents in these models. Studyingnthe e?ect of learning algorithms on the convergence rate of players is crucial for developing a solidnunderstanding of the corresponding games.nIn this tutorial, we ﬁrst describe an overview of the required terminology from game theory. Then, wensurvey results about the convergence of myopic and learning-based best responses of players to equilibrianand approximately optimal solutions, and study the e?ect of various learning algorithms in convergencen(rate). Throughout the tutorial, we describe fundamental connections between local search algorithmsnand learning algorithms with the convergence of best-response dynamics in multi-agent games.
9520 en Learning with Dependencies between Several Response Variables We analyze situations where modeling several response variables for a given input improves the prediction accuracy for each individual response variable. Interestingly, this setting has appeared in different context and a number of different but related approaches have been proposed. In all these approaches some assumptions about the dependency structure between the response variables is made.n nHere is a small selection of labels describing relevant work:nmultitask learning, multi-class classification, multi-label prediction, hierarchical Bayes, inductive transfer learning, hierarchical linear models, mixed effect models, partial least squares, canonical correlation analysis, maximal covariance regression, multivariate regression, structured prediction, relational learning, ...n nThe large number of approaches is confusing for the novice, and often even for the expert. In this tutorial we systematically introduce some of the major approaches and describe them from a common viewpoint.
9521 en Survey of Boosting from an Optimization Perspective Boosting has become a well known ensemble method. The algorithm maintainsna distribution on the ±-labeled examples and a new base learner is added in angreedy fashion. The goal is to obtain a small linear combination of base learnersnthat clearly separates the examples. We focus on a recent view of Boostingnwhere the update algorithm for distribution on the examples is characterized byna minimization problem that uses a relative entropy as a regularization.nThe most well known boosting algorithms is AdaBoost. This algorithmnapproximately maximizes the hard margin, when the data is separable. Wenfocus on recent algorithms that provably maximize the soft margin when thendata is noisy. We will teach the new algorithms, give a unied and versatilenview of Boosting in terms of relative entropy regularization, and show how tonsolve large scale problems based on state of the art optimization techniques.nOur goal is to motivate people to mimic the recent successes of the SVMncommunity for scaling up the solvable problem size. This goal is challengingnbecause in Boosting the regularization (relative entropy) is more complicatednthan the one used for SVMs (squared Euclidean distance). Nevertheless we cannsolve dense problems with 200K examples in less than a minute on a laptop.
9522 en The Neuroscience of Reinforcement Learning Overview and goals:nnOne of the most influential contributions of machine learning to understanding the human brain is the (fairly recent) formulation of learning in real world tasks in terms of the computational framework of reinforcement learning. This confluence of ideas is not limited to abstract ideas about how trial and error learning should proceed, but rather, current views regarding the computational roles of extremely important brain substances (such as dopamine) and brain areas (such as the basal ganglia) draw heavily from reinforcement learning. The results of this growing line of research stand to contribute not only to neuroscience and psychology, but also to machine learning: human and animal brains are remarkably adept at learning new tasks in an uncertain, dynamic and extremely complex world. Understanding how the brain implements reinforcement learning efficiently may suggest similar solutions to engineering and artificial intelligent problems. This tutorial will present the current state of the study of neural reinforcement learning, with an emphasis on both what it teaches us about the brain, and what it teaches us about reinforcement learning. nnTarget Audience:nThe target audience are researchers working in the field of reinforcement learning, who are interested in the current state-of-the-art of neuroscientific applications of this theoretical framework, as well as researchers working in related fields of machine learning such as engineering and robotics. Familiarity/basic knowledge of reinforcement learning (MDPs, dynamic programming, online temporal difference methods) will be assumed; basic knowledge in neuroscience or psychology will not. nnTutorial outline:nIntroduction: A coarse-grain overview of the brain and what we currently know about how it worksnLearning and decision making in animals and humans: is this really a reinforcement learning problem?nDopamine and prediction errors: what we know about dopamine, why we think it computes a temporal difference prediction error, and why should we care? Evidence for the prediction error hypothesis of dopaminenActor/Critic architectures in the basal ganglia: a distribution of functions in a learning networknSARSA versus Q-learning: can dopamine reveal what algorithm the brain actually uses?nMultiple learning systems in the brain: what is the evidence for both model based and model free reinforcement learning systems in the brain, why have more than one system, and how to arbitrate between themnBeyond phasic dopamine: average reward reinforcement learning, tonic dopamine and the control of response vigornRisk and reinforcement learning: can the brain tell us something about learning of the variance of rewards?nOpen challenges and future directions: what more can reinforcement learning teach us about the brain, and where can we expect the brain to teach us about reinforcement learning?
9523 en Active Learning Active learning is defined by contrast to the passive model of supervised learning where all the labels for learning are obtained without reference to the learning algorithm, while in active learning the learner interactively chooses which data points to label. The hope of active learning is that interaction can substantially reduce the number of labels required, making solving problems via machine learning more practical. This hope is known to be valid in certain special cases, both empirically and theoretically.nnVariants of active learning have been investigated over several decades and fields. The focus of this tutorial is on general techniques which are applicable to many problems. At a mathematical level, this corresponds to approaches with provable guarantees under weakest-possible assumptions since real problems are more likely to fit algorithms which work under weak assumptions.nnWe believe this tutorial should be of broad interest. People working on or using supervised learning are often confronted with the need for more labels, where active learning can help. Similarly, in reinforcement learning, generalizing while interacting in more complex ways is an active research topic. Please join us.
9524 en Modeling Social and Information Networks: Opportunities for Machine Learning Emergence of the web, social media and online social networking websites gave rise to detailed traces of human social activity. This offers many opportunities to analyze and model behaviors of millions of people. For example, we can now study ''planetary scale'' dynamics of a full Microsoft Instant Messenger network of 240 million people, with more than 255 billion exchanged messages per month. nMany types of data, especially web and "social" data, come in a form of a network or a graph. This tutorial will cover several aspects of such network data: macroscopic properties of network data sets; statistical models for modeling large scale network structure of static and dynamic networks; properties and models of network structure and evolution at the level of groups of nodes and algorithms for extracting such structures. I will also present several applications and case studies of blogs, instant messaging, Wikipedia and web search. nMachine learning as a topic will be present throughout the tutorial. The idea of the tutorial is to introduce the machine learning community to recent developments in the area of social and information networks that underpin the Web and other on-line media.
9525 en Tutorial on Learning Deep Architectures This short tutorial on deep learning will review a variety of methods for learning multi-level, hierarchical representations, emphasizing their common traits. While deep architectures have theoretical advantages in terms of expressive power and efficiency of representation, they also provide a possible model for information processing in the mammalian cortex, which seems to rely on representations with multiple levels of abstractions. A number of deep learning methods have been proposed since 2005, that have yielded surprisingly good performance in several areas, particularly in vision (object recognition), and natural language processing. They all learn multiple levels of representation using some form of unsupervised learning. Hypotheses to explain why these algorithms work well will be discussed in the light of new experimental results. Many of these algorithms can be cast in the framework of the energy-based view of unsupervised learning, which generalizes graphical models used as building blocks for deep architectures, such as the Restricted Boltzmann Machines (RBM) and variations of regularized auto-encoders. Old and new algorithms will be presented for training, sampling, and estimating the partition function of RBMs and Deep Belief Networks. Applications of deep architectures to computer vision and natural language processing will be described. A number of open problems and future research avenues will be discussed, with active participation from the audience.
9526 en Unsupervised Structure Learning: Hierarchical Recursive Composition, Suspicious Coincidence and Competitive Exclusion We describe a new method for unsupervised structure learning of a hierarchical compositional model (HCM) for deformable objects. The learning is unsupervised in the sense that we are given a training data set of images containing the object in cluttered backgrounds but we do not know the position or boundary of the object. The structure learning is performed by a bottom-up and top-down process. The bottom-up process is a novel form of hierarchical clustering which recursively composes proposals for simple structures to generate proposals for more complex structures. We combine standard clustering with the suspicious coincidence principle and the competitive exclusion principle to prune the number of proposals to a practical number and avoid an exponential explosion of possible structures. The hierarchical clustering stops automatically, when it fails to generate new proposals, and outputs a proposal for the object model. The top-down process validates the proposals and fills in missing elements. We tested our approach by using it to learn a hierarchical compositional model for parsing and segmenting horses on Weizmann dataset. We show that the resulting model is comparable with (or better than) alternative methods. The versatility of our approach is demonstrated by learning models for other objects (e.g., faces, pianos, butterflies, monitors, etc.). It is worth noting that the low-levels of the object hierarchies automatically learn generic image features while the higher levels learn object specific features. We then describe more recent work which uses similar principles to learn hierarchies for many objects simultaneously.nnThis talk is based on two research projects. The full authors for these projects are:nn  * Project 1 (ECCV 2008) L. Zhu (UCLA), C. Lin (Microsoft Beijing), H. Huang (Microsoft Beijing), Y.Chen (USTC), and A.L. Yuille (UCLA),nn  * Project 2. L. Zhu (MIT), Y. Chen (USTC), W. Freeman (MIT), A. Torrabla (MIT), and A.L. Yuille (UCLA).
9527 en Deep Learning via Semi-Supervised Embedding We show how nonlinear embedding algorithms popular for use with shallow semi-supervised learning techniques such as kernel methods can be applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.nnWe then go on to generalize this approach to take advantage of sequential data: for images, and text.nnFor images, we take advantage of the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. We demonstrate the effectiveness of this method in a semi-supervised setting on some pose invariant object and face recognition tasks.nnFor text, we describe a unified approach to tagging: a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, and semantic roles. State-of-the-art performance is attained by learning word embeddings using a text specific semi-supervised task called a language model.nnJoint work with: Ronan Collobert, Frederic Ratle, Hossein Mobahi, Pavel Kuksa and Koray Kavukcuoglu.
9528 en Convex Sparse Methods for Feature Hierarchies Sparse methods usually deal with the selection of a few elements from a large collection of pre-computed features. While theoretical results suggest that techniques based on the L1-norm can deal with exponentially many irrelevant features, current algorithms cannot handle more than millions of variables. In this talk, I will show how structured norms can deal in polynomial time with exponentially many features that are organized in a directed acyclic graph.
9529 en Unsupervised Discovery of Structure, Succinct Representations and Sparsity We describe a class of unsupervised learning methods that learn sparse representations of the training data, and thereby identify useful features. Further, we show that deep learning (multilayer) versions of these ideas, ones based on sparse DBNs, learn rich feature hierarchies, including part-whole decompositions of objects. Central to this is the idea of "probabilistic max pooling", which allows us to implement convolutional DBNs at a large scale, while maintaining probabilistically sound semantics. In the case of images, at the lowest level this method learns to detect edges; at the next level, it puts together edges to form "object parts"; and finally, at the highest level puts together object parts to form whole "object models". The features this method learns are useful for a wide range of tasks, including object recognition, text classification, and audio classification. We also present the result of comparing a two-layer version of the model (trained on natural images) to visual cortical areas V1 and V2 in the brain (the first and second stages of visual processing in the cortex). Finally, we'll conclude with a discussion on some open problems and directions for future research.
9530 en A Factor Model for Learning Higher Order Features in Natural Images The visual system is a hierarchy of processing stages. Each stage in this pathway, in addition to encoding increasingly complex features of the input, performs complex non-linear computations. What is the functional role of these non-linear behaviors and how do we incorporate them into generative models of natural images?nnA number of non-linear properties of visual neurons can be predicted from the statistical dependencies observed in natural images. For example, the magnitudes of linear filter outputs are correlated; normalizing filter responses removes this correlation (making the responses more independent and marginally Gaussian) and reproduces neural gain control. In addition, the pattern in these correlations is itself highly informative, and can be used to infer the context of patches sampled from a large scene. Here I will focus on these statistical patterns and describe a generative model that captures them using a set of factors in the space of log-covariance of a multivariate Gaussian distribution. Trained on natural images, the model learns a compact code for correlations observed in pixel (or linear feature) distributions that represents more abstract properties of the image. I will also connect this work to recent generative models that incorporate multiplicative interactions between observed and latent variables.
9531 en Strategies for Prediction under Imperfect Monitoring We propose simple randomized strategies for sequential decision (or prediction) under imperfect monitoring, that is, when the decision maker (forecaster) does not have access to the past outcomes but rather to a feedback signal. The proposed strategies are consistent in the sense that they achieve, asymptotically, the best-possible average reward among all fixed actions. It was Rustichini who first proved the existence of such consistent predictors. The forecasters presented in this talk offer the first constructive proof of consistency. Moreover, the proposed algorithms are computationally efficient. We also establish upper bounds for the rates of convergence. In the case of deterministic feedback signals, these rates are optimal up to logarithmic terms. (Joint work with Shie Mannor and Gilles Stoltz.)
9532 en Trading Regret Rate for Computational Efficiency in Online Learning with Limited Feedback We study low regret algorithms for online learning with limited feedback, where there is an additional constraint on the computational power of the learner. Focusing on multi-armed bandit with side information, we demonstrate cases in which there is a trade-off between the regret rate and the computational efficiency of the online learning algorithm. In particular, for the class of linear hypotheses we show that the EXP4 prediction strategy achieves the optimal regret but is not efficient. In contrast, we propose much more efficient strategies, still with a vanishing regret, but a worse regret rate.
9536 en Strategic Considerations in Bandit Settings - The Price of Truthfulness in Online Ad Auctions Research at the intersection of machine learning and economics has flourished in recent years due to the realization that many technological systems (such as the internet) are better understood and managed when they are viewed as economic systems rather than just merely technological ones. In particular, there is much recent work on understanding learning algorithms and models in settings where the strategic considerations of the participants must be taken into account (e.g. spam detection).nThis talk will examine the this broader issue in the setting of online ad-auctions (in particular "pay-per-click" auctions, where advertisers are charged only those rounds when their ad is clicked on). Designing such an auction faces the classic explore/exploit dilemma: while gathering information about the "click-through-rates" of advertisers, the mechanism may loose revenue; however, this gleaned information may prove valuable in the future for a more profitable allocation. In this sense, such mechanisms are prime candidates to be designed using multi-armed bandit techniques. However, a naive application of multi-armed bandit algorithms would not take into account the strategic considerations of the players - players might manipulate their bids (which determine the auction's revenue) in a way as to maximize their own utility. Hence, we consider the natural restriction that the auction be truthful.nThis work sharply characterizes what regret is achievable, under this truthful restriction. Interestingly, we show that this restriction imposes statistical limits on the achievable regret - that it is Theta(T^2/3), while for traditional bandit algorithms (without this truthful restriction) the achievable regret is O(T^1/2) (where T is the number of rounds). We term the extra T^1/6 factor, the "price of truthfulness".
9539 en Bandit Algorithms for Online Linear Optimization In the online linear optimization problem a forecaster chooses, at each time instance, a vector x from a certain given subset S of the D-dimensional Euclidean space and suffers a time-dependent loss that is linear in x. The goal of the forecaster is to achieve that, in the long run, the accumulated loss is not much larger than that of the best possible vector in S. In this talk we consider the "bandit" setting of the linear optimization problem, in which the forecaster has only access to the losses of the chosen vectors. We survey some recent algorithms that solve this problem. For the special case in which S is a subset of the d-dimensional Boolean hypercube, we describe a new forecaster whose performance, for a variety of concrete choices of S, is better than all previously known bounds, and not improvable in general. We also point out that computationally efficient implementations for various interesting choices of S exist. Joint work with Gabor Lugosi (Barcelona).
9543 en Piecewise-Stationary Bandit Problems with Side Information We consider a sequential decision problem wherenthe rewards are generated by a piecewise-stationaryndistribution. However, the different reward distributionsnare unknown and may change at unknownninstants. Our approach uses a limited number ofnside observations on past rewards, but does not requirenprior knowledge of the frequency of changes.nIn spite of the adversarial nature of the reward process,nwe provide an algorithm whose regret, withnrespect to the baseline with perfect knowledge ofnthe distributions and the changes, is O(k log(T)),nwhere k is the number of changes up to time T.nThis is in contrast to the case where side observationsnare not available, and where the regret is atnleast ?(√T). We also show that our bound is tightnfor a natural class of algorithms. An earlier versionnof this work appears in [YM09].
9545 en Multi-Armed Bandits with Betting We study an extension to the stochastic multiarmednbandit problem where the learner has anbudget ofK “coins” it can use in each round. Thenlearner can use the coins to play multiple arms inneach round, having the option to “bet” multiplencoins on an arm. At the end of the round, thenarms generate a reward that is proportional to thenamount of coins invested in them.
9546 en Forced-Exploration Based Algortihms for Playing in Stochastic Linear Bandits 
9547 en The Offset Tree for Learning with Partial Labels 
9550 en One-Pass Approximate k-Means Optimization 
9552 en Monte-Carlo Sampling for Regret Minimization in Extensive Games 
9555 en Introduction and General Problem Statement Most machine learning (ML) algorithms rely fundamentally on concepts of numerical mathematics. Standard reductions to black-box computational primitives do not usually meet real-world demands and have to be modified at all levels. The increasing complexity of ML problems requires layered approaches, where algorithms are components rather than stand-alone tools fitted individually with much human effort. In this modern context, predictable run-time and numerical stability behavior of algorithms become fundamental. Unfortunately, these aspects are widely ignored today by ML researchers, which limits the applicability of ML algorithms to complex problems.nBackground and ObjectivesnnOur workshop aims to address these shortcomings, by trying to distill a compromise between inadequate black-box reductions and highly involved complete numerical analysis. We will invite speakers with interest in *both* numerical methodology *and* real problems in applications close to machine learning. While numerical software packages of ML interest will be pointed out, our focus will rather be on how to best bridge the gaps between ML requirements and these computational libraries. A subordinate goal will be to address the role of parallel numerical computation in ML. Examples of machine learning founded on numerical methods include low level computer vision and image processing, non-Gaussian approximate inference, Gaussian filtering / smoothing, state space models, approximations to kernel methods, and many more.nImpact and Expected OutcomennWe will call the community's attention to the increasingly critical issue of numerical considerations in algorithm design and implementation. A set of essential rules for how to use and modify numerical software in ML is required, for which we aim to lay the groundwork in this workshop. These efforts should lead to an awareness of the problems, as well as increased focus on efficient and stable ML implementations. We will encourage speakers to point out useful software packages, together with their caveats, asking them to focus on examples of ML interest. Raising awareness about the increasing importance of stability and predictable run-time behaviour of numerical machine learning algorithms and primitives. Establishing a code of conduct for how to best select and modify existing numerical mathematics code for machine learning problems. Learning about developments in current numerical mathematics, a major backbone of most machine learning methods.
9556 en Variance Approximation in Large-Scale Gaussian Markov Random Fields In this talk we discuss a framework for computing accurate approximate variances in large scale Gaussian Markov Random Fields. We start by motivating the need to compute variances in GMRFs, and discuss related problems in machine learning. Our approach is based on constructing a certain low-rank aliasing matrix which takes advantage of the Markov graph of the model. We first construct such a matrix for models with short-range correlation, and then describe a wavelet-based construction for models with long-range correlation. The approach is based on fast solution of sparse linear systems, and we describe suitable preconditioners. We also describe how the approach can be used for problems with sparse plus low-rank structure, for example in approximate Kalman filtering with large state spaces.
9557 en Statistical Leverage and Improved Matrix Algorithms Given an m x n matrix A and a rank parameter k, define the leverage of the i-th row of A to be the i-th diagonal element of the projection matrix onto the span of the top k left singular vectors of A. In this case, "high leverage" rows have a disproportionately large amount of the "mass" in the top singular vectors. Historically, this statistical concept (and generalizations of it) has found extensive applications in diagnostic regression analysis. Recently, this concept has also been central in the development of improved randomized algorithms for several fundamental matrix problems that have broad applications in machine learning and data analysis. Two examples of the use of statistical leverage for improved worst-case analysis of matrix algorithms will be described. The first problem is the least squares approximation problem, in which there are n constraints and d variables. Classical algorithms, dating back to Gauss and Legendre, use O(nd2) time. We describe a randomized algorithm that uses only O(n d log d) time to compute a relative-error, i.e., 1+/-epsilon, approximation. The second problem is the problem of selecting a "good" set of exactly k columns from an m x n matrix, and the algorithm of Gu and Eisenstat provides the best previously existing result. We describe a two-stage algorithm that improves on their result. Recent applications of statistical leverage ideas in modern large-scale machine learning and data analysis will also be briefly described. This concept has proven to be particularly fruitful in large data applications where modeling decisions regarding what computations to perform are made for computational reasons, as opposed to having any realistic hope that the statistical assumptions implicit in those computations are satisfied by the data.
9558 en Matrix Computations in Machine Learning Matrix Computations are ubiquitous in all areas of science and engineering. In this talk, I will first survey some traditional problems in matrix computations and discuss issues that arise in solving them, such as, accuracy, algorithms and software. Then, I will discuss various matrix computation problems that arise in machine learning, especially specialized computations, such as non-negative matrix factorization, multilevel graph clustering and kernel learning. I will conclude with a pointer to resources and a discussion.
9559 en Using Interior Point Methods for Optimization in Training Very Large Scale Support Vector Machines In this talk we shall discuss the issues of Interior Point Methods (IPMs) applied to solve optimization problems arising in the context of very large-scale Support Vector Machine (SVM) training. First, we will briefly introduce IPMs for linear and quadratic programming and comment on their advantages: (a) polynomial complexity, (b) ability to solve very large problems, (c) excellent practical behaviour (much better than that predicted by the worst-case complexity analysis), and (d) eligibility to parallelisation [1]. We will then address specific features of optimization problems arising in SVM training, in particular, the presence of large datasets which are stored as dense matrices and make problems very well-suited to the use of IPMs. We will survey numerical techniques applicable in this context such as suitable factorization methods and we will comment on several interesting developments made over the last decade which aimed at using IPMs for SVM training. We will demonstrate that the key to success in applying IPMs is the ability to re-formulate SVM problems as separable quadratic optimization ones which then can be successfully tackled by appropriate linear algebra tools of IPMs [2,3]. Finally, we will comment on the existing challenges for IPMs in SVM training context and touch a number of research issues which still remain open. In particular, we will discuss a need of developing new algorithms which may take advantage of new multi-core architectures, challenges of problems getting larger and larger, and the use of non-linear and indefinite kernels. This is joint work with Kristian Woodsend.
9560 en Gaussian Processes and Fast Matrix-Vector Multiplies 
9561 en The Parallel Machine Learning (PML) Framework and Numerical Aspects of the Transform Regression Algorithm 
9562 en GADGET SVM: a Gossip-bAseD sub-GradiEnT SVM solver 
9563 en Condition Number Analysis of Kernel-Based Density Ratio Estimation 
9564 en Final Discussion 
9565 en High-Level Actions 
9566 en Dyna(k): A Multi-Step Dyna Planning Dyna planning is an efficient way of learningnfrom real and imaginary experience. Existingntabular and linear Dyna algorithms arensingle-step, because an "imaginary" featurenis predicted only one step into the future. Innthis paper, we introduce a multi-step Dynanplanning that predicts more steps into thenfuture. Multi-step Dyna is able to figure outna sequence of multi-step results when a realninstance happens, given that the instance itself,nor a similar experience has been imaginedn(i.e., simulated from the model) andnplanned. Our multi-step Dyna is based on anmulti-step model, which we call the ?-model.nThe ?-model interpolates between the onestepnmodel and an innite-step model, andncan be learned efficiently online. The multistepnDyna algorithm, Dyna(k), uses the ?-nmodel to generate predictions k steps aheadnof the imagined feature, and applies TD onnthis imaginary multi-step transitioning.
9567 en Manifold Embeddings for Model-Based Reinforcement Learning of Neurostimulation Policies Real-world reinforcement learning problems often exhibit nonlinear, continuous-valued,nnoisy, partially-observable state-spaces that are prohibitively expensive to explore. Thenformal reinforcement learning framework, unfortunately, has not been successfully demonstrated in a real-world domain having all of these constraints. We approach this domainnwith a two-part solution. First, we overcome continuous-valued, partially observable state-spaces by constructing manifold embeddings of the system’s underlying dynamics, whichnsubstitute as a complete state-space representation. We then define a generative modelnover this manifold to learn a policy off-line. The model-based approach is preferred because it enables simplification of the learning problem by domain knowledge. In this work we formally integrate manifold embeddings into the reinforcement learning framework, summarize a spectral method for estimating embedding parameters, and demonstrate the model-based approach in a complex domain-adaptive seizure suppression of an epileptic neural system.
9568 en An Empirical Comparison of Abstraction in Models of Markov Decision Processes Reinforcement learning studies the problem of solving sequential decision making problems. Model-based methods learn an effective policy in few actions by learning a model of the domain and simulating experience in their models. Typical model-based methods must visit each state at least once, which can be infeasible in large domains. To overcome this problem, the model learning algorithm needs to generalize knowledge to unseen states and provide information about the states in which it needs more experience. In this paper, we use existing supervised learning techniques to learn the model of the domain. We empirically compare theirneffectiveness at generalizing knowledge across states on three different domains. Our resultsnindicate that tree-based models perform the best after training on a small number of transitions, while support vector machines perform the best after a large number of transitions.
9569 en Basis Function Construction for Hierarchical Reinforcement Learning This paper introduces an approach to automatic basis function construction for HierarchicalnReinforcement Learning (HRL) tasks. We describe some considerations that arisenwhen constructing basis functions for multilevel task hierarchies. We extend previousnwork on using Laplacian bases for value function approximation to situations where thenagent is provided with a multi-level action hierarchy. We experimentally evaluate thesentechniques on the Taxi domain.
9570 en Poster Spotlights: Situation Dependent Spatial Abstraction in Reinforcement Learning Based on Structural Knowledge State space abstraction reduces the size of a representation by factoring out details thatnare not relevant for solving a task at hand. But even in abstract representations not every detail is relevant in any situation. In cases where the structure of the environmentnonly allows for one particular action selection, all information that does not relate tonthe structure can be omitted. We present a method to identify such cases in a reinforcement learning setting and abstract from non-structural details when appropriate tonshrink the state space and allow for knowledge reuse. A significant performance improvement of this approach is demonstrated in a goal-directed robot navigation task.
9571 en Poster Spotlights: Automated Discovery of Options in Factored Reinforcement Learning Factored Reinforcement Learning (FRL) is a method to solve Factored Markov Decision Processesnwhen the structure of the transition and reward functions of the problem must be learned.nIn this paper, we present TeXDYNA, an algorithm that combines the abstraction techniquesnof Semi-Markov Decision Processes to perform the automatic hierarchical decomposition of thenproblem with an FRL method. The algorithm is evaluated on the taxi problem.
9572 en Poster Spotlights: Hierarchical Skill Learning for High-Level Planning We present skill bootstrapping, a proposednnew research direction for agent learning andnplanning that allows an agent to start withnlow-level primitive actions, and develop skillsnthat can be used for higher-level planning.nSkills are developed over the course of solvingnmany different problems in a domain,nusing reinforcement learning techniques toncomplement the benefits and disadvantagesnof heuristic-search planning. We describenthe overall architecture of the proposed approach,ndiscuss how it relates to other work,nand give motivating examples for why thisnapproach would be successful.
9573 en Welcome Statment In the last 25 years, reinforcement learning research has made great strides and has had a significant impact within several fields, includingnn  * artificial intelligencen  * optimal controln  * neurosciencen  * psychologyn  * economicsn  * operations researchnnThese are diverse areas, with different goals and different evaluation criteria. It is striking that reinforcement learning ideas are playing new roles in all of them. The purposes of this meeting are nn  * to recognize and assess this confluence of fieldsn  * to celebrate the diversity of reinforcement learning researchn  * to exchange information among the fieldsnnMSRL consisted of a series of invited talks and an evening poster session. Participation in the poster session was based on extended abstracts.nnMSRL was co-located with a cluster of other meetings, including the International Conference on Machine Learning, the Conference on Uncertainty in Artificial Intelligence, and the Conference on Learning Theory.
9574 en Deconstructing Reinforcement Learning The premise of this symposium is that the ideas of reinforcement learning have impacted many fields, including artificial intelligence, neuroscience, control theory, psychology, and economics. But what are these ideas and which of them is key? Is it the idea of reward and reward prediction as a way of structuring the problem facing both natural and artificial systems? Is it temporal-difference learning as a sample-based algorithm for approximating dynamic programming? Or is it the idea of learning online, by trial and error, searching to find a way of behaving that might not be known by any human supervisor? Or is it all of these ideas and others, all coming to renewed prominence and significance as these fields focus on the common problem that faces animals, machines, and societies - how to predict and control a hugely complex world that can never be understood incompletely, but only as a gross, ever-changing approximation? In this talk I seek to start the process of phrasing and answering these questions. In some cases, from my own experience, I can identify which ideas have been the most important, and guess which will be in the future. For others I can only ask the other speakers and attendees to provide informed perspectives from their own fields.nnn
9575 en Birdsong Learning Zebra finch are born in the spring when they are exposed to a con-specific male song. Later in the spring the bird begins to vocalize and the song gradually converges to the tutor song. There is increasing evidence that birdsong learning occurs through associative reward-penalty reinforcement learning.
9576 en Fifty Years of RL in Games Many researchers have advocated game domains as highly useful testbeds where one can cleanly isolate and study important issues faced by RL and more general AI methods in tackling messy real-world problems. In this talk, I'd like to survey some of the highlights of the numerous studies of RL in various game domains since Samuel's seminal work of fifty years ago. My definition of "games" is broad and will include puzzles, competitions, simulated marketplaces, and video/online games. I will also talk about the relationship and differences between traditional single-agent RL and more recent multiagent learning algorithms, which are likely necessary in general multi-player games. The goal of the talk is to draw a larger perspective on what we have learned from studying RL in games, and where promising future opportunities may lie, not only as RL theory advances, but as "games" themselves continue to evolve with advancing technology.
9577 en Reinforcement Learning, Apprenticeship Learning and Robotic Control Reinforcement learning has proved to be a powerful method for robotic control. In this talk, drawing on examples from autonomous helicopter flight, quadruped robot control and autonomous driving, I'll describe some of the challenges we've faced in applying RL algorithms to various control problems, such as (i) Problems where the reward function is exceedingly difficult to specify by hand, and must itself be learned, (ii) Safe exploration, where one wishes to explore without damaging the robot, and (iii) Learning high performance controllers even if we have only an extremely inaccurate model of our robot's dynamics. Using apprenticeship learning - in which we learn by watching an expert demonstration - as a unifying theme, I'll also describe a few algorithms for addressing these challenges.
9578 en Third Annual Reinforcement Learning Competition Building on last year's competition and the benchmarking events that preceded it, this event will be a forum for reinforcement learning researchers to rigorously compare the performance of their methods on a suite of challenging domains. The competition finals and workshop will take place during the Multidisciplinary Symposium on Reinforcement Learning, a part of the 2009 International Conference on Machine Learning (ICML'09) in Montreal, Canada. To encourage student participation, we will be awarding travel scholarships for student competitors.nnIn order to encourage even greater participation, our technical committee has been working hard to lower the bar for entry. The competition software is easy to install, and contains sample code so that you can get started with minimal effort. In addition to travel scholarships there are a number of exciting prizes for grabs. We are also including resources to make it easy for instructors to use the competition software as a part of a course on Reinforcement Learning, Machine Learning, or Artificial Intelligence.nn[[http://2009.rl-competition.org/index.php]]
9584 en Near-Bayesian Exploration in Polynomial Time We consider the exploration/exploitation problem in reinforcement learning (RL). The Bayesian approach to model-based RL offers an elegant solution to this problem, by considering a distribution over possible models and acting to maximize expected reward; unfortunately, the Bayesian solution is intractable for all but very restricted cases. In this paper we present a simple algorithm, and prove that with high probability it is able to perform epsilon-close to the true (intractable) optimal Bayesian policy after some small (polynomial in quantities describing the system) number of time steps. The algorithm and analysis are motivated by the so-called PAC-MDP approach, and extend such results into the setting of Bayesian RL. In this setting, we show that we are able to achieve lower sample complexity bounds than existing PAC-MDP algorithms, while using exploration strategies that are much greedier than the (extremely cautious) exploration strategies used by these existing algorithms.
9588 en Scalable Link Mining and Analysis on Information Networks With the ubiquity of information networks and their broad applications, there have been numerous studies on the construction, online analytical processing, and mining of information networks in multiple disciplines, including social network analysis, World-Wide Web, database systems, data mining, machine learning, and networked communication and information systems. Algorithms like PageRank and HITS have been developed in late 1990s to explore links among Web pages to discover authoritative pages and hubs. Links have also been popularly used in citation analysis and social network analysis. However, there is a lack of systematic treatment on how to fully explore the power of links in scalable data analysis. In this talk, the power of links are examined in details to improve the effectiveness and efficiency of typical data analysis tasks, including information integration, on-line analytic processing, and other interesting data mining tasks, especially in the multi-relational databases and/or the World-Wid e Web environments.
9589 en Bottom-Up Search and Transfer Learning in SRL This talk addresses two important issues motivated by of our recent research in SRL. First, is the value of data-driven, "bottom-up" search in learning the structure of SRL models. Bottom-up induction has a long history in traditional ILP; however, its use in SRL has been somewhat limited. We review recent results on several structure-learning methods for Markov Logic Networks (MLNs) that highlight the value of bottom-up search. Second, is the value of transfer learning in reducing the data and computational demands of SRL. By inducing a predicate mapping between seemingly disparate domains, effective SRL models can be efficiently learned from very small amounts of in-domain training data. For example, by transferring a model learned from data about a CS department, we have induced reasonably accurate models for IMDB movie data given training data about only a single person.
9590 en Large Networks, Clusters and Kronecker Products Emergence of the web and online computing applications gave rich datanon human social activity that can be represented in a form of anninteraction graph. One of the principal challenges then is to buildnmodels and understanding of the structure of such large networks. Innthis talk I will present our work on the cluster or communitynstructure in large networks, where clusters are thought of as sets ofnnodes that are better connected internally than to the rest of thennetwork. We find that large networks have very different clusteringnstructure from well studied small social networks and graphs that arenwell-embeddable in a low-dimensional structure. In networks ofnmillions of nodes tight clusters exist at only very small size scalesnup to around 100 nodes, while at large size scales networks becomesnexpander like. As this behavior is not explained, even at anqualitative level, by any of the commonly-used network generationnmodels I will then present a network model based on Kronecker productsnthat is able to produce graphs exhibiting a network structure similarnto our observations.
9591 en A Tutorial on Logic-Based Approaches to SRL The relations in Statistical Relational Learning are often expressed using first-order logic, leading to formalisms which combine both logical and probabilistic representations. In this talk I intend to explain the most important consequences of adopting a logical approach to SRL. Defining distributions over 'possible worlds' is a common theme to many such approaches. Two prominent logic-based formalisms - Markov logic networks and PRISM programs - will be used as exemplars. Although the talk is tutorial in nature, I hope to make it interesting to those already familiar with this area!
9592 en First-Order Models for Sequential Decision-Making In this talk I will discuss first-order models and algorithms for sequential decision-making, specifically those approaches that admit exact lifted solutions. The first emphasis of the talk will be on the insights that underlie these models and algorithms along with potential caveats for their practical application. The second emphasis of the talk will be on a variety of extensions of the first-order Markov decision process (MDP) framework such as the factored first-order MDP and the first-order partially observable MDP. The third emphasis of the talk will be on the algorithmic tricks-of-the-trade that allow the practical application of these models; this includes (a) useful data structures, (b) efficient solution techniques for first-order linear programs, (c) new techniques for first-order variable elimination, and (d) practical methods for maintaining compact, consistent first-order representations without theorem proving.
9593 en Weighted Deductionas an Abstraction Level for AI The field of AI has become implementation-bound. We have plenty of ideas, but it is increasingly laborious to try them out, as our models become more ambitious and our datasets become larger, noisier, and more heterogeneous. The software engineering burden makes it hard to start new work; hard to reuse and combine existing ideas; and hard to educate our students. In this talk, I'll propose to hide many common implementation details behind a new level of abstraction that we are developing. Dyna is a declarative programming language that combines logic programming with functional programming. It also supports modularity. It may be regarded as a kind of deductive database, theorem prover, truth maintenance system, or equation solver. I will illustrate how Dyna makes it easy to specify the combinatorial structure of typical computations needed in natural language processing, machine learning, and elsewhere in AI. Then I will sketch implementation strategies and program transformations that can help to make these computations fast and memory-efficient. Finally, I will suggest that machine learning should be used to search for the right strategies for a program on a particular workload.
9594 en Panel Discussion 
9606 en Measuring Language in the Brain / Merjenje jezika v mo?ganih Attempts to understand the relationship between language and the brain have a long history. Recent theoretical, technological, methodological and analytic developments have provided us with the opportunity to look a little closer at the workings of the intact and damaged brain. In this talk I will give an overview of recent attempts to measure brain activity related to Language processes. One important aspect of this endeavour is that it is truly multi-disciplinary, involving cooperation between linguists, psychologists, mathematicians, geneticists, physicists and others. This level of scientific integration is challenging. It requires flexibility and a willingness to reconceptualise problems and to recognize common interests, but it returns synergistic results. Current best practice in neuro-linguistic research not only uses the available methods, analytic approaches and technologies to reveal the neural architecture and processes underpinning language but also use language as a vehicle to probe and extend the limits of the methods, analytic approaches and technologies. I will try to survey this multi-disciplinary landscape by addressing three basic questions:nn  * where - what parts of the brain are associated with language use?n  * when - what is the time course of language processing?n  * how - what mechanisms support the processing of language?nnand maybe even a little why. n----nZnanstveni ve?er bo namenjen pregledu najnovej?ih poskusov merjenja mo?ganske aktivnosti, povezane s procesiranjem jezika. Podan bo natan?nej?i pregled trenutne dobre prakse v nevrolingvisti?nih raziskavah ter uporabe jezika kot na?ina za preizku?anje metod, analiti?nih pristopov in tehnologij v nevrologiji ter ?irjenje mej njihovih zmogljivosti. Obenem bo poskusil odgovoriti na tri osnovna vpra?anja:nn  * kje – kateri deli mo?ganov so povezani z rabo jezika?n  * kdaj – kak?en je ?asovni potek jezikovnega procesiranja?n  * kako – kateri mehanizmi podpirajo procesiranje jezika?n
9608 en A tutorial on Deep Learning Complex probabilistic models of unlabeled data can be created by combining simpler models. Mixture models are obtained by averaging the densities of simpler models and "products of experts" are obtained by multiplying the densities together and renormalizing. A far more powerful type of combination is to form a "composition of experts" by treating the values of the latent variables of one model as the data for learning the next model. The first half of the tutorial will show how deep belief nets -- directed generative models with many layers of hidden variables -- can be learned one layer at a time by composing simple, undirected, product of expert models that only have one hidden layer. It will also explain why composing directed models does not work. Deep belief nets are trained as generative models on large, unlabeled datasets, but once multiple layers of features have been created by unsupervised learning, they can be fine-tuned to give excellent discrimination on small, labeled datasets. The second half of the tutorial will describe applications of deep belief nets to several tasks including object recognition, non-linear dimensionality reduction, document retrieval, and the interpretation of medical images. It will also show how the learning procedure for deep belief nets can be extended to high-dimensional time series and hierarchies of Conditional Random Fields.
9647 en Electroencephalographic (EEG) Coherence Study of Working Memory Brain Oscillations Different brain areas process various aspects of information in parallel as well as segregated way. It is not known, how is this information integrated into a unitary percept or action. The binding problem is one of the key problems in understanding brain function. Synchronized oscillatory activity of neurons is one possible mechanism of the functional integration of different communicating brain areas. The binding has been well-studied in the visual system, but it could also serve as a mechanism in visuomotor integration or functional coupling present with other brain processes and behavioural modes (perception, complex motor behaviour, selective attention, learning, working memory, etc.). Interregional synchronization of the electroencephalographic (EEG) signal can be determined by EEG coherence analysis. In the article we present a research example of coherence changes in a visuomotor task. During this task, coherence between visual and motor brain areas increased. This might reflect functional coupling between those areas, but it could also be influenced by other cognitive processes (e.g. selective attention). Coherence analysis is suitable for studying integrative brain function. Because it measures only one of the possible mechanisms of integration, it offers promise especially when combined with other electrophysiological and functional imaging methods.
9703 en Structured Prediction for Natural Language Processing This tutorial will discuss the use of structured prediction methods from machine learning in natural language processing. The field of NLP has, in the past two decades, come to simultaneously rely on and challenge the field of machine learning. Statistical methods now dominate NLP, and have moved the field forward substantially, opening up new possibilities for the exploitation of data in developing NLP components and applications. However, formulations of NLP problems are often simplified for computational or practical convenience, at the expense of system performance. This tutorial aims to introduce several structured prediction problems from NLP, current solutions, and challenges that lie ahead. Applications in NLP are a mainstay at ICML conferences; many ML researchers view NLP as a primary or secondary application area of interest. This tutorial will help the broader ML community understand this important application area, how progress is measured, and the trade-offs that make it a challenge.
9704 en Opening 
9705 en Networking Genes And Drugs: Understanding Gene Function And Drug Mode Of Action From Large-Scale Experimental Data A cell can be described as a synergistic ensemble of biological entities (mRNA, proteins, ncRNA, metabolites, etc) interacting with each other, whose collective behaviour causes the observed phenotypes. A great research effort is ongoing in identifying and mapping the network of interactions among biomolecules in mammalian species.nThe idea of harnessing this network to understand human diseases at the molecular level, and possibly to find suitable drugs for their treatment, is fascinating but still unfulfilled.nWe will show how it is possible to harness experimental data on human cells and tissue to identify the gene regulatory networks among tens of thousands of genes, and how to use this information to analyse the modular structure of the cell and predict the function of each gene. Moreover, we will show how using these data it is also possible to identify a suitable drug, or a combination of drugs, that can restore the physiological behaviour of the affected pathways in human diseases.
9706 en Predicting the Functions of Proteins in PPI Networks from Global Information 
9707 en Integrated Network Construction Using Event Based Text Mining 
9708 en Quantitative Microscopy: Bridge Between "Wet" Biology and Computer Science Quantification of experimental evidence is an important aspect of modern life science. In microscopy, this causes a shift from pure presentation of "supporting cases" toward the quantification of the processes under study. Computer image processing breaks through the light microcopy diffraction limit, it allows to track individual molecules in the life specimen, quantify distribution and co-localization of compartment markers, etc. The quantified experimental data forms a basis for the models of the biological processes. Quality of predictive models is crucially dependent on the accuracy of the quantified experimental data. The quality of experimental data is a function of algorithms as well as the imperfections of the "wet" experiment. The number of research papers devoted to the algorithms of microcopy image analysis, segmentation, classification and tracking has grown very fast in the last two decades. The analysis of the source of noise in "wet" biology and microscopy has gotten less attention. In this talk I will focus on the correction of experimental data before applying analysis algorithms. These corrections have two faces. They are obligatory to compensate for imperfections of "wet" microscopy while at the same time this correction can break some assumptions, which form the basis of algorithms for subsequent analysis. The examples of the different approaches for "pre-" and "post-" correction will be presented.
9709 en On Utility Of Gene Set Signatures In Gene Expression-Based Class Prediction 
9710 en Evaluation Method For Feature Rankings And Their Aggregations For Biomarker Discovery 
9711 en Matching Models To Data In Modelling Morphogen Diffusion 
9712 en Evaluation Of Signaling Cascades Based On The Weights From Microarray And ChIP-seq Data 
9713 en Synthetic Biology Achievements And Future Prospects Synthetic biology, which combines engineering approach in biological systems is getting a strong momentum due to the recent technological advances, which allow us to manipulate the genetic information at an unprecedented scale.nCurrently synthetic biology is exploiting its potentials and advantages but also bottlenecks. We will review some success stories of synthetic biology in different field of applications, such as medicine, energy and materials. Medical applications of synthetic biology are some of the most promising areas of synthetic biology, particularly for the alternative methods of drug production, biosensors and also different therapeutic applications. Recent developments in our understanding of cellular signaling and host-pathogen interactions provide the opportunity for new types of medical intervention, where we can utilize parts of the existing or reengineer signaling responses connected to various pathological conditions. Knowledge of the ways that microbes use to avoid the human immune response allows us to devise approach to bypass those microbial strategies.nWe will look at three different applications of synthetic biology, which involve reengineering of cell signaling pathways, which we have prepared for the international genetically engineered machines competition in years 2006-2008. We have designed and demonstrated proof of the concept of antiviral detection and defense system based on essential viral functions that is independent on mutations and a synthetic vaccine that activates both innate and adaptive immune response.
9714 en Machine Learning Methods For Protein Analyses Computational biologists, and biologists more generally, spend a lot of time trying to more fully characterize proteins. In this talk, I will describe several of our recent efforts to use machine learning methods to gain a better understanding of proteins. First, we tackle one of the oldest problems in computational biology, the recognition of distant evolutionary relationships among protein sequences. We show that by exploiting a global protein similarity network, coupled with a latent space embedding, we can detect remote protein homologs more accurately than state-of-the-art methods such as PSI-BLAST and HHPred. Second, we use machine learning methods to improve our ability to identify proteins in complex biological samples on the basis of shotgun proteomics data. I will describe two quite different approaches to this problem, one generative and one discriminative.
9715 en A Comparison Of AUC-Estimators In Small-Sample Studies 
9716 en Accuracy?Rejection Curves (ARCs) For Comparison Of Classification Methods With Reject Option 
9717 en Metadata For Systems Biology The ease with which modern computational and theoretical tools can be applied to modeling has led to an exponential increase in the size and complexity of computational models in biology. At the same time, the accelerating pace of progress also highlights limitations in current approaches to modeling. One of these limitations is the insufficient degree to which the semantics and qualitative behaviour of models are systematised and expressed formally enough to support unambiguous interpretation by software systems. As a result, human intervention is required to interpret and connect a model's mathematical structures with information about the its meaning (semantics). Often, this critical information is usually communicated through free-text descriptions or non-standard annotations; however, free-text descriptions cannot easily be interpreted by current modeling tools.nWe will describe three efforts to standardize the encoding of missing semantics for kinetic models. The overall approach involves connecting model elements to common, external sources of information that can be extended as existing knowledge is expanded and refined. These external sources are carefully managed public, free, consensus ontologies: the Systems Biology Ontology (SBO), the Kinetic Simulation Algorithm Ontology (KiSAO), and the Terminology for the Description of Dynamics (TeDDy). Together they provide a means for annotating a model with stable and perennial identifiers which reference machine readable regulated terms defining the semantics of the three facets of the modeling process 1) the relationship between the model and the biology it aims to describe, 2) the process used to simulate the model and obtain expected results, and 3) the results themselves.
9718 en Hierarchical Cost-Sensitive Algorithms For Genome-Wide Gene Function Prediction 
9719 en Simple Ensemble Methods Are Competitive With State-Of-The-Art Data Integration Methods For Gene Function Prediction 
9720 en A Subgroup Discovery Approach For Relating Chemical Structure And Phenotype Data In Chemical Genomics 
9721 en Evaluation Of Methods In Gene Association Studies: Yet Another Case For Bayesian Networks 
9722 en Automating Science The basis of science is the hypothetico-deductive method and the recording of experiments in sufficient detail to enable reproducibility. We report the development of the Robot Scientist "Adam" which advances the automation of both. Adam has autonomously generated functional genomics hypotheses about the yeast Saccharomyces cerevisiae, and experimentally tested these hypotheses using laboratory automation. We have confirmed Adam's conclusions through manual experiments. To describe Adam's research we have developed an ontology and logical language. The resulting formalization involves over 10,000 different research units in a nested tree-like structure, ten levelsdeep, that relates the 6.6 million biomass measurements to their logical description. This formalization describes how a machine discovered new scientific knowledge. Describing scientific investigations in this way opens up new opportunities to apply machine learning and data-mining to discover new knowledge.
9723 en Closing Remarks 
9737 en Introduction To Bayesian Inference 
9738 en Graphical Models 
9739 en Markov Chain Monte Carlo 
9740 en Information Theory 
9741 en Kernel Methods 
9742 en Approximate Inference 
9743 en Topic Models 
9744 en Gaussian Processes 
9745 en Convex Optimization 
9746 en Learning Theory 
9748 en Computer Vision 
9749 en Nonparametric Bayesian Models 
9750 en Machine Learning and Cognitive Science 
9751 en Reinforcement Learning 
9753 en Foundations of Nonparametric Bayesian Methods 
9754 en Deep Belief Networks 
9755 en Particle Filters 
9756 en Causality 
9757 en Information Retrieval 
9758 en Bayesian or Frequentist, Which Are You? 
9760 en Towards a Simulation-based Communication Tool to Support Semantic Business Process Management Successfully communicating a Business Executive’s goals andndesires to an IT Architect with regard to organizational change presents a majornchallenge. The most significant problem is relating the changes desired in ansemantically consistent and understandable manner and then reflecting thenpotential impact of those changes on the organizational structure and thenbusiness processes carried out within that organization. This paper presents anproposal for a simulation-based communication tool that employs ansemantically driven natural-language component to capture a BusinessnExecutive’s needs. These needs are translated into a simulation of a businessnprocess consisting of semantic web services that represent the evolution of thenorganizations IT infrastructure and policies. Through an iterativencommunication loop with the IT Architect the simulation can be used tonaccurately represent the changes necessary to meet the Business Executive’snneeds.
9761 en Contextualized Ontology-Based Similarity in the Human Resources Domain: A Business Use Case We are currently devoloping an integrated business applicationnplatform named Corinna3 where the long-term goal is to combinenSemantic Web technologies with Natural Language Processing (NLP) tonincrease the efficiency of the enterprise search process.nIn this paper we consider one of the Corinna use cases, namely thenHuman Resource (HR) use case, which has already reached a prototypenimplementation. While NLP is used for automatic categorizationnof an unstructured resource, ontologies represent these resources in antaxonomical way with arbitrary properties. On top of the resource ontologiesnwe use both the NLP for the information extraction as well asnontology-based similarity in order to get more acceptable similar searchnresults. Our implemented approach of contextualized hybrid similaritynsearch within the Corinna platform derives strongly from the definednbusiness use cases. It combines and extends existing similarity measuresnunder consideration of additional context information.
9762 en Quest for Superconductivity in Compounds of Divalent Silver: 8 Years after the Prediction Fluorine is the most electronegative among all chemical bond–forming elements. Because of this, the vast majority of binary and higher inorganic fluorides are high–melting large–band gap electronic insulators, which are transparent in the visible region of the electromagnetic spectrum. Rare examples of metallic fluorides are known, but valence orbitals of F marginally participate in the electronic transport in these compounds. In this account we describe recent theory–driven attempts to turn a family of TM fluorides into a novel class of high–temperature superconductors. Substantial mixing of F’s 2p orbitals and metal valence functions (‘covalence’) as well as partial band occupation are needed to generate band crossing judged responsible for unusually strong vibronic coupling and for the appearance of superconductivity. This precondition is satisfied for unusual fluorides of divalent silver – fluoroargentates (II). These fascinating materials share lots of common features with oxides of Cu(I). Unfortunately, in all known layered fluorides of silver (II) the unpaired d electrons of silver order ferromagnetically. This is in contrast to antiferromagnetic ordering observed for undoped oxocuprates. In this account attempts will be described of crystal-engineering of fluoroargentates (II) which target a layered antiferromagnetic precursor of a superconductor.nn
9768 en Opening of the Meeting 
9769 en How Is InJo as a Topic Placed Globally: The Overview of InJo Practice in the World 
9770 en Innovation in Journalism 
9771 en Innovation in Swedish Media 
9772 en Innovation in Slovenian Media: Findings and Recommendations from the Slovenian Jury of InJo Award from 2006 to 2009 
9773 en Is Innovation a Journalistic Topic? 
9774 en Experience from Israel 
9775 en Experience from Finland 
9776 en Experience from Europe 
9777 en Experience from Slovenia 
9778 en Presentation of the Slovene Scholarship for Innovation Journalism Fellowship Program at University of Stanford in 2010 
9779 en Experience from Innovation Journalism Fellowship Program 
9780 en How to Gain the Right Knowledge for Efficient Reporting on Innovation 
9781 en Introduction to New Media Business Models 
9782 en Story of SamaaTV Check the YouTube video mentioned in the presentation [[http://www.youtube.com/watch?v=jehP0BOeBJI|here]]
9783 en BBC as Innovative Media Enterprise Check the video mentioned in the presentation [[http://gallery.me.com/hgyr#100005|here]].
9784 en Story about Community Acting Reporting 
9785 en What Can We Learn from the New Media Business Model? 
9786 en Interview with Andreas Walter 
9787 en Interview with Miha Gr?ar 
9788 en Interview with Holger Lausen 
9789 en Interview with Mick Kerrigan 
9790 en Interview with Katharina Siorpaes 
9791 en Interview with Sofia Angeletou 
9792 en Interview with Klaas Dellschaft 
9793 en Interview with Sebastian Dietzold 
9804 en Learning to Herd and Herding to Learn Learning in the traditional sense focuses on finding a point estimate for the parameters of its model. Bayesian approaches extend this to posterior distributions but are computationally intractable for Markov random fields and inference can easily get trapped in local modes. A third possibility is to define a dynamical system, herding, that mixes very efficiently over an attractor set and where each point on this set defines an "energy function" over some state space. The collection of all these energy minima represent a sample collection that shares certain moment statistics with the input data.nI will briefly introduce this system and present very preliminary ideas on the following issues:nn  * Can we learn (hyper)parameters for herding so that it can be run with the data decoupled from it?n  * Which herding systems should be considered equivalent and what are the implications for the use of fast weights in ML learning?n  * Among all non-equivalent herding systems that reproduce the same moment constraints, can we learn the one that performs optimal in terms of generalization?n  * Can we characterize the attractor set of herding and is its dynamics chaotic?n  * How useful can herding be for producing deep representations?
9829 en Considering Unseen States as Impossible in Factored Reinforcement Learning The Factored Markov Decision Process (FMDP) framework is a standard representation for sequential decision problems under uncertainty where the state is represented as a collection of random variables. Factored Reinforcement Learning (FRL) is an Model-based Reinforcement Learning approach to FMDPs where the transition and reward unctions of the problem are learned. In this paper, we show how to model in a theoretically well-founded way the problems where some combinations of state variable values may not occur, giving rise to impossible states. Furthermore, we propose a new heuristics that considers as impossible the states that have not been seen so far. We derive an algorithm whose improvement in performance with respect to the standard approach is illustrated through benchmark experiments.
9830 en A Convex Method for Locating Regions of Interest with Multi-Instance Learning In content-based image retrieval (CBIR) and image screening, it is often desirable to automatically locate the regions of interest (ROI) in the images. This can be accomplished with multi-instance learning techniques by treating each image as a bag of instances (regions). Many SVM-based methods are successful in predicting the bag label. However, very few of them can locate the ROIs and often they are based on either the local search or an EM-type strategy, which may get stuck in local minima. To address this problem, we propose in this paper two convex optimization methods which maximize the margin of concepts via key instance generation in the instance-level and bag-level, respectively. Moreover, this can be efficiently solved with a cutting plane algorithm. Experiments show that the proposed methods can effectively locate ROIs. Moreover, on the benchmark data sets, they achieve performance that are competitive with state-of-the-art algorithms.
9831 en Efficient Decoding of Ternary Error-Correcting Output Codes for Multiclass Classification We present an adaptive decoding algorithm for ternary ECOC matrices which reduces the number of needed classifier evaluations for multiclass classification. The resulting predictions are guaranteed to be equivalent with the original decoding strategy except for ambiguous final predictions. The technique works for Hamming Decoding and several commonly used alternative decoding strategies. We show its effectiveness in an extensive empirical evaluation considering various code design types: Nearly in all cases, a considerable reduction is possible. We also show that the performance gain depends on the sparsity and the dimension of the ECOC coding matrix.
9832 en Simulated Iterative Classification: A new Learning Procedure for Graph Labeling Collective classification refers to the classification of interlinked and relational objects described as nodes in a graph. The Iterative Classification Algorithm (ICA) is a simple, efficient and widely used method to solve this problem. It is representative of a family of methods for which inference proceeds as an iterative process: at each step, nodes of the graph are classified according to the current predicted labels of their neighbors. We show that learning in this class of models suffers from a training bias. We propose a new family of methods, called Simulated ICA, which helps reducing this training bias by simulating inference during learning. Several variants of the method are introduced. They are both simple, efficient and scale well. Experiments performed on a series of 7 datasets show that the proposed methods outperform representative state-of-the-art algorithms while keeping a low complexity.
9833 en Combining Instance-Based Learning and Logistic Regression for Multi-Label Classification Multilabel classification is an extension of conventional classification innwhich a single instance can be associated with multiple labels. Recentnresearch has shown that, just like for conventional classification, instance-based learning algorithms relying on the nearest neighbor estimation principle can be used quite successfully in this context. However, since hitherto existing algorithms do not take correlations and interdependencies between labels into account, their potential has not yet been fully exploited. In this paper, we propose a new approach to multilabel classification, which is based on a framework that unifies instance-based learning and logistic regression, comprising both methods as special cases. This approach allows one to capture interdependencies between labels and, moreover, to combine model-based and similarity-based inference for multilabel classification. As will be shown by experimental studies, our approach is able to improve predictive accuracy in terms of several evaluation criteria for multilabel prediction.
9834 en Within-Network Classification Using Local Structure Similarity Within-network classification, where the goal is to classify the nodes of a partly labeled network, is a semi-supervised learning problem that has applications in several important domains like image processing, the classification of documents, and the detection of malicious activities. While most methods for this problem infer the missing labels collectively based on the hypothesis that linked or nearby nodes are likely to have the same labels, there are many types of networks for which this assumption fails, e.g., molecular graphs, trading networks, etc. In this paper, we present a collective classification method, based on relaxation labeling, that classifies entities of a network using their local structure. This method uses a marginalized similarity kernel that compares the local structure of two nodes with random walks in the network. Through experimentation on different datasets, we show our method to be more accurate than several state-of-the-art approaches for this problem.n
9835 en Learning Preferences with Hidden Common Cause Relations Gaussian processes have successfully been used to learn preferences among entities as they provide nonparametric Bayesian approaches for model selection and probabilistic inference. For many entities encountered in real-world applications, however, there are complex relations between them. In this paper, we present a preference model which incorporates information on relations among entities. Specifically, we propose a probabilistic relational kernel model for preference learning based on Silva et al.’s mixed graph Gaussian processes: a new prior distribution, enhanced with relational graph kernels, is proposed to capture the correlations between preferences. Empirical analysis on the LETOR datasets demonstrates that relational information can improve the performance of preference learning.
9836 en An l1 Regularization Framework for Optimal Rule Combination In this paper l1 regularization is introduced into relational learning to produce sparse rule combination. In other words, as few as possible rules are contained in the final rule set. Furthermore, we design a rule complexity penalty to encourage rules with fewer literals. The resulted optimization problem has to be formulated in an infinite dimensional space of horn clauses $R_m$ associated with their corresponding complexity $\mathcal{C}_m$. It is proved that if a locally optimal rule is generated at each iteration, the final obtained rule set will be globally optimal. The proposed meta-algorithm is applicable to any single rule generator. We bring forward two algorithms, namely, $\ell_1$FOIL and $\ell_1$Progol. Empirical analysis is carried on ten real world tasks from bioinformatics and cheminformatics. The results demonstrate that our approach offers competitive prediction accuracy while the interpretability is straightforward.
9838 en Enhancing the Performance of Centroid Classifier by ECOC and Model-Refinement With the aim of improving the performance of centroid text classifier, we attempt to make use of the advantages of Error-Correcting Output Codes (ECOC) strategy. The framework is to decompose one multi-class problem into multiple binary problems and then learn the individual binary classification problems by centroid classifier. However, this kind of decomposition incurs considerable bias for centroid classifier, which results in noticeable degradation of performance for centroid classifier. In order to address this issue, we use Model-Refinement strategy to adjust this so-called bias. The basic idea is to take advantage of misclassified examples in the training datanto iteratively refine and adjust the centroids of text data. The experimental results reveal that Model-Refinement strategy can dramatically decrease the bias introduced by ECOC, and the combined classifier is comparable to or even better than SVM classifier in performance.
9839 en PLSI: The True Fisher Kernel and Beyond The Probabilistic Latent Semantic Indexing model, introduced by T. Hofmann (1999), has engendered applications in numerous fields, notably document classification and information retrieval. In this context, the Fisher kernel was found to be an appropriate document similarity measure. However, the kernels published so far contain unjustified features, some of which hinder their performances. Furthermore, PLSI is not generative for unknown documents, a shortcoming usually remedied by ”folding them in” the PLSI parameter space. This paper contributes on both points by (1) introducing a new, rigorous development of the Fisher kernel for PLSI, addressing the role of the Fisher Information Matrix, and uncovering its relation to the kernels proposed so far; and (2) proposing a novel and theoretically sound document similarity, which avoids the problem of ”folding in” unknown documents. For both aspects, experimental results are provided on several information retrieval evaluation sets.
9840 en Dependency Tree Kernels for Relation Extraction from Natural Language Text The automatic extraction of relations from unstructured natural text is challenging but offers practical solutions for many problems like automatic text understanding and semantic retrieval. Relation extraction can be formulated as a classification problem using support vector machines and kernels for structured data that may include parse trees to account for syntactic structure. In this paper we present new tree kernels over dependency parse trees automatically generated from natural language text. Experiments on a public benchmark data set show that our kernels with richer structural features significantly outperform all published approaches for kernel-based relation extraction from dependency trees. In addition we optimize kernel computations to improve the actual runtime compared to previous solutions.
9841 en The Sensitivity of Latent Dirichlet Allocation for Information Retrieval It has been shown that the use of topic models for Information retrieval provides an increase in precision when used in the appropriate form. Latent Dirichlet Allocation (LDA) is a generative topic model that allows us to model documents using a Dirichlet prior. Using this topic model, we are able to obtain a fitted Dirichlet parameter that provides the maximum likelihood for the document set. In this article, we examine the sensitivity of LDA with respect to the Dirichlet parameter when used for Information retrieval. We compare the topic model computation times, storage requirements and retrieval precision of fitted LDA to LDA with a uniform Dirichlet prior. The results show there there is no significant benefit of using fitted LDA over the LDA with a constant Dirichlet parameter, hence showing that LDA is insensitive with respect to the Dirichlet parameter when used for Information retrieval.
9842 en Protein Identification from Tandem Mass Spectra with Probabilistic Language Modeling This paper presents an interdisciplinary investigation of statistical information retrieval (IR) techniques for protein identification from tandem mass spectra, a challenging problem in proteomic data analysis. We formulate the task as an IR problem, by constructing a “query vector” whose elements are system-predicted peptides with confidence scores based on spectrum analysis of the input sample, and by defining the vector space of “documents” with protein profiles, each of which is constructed based on the theoretical spectrum of a protein. This formulation establishes a new connection from the protein identification problem to a probabilistic language modeling approach as well as the vector space models in IR, and enables us to compare fundamental differences in the IR models and common approaches in protein identification. Our experiments on benchmark spectrometry query sets and large protein databases demonstrate that the IR models significantly outperform well-established methods in protein identification, by enhancing precision in high-recall regions in particular, where the conventional approaches are weak.
9843 en One Graph is Worth a Thousand Logs: Uncovering Hidden Structures in Massive System Event Logs In this paper we describe our work on pattern discovery in system event logs. For discovering the patterns we developed two novel algorithms. The first is a sequential and efficient text clustering algorithm which automatically discovers the templates generating the messages. The second, the PARIS algorithm (Principle Atom Recognition In Sets), is a novel algorithm which discovers patterns of messages that represent processes occurring in the system. We demonstrate the usefulness of our analysis, on real world logs from various systems, for debugging of complex systems, efficient search and visualization of logs and characterization of system behavior.
9844 en Leveraging Higher Order Dependencies Between Features for Text Classification Traditional machine learning methods only consider relationships between feature values within individual data instances while disregarding the dependencies that link features across instances. In this work, we develop a general approach to supervised learning by leveraging higher-order dependencies between features. We introduce a novel Bayesian framework for classification named Higher Order Naive Bayes (HONB). Unlike approaches that assume data instances are independent, HONB leverages co-occurrence relations between feature values across different instances. Additionally, we generalize our framework by developing a novel data-driven space transformation that allows any classifier operating in vector spaces to take advantage of these higher-order co-occurrence relations. Results obtained on several benchmarkntext corpora demonstrate that higher-order approaches achieve significant improvements in classification accuracy over the baseline (first-order) methods.
9845 en Mining Peculiar Compositions of Frequent Substrings from Sparse Text Data Using Background Texts We consider mining unusual patterns from text T. Unlike existing methods which assume probabilistic models and use simple estimation methods, we employ a set B of background text in addition to T and compositions w=xy of x and y as patterns. A string w is peculiar if there exist x and y such that w=xy, each of x and y is more frequent in B than in T, and conversely w=xy is more frequent in T. The frequency of xy in T is very small since x and y are infrequent in T, but xy is relatively abundant in T compared to xy in B. Despite these complex conditions for peculiar compositions, we develop a fast algorithm to find peculiar compositions using the suffix tree. Experiments using DNA sequences show scalability of our algorithm due to our pruning techniques and the superiority of the concept of the peculiar composition.
9846 en Learning to Disambiguate Search Queries from Short Sessions Web searches tend to be short and ambiguous. It is therefore not surprising that Web query disambiguation is an actively researched topic. To provide a personalized experience for a user, most existing work relies on search engine log data in which the search activities of that particular user, as well as other users, are recorded over long periods of time. Such approaches may raise privacy concerns and may be difficult to implement for pragmatic reasons. We present an approach to Web query disambiguation that bases its predictions only on a short glimpse of user search activity, captured in a brief session of 4-6 previous searches on average. Our method exploits the relations of the current search session to previous similarly short sessions of other users in order to predict the user’s intentions and is based on Markov logic, a statistical relational learning model that has been successfully applied to challenging language problems in the past. We present empirical results that demonstrate the effectiveness of our proposed approach on data collected from a commercial general-purpose search engine.
9847 en Topic Significance Ranking of LDA Generative Models Topic models, like Latent Dirichlet Allocation (LDA), have been recently used to automatically generate text corpora topics, and to subdivide the corpus words among those topics. However, not all the estimated topics are of equal importance or correspond to genuine themes of the domain. Some of the topics can be a collection of irrelevant words, or represent insignificant themes. Current approaches to topic modeling perform manual examination to find meaningful topics. This paper presents the first automated unsupervised analysis of LDA models to identify junk topics from legitimate ones, and to rank the topic significance. Basically, the distance between a topic distribution and three definitions of “junk distributions” is computed using a variety of measures, from which an expressive figure of the topic significance is implemented using 4-phase Weighted Combination approach. Our experiments on synthetic and benchmark datasets show the effectiveness of the proposed approach in ranking the topic significance.
9848 en Identifying the Original Contribution of a Document via Language Modeling One major goal of text mining is to provide automatic methods to help humans grasp the key ideas in ever-increasing text corpora. To this effect, we propose a statistically well-founded method for identifying the original ideas that a document contributes to a corpus, focusing on self-referential diachronic corpora such as research publications, blogs, email, and news articles. Our statistical model of passage impact defines (interesting) original content through a combination of impact and novelty, and the model is used to identify each document’s most original passages. Unlike heuristic approaches, the statistical model is extensible and open to analysis. We evaluate the approach both on synthetic data and on real data in the domains of research publications and news, showing that the passage impact model outperforms a heuristic baseline method.
9849 en Stanford University 
9850 en Bloomberg 
9851 en Zemanta 
9852 en NYTimes Research 
9853 en Accenture Labs 
9854 en eBay Research 
9855 en Yahoo Research 
9856 en Discovering Patterns in Flows: A Privacy Preserving Approach with the ACSM 
9857 en Enhanced Web Page Content Visualization with Firefox 
9858 en Semi-Automatic Categorization of Videos on VideoLectures.net / Visual OntoBridge: Semi-Automatic Semantic Annotation Software 
9860 en TeleComVis: Exploring Temporal Communities in Telecom Networks 
9861 en OTTHO: On the Tip of My Thought 
9862 en Found in Translation (WEB) 
9863 en Omiotis: A Thesaurus-based Measure of Text Relatedness (WEB) 
9864 en Protecting Sensitive Topics in Text Documents with PROTEXTOR 
9865 en A Community-Based Platform for Machine Learning Experimentation 
9866 en A Flexible and Efficient Algorithm for Regularized Fisher Discriminant Analysis Fisher linear discriminant analysis (LDA) and its kernel extension—kernel discriminant analysis (KDA)—are well known methods that consider dimensionality reduction and classification jointly. While widely deployed in practical problems, there are still unresolved issues surrounding their efficient implementation and their relationship with least mean squared error procedures. In this paper we address these issues within the framework of regularized estimation. Our approach leads to a flexible and efficient implementation of LDA as well as KDA. We also uncover a general relationship between regularized discriminant analysis and ridge regression. This relationship yields variations on conventional LDA based on the pseudoinverse and a direct equivalence to an ordinary least squares estimator. Experimental results on a collection of benchmark data sets demonstrate the effectiveness of our approach.
9867 en Syntactic Structural Kernels for Natural Language Interfaces to Databases A core problem in data mining is to retrieve data in a easy and human friendly way. Automatically translating natural language questions into SQL queries would allow for the design of effective and useful database systems from a user viewpoint. Interesting previous work has been focused on the use of machine learning algorithms for automatically mapping natural language (NL) questions to SQL queries. In this paper, we present many structural kernels and their combinations for inducing the relational semantics between pairs of NL questions and SQL queries. We measure the effectiveness of such kernels by using them in Support Vector Machines to select the queries that correctly answer to NL questions. Experimental results on two different datasets show that our approach is viable and that syntactic information under the form of pairs ofnsyntactic tree fragments (from queries and questions) plays a major role in deriving the relational semantics between the two languages.nnn
9868 en Kernels for Periodic Time Series Arising in Astronomy We present a method for applying machine learning algorithms to the automatic classification of astronomy star surveys using time series of star brightness. Currently such classification requires a large amount of domain expert time. We show that a combination of phase invariant similarity and explicit features extracted from the time series provide domain expert level classification. To facilitate this application, we investigate the cross-correlation as a general phase invariant similarity function for time series. We establish several theoretical properties of cross-correlation showing that it is intuitively appealing and algorithmically tractable, but not positive semidefinite, and therefore not generally applicable with kernel methods. As a solution we introduce a positive semidefinite similarity function with the same intuitive appeal as cross-correlation. An experimental evaluation in the astronomy domain as well as several other data sets demonstrates the performance of the kernel and related similarity functions.
9869 en Kernel-Based Copula Processes Kernel-based Copula Processes (KCPs), a new versatile tool for analyzing multiple time-series, are proposed here as a unifying framework to model the interdependency across multiple time-series and the long-range dependency within an individual time-series. KCPs build on the celebrated theory of copula which allows for the modeling of complex interdependence structure, while leveraging the power of kernel methods for efficient learning and parsimonious model specification. Specifically, KCPs can be viewed as a generalization of the Gaussian processes enabling non-Gaussian predictions to be made. Such non-Gaussian features are extremely important in a variety of application areas. As one application, we consider temperature series from weather stations across the US. Not only are KCPs found to have modeled the heteroskedasticity of the individual temperature changes well, the KCPs also successfully discovered the interdependencies among different stations. Such results are beneficial for weather derivatives trading and risk management, for example.
9870 en Parameter-Free Hierarchical Co-Clustering by n-Ary Splits Clustering high-dimensional data is challenging. Classic metrics fail in identifying real similarities between objects. Moreover, the huge number of features makes the cluster interpretation hard. To tackle these problems, several co-clustering approaches have been proposed which try to compute a partition of objects and a partition of features simultaneously. Unfortunately, these approaches identify only a predefined number of flat co-clusters. Instead, it is useful if the clusters are arranged in a hierarchical fashion because the hierarchy provides insides on the clusters.In this paper we propose a novel hierarchical co-clustering, which builds two coupled hierarchies, one on the objects and one on features thus providing insights on both them. Our approach does not require a pre-specified number of clusters, and produces compact hierarchies because it makes ro-ary splits, where ro is automatically determined. We validate our approach on several high-dimensional datasets with state of the art competitors.
9871 en A Matrix Factorization Approach for Integrating Multiple Data Views In many domains there will exist different representations or “views” describing the same set of objects. Taken alone, these views will often be deficient or incomplete. Therefore a key problem for exploratory data analysis is the integration of multiple views to discover the underlying structures in a domain. This problem is made more difficult when disagreement exists between views. We introduce a new unsupervised algorithm for combining information from related views, using a “late integration” strategy. Combination is performed by applying an approach based on matrix factorization to group related clusters produced on individual views. This yields a projection of the original clusters in the form of a new set of “meta-clusters” covering the entire domain. We also provide a novel model selection strategy for identifying the correct number of meta-clusters. Evaluations performed on a number of multi-view text clustering problems demonstrate the effectiveness of the algorithm.
9873 en MACs: Multi-Attribute Co-Clusters with High Correlation Information In many real-world applications that analyze correlations between two groups of diverse entities, each group of entities can be characterized by multiple attributes. As such, there is a need to co-cluster multiple attributes’ values into pairs of highly correlated clusters. We denote this co-clustering problem as the multi-attribute co-clustering problem. In this paper, we introduce a generalization of the mutual information between two attributes into mutual information between two attribute sets. The generalized formula enables us to use correlation information to discover multi-attribute co-clusters (MACs). We develop a novel algorithm MACminer to mine MACs with high correlation information from datasets. We demonstrate the mining efficiency of MACminer in datasets with multiple attributes, and show that MACs with high correlation information have higher classification and predictive power, as compared to MACs generated by alternative high-dimensional data clustering and pattern mining techniques.
9874 en Integrating Logical Reasoning and Probabilistic Chain Graphs Probabilistic logics have attracted a great deal of attention during the past few years. While logical languages have taken a central position in research on knowledge representation and automated reasoning, probabilistic graphical models with their probabilistic basis have taken up a similar position when it comes to reasoning with uncertainty. The formalism of chain graphs is increasingly seen as a natural probabilistic graphical formalism as it generalises both Bayesian networks and Markov networks, and has a semantics which allows any Bayesian network to have a unique graphical representation. At the same time, chain graphs do not support modelling and learning of relational aspects of a domain. In this paper, a new probabilistic logic, chain logic, is developed along the lines of probabilistic Horn logic. The logic leads to relational models of domains in which associational and causal knowledge are relevant and where probabilistic parameters can be learned from data.
9875 en Relevance Grounding for Planning in Relational Domains Probabilistic relational models are an efficient way to learn and represent the dynamics in realistic environments consisting of many objects. Autonomous intelligent agents that ground this representation for all objects need to plan in exponentially large state spaces and large sets of stochastic actions. A key insight for computational efficiency is that successful planning typically involves only a small subset of relevant objects. In this paper, we introduce a probabilistic model to represent planning with subsets of objects and provide a definition of object relevance. Our definition is sufficient to prove consistency between repeated planning in partially grounded models restricted to relevant objects and planning in the fully grounded model. We propose an algorithm that exploits object relevance to plan efficiently in complex domains. Empirical results in a simulated 3D blocksworld with an articulated manipulator and realistic physics prove the effectiveness of our approach.
9876 en On Structured Output Training: Hard Cases and an Efficient Alternative We consider a class of structured prediction problems for which the assumptions made by state-of-the-art algorithms fail. To deal with exponentially sized output sets, these algorithms assume, for instance, that the best output for a given input can be found efficiently. While this holds for many important real world problems, there are also many relevant and seemingly simple problems where these assumptions do not hold. In this paper, we consider route prediction, which is the problem of finding a cyclic permutation of some points of interest, as an example and show that state-of-the-art approaches cannot guarantee polynomial runtime for this output set. We then present a novel formulation of the learning problem that can be trained efficiently whenever a particular ’super-structure counting’ problem can be solved efficiently for the output set. We also list several output sets for which this assumption holds and report experimental results.
9877 en Applying Electromagnetic Field Theory Concepts to Clustering with Constraints This work shows how concepts from the electromagnetic field theory can be efficiently used in clustering with constraints. The proposed framework transforms vector data into a fully connected graph, or just works straight on the given graph data. User constraints are represented by electromagnetic fields that affect the weight of the graph’s edges. A clustering algorithm is then applied on the adjusted graph, using k-distinct shortest paths as the distance measure. Our framework provides better accuracy compared to MPCK-Means, SS-Kernel-KMeans and KMeans+Diagonal Metric even when very few constraints are used, significantly improves clustering performance on some datasets that other methods fail to partition successfully, and can cluster both vector and graph datasets. All these advantages are demonstrated through thorough experimental evaluation.
9878 en Max-Margin Weight Learning for Markov Logic Networks Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing discriminative weight learning methods for MLNs all try to learn weights that optimize the Conditional Log Likelihood (CLL) of the training examples. In this work, we present a new discriminative weight learning method for MLNs based on a max-margin framework. This results in a new model, Max-Margin Markov Logic Networks (M3LNs), that combines the expressiveness of MLNs with the predictive accuracy of structural Support Vector Machines (SVMs). To train the proposed model, we design a new approximation algorithm for loss-augmented inference in MLNs based on Linear Programming (LP). The experimental result shows that the proposed approach generally achieves higher $F_1$ scores than the current best discriminative weight learner for MLNs.
9879 en Adaptive XML Tree Classification on Evolving Data Streams We propose a new method to classify patterns, using closed and maximal frequent patterns as features. Generally, classification requires a previous mapping from the patterns to classify to vectors of features, and frequent patterns have been used as features in the past. Closed patterns maintain the same information as frequent patterns using less space and maximal patterns maintain approximate information. We use them to reduce the number of classification features. We present a new framework for XML tree stream classification. For the first component of our classification framework, we use closed tree mining algorithms for evolving data streams. For the second component, we use state of the art classification methods for data streams. To the best of our knowledge this is the first work on tree classification in streaming data varying with time. We give a first experimental evaluation of the proposed classification method.
9880 en Integrating Novel Class Detection with Classification for Concept-Drifting Data Streams In a typical data stream classification task, it is assumed that the total number of classes are fixed. This assumption may not be valid in a real streaming environment, where new classes may evolve. Traditional data stream classification techniques are not capable of recognizing novel class instances until the appearance of the novel class is manually identified, and labeled instances of that class are presented to the learning algorithm for training. The problem becomes more challenging in the presence of concept-drift, when the underlying data distribution changes over time. We propose a novel and efficient technique that can automatically detect the emergence of a novel class in the presence of concept-drift by quantifying cohesion among unlabeled test instances, and separation of the test instances from training instances. Our approach is non-parametric, meaning, it does not assume any underlying distributions of data. Comparison with the state-of-the-art stream classification techniques prove the superiority of our approach.
9881 en Harnessing the Strengths of Anytime Algorithms for Constant Data Streams Anytime algorithms have been proposed for many different applications e.g. in data mining. Their strengths are the ability to first provide a result after a very short initialization and second to improve their result with additional time. Therefore, anytime algorithms have so far been used when the available processing time varies, e.g. on varying data streams. In this paper we propose to employ anytime algorithms on constant data streams, i.e. for tasks with constant time allowance. We introduce two approaches that harness the strengths of anytime algorithms on constant data streams and thereby improve the over all quality of the result with respect to the corresponding budget algorithm. We derive formulas for the expected performance gain and demonstrate the effectiveness of our novel approaches using existing anytime algorithms on benchmark data sets. The goal that was set and reached in this paper is to improve the quality of the result over that of traditional budget approaches, which are used in annabundance of stream mining applications. Using anytime classification as annexample application we show for SVM, Bayes and nearest neighbor classifiers that both our novel approaches improve the classification accuracy for slow and fast data streams. The results confirm our general theoretic models and show the effectiveness of our approaches. The simple yet effective idea can be employed for any anytime algorithm along with a quality measure and motivates further research in e.g. classification confidence measures or anytime algorithms.
9882 en Bi-Directional Joint Inference for Entity Resolution and Segmentation Using Imperatively-Defined Factor Graphs There has been growing interest in using joint inference across multiple subtasks as a mechanism for avoiding the cascading accumulation of errors in traditional pipelines. Several recent papers demonstrate joint inference between the segmentation of entity mentions and their de-duplication, however, they have various weaknesses: inference information flows only in one direction, the number of uncertain hypotheses is severely limited, or the subtasks are only loosely coupled. This paper presents a highly-coupled, bi-directional approach to joint inference based on efficient Markov chain Monte Carlo sampling in a relational conditional random field. The model is specified with our new probabilistic programming language that leverages imperative constructs to define factor graph structure and operation. Experimental results show that our approach provides a dramatic reduction in error while also running faster than the previous state-of-the-art system.
9883 en Variational Graph Embedding for Globally and Locally Consistent Feature Extraction Existing feature extraction methods explore either global statistical or local geometric information underlying the data. In this paper, we propose a general framework to learn features that account for both types of information based on variational optimization of nonparametric learning criteria. Using mutual information and Bayes error rate as example criteria, we show that high-quality features can be learned from a variational graph embedding procedure, which is solved through an iterative EM-stylenalgorithm where the E-Step learns a variational affinity graph and the M-Step in turn embeds this graph by spectral analysis. The resulting feature learner has several appealing properties such as maximum discrimination, maximum-relevance-minimum-redundancy and locality-preserving. Experiments on benchmark face recognition data sets confirm the effectiveness of our proposed algorithms.
9884 en Dynamic Factor Graphs for Time Series Modeling This article presents a method for training Dynamic Factor Graphs (DFG) with continuous latent state variables. A DFG includes factors modeling jointnprobabilities between hidden and observed variables, and factors modelingndynamical constraints on hidden variables. The DFG assigns a scalar energy to each configuration of hidden and observed variables. A radient-based inference procedure finds the minimum-energy state sequence for a givennobservation sequence. Because the factors are designed to ensure a constant partition function, they can be trained by minimizing the expected energy over training sequences with respect to the factors’ parameters. These alternated inference and parameter updates can be seen as a deterministic EM-like procedure. Using smoothing regularizers, DFGs are shown to reconstruct chaotic attractors and to separate a mixture of independent oscillatory sources perfectly. DFGs outperform the best known algorithm on the CATS competition benchmark for time series prediction. DFGs also successfully reconstruct missing motion capture data.
9885 en RTG: A Recursive Realistic Graph Generator using Random Typing We propose a new, recursive model to generate realistic graphs,nevolving over time. Our model has the following properties: it is (a)nflexible, capable of generating the cross product of weighted/unweighted, directed/undirected, uni/bipartite graphs; (b) realistic, giving graphs thatnobey eleven static and dynamic laws that real graphs follow (we formallynprove that for several of the (power) laws and we estimate their exponentsnas a function of the model parameters); (c) parsimonious, requiring only four parameters. (d) fast, being linear on the number of edges; (e) simple, intuitively leading to the generation of macroscopic patterns. We empirically show that our model mimics two real-world graphs very well: Blognet (unipartite, undirected, unweighted) with 27K nodes and 125K edges; and Committee-to-Candidate campaign donations (bipartite, directed,nweighted) with 23K nodes and 880K edges. We also show how to handle time so that edge/weight additions are bursty and self-similar.
9886 en Mining Graph Evolution Rules In this paper we introduce graph-evolution rules, a novel type of frequency-based pattern that describe the evolution of large networks over time, at a local level. Given a sequence of snapshots of an evolving graph, we aim at discovering rules describing the local changes occurring in it. Adopting a definition of support based on minimum image we study the problem of extracting patterns whose frequency is larger than a minimum support threshold. Then, similar to the classical association rules framework, we derive graph-evolution rules from frequent patterns that satisfy a given minimum confidence constraint. We discuss meritsand limits of alternative definitions of support and confidence, justifying the chosen framework. To evaluate our approach we devise GERM (Graph Evolution Rule Miner), an algorithm to mine all graph-evolution rules whose support and confidence are greater than given thresholds. The algorithm is applied to analyze four large real-world networks (i.e., two social networks, and two co-authorship networks from bibliographic data), using different time granularities. Our extensive experimentation confirms the feasibility and utility of the presented approach. It further shows that different kinds of networks exhibit different evolution rules, suggesting the usage of these local patterns to globally discriminate different kind of networks.
9887 en Binary Decomposition Methods for Multipartite Ranking Bipartite ranking refers to the problem of learning a ranking function from a training set of positively and negatively labeled examples. Applied to a set of unlabeled instances, a ranking function is expected to establish a total order in which positive instances precede negative ones. The performance of a ranking function is typically measured in terms of the AUC. In this paper, we study the problem of multipartite ranking, an extension of bipartite ranking to the multi-class case. In this regard, we discuss extensions of the AUC metric which are suitable as evaluation criteria for multipartite rankings. Moreover, to learn multipartite ranking functions, we propose methods on the basis of binary decomposition techniques that have previously been used for multi-class and ordinal classification. We compare these methods both analytically and experimentally, not only against each other but also to existing methods applicable to the same problem.
9888 en Cost-sensitive learning based on Bregman divergences This paper analyzes the application of a particular class of Bregman divergences to design cost-sensitive classifiers for multiclass problems. We show that these divergence measures can be used to estimate posterior probabilities with maximal accuracy for the probability values that are close to the decision boundaries. Asymptotically, the proposed divergence measures provide classifiers minimizing the sum of decision costs in non-separableproblems, and maximizing a margin in separable MAP problems.
9889 en A Self-Training Approach to Cost Sensitive Uncertainty Sampling Uncertainty sampling is an effective method for performing active learning that is computationally efficient compared to other active learning methods such as loss-reduction methods. However, unlike lossreduction methods, uncertainty sampling cannot minimize total misclassification costs when errors incur different costs. This paper introduces a method for performing cost-sensitive uncertainty sampling that makes use of self-training. We show that, even when misclassification costs are equal, this self-training approach results in faster reduction of loss as a function of number of points labeled and more reliable posterior probability estimates as compared to standard uncertainty sampling. We also show why other more naive methods of modifying uncertainty sampling to minimize total misclassification costs will not always work well.
9890 en Statistical Relational Learning with Formal Ontologies We propose a learning approach for integrating formal knowledge into statistical inference by exploiting ontologies as a semantically rich and fully formal representation of prior knowledge. The logical constraints deduced from ontologies can be utilized to enhance and control the learning task by enforcing description logic satisfiability in a latent multi-relational graphical model. To demonstrate the feasibility of our approach we provide experiments using real world social network data in form of a SHOIN(D) ontology. The results illustrate two main practical advancements: First, entities and entity relationships can be analyzed via the latent model structure. Second, enforcing the ontological constraints guarantees that the learned model does not predict inconsistent relations. In our experiments, this leads to an improved predictive performance.
9891 en Inference and Validation of Networks We develop a statistical methodology to validate the result of network inference algorithms, based on principles of statistical testing and machine learning. The comparison of results with reference networks, by means of similarity measures and null models, allows us to measure the significance of results, as well as their predictive power. The use of Generalised Linear Models allows us to explain the results in terms of available ground truth which we expect to be partially relevant. We present these methods for the case of inferring a network of News Outlets based on their preference of stories to cover. We compare three simple network inference methods and show how our technique can be used to choose between them. All the methods presented here can be directly applied to other domains where network inference is used.
9892 en Neural Networks for State Evaluation in General Game Playing Unlike traditional game playing, General Game Playing is concerned with agents capable of playing classes of games. Given the rules of an unknown game, the agent is supposed to play well without human intervention. For this purpose, agent systems that use deterministic game tree search need to automatically construct a state value function to guide search. Successful systems of this type use evaluation functions derived solely from the game rules, thus neglecting further improvements by experience. In addition, these functions are fixed in their form and do not necessarily capture the game’s real state value function. In this work we present an approach for obtaining evaluation functions on the basis of neural networks that overcomes the aforementioned problems. A network initialization extracted from the game rules ensures reasonable behavior without the need for prior training. Later training, however, can lead to significant improvements in evaluation quality, as our results indicate.
9893 en Learning Multi-Linear Representations of Probability Distributions for Efficient Inference We examine the class of multi-linear polynomial representations (MLR) for expressing probability distributions over discrete variables. Recently, MLR have been considered as intermediate representations that facilitate inference in distributions represented as graphical models. We show that MLR is an expressive representation of discrete distributions and can be used to concisely represent classes of distributions which have exponential size in other commonly used representations, while supporting probabilistic inference in time linear in the size of the representation. Our key contribution is presenting techniques for learning bounded-size distributions represented using MLR, which support efficient probabilistic inference. We propose algorithms for exact and approximate learning for MLR and, through a comparison with Bayes Net representations, demonstrate experimentally that MLR representations provide faster inference without sacrificing inference accuracy.
9894 en Subspace Regularization: A New Semi-Supervised Learning Method Most existing semi-supervised learning methods are based on the smoothness assumption that data points in the same high density region should have the same label. This assumption, though works well in many cases, has limitations. To overcome this problems, we introduce into semi-supervised learning the classic low-dimensionality embedding assumption, stating that most geometric information of high dimensional data is embedded in a low dimensional manifold. Based on this, we formulate the problem of semi-supervised learning as a task of finding a subspace and a decision function on the subspace such that the projected data are well separated and the original geometric information is preserved as much as possible. Under this framework, the optimal subspace and decision function are iteratively found via a projection pursuit procedure. The low computational complexity of the proposed method lends it to applications on large scale data sets. Experimental results demonstrates the effectiveness of our method.
9895 en Active and Semi-Supervised Data Domain Description Data domain description techniques aim at deriving concise descriptions of objects belonging to a category of interest. For instance, the support vector domain description (SVDD) learns a hypersphere enclosing the bulk of provided unlabeled data such that points lying outside of the ball are considered anomalous. However, relevant information such as expert and background knowledge remain unused in the unsupervised setting. In this paper, we rephrase data domain description as a semi-supervised learning task, that is, we propose a semi-supervised generalization of data domain description (SSSVDD) to process unlabeled and labeled examples. The corresponding optimization problem is non-convex. We translate it into an unconstraint, continuous problem that can be optimized accurately by gradient-based techniques. Furthermore, we devise an effective active learning strategy to query low-confidence observations. Our empirical evaluation on network intrusion detection and object recognition tasks shows that our SSSVDDs consistently outperform baseline methods in relevant learning settings.
9896 en Semi-Supervised Multi-Task Regression Labeled data are needed for many machine learning applications but the amount available in some applications is scarce. Semi-supervised learning and multi-task learning are two of the approaches that have been proposed to alleviate this problem. In this paper, we seek to integrate these two approaches for regression applications. We first propose a new supervised multi-task regression method called SMTR, which is based on Gaussian processes (GP) with the assumption that the kernel parameters for all tasks share a common prior. We then incorporate unlabeled data into SMTR by changing the kernel function of the GP prior to a data-dependent kernel function, resulting in a semi-supervised extension of SMTR, called SSMTR. Moreover, we incorporate pairwise information into SSMTR to further boost the learning performance for applications in which such information is available. Experiments conducted on two commonly used data sets for multi-task regression demonstrate the effectiveness of our methods.
9897 en Transductive Classification via Dual Regularization Semi-supervised learning has witnessed increasing interest in the past decade. One common assumption behind semi-supervised learning is that the data labels should be sufficiently smooth with respect to the intrinsic data manifold. Recent research has shown that the features also lie on a manifold. Moreover, there is a duality between data points and features, that is, data points can be classified based on their distribution on features, while features can be classified based on their distribution on the data points.nHowever, existing semi-supervised learning methods neglect these points. In this paper, we present a dual regularization, which consists of two graph regularizers and a co-clustering type regularizer. Furthermore, we propose a novel transductive classification framework based on dual regularization, which can be solved by alternating minimization algorithm and its convergence is theoretically guaranteed. Experiments demonstrate that the proposed methods outperform many state of the art transductive classification methods.
9898 en New Regularized Algorithms for Transducitve Learning We propose a new graph-based label propagation algorithm for transductive learning. Each example is associated with a vertex in an undirected graph and a weighted edge between two vertices represents similarity between the two corresponding example. We build on Adsorption, a recently proposed algorithm and analyze its properties. We then state our learning algorithm as a convex optimization problem over multi-label assignments and derive an efficient algorithm to solve this problem. We state the conditions under which our algorithm is guaranteed to converge. We provide experimental evidence on various real-world datasets demonstrating the effectiveness of our algorithm over other algorithms for such problems. We also show that our algorithm can be extended to incorporate additional prior information, and demonstrate it with classifying data where the labels are not mutually exclusive.
9899 en Graph-Based Discrete Differential Geometry for Critical Instance Filtering Graph theory has been shown to provide a powerful tool for representing andntackling machine learning problems, such as clustering, semi-supervised learning, and feature ranking. This paper proposes a graph-based discrete differential operator for detecting and eliminating competence-critical instances and class label noise from a training set in order to improve classification performance. Results of extensive experiments on artificial and real-life classification problems substantiate the effectiveness of the proposed approach.
9900 en Active Learning for Reward Estimation in Inverse Reinforcement Learning Inverse reinforcement learning addresses the general problem of recovering a reward function from samples of a policy provided by an expert/demonstrator. In this paper, we introduce active learning for inverse reinforcement learning. We propose an algorithm that allows the agent to query the demonstrator for samples at specific states, instead of relying only on samples provided at ”arbitrary” states. The purpose of our algorithm is to estimate the reward function with similar accuracy as other methods from the literature while reducing the amount of policy samples required from the expert. We also discuss the use of our algorithm in higher dimensional problems, using both Monte Carlo and gradient methods. We present illustrative results of our algorithm in several simulated examples of different complexities.
9901 en Heteroscedastic Probabilistic Linear Discriminant Analysis with Semi-Supervised Extension Linear discriminant analysis (LDA) is a commonly used method for dimensionality reduction. Despite its successes, it has limitations under some situations, including the small sample size problem, the homoscedasticity assumption that different classes have the same Gaussian distribution, and its inability to produce probabilistic output and handle missing data. In this paper, we propose a semi-supervised and heteroscedastic extension of probabilistic LDA, called S$^2$HPLDA, which aims at overcoming all these limitations under a common principled framework. Moreover, we apply automatic relevance determination to determine the required dimensionality of the low-dimensional space for dimensionality reduction. We empirically compare our method with several related probabilistic subspace methods on some face and object databases. Very promising results are obtained from the experiments showing the effectiveness of our proposed method.
9902 en Two-Way Analysis of High-Dimensional Collinear Data We present a Bayesian model for two-way ANOVA-type analysis of high-dimensional, small sample-size datasets with highly correlated groups of variables. Modern cellular measurement methods are a main application area; typically the task is differential analysis between diseased and healthy samples, complicated by additional covariates requiring a multi-way analysis. The main complication is the combination of high dimensionality and low sample size, which renders classical multivariate techniques useless. We introduce a hierarchical model which does dimensionality reduction by assuming that the input variables come in similarly-behaving groups, and performs an ANOVA-type decomposition for the set of reduced-dimensional latent variables. We apply the methods to study lipidomic profiles of a recent large-cohort human diabetes study.
9903 en Feature Weighting Using Margin and Radius Based Error Bound Optimization in SVMs The Support Vector Machine error bound is a function of the margin and radius. Standard SVM algorithms maximize the margin within a given feature space, therefore the radius is fixed and thus ignored in the optimization. We propose an extension of the standard SVM optimization in which we also account for the radius in order to produce an even tighter error bound than what we get by controlling only for the margin. We use a second set of parameters, ${\vect \mu}$, that control the radius introducing like that an explicit feature weighting mechanism in the SVM algorithm. We impose an $l_1$ constraint on ${\vect \mu}$ which results in a sparse vector, thus performing feature selection. Our original formulation is not convex, we give a convex approximation and show how to solve it. We experiment with real world datasets and report very good predictive performance compared to standard SVM.
9904 en Margin and Radius Based Multiple Kernel Learning A serious drawback of kernel methods, and Support Vector Machines (SVM) in particular, is the difficulty in choosing a suitable kernel function for a given dataset. One of the approaches proposed to address this problem is Multiple Kernel Learning (MKL) in which several kernels are combined adaptively for a given dataset. Many of the existing MKL methods use the SVM objective function and try to find a linear combination of basic kernels such that the separating margin between the classes is maximized. However, these methods ignore the fact that the theoretical error bound depends not only on the margin, but also on the radius of the smallest sphere that contains all the training instances. We present a novel MKL algorithm that optimizes the error bound taking account of both the margin and the radius. The empirical results show that the proposed method compares favorably with other state-of-the-art MKL methods.
9905 en Sparse Kernel SVMs via Cutting-Plane Training We explore an algorithm for training SVMs with Kernels that can represent the learned rule using arbitrary basis vectors, not just the support vectors (SVs) from the training set. This results in two benefits. First, the addednflexibility makes it possible to find sparser solutions of good quality,nsubstantially speeding-up prediction. Second, the improved sparsity can also make training of Kernel SVMs more efficient, especially for high-dimensional and sparse data (e.g. text classification). This has the potential to make training of Kernel SVMs tractable for large training sets, where conventional methods scale quadratically due to the linear growth of the number of SVs. In addition to a theoretical analysis of the algorithm, we also present an empirical evaluation.
9906 en Kernel Polytope Faces Pursuit Polytope Faces Pursuit (PFP) is a greedy algorithm that approximates the sparse solutions recovered by 1 regularised least-squares (Lasso) [4, 10] in a similar vein to (Orthogonal) Matching Pursuit (OMP) [16]. The algorithm is based on the geometry of the polar polytope where at each step a basis function is chosen by finding the maximal vertex using a path-following method. The algorithmic complexity is of a similar order to OMP whilst being able to solve problems known to be hard for (O)MP. Matching Pursuit was extended to build kernel-based solutions to machine learning problems, resulting in the sparse regression algorithm, Kernel Matching Pursuit (KMP) [17]. We develop a new algorithm to build sparse kernel-based solutions using PFP, which we call Kernel Polytope Faces Pursuit (KPFP). We show the usefulness of this algorithm by providing a generalisation error bound [7] that takes into account a natural regression loss and experimental results on several benchmark datasets.
9907 en Decomposition Algorithms for Training Large-scale Semiparametric Support Vector Machines We describe a method for solving large-scale semiparametric support vector machines (SVMs) for regression problems. Most of the approaches proposed to date for large-scale SVMs cannot accommodate the multiple equality constraints that appear in semiparametric problems. Our approach uses a decomposition framework, with a primal-dual algorithm to find an approximate saddle point for the min-max formulation of each subproblem. We compare our method with algorithms previously proposed for semiparametric SVMs, and show that it scales well as the number of training examples grows.
9908 en Universal Learning over Related Distributions and Adaptive Graph Transduction The basis assumption “training and test data drawn from the same distribution” is often violated. We propose one common solution to cover various scenarios of learning under “different but related distributions” in a single framework. Examples include (a) sample selection bias (b) transfer learning and (c) uncertain training data. The main motivation is that one could ideally solve as many problems as possible with a single approach. The proposed solution extends graph transduction using the maximum margin principle over unlabeled data. The error of the proposed method is bounded even when the training and testing distributions are different. Experiment results demonstrate that the proposed method improves the traditional graph transduction by as much as 15% in accuracy and AUC in all common situations of distributionndifference. Most importantly, it outperforms, by up to 10% in accuracy, several state-of-art approaches proposed to solve specific category of distribution difference.
9909 en Reconstructing Data Perturbed by Random Projections when the Mixing Matrix is Known Random Projection (RP) has drawn great interest from the research of privacy-preserving data mining due to its high efficiency and security. It was proposed in \cite{Liu} where the original data set composed of $m$ attributes, is multiplied with a mixing matrix of dimensions $k\times m ~(m>k)$ which is random and orthogonal on expectation, and then the $k$ series of perturbed data are released for mining purposes. To our knowledge little work has been done from the view of the attacker, to reconstruct the original data to get some sensitive information, given the data perturbed by RP and some priori knowledge, e.g. the mixing matrix, the means and variances of the original data. In the case that the attributes of the original data are mutually independent and sparse, the reconstruction can be treated as a problem of Underdetermined Independent Component Analysis (UICA), but UICA has some permutation and scaling ambiguities. In this paper we propose a reconstruction framework based on UICA and also some techniques to reduce the ambiguities. The cases that the attributes of the original data are correlated and not sparse are also common in data mining. We also propose a reconstruction method for the typical case of Multivariate Gaussian Distribution, based on the method of Maximum A Posterior (MAP). Our experiments show that our reconstructions can achieve high recovery rates, and outperform the reconstructions based on Principle Component Analysis (PCA).
9910 en On Subgroup Discovery in Numerical Domains Subgroup discovery is a Knowledge Discovery task that aims at finding subgroups of a population with high generality and distributional unusualness. While several subgroup discovery algorithms have been presented in the past, they focus on databases with nominal attributes or make use of discretization to get rid of the numerical attributes. In this paper, we illustrate why the replacement of numerical attributes by nominal attributes can result in suboptimal results. Thereafter, we present a new subgroup discovery algorithm that prunes large parts of the search space by exploiting bounds between related numerical subgroup descriptions. The same algorithm can also be applied to ordinal attributes. In an experimental section, we show that the use of our new pruning scheme results in a huge performance gain when more that just a few split-points are considered for the numerical attributes.
9911 en Evaluation Measures for Multi-Class Subgroup Discovery Subgroup discovery aims at finding subsets of a population whose class distribution is significantly different from the overall distribution. It has previously predominantly been investigated in a two-class context. This paper investigates multi-class subgroup discovery methods. We consider six evaluation measures for multi-class subgroups, four of them new, and study their theoretical properties. We extend the two-class subgroup discovery algorithm CN2-SD to incorporate the new evaluation measures and a new weighting scheme inspired by AdaBoost. We demonstrate the usefulness of multi-class subgroup discovery experimentally, using discovered subgroups as features for a decision tree learner. Not only is the number of leaves of the decision tree reduced with a factor between 8 and 16 on average, but significant improvements in accuracy and AUC are achieved with particular evaluation measures and settings. Similar performance improvements can be observed when using naive Bayes.
9912 en Non-Redundant Subgroup Discovery Using a Closure System Subgroup discovery is a local pattern discovery task, in which descriptions of subpopulations of a database are evaluated against some quality function. As standard quality functions are functions of the described subpopulation, we propose to search for equivalence classes of descriptions with respect to their extension in the database rather than individual descriptions. These equivalence classes have unique maximal representatives forming a closure system. We show that minimum cardinality representatives of each equivalence class can be found during the enumeration process of that closure system without additional cost, while finding a minimum representative of a single equivalence class is NP-hard. With several real-world datasets we demonstrate that search space and output are significantly reduced by considering equivalence classes instead of individual descriptions and that the minimum representatives constitute a family of subgroup descriptions that is of same or better expressive power than those generated by traditional methods.
9913 en Debt Detection in Social Security by Sequence Classification Using Both Positive and Negative Patterns Debt detection is important for improving payment accuracy in social security. Since debt detection from customer transactional data can be generally modelled as a fraud detection problem, a straightforward solution is to extract features from transaction sequences and build a sequence classifier for debts. The existing sequence classification methods based on sequential patterns consider only positive patterns. However, according to our experience in a large social security application, negative patterns are very useful in accurate debt detection. In this paper, we present a successful case study of debt detection in a large social security application. The central technique is building sequence classification using both positive and negative sequential patterns.
9914 en A Condensed Representation of Itemsets for Analyzing their Evolution over Time Driven by the need to understand change within domains there is emerging research on methods which aim at analyzing how patterns and in particular itemsets evolve over time. In practice, however, these methods suffer from the problem that many of the observed changes in itemsets are temporally redundant in the sense that they are the side-effect of changes in other itemsets, hence making the identification of the fundamental changes difficult. As a solution we propose temporally closed itemsets, a novel approach for a condensed representation of itemsets which is based on removing temporal redundancies. We investigate how our approach relates to the well-known concept of closed itemsets if the latter would be directly generalized to account for the temporal dimension. Our experiments support the theoretical results by showing that the set of temporally closed itemsets is significantly smaller than the set of closed itemsets.
9915 en Taxonomy-Driven Lumping for Sequence Mining Given a taxonomy of events and a dataset of sequences of these events,nwe study the problem of finding efficient and effective ways to produce a compact representation of the sequences. We model sequences with Markov models whose states correspond to nodes in the provided taxonomy, and each state represents the events in the subtree under the corresponding node. By lumping observed events to states that correspond to internalnnodes in the taxonomy, we allow more compact models that are easier tonunderstand and visualize, at the expense of a decrease in the data likelihood.nWe formally define and characterize our problem, and propose a scalable search method for finding a good trade-off between two conflicting goals: maximizing the data likelihood, and minimizing the model complexity. We implement these ideas in Taxomo, a taxonomy-driven modeler, which we apply in two different domains, query-log mining and mining of trajectories.
9916 en Identifying the Components Most, if not all, databases are mixtures of samples from different distributions. In many cases, however, nothing is known about the source components of these mix-tures. Therefore, many methods that induce models regard a database as sampled from a single data distribution. Models that do take into account that databases actu-ally are sampled from mixtures of distributions are often superior to those that do not, independent of whether this is modelled explicitly or implicitly. Transaction databases are no different with regard to data distribution.
9917 en On Feature Selection, Bias-Variance, and Bagging We examine the mechanism by which feature selection improves the accuracy of supervised learning. An empirical bias/variance analysis as feature selection progresses indicates that the most accurate feature set corresponds to the best bias-variance trade-off point for the learning algorithm. Often, this is not the point separating relevant from irrelevant features, but where increasing variance outweighs the gains from adding more (weakly) relevant features. In other words, feature selection can be viewed as a variance reduction method that trades off the benefits of decreased variance (from the reduction in dimensionality) with the harm of increased bias (from eliminating some of the relevant features). If a variance reduction method like bagging is used, more (weakly) relevant features can be exploited and the most accurate feature set is usually larger. In many cases, the best performance is obtained by using all available features.
9918 en Multi-Task Feature Selection Using the Multiple Inclusion Criterion (MIC) We address the problem of joint feature selection in multiple related classification or regression tasks. When doing feature selection with multiple tasks, usually one can “borrow strength” across these tasks to get a more sensitive criterion for deciding which features to select. We propose a novel method, the Multiple Inclusion Criterion (MIC), which modifies stepwise feature selection to more easily select features that are helpful across multiple tasks. Our approach allows each feature to be added to none, some, or all of the tasks. MIC is most beneficial for selecting a small set of predictive features from a large pool of potential features, as is common in genomic and biologicalndatasets. Experimental results on such datasets show that MIC usually outperforms other competing multi-task learning methods not only in terms of accuracy but also by building simpler and more interpretable models.
9919 en Stable and Accurate Feature Selection In addition to accuracy, stability is also a measure of success for a feature selection algorithm. Stability could especially be a concern when the number of samples in a data set is small and the dimensionality is high. In this study, we introduce a stability measure, and perform both accuracy and stability measurements of MRMR (Minimum Redundancy Maximum Relevance) feature selection algorithm on different data sets. The two feature evaluation criteria used by MRMR, MID (Mutual Information Difference) and $MIQ$ (Mutual Information Quotient), result in similar accuracies, but MID is more stable. We also introduce a new feature selection criterion, MIDalpha, where redundancy and relevance of selected features are controlled by parameter alpha.
9920 en Feature Selection by Transfer Learning with Linear Regularized Models This paper presents a novel feature selection method for classification of high dimensional data, such as those produced by microarrays. It includes a partial supervision to smoothly favor the selection of some dimensions (genes) on a new dataset to be classified. The dimensions to be favored are previously selected from similar datasets in large microarray databases, hence performing inductive transfer learning at the feature level. This technique relies on a feature selection method embedded within a regularized linear model estimation. A practical approximation of this technique reduces to linear SVM learning with iterative input rescaling. The scaling factors depend on the selected dimensions from the related datasets. The final selection may depart from those whenever necessary to optimize the classification objective. Experiments on several microarray datasets show that the proposed method both improves the selected gene lists stability, with respect to sampling variation, as well as the classification performances.
9921 en Feature Selection for Value Function Approximation Using Bayesian Model Selection Feature selection in reinforcement learning (RL), i.e. choosing basis functions such that useful approximations of the unkown value function can be obtained, is one of the main challenges in scaling RL to real-world applications. Here we consider the Gaussian process based framework GPTD for approximate policy evaluation, and propose feature selection through marginal likelihood optimization of the associated hyperparameters. Our approach has two appealing benefits: (1) given just sample transitions,nwe can solve the policy evaluation problem fully automatically (without looking at the learning task, and, in theory, independent of the dimensionality of the state space), and (2) model selection allows us to consider more sophisticated kernels, which in turn enable us to identify relevant subspaces and eliminate irrelevant state variablesnsuch that we can achieve substantial computational savings and improved prediction performance.
9922 en Capacity Control for Partially Ordered Feature Sets Partially ordered feature sets appear naturally in classification settings with structured instances. For example, when the instances are graphs and the features represent subgraph-occurrence-checks, the features can be partially ordered according to an “is subgraph of” relation. We investigate how the redundancy in such datasets affects the capacity control behavior of linear classification methods. While the capacity does not decrease in general, we derive better capacity bounds for distributions, which assign lower probabilities to instances in the lower levels of the feature hierarchy. For itemset, subsequence and subtrees, the capacity is finite even for data with an infinite number of features. We validate these results empirically and show that the limited capacity of linear classifiers makes underfitting rather than overfitting the more prominent capacity control problem. To avoid underfitting, we propose substructure classes with “elastic edges”, and we demonstrate how such broad feature classes can be used with large datasets.
9923 en Feature Selection for Density Level-Sets A frequent problem in density level-set estimation is the choice of the right features that give rise to compact and concise representations of the observed data. We present an efficient feature selection method for density level-set estimation where optimal kernel mixing coefficients and model parameters are determined simultaneously. Our approach generalizes one-class support vector machines and can be equivalently expressed as a semi-infinite linear program that can be solved with interleaved cutting plane algorithms. The experimental evaluation of the new method on network intrusion detection and object recognition tasks demonstrate that our approach not only attains competitive performance but also spares practitioners from a priori decisions on feature sets to be used.
9924 en The Feature Importance Ranking Measure Most accurate predictions are typically obtained by learning machines with complex feature spaces (as e.g. induced by kernels). Unfortunately, such decision rules are hardly accessible to humans and cannot easily be used to gain insights about the application domain. Therefore, one often resorts to linear models in combination with variable selection, thereby sacrificing some predictive power for presumptive interpretability. Here, we introduce the Feature Importanc Ranking Measure(FIRM), which by retrospective analysis of arbitrary learning machines allows to achieve both excellent predictive performance and superior interpretation. In contrast to standard raw feature weighting, FIRM takes the underlying correlation structure of the features into account. Thereby, it is able to discover the most relevant features, even if their appearance in the training data is entirely prevented by noise. The desirable properties of FIRM are investigated analytically and illustrated in simulations.
9926 en Learning the Difference between Partially Observable Dynamical Systems We propose a new approach for estimating the difference between two partially observable dynamical systems. We assume that one can interact with the systems by performing actions and receiving observations. The key idea is to define a Markov Decision Process (MDP) based on the systems to be compared, in such a way that the optimal value of the MDP initial state can be interpreted as a divergence (or dissimilarity) between the systems. This dissimilarity can then be estimated by reinforcement learning methods. Moreover, the optimal policy will contain information about the actions which most distinguish the systems. Empirical results show that this approach is useful in detecting both big and small differences, as well as in comparing systems with different internal structure.
9927 en Optimal Online Learning Procedures for Model-Free Policy Evaluation In this study, we extend the framework of semiparametric statistical inference introduced recently to reinforcement learning (Ueno, et.al., 2008) to online learning procedures for policy evaluation. This generalization enables us to investigate statistical properties of value function estimators both by batch and online procedures in a unified way in terms of estimating functions. Furthermore, we propose a novel online learning algorithm with optimal estimating functions which achieve the minimum estimation error. Our theoretical developments are confirmed using a simple chain walk problem.
9928 en Boosting Active Learning to Optimality: a Tracable Monte-Carlo, Billiard-Based Algorithm This paper focuses on Active Learning with a limited number of queries; in application domains such as Numerical Engineering, the size of the training set might be limited to a few dozen or hundred examples due to computational constraints. Active Learning under bounded resources is formalized as a finite horizon Reinforcement Learning problem, where the sampling strategy aims at minimizing the expectation of the generalization error. A tractable approximation of the optimal (intractable) policy is presented, the Bandit-based Active Learner (Baal) algorithm. Viewing Active Learning as a single-player game, Baal combines UCT, the tree structured multi-armed bandit algorithm proposed by Kocsis and Szepesvari (2006), and billiard algorithms. A proof of principle of the approach demonstrates its good empirical convergence toward an optimal policy and its ability to incorporate prior AL criteria. Its hybridization with the Query-by-Committee approach is found to improve on both stand-alone Baal and stand-alone QbC.
9930 en Opening of the First International Workshop on LEarning and data Mining for Robotics (LEMIR 2009) 
9931 en Controlling Humanoid Robots by Means of Genetic Programming We show the real-world applications of EC (evolutionary computation)nto robotics, which is called "evolutionary robotics".nMachine Learning techniques can be applied tona robot in order to achieve a task for it if the appropriatenactions are not predetermined. In such a situation, the robot cannlearn the appropriate actions by using trial-and-error in a realnenvironment. GP (Genetic Programming) can generate programs to controlna robot directly, and many studies have been done showing this.nGA (Genetic Algorithms) in combinationnwith neural networks (NN) can also be used to control robots.nRegardless of the method used, the evaluation of real robotsnrequires a significant amount of time partly due to their complexnmechanical actions. Moreover, evaluations have to be repeated overnseveral generations for many individuals in both GP and GA.nTherefore, in most studies, the learning is conducted innsimulation, and the acquired results are applied to real robots.nTo solve these difficulties, we propose an integrated technique ofngenetic programming and reinforcement learning (RL) to enable anreal robot to adapt its actions in a real environment. Ourntechnique does not require a precise simulator because learning isnachieved through the real robot. In addition, our technique makesnit possible for real robots to learn effective actions. Based onnthis proposed technique, we evolve common programs using GP,nwhich are applicable to various types of robots. Using thisnevolved program, we execute reinforcement learning in a realnrobot. With our method, the robot can adapt to its own operationalncharacteristics and learn effective actions. The effectiveness ofnour proposed approach is demonstrated by performing experimentsnwith real humanoid robots.
9932 en Quantification and Minimization of the Simulation-Reality-Gap on a BRIO(R) Labyrinth Game In this paper we present a new method, the so-called -ntunnel, that can be used to quantify the simulation-reality-gap by comparingnthe behavior (the state-trajectory) of a baseline system (for instancena real robotic system) and a model of this system (e.g. a physicbasednsimulation). With this -tunnel, the impact of a change of thenmodel's parameter can be analyzed. Furthermore, we present an approachnto automate the optimization of these parameters and presentnsome results obtained on a robot system that is based on a BRIOR labyrinthngame.
9933 en Toward Using Symbolic Discovery in Designing Controllers of Autonomous Swarm Robots In this paper, we propose an approach which iterates a designtest-nanalysis cycle using symbolic data mining methods for designing controllersnof autonomous swarm robots. The approach is applicable even ifnthe onboard signal is unavailable to the designer, which is common fornsuch kinds of robots. As the first step, we tackle a specific task in whichntwo swarm robots try to visit as many cells as possible in a square fieldnbefore a fatal collision. Quick analysis using conventional techniques, relyingnalso on human inspection revealed interesting essentials including andesirable type of interaction between the swarm robots and possible refinementsnof the controllers. We consider possible usages of data miningnmethods including an efficient trajectory discovery method, an effectivenminority subset discovery method, and a robust partial classifier discoverynmethod.
9934 en Robotics in Planetary Exploration Some of the problems to be faced in designing robots fornplanetary exploration will be described. The context refers to the projectnSTEPS, targeting future Mars missions, specically the design of the lan-nder (the capsule for reaching the planet surface) and the rover (a robot fornexploring the environment). Both the lander and the rover are equippednwith several sensors, among which a camera plays a fundamental role.nVision-based landing and environment reconstruction are considered pri-nmary goals for the success of the mission.
9935 en A Road Map for Motor Skill Learning Autonomous robots that can assist humans in situations of daily lifenhave been a long standing vision of robotics, artificial intelligence,nand cognitive sciences. A first step towards this goal is to createnrobots that can learn tasks triggered by environmental context ornhigher level instruction. However, learning techniques have yet tonlive up to this promise as only few methods manage to scale tonhigh-dimensional manipulator or humanoid robots. In this tutorial, wengive a general overview on motor skill learning. For doing so, wendiscuss task-appropriate representations and algorithms for learningnin robotics. Among the topics are the learning basic movements ornmotor primitives by imitation and reinforcement learning, learningnrhytmic and discrete movements, fast regression methods for learningninverse dynamics and setups for learning task-space policies. Examplesnon various robots will be shown; these include ball-paddling,nball-in-a-cup, robot darts, robot table tennis, learning inversendynamics, learning operational space control, and many others.
9936 en Solving Deterministic Policy (PO)MDPs using The viewpoint of solving Markov Decision Processes andntheir partially observable extension refers to nding policies that max-nimise the expected reward. We follow the rephrasing of this problem asnlearning in a related probabilistic model. Our trans-dimensional distri-nbution formulation obtains equivalent results to previous work in theninnite horizon case and also rigorously handles the nite horizon casenwithout discounting. In contrast to previous expositions, our frameworknelides auxiliary variables, simplifying the algorithm development. For anynMDP the optimal policy is deterministic, meaning that this importantncase needs to be dealt with explicitly. Whilst this case has been discussednby previous authors, their treatment has not been formally equivalent tonan EM algorithm, but rather based on a xed point iteration analogousnto policy iteration. In contrast we derive a true EM approach for thisncase and show that this has a signicantly faster convergence rate thannnon-deterministic EM. Our approach extends naturally to the POMDPncase as well. In the special case of deterministic environments, standardnEM algorithms break down and we show how this can be addressed us-ning a convex combination of the original deterministic environment andna ctitious stochastic `antifreeze' environment.
9937 en Panel: Challenges of Machine Learning-Based Abutonomouas Robotics 
9938 en Invited Talk: Towards Theoretical Understanding of Domain Adaptation Learning Machine learning enjoys a deep and powerful theory that has led to a wide variety of highly successful practical tools. However, most of this theory is developed under some simplifying assumptions that clearly fail in the real world. In particular, a fundamental assumption of the theory is that the data available for training and the data of the target application come from the same source. When this assumption fails, the learner is faced with a “domain adaptation” challenge. In the past few years, the range of machine learning applications have been expanded to include various tasks requiring domain adaptation. Such application have been addressed by several heuristic paradigms. However, the common theoretical models fall short of providing useful analysis of these techniques. The key to domain adaptation is the similarity between the training and target domains. In this talk I will discuss several parameters along which task similarity can be defined and measured and discuss to what extent can they be utilized to direct learning algorithms and guarantee their success. Recent work can provide theoretical justification to some existing practical heuristics, as well as guide the development of novel algorithms for handling some types of data discrepancies. However, our current understanding leaves much to be desired. I shall devote the last part of the talk to describing some of the challenges and open questions that will have to be addressed before one can claim satisfactory understanding of learning in the presence of training-test discrepancies. The talk is based on joint works with John Blitzer, Koby Crammer and Fernando Pereira and with my students, David Pal, Teresa Luu and Tyler Lu.n
9939 en Invited Talk: Empirical Risk Minimization with Statistics of Higher Order with Examples from Bipartite Ranking Statistical learning theory was mainly developed in the framework of binary classification under the assumption that observations in the training set form an i.i.d. sample. The techniques involved in order to provide statistical guarantees for state-of-the-art learning algorithms are borrowed from the theory of empirical processes. This is made possible not only because of the "i.i.d." assumption on the data but also because of the nature of the performance measures, such as classification error or margin error, which are statistics of order one. In the talk, I will discuss a variety of questions which arise in the theory when more involved criteria are considered. The problem of bipartite ranking through ROC curve optimization provides a prolific source of optimization functionals which are statistics of order strictly larger than one and several examples will be presented.
9940 en Robustness and Generalizability for Markovian Samples We consider robustness of learning algorithms and prove thatnunder a very general setup, robustness of an algorithm implies that it generalizes,nand consequently, a robust algorithm that asymptotically minimizesnempirical risk is consistent. In particular, this relationship holds innthe case where both training samples and testing samples are generatednaccording to evolving of a Markovian chain satisfying the Doeblin condition.nWe further provide conditions that ensure robustness and hencengeneralizability and in some cases consistency, all under the Markoviannsetup. Two notable examples are support vector machines and Lasso.
9941 en PAC-Bayesian Bounds in the Non IID Case 
9942 en Hybrid Stochastic-Adversarial On-Line Learning Most of the research in online learning focused either on thenproblem of adversarial classification (i.e., both inputs and labels are arbitrarilynchosen by an adversary) or on the traditional supervised learningnproblem in which samples are i.i.d. according to a probability distribution.nNonetheless, in a number of domains the relationship betweenninputs and labels may be adversarial, whereas inputs are generated accordingnto a fixed distribution. This scenario can be formalized as annhybrid classification problem in which inputs are i.i.d., while labels arenadversarial. In this paper we introduce the hybrid stochastic-adversarialnproblem, we propose an online learning algorithm for its solution, andnwe analyze its performance. In particular, we show that, given a hypothesisnspace H with finite VC dimension, it is possible to incrementallynbuild a suitable finite set of hypotheses that can be used as input for annexponentially weighted forecaster achieving a cumulative regret over nnrounds of order O(npnnV C(H) log n) with overwhelming probability.
9943 en Nonparametric ICA for Nonstationary Instantaneous Mixtures In this work, we use nonparametric sample Fisher informationnlike matrix instead of the sample covariance matrix to find the mixingnmatrix in a nonstationary ICA model. This replacement may resultsnin faster and robust separation.
9944 en Descriptive Subgroup Mining of Folk Music Descriptive analysis of music corpora is important to musicologistsnwho are interested in identifying the properties that characterizenspecific genres of music. In this study we present such an analysis of anlarge corpus of folk tunes, all labeled by their origin. Subgroup Discoveryn(SD) is a rule learning technique located at the intersection of predictivenand descriptive induction. One of the advantages of using this techniquenis the intuitive and interpretable result in the form of a collection of simplenrules. Classification accuracy is not the goal of this study. Instead, wendiscuss some of the highest scoring rules with respect to their descriptivenpower.
9945 en Detecting Key Features in Popular Music: Case Study - Singing Voice Detection Detecting distinct features in modern pop music is an importantnproblem that can have significant applications in areas such as multimedianentertainment. They can be used, for example, to give a visually coherentnrepresentation of the sound. We propose to integrate a singing voice detectornwith a multimedia, multi-touch game where the user has to perform simplentasks at certain key points in the music. While the ultimate goal is tonautomatically create visual content in response to features extracted from thenmusic, here we give special focus to the detection of voice segments in musicnsongs. The solution presented extracts the Mel-Frequency Cepstral Coefficientsnof the sound and uses a Hidden Markov Model to infer if the sound has voice.nThe classification rate obtained is high when compared to other singing voicendetectors that use Mel-Frequency Cepstral Coefficients.
9946 en A Framework for Performer Identification in Audio Recordings We present a general framework for the task of identifyingnperformers from their playing styles. We investigate how musicians express and communicate their view of the musical content in pieces andnhow to use this information in order to automatically identify performers. We study note-level deviations of parameters such as timing andnamplitude. Our approach to performer identification consists of inducingnan expressive performance model for each of the interpreters (essentiallynestablishing a performer dependent mapping of inter-note features to antiming and amplitude expressive transformations). We outline two successful performer identification case studies.
9947 en Melodic Models for Polyphonic Music Classification The classification of polyphonic music still presents challengesnfor current music data mining methods. In this paper we explorenthe performance of classifiers specifically created for melody on the polyphonicnclassification task. On a small dataset of string quartet movementsnof Haydn and Mozart, the melodic n-gram model outperforms thenmelodic global feature model for composer recognition. Furthermore, ansimple model that combines the predictions made from different instrumentalnparts outperforms models created from any single voice. Thenresults indicate that models taking into account polyphonic informationnachieve higher classification accuracy.
9948 en Audio Genre Classification with Semi-Supervised Feature Ensemble Learning Widespread availability and use of music have madenautomated audio genre classification an important field of research.nThanks to feature extraction systems, not only music data, but alsonfeatures for them have become readily available. However, handlabelingnof a large amount of music data is time consuming. Innthis study, we introduce a semi-supervised random feature ensemblenmethod for audio classification which uses labeled and unlabeledndata together for better genre classification. In order to have diversensubsets of features which are both relevant and non-redundant withinnthemselves, we introduce the Prob-mRMR (Probabilistic minimumnRedundancy Maximum Relevance) feature selection algorithm. ProbmRMRnis based on mRMR of Ding and Peng 2003 and it selectsnthe features probabilistically according to relevance and redundancynmeasures. Experimental results show that ensembles of classifiers usingnProb-mRMR feature subsets outperform both Co-training and RASCOn(Random Subspace Method for Co-training, Wang 2008) which usesnrandom feature subsets.
9949 en Modeling the Influence of Performance Controls on Violin Timbre By means of a sensing system we are able to capture bowingnand ¯ngering actions executed by a violinist during real performances.nThe aim of this research is to model the relation between those actionsnand the sound produced. We describe the process for training and optimizing the model by means of neural networks. Given a set of controlnactions, the model is able to predict the spectral envelopes of the harmonic and noisy components of the sound which are used for soundnsynthesis. The model is validated by comparing a real recording with thencorresponding synthetic sound produced.
9950 en Modeling Expressive Performances of the Singing Voice The long term goal of this work is to develop models of operaticnsingers and use them to generate expressive performances similar innvoice quality and style with what original performances by those singersnwould sound like. This paper focuses on learning timing models of expressivenperformance by using high-level descriptors extracted from existingnaudio recordings. Our approach is based on applying machine learningnto discover singer-specific timing patterns of expressive singing basednon existing performances. The experimental results show a significantncorrelation between the note durations of real performances and thosenpredicted by our model.
9951 en A Probabilistic Approach to Melodic Similarity Melodic similarity is an important research topic in musicninformation retrieval. The representation of symbolic music by meansnof trees has proven to be suitable in melodic similarity computation,nbecause they are able to code rhythm in their structure leaving only pitchnrepresentations as a degree of freedom for coding. In order to comparentrees, different edit distances have been previously used. In this paper,nstochastic k-testable tree-models, formerly used in other domains likenstructured document compression or natural language processing, havenbeen used for computing a similarity measure between melody trees as anprobability and their performance has been compared to a classical treenedit distance.
9952 en Interactive Segmentation of Electro-Acoustic Music 
9953 en Information Networks: State of the Art This paper provides an overview of different types of information networks and categorizes them by identifying several key properties of information units and relations. These properties reflect the expressiveness and thus ability of an information network to model data of a diverse nature.
9954 en Characterizing Semantic Relatedness of Search Query Terms Mining for semantic information in search engine query logsnbears great potential for both the optimization of search engines and bootstrapping Semantic Web applications. The interaction of a user with a search engine (more specifically clicklog information) has recently beennviewed as implicit tagging of resources by query terms. The resulting structure, previously called a logsonomy, exhibits structural similarities to folksonomies, which evolve during the explicit process of annotating resourcesnwith freely chosen keywords in social bookmarking systems. Fornthe folksonomy case, appropriate measures of relatedness have shown tonbe capable to harvest the emerging semantics inherent in the tripartitengraph of users, tags and resources. Motivated by the reported structuralnsimilarities, in this work we extend this methodology to logsonomies. Morenspecifically, we apply several measures of query term relatedness to thenlogsonomy graph and provide a semantic characterization for each measurenby grounding it against user-validated relatedness measures based onnWordNet. Comparing the outcome with prior results of analyzing folksonomyndata we nd that the formalization of log data in logsonomies retainsnthe semantic information. Some relatedness measures we applied prove tonbe able to capture these emergent semantics similarly to the folksonomyncase, while others exhibit different characteristics. In this way we providena novel and systematic approach to compare the emergent semantics ofnuser interactions with search engines and social bookmarking systems. Wenconclude that the type of semantic information inherent in both emergingnstructures is similar, and inform the choice of an appropriate measure ofnquery term relatedness for a given task.
9955 en Constructing Information Networks from Text Documents A major challenge for next generation data mining systems isncreative knowledge discovery from diverse and distributedndata/knowledge sources. In this task, an important challenge isninformation fusion of diverse representations into a uniquendata/knowledge format. This paper focuses on the graph representationnof data/knowledge generated from text documents available on the web.nThe problem addressed is how to efficiently and effectively create anninformation network, named a BisoNet, from large text corpora. Severalnoptions concerning node and arc representation are discussed, and ancase study information network is created from articles concerningnautism, downloaded from the PubMed repository of medicalnpublications. Open issues and lessons learned concerning representationnchoices are discussed.
9956 en Gene Analytics: Discovery and Contextualization of Enriched Gene Sets The paper present a preliminary study of creative knowledgendiscovery through bisociative data analysis. Bisociative reasoning is atnthe heart of creative, accidental discovery (serendipity), and is focused on finding unexpected links by crossing different contexts. Contextualizationnand linking between highly diverse and distributed data and knowledgensources is therefore crucial for implementation of bisociative reasoning.nIn the paper we explore these ideas on the problem of analysis of microarrayndata. We show how enriched gene sets are found by using ontologyninformation as background knowledge in semantic subgroup discovery.nThese genes are then contextualized by the computation of probabilisticnlinks to diverse bioinformatics resources. Results of two case studies are used to illustrate the approach.
9957 en Review of Network Abstraction Techniques Networks are a common way of representing linked information.nThe goal of network abstraction is to transform a large networkninto a smaller one, so that the smaller is a useful summary of the originalngraph.nIn this paper we review dierent approaches and techniques proposed tonabstract a large network. We classify the approaches along two axes. Thenrst one consists of elementary simplication techniques used: pruningnof (irrelevant) nodes and edges, partitioning to several smaller networks,nand generalization by replacement of subnetworks by more general structures.nThe other axis is objective vs. subjective methods; the latter onesnaim to maintain more information about those parts of a network thatnthe user has indicated as interesting.nWe conclude the review by a brief analysis of which intersections of thentwo axes are least researched and could therefore have future potential.
9958 en Probabilistic and Logical Inference for Network Mining In this talk I shall analyze network mining and bisociationnfrom a logical and probabilistic inference point of view. This is inspirednby the work on ProbLog for link mining [De Raedt, Kimmig, Toivonen,nIJCAI 2007], which is - in turn -based on Biomine [Sevonen et al. DILSn06]. The talk shall introduce a probabilistic semantics for networks andndatabases and use it to clarify notions of deduction, abduction and explanations, induction, analogy, abstraction and spread of influence. Allnnotions will be illustrated in the context of the Biomine network.
9959 en Finding Representative Nodes in Probabilistic Graphs We introduce the problem of identifying representative nodesnin probabilistic graphs, motivated by the need to produce different simplenviews to large networks. We define a probabilistic similarity measurenfor nodes, and then apply clustering methods to nd groups of nodes.nFinally, a representative is output from each cluster. We report on experimentsnwith real biomedical data, using both the k-medoids and hierarchicalnclustering methods in the clustering step. The results suggest thatnthe clustering based approaches are capable of finding a representativenset of nodes.
9960 en Pure Spreading Activation is Pointless Spreading activation is a popular technique for retrievingnand ranking indirectly related information by activating query items andnspreading their activation along relatedness links. Almost every use ofnthe technique is accompanied by its own set of restrictions on the dynamics, though, and the usual motivation is a reduced computationalndemand or an improved t to specific types of data. We show thatnin linear, constraint-free scenarios spreading activation would actuallynyield query-independent results, so that applications crucially depend onnthe imposed restrictions. To avoid this undesirable behavior, we studynnatural modifications that ensure query-dependent results even withoutnheuristic restrictions and provide experimental evidence for their effectiveness.
9961 en "BisoNet" Generation Using Textual Data According to Koestler, the notion of a bisociation denotesna connection between pieces of information from habitually separatedndomains or categories. In this paper, we consider a methodology to findnsuch bisociations using a network representation of knowledge, which isncalled a BisoNet, because it promises to contain bisociations. In a firstnstep, we consider how to create BisoNets from several textual databasesntaken from different domains using simple text-mining techniques. Tonachieve this, we introduce a procedure to link nodes of a BisoNet andnto endow such links with weights, which is based on a new measure forncomparing text frequency vectors. In a second step, we try to rediscovernknown bisociations, which were originally found by a human domainnexpert, namely indirect relations between migraine and magnesium asnthey are hidden in medical research articles published before 1987. Wenobserve that these bisociations are easily rediscovered by simply following the strongest links. Future work includes extending our methods tonnon-textual data, improving the similarity measure, and applying morensophisticated graph mining methods.
9962 en Creative Knowledge Discovery by Literature Outlier Detection This paper investigates the role of outliers in literature-basednknowledge discovery. It shows that detecting interesting outliers that appear innthe literature about a given phenomenon can help generate novel plausiblenscientific hypotheses. The underlying assumption is that whereas the majoritynof domain literatures describe matters related to common understanding of thendomain, some particular observations that appear rarely in the literature cannindicate a promising direction towards novel discoveries. This rarity principle isnused in our method called RaJoLink to guide the knowledge discovery process.nThe presented method focuses on the role of outliers in the closed discoverynprocess as implemented in the RaJoLink literature mining methodology.
9963 en Interactive Visualization of Continuous Node Features in Graphs Ordinary graphs only support discrete structures. In this paper we presentnan approach towards embedding continuous data – like time stamps or series of measurementsn– in discrete graph models. These continuous meta-information implicitlyndefine relations between vertices which are not explicitly defined in the graph itself.nWe call this an induced Non-Discrete Graph Structure (NoDeS). The model is helpfulnfor visualization of time-dependent models or values from physical domains. We providena formal definition of NoDeS based on graphs and two mappings, instance andnannotation based, to already known graph structures and visualizations. A visualizationnof multi-partite projection provides a representation of information from severalncontexts, enabling NoDeS for a generic context switching mechanism which is usednfor interaction with these structures. Finally, we introduce an application concept fornagent-driven event scheduling using NoDeS.
9964 en Object Ranking 
9965 en From Ranking to Intransitive Preference Learning: Rock-Paper-Scissors and Beyond Incorporating Exceptions 
9966 en Kernel Principal Component Ranking: Robust Ranking on Noisy Data 
9967 en Combining Ordinal Preferences by Boosting 
9968 en Decision Rule-Based Algorithm for Ordinal Classification Based on Rank Loss Minimization 
9969 en Learning Various Classes of Models of Lexicographic Orderings 
9970 en On the Combination of Two Decompositive Multi-Label Classification Methods 
9971 en Label Ranking with Partial Abstention Using Ensemble Learning 
9972 en Preference Learning in Recommender Systems 
9973 en UTA - NM: Explaining Stated Preferences with Additive Non-Monotonic Utility Functions 
9977 en Evolution in Four Dimensions Ideas about heredity and evolution are undergoing a revolutionary change. New findings in molecular biology challenge the gene-centered version of Darwinian theory according to which adaptation occurs only through natural selection of chance DNA variations. In Evolution in Four Dimensions, Eva Jablonka and Marion Lamb argue that there is more to heredity than genes. They trace four "dimensions" in evolution—four inheritance systems that play a role in evolution: genetic, epigenetic (or non-DNA cellular transmission of traits), behavioral, and symbolic (transmission through language and other forms of symbolic communication). These systems, they argue, can all provide variations on which natural selection can act. Evolution in Four Dimensions offers a richer, more complex view of evolution than the gene-based, one-dimensional view held by many today. The new synthesis advanced by Jablonka and Lamb makes clear that induced and acquired changes also play a role in evolution.nnAfter discussing each of the four inheritance systems in detail, Jablonka and Lamb "put Humpty Dumpty together again" by showing how all of these systems interact. They consider how each may have originated and guided evolutionary history and they discuss the social and philosophical implications of the four-dimensional view of evolution. Each chapter ends with a dialogue in which the authors engage the contrarieties of the fictional (and skeptical) "I.M.," or Ifcha Mistabra—Aramaic for "the opposite conjecture"—refining their arguments against I.M.'s vigorous counterarguments. The lucid and accessible text is accompanied by artist-physician Anna Zeligowski's lively drawings, which humorously and effectively illustrate the authors' points.
9978 en NMR and mSR in SmFeAsO1-xFx Superconductors Among the recently discovered pnictide superconductors the highest superconducting critical temperature has been measured for SmFeAsO1-xFx family (TC < 55 K). A brief overview on recent results obtained by means of 19F nuclear magnetic resonance and muon spin rotation in SmFeAsO1-xFx superconductors will be presented. 19F nuclear spin-lattice relaxation, which allows to probe the low-energy excitations within the Sm layers, shows a behavior which is rather characteristic of heavy-fermion compounds with a magnetic ground-state. From the temperature dependence of the NMR linewidth information on the static properties of the flux lines lattice and on the superconducting order parameter can be derived. These results are compared with the ones obtained by muon spin rotation, which represents a powerful tool to study the doping and temperature dependence of the London penetration depth and to evidence the possible coexistence of superconductivity and magnetism at the microscopic level. A comparison between the phenomenology observed in cuprates and Fe-based superconductors will be made throughout the seminar.
9980 en Learning Deep Hierarchies of Representations Whereas theoretical work suggests that deep architectures might be computationally and statistically more efficient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pre-training of each level of a hierarchically structured model. Several unsupervised criteria and procedures were proposed for this purpose, starting with the Restricted Boltzmann Machine (RBM), which when stacked gives rise to Deep Belief Networks (DBN). Although the partition function of RBMs is intractable, inference is tractable and we review several successful learning algorithms that have been proposed, in particular those using weights that change quickly during learning instead of converging. In addition to being impressive as generative models, deep architectures based on RBMs and other unsupervised learning methods have made an impact by being used to initialize deep supervised neural networks. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. We attempt to shed some light on these questions by comparing different successful approaches to training deep architectures and through extensive simulations investigating explanatory hypotheses. Finally, we describe our current research program, objectives and challenges, regarding learning representations at multiple levels of abstraction, to compare web objects such as images, documents, and search engine requests, comparisons that are at the core of several information retrieval applications.
9998 en Parameters influencing noise emissions – proposal for an Interdependency Matrix 
9999 en An overviewof the TYROSAFE project / Tyre and Road Surface Optimisation for Skid Resistance and Further Effects 
10002 en WP3 Experts Workshop on Contributory Factors 
10004 en Task 3.2 Parameters influencing rolling resistance 
10007 en TYROSAFE - basic information on Tyrp Running resistance 
10051 en Structured Output Prediction of Enzyme Function via Reaction Kernels Enzyme function prediction is an important problem in post-genomicnbioinformatics. There are two general methods for solving the problem:ntransfer of annotation from a similar, already annotated protein, andnmachine learning approaches that treat the problem as classificationnagainst a fixed taxonomy, such as Gene Ontology or the EC hierarchy.nThese methods are suitable in cases where the function has beennpreviously characterized and included in the taxonomy. However, given annew function that is not previously described, existing approachesnarguably do not offer adequate support for the human expert.nnIn this presentation, we I will present a structured output learningnapproach, where the enzyme function, an enzymatic reaction, is describednin fine-grained fashion with so called reaction kernels which allowninterpolation and extrapolation in the output (reaction) space. Anstructured output model is learned to predict enzymatic reactions fromnsequence motifs. We bring forward several choices for constructingnreaction kernels and experiment with them in the remote homology casenwhere the functions in the test set have not been seen in the trainingnphase. Our experiments demonstrate the viability of our approach.
10094 en Centre for knowledge transfer, VideoLectures.NET, World Summit award and future plans Centre for knowledge transfer in information technologies performs educational, promotional and infrastructural activities and provides direct exchange of information and experience between researchers and the users of their research results. By partnering and active engagement in the different European research projects the Centre successfully extends its activities to the research and development. Most of the research is performed in the area of knowledge management for traditional and emerging forms of organizations like networked and virtual organizations.nnThe Centre is operating two web portals. The first one is VideoLectures.NETwhich is now becoming a reference portal presenting high quality scientific lectures and a second one is IST World that offers services for automatic data collection and analysis of the European research.
10104 en Brains, Meaning and Corpus Statistics Google Tech TalksnMarch 27, 3009nnABSTRACTnnPresented bynnTom M. MitchellnE. Fredkin Professor and Department HeadnMachine Learning DepartmentnCarnegie Mellon UniversitynnHow does the human brain represent meanings of words and pictures in terms of the underlying neural activity? This talk will present our research using machine learning methods together with fMRI brain imaging to study this question. One line of our research has involved training classifiers that identify which word a person is thinking about, based on their neural activity observed using fMRI. A more recent line involves developing a computational model that predicts the neural activity associated with arbitrary English words, including words for which we do not yet have brain image data. This computational model is trained using a combination of fMRI data associated with several dozen concrete nouns, together with statistics gathered from a trillion-word text corpus. Once trained, the model predicts fMRI activation for any other concrete noun appearing in the text corpus, with highly significant accuracies over the 60 nouns for which we currently have fMRI data.nnTom M. Mitchell is the E. Fredkin Professor and head of the Machine Learning Department at Carnegie Mellon University. Mitchell is a past President of the American Association of Artificial Intelligence (AAAI), and a Fellow of the AAAS and of the AAAI. His general research interests lie in machine learning, artificial intelligence, and cognitive neuroscience. Mitchell believes the field of machine learning will be the fastest growing branch of computer science during the 21st century.nnMitchell's web home page is www.cs.cmu.edu/~tom
10182 en Multiframe Motion Segmentation via Penalized MAP Estimation and Linear Programming 
10215 en Road Assessment and Monitoring 
10216 en Welcome and introduction 
10217 en Task 2.2 – Non-destructive testing 
10218 en Arches/SPENS: The context 
10219 en Guidelines on a Systematic Decision 
10220 en Sustainable Pavements for European New member states 
10221 en Guidelines for the environmental assessment of various pavement types including recommendations to road authorities in New Member States 
10222 en A gaze on central European highway structures 
10223 en Bridge safety assessment and maintenance wizh the use of monitoring techniques 
10224 en Traffic load models for bridges in Central and Eastern European Countries 
10225 en Recommendations for dynamic allowance in bridge assessment 
10226 en Evaluation of materials for road 
10227 en Role of measurements and experimental data in optimised bridge assessment 
10228 en Laboratory and field implementation of high modulus asphalt concrete 
10229 en Discussion on Measurements and loadings 
10230 en Soft, diagnostic and proof load testing in routine bridge assessment 
10231 en Accelerated Field Tests 
10232 en Upgrading of asphalt macadam and light asphalt pavements to the bearing capacity level needed by EU-regulations 
10233 en The use of corrosion resistant reinforcement - the chance for durable concrete reinforced structures 
10234 en Improvement of Pavement Structures & Long-Term Performance of Reinforced Pavements 
10235 en Application of steel slag aggregate in road construction 
10236 en Cathodic protection for extending the life of concrete bridges 
10237 en A methodology for testing and implementing crushed concrete in road construction 
10238 en Discussion on Monitoring and preventing 
10239 en Systematic decision making processes within Bridge Management System 
10240 en Strengthening with FRP glued materials 
10241 en Ultra High Performance Fibre Reinforced Concretes (UHPFRC) for rehabilitation of bridges - recent advances in Slovenia 
10242 en Practical mix design model for asphalt mixtures 
10243 en Composite UHPFRC-concrete construction for CO2 emission savings 
10244 en Discussion on Structures Hardening and Strengthening 
10245 en Encouraging the transfer of knowledge 
10246 en Implementation activities of SHRP2 
10247 en Next steps in the CERTAIN project 
10248 en Implementation Putting research into practice 
10249 en Discussion on Implementation of Results 
10250 en Conclusion 
10251 en The importance of corrosion monitoring for the durability of structures 
10252 en Load test results Internet data base - a new tool in bridge assessment 
10407 en Introduction 
10408 en Overview 
10409 en Background regarding the harmonisation process 
10410 en The common scale question 
10411 en Group discussions 
10412 en The TYROSAFE implementation plan 
10413 en A buyer's guide to continious optimization 
10414 en Compressive Sensing for Computer Vision: Hype vs Hope 
10415 en On the completeness of coding with image features 
10416 en Learning Models for Object Recognition from Natural Language Descriptions 
10417 en Better appearance models for pictorial structures 
10418 en Bilateral Symmetry Detection via Symmetry-Growing 
10419 en Real-time texture boundary detection from ridges in the standard deviation space 
10420 en Category-Specific Object Recognition and Segmentation Using a Skeletal Shape Model 
10421 en A Constant-Time Efficient Stereo SLAM System 
10422 en Multi-View Geometry of the Refractive Plane 
10423 en Estimation of Location Uncertainty for Scale Invariant Features Points 
10424 en Multiple Target Localisation at over 100 FPS 
10425 en What can the world tell us about an image? 
10426 en Semantic Scene Segmentation using Random Multinomial Logit 
10427 en Fast Segmentation via Randomized Hashing 
10428 en Combining Appearance and Structure from Motion Features for Road Scene Understanding 
10429 en Object Localization with Global and Local Context Kernels 
10430 en PRISM: PRincipled Implicit Shape Model 
10431 en Gait Representation Using Flow Fields 
10432 en Specularity and Shadow Interpolation via Robust Polynomial Texture Maps 
10433 en Learning generative texture models with extended Fields-of-Experts 
10434 en Get Out of my Picture! Internet-based Inpainting 
10435 en Stochastic Image Denoising 
10436 en 3D head pose estimation from multiple distant views 
10437 en 3D-assisted Facial Texture Super-Resolution 
10438 en Head Pose Classification in Crowded Scenes 
10439 en Subtitle-free Movie to Script Alignment 
10440 en Multi-view synchronization of human actions and dynamic scenes 
10441 en Attribute Multiset Grammars for Global Explanations of Activities 
10442 en Evaluation of local spatio-temporal features for action recognition 
10443 en Mining visual actions from movies 
10444 en Introduction to Neon NeOn is a 14.7 million Euros project involving 14 European partners and co-funded by the European Commission’s Sixth Framework Programme under grant number IST-2005-027595. NeOn started in March 2006 and has a duration of 4 years. Our aim is to advance the state of the art in using ontologies for large-scale semantic applications in the distributed organizations. Particularly, we aim at improving the capability to handle multiple networked ontologies that exist in a particular context, are created collaboratively, and might be highly dynamic and constantly evolving.
10446 en Discussion 
10447 en Lecture 1 - How Do You Know? Professor McBride outlines the course with its goals and requirements, including the required laboratory course. To the course's prime question "How do you know" he proposes two unacceptable answers (divine and human authority), and two acceptable answers (experiment and logic). He illustrates the fruitfulness of experiment and logic using the rise of science in the seventeenth century. London's Royal Society and the "crucial" experiment on light by Isaac Newton provide examples. In his correspondence with Newton Samuel Pepys, diarist and naval purchasing officer, illustrates the attitudes and habits which are most vital for budding scientists - especially those who would like to succeed in this course. The lecture closes by introducing the underlying goal for the first half of the semester: understanding the Force Law that describes chemical bonds.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L01|Professor McBride's web resources for CHEM 125 (Fall 2008)]]n
10448 en Lecture 2 - Force Laws, Lewis Structures and Resonance Professor McBride begins by following Newton's admonition to search for the force law that describes chemical bonding. Neither direct (Hooke's Law) nor inverse (Coulomb, Gravity) dependence on distance will do - a composite like the Morse potential is needed. G. N. Lewis devised a "cubic-octet" theory based on the newly discovered electron, and developed it into a shared pair model to explain bonding. After discussing Lewis-dot notation and formal charge, Professor McBride shows that in some "single-minimum" cases the Lewis formalism is inadequate and salvaging it required introducing the confusing concept of "resonance."nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L02|Professor McBride's web resources for CHEM 125 (Fall 2008)]]n
10449 en Lecture 3 - Double Minima, Earnshaw's Theorem and Plum-Puddings Continuing the discussion of Lewis structures and chemical forces from the previous lecture, Professor McBride introduces the double-well potential of the ozone molecule and its structural equilibrium. The inability for inverse-square force laws to account for stable arrangements of charged particles is prescribed by Earnshaw's Theorem, which may be visualized by means of lines of force. J.J. Thomson circumvented Earnshaw's prohibition on structure by postulating a "plum-pudding" atom. When Rutherford showed that the nucleus was a point, Thomson had to conclude that Coulomb's law was invalid at small distances.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L03|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10450 en Lecture 4 - Coping with Smallness and Scanning Probe Microscopy This lecture asks whether it is possible to confirm the reality of bonds by seeing or feeling them. It first describes the work of "clairvoyant" charlatans from the beginning of the twentieth century, who claimed to "see" details of atomic and molecular structure, in order to discuss proper bases for scientific belief. It then shows that the molecular scale is not inconceivably small, and that Newton and Franklin performed simple experiments that measure such small distances. In the last 25 years various realizations of Scanning Probe Microscopy have enabled chemists to "feel" individual molecules and atoms, but not bonds.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L04|Professor McBride's web resources for CHEM 125 (Fall 2008)]]n
10451 en Lecture 5 - X-Ray Diffraction Professor McBride introduces the theory behind light diffraction by charged particles and its application to the study of the electron distribution in molecules by x-ray diffraction. The roles of molecular pattern and crystal lattice repetition are illustrated by shining laser light through diffraction masks to generate patterns reminiscent of those encountered in X-ray studies of ordered solids.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L05|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10452 en Lecture 6 - Seeing Bonds by Electron Difference Density Professor McBride uses a hexagonal "benzene" pattern and Franklin's X-ray pattern of DNA, to continue his discussion of X-ray crystallography by explaining how a diffraction pattern in "reciprocal space" relates to the distribution of electrons in molecules and to the repetition of molecules in a crystal lattice. He then uses electron difference density mapping to reveal bonds, and unshared electron pairs, and their shape, and to show that they are only one-twentieth as dense as would be expected for Lewis shared pairs. Anomalous difference density in the carbon-fluorine bond raises the course's second great question, "Compared to what?"nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L06|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10453 en Lecture 7 - Quantum Mechanical Kinetic Energy After pointing out several discrepancies between electron difference density results and Lewis bonding theory, the course proceeds to quantum mechanics in search of a fundamental understanding of chemical bonding. The wave function ?, which beginning students find confusing, was equally confusing to the physicists who created quantum mechanics. The Schrödinger equation reckons kinetic energy through the shape of ?. When ? curves toward zero, kinetic energy is positive; but when it curves away, kinetic energy is negative!nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L07|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10454 en Lecture 8 - One-Dimensional Wave Functions Professor McBride expands on the recently introduced concept of the wave function by illustrating the relationship of the magnitude of the curvature of the wave function to the kinetic energy of the system, as well as the relationship of the square of the wave function to the electron probability density. The requirement that the wave function not diverge in areas of negative kinetic energy leads to only certain energies being allowed, a property which is explored for the harmonic oscillator, Morse potential, and the Columbic potential. Consideration of the influence of mass reveals an "isotope effect" on dynamics, on the energy, vibration frequency, and length of bonds.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L08|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10455 en Lecture 9 - Chladni Figures and One-Electron Atoms After showing how a double-minimum potential generates one-dimensional bonding, Professor McBride moves on to multi-dimensional wave functions. Solving Schrödinger's three-dimensional differential equation might have been daunting, but it was not, because the necessary formulas had been worked out more than a century earlier in connection with acoustics. Acoustical "Chladni" figures show how nodal patterns relate to frequencies. The analogy is pursued by studying the form of wave functions for "hydrogen-like" one-electron atoms. Removing normalizing constants from the formulas for familiar orbitals reveals the underlying simplicity of their shapes.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L09|Professor McBride's web resources for CHEM 125 (Fall 2008)]]n
10456 en Lecture 10 - Reality and the Orbital Approximation In discussions of the Schrödinger equation thus far, the systems described were either one-dimensional or involved a single electron. After discussing how increased nuclear charge affects the energies of one-electron atoms and then discussing hybridization, this lecture finally addresses the simple fact that multi-electron systems cannot be properly described in terms of one-electron orbitals.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L10|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10457 en Lecture 11 - Orbital Correction and Plum-Pudding Molecules The lecture opens with tricks ("Z-effective" and "Self Consistent Field") that allow one to correct approximately for the error in using orbitals that is due to electron repulsion. This error is hidden by naming it "correlation energy." Professor McBride introduces molecules by modifying J.J. Thomson's Plum-Pudding model of the atom to rationalize the form of molecular orbitals. There is a close analogy in form between the molecular orbitals of CH4 and NH3 and the atomic orbitals of neon, which has the same number of protons and neutrons. The underlying form due to kinetic energy is distorted by pulling protons out of the Ne nucleus to play the role of H atoms.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L11|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10458 en Lecture 12 - Overlap and Atom-Pair Bonds This lecture begins by applying the united-atom "plum-pudding" view of molecular orbitals, introduced in the previous lecture, to more complex molecules. It then introduces the more utilitarian concept of localized pairwise bonding between atoms. Formulating an atom-pair molecular orbital as the sum of atomic orbitals creates an electron difference density through the cross product that enters upon squaring a sum. This "overlap" term is the key to bonding. The hydrogen molecule is used to illustrate how close a simple sum of atomic orbitals comes to matching reality, especially when the atomic orbitals are allowed to hybridize.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L12|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10459 en Lecture 13 - Overlap and Energy-Match Professor McBride uses this lecture to show that covalent bonding depends primarily on two factors: orbital overlap and energy-match. First he discusses how overlap depends on hybridization; then how bond strength depends on the number of shared electrons. In this way quantum mechanics shows that Coulomb's law answers Newton's query about what "makes the Particles of Bodies stick together by very strong Attractions." Energy mismatch between the constituent orbitals is shown to weaken the influence of their overlap. The predictions of this theory are confirmed experimentally by measuring the bond strengths of H-H and H-F during heterolysis and homolysis.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L13|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10460 en Lecture 14 - Checking Hybridization Theory with XH3 This lecture brings experiment to bear on the previous theoretical discussion of bonding by focusing on hybridization of the central atom in three XH3 molecules. Because independent electron pairs must not overlap, hybridization can be related to molecular structure by a simple equation. The "Umbrella Vibration" and the associated rehybridization of the central atom is used to illustrate how a competition between strong bonds and stable atoms works to create differences in molecular structure that discriminate between bonding models. Infrared and electron spin resonance experiments confirm our understanding of the determinants of molecular structure.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L14|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10461 en Lecture 15 - Chemical Reactivity: SOMO, HOMO, and LUMO Professor McBride begins by using previous examples of "pathological" bonding and the BH3 molecule to illustrate how a chemist's use of localized bonds, vacant atomic orbitals, and unshared pairs to understand molecules compares with views based on the molecule's own total electron density or on computational molecular orbitals. This lecture then focuses on understanding reactivity in terms of the overlap of singly-occupied molecular orbitals (SOMOs) and, more commonly, of an unusually high-energy highest occupied molecular orbital (HOMO) with an unusually low-energy lowest unoccupied molecular orbital (LUMO). This is shown to be a generalization of the traditional concepts of acid and base. Criteria for assessing reactivity are outlined and illustrated.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L15|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10462 en Lecture 16 - Recognizing Functional Groups This lecture continues the discussion of the HOMO/LUMO view of chemical reactivity by focusing on ways of recognizing whether a particular HOMO should be unusually high in energy (basic), or a particular LUMO should be unusually low (acidic). The approach is illustrated with BH3, which is both acidic and basic and thus dimerizes by forming unusual "Y" bonds. The low LUMOs that make both HF and CH3F acidic are analyzed and compared underlining the distinction between MO nodes that derive from atomic orbitals nodes (AON) and those that are antibonding (ABN). Reaction of HF as an acid with OH- is shown to involve simultaneous bond-making and bond-breaking.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L16|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10463 en Lecture 17 - Reaction Analogies and Carbonyl Reactivity Continuing the examination of molecular orbital theory as a predictor of chemical reactivity, this lecture focuses on the close analogy among seemingly disparate organic chemistry reactions: acid-base, SN2 substitution, and E2 elimination. All these reactions involve breaking existing bonds where LUMOs have antibonding nodes while new bonds are being formed. The three-stage oxidation of ammonia by elemental chlorine is analyzed in the same terms. The analysis is extended to the reactivity of the carbonyl group and predicts the trajectory for attack by a high HOMO. This predicted trajectory was validated experimentally by Bürgi and Dunitz, who compared numerous crystal structures determined by X-ray diffraction.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L17|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10464 en Lecture 18 - Amide, Carboxylic Acid and Alkyl Lithium This lecture completes the first half of the semester by analyzing three functional groups in terms of the interaction of localized atomic or pairwise orbitals. Many key properties of biological polypeptides derive from the mixing of such localized orbitals that we associate with "resonance" of the amide group. The acidity of carboxylic acids and the aggregation of methyl lithium into solvated tetramers can be understood in analogous terms. More amazing than the panoply of modern experimental and theoretical tools is that their results would not have surprised traditional organic chemists who already had developed an understanding of organic structure with much cruder tools. The next quarter of the semester is aimed at understanding how our scientific predecessors developed the structural model and nomenclature of organic chemistry that we still use.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L18|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10465 en Lecture 19 - Oxygen and the Chemical Revolution (Beginning to 1789) This lecture begins a series describing the development of organic chemistry in chronological order, beginning with the father of modern chemistry, Lavoisier. The focus is to understand the logic of the development of modern theory, technique and nomenclature so as to use them more effectively. Chemistry begins before Lavoisier's "Chemical Revolution," with the practice of ancient technology and alchemy, and with discoveries like those of Scheele, the Swedish apothecary who discovered oxygen and prepared the first pure samples of organic acids. Lavoisier's Traité Élémentaire de Chimie launched modern chemistry with its focus on facts, ideas, and words. Lavoisier weighed gases and measured heat with a calorimeter, as well as clarifying language and chemical thinking. His key concepts were conservation of mass for the elements and oxidation, a process in which reaction with oxygen could make a "radical" or "base" into an acid.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L19|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10466 en Lecture 20 - Rise of the Atomic Theory (1790-1805) This lecture traces the development of elemental analysis as a technique for the determination of the composition of organic compounds beginning with Lavoisier's early combustion and fermentation experiments, which showed a new, if naïve, attitude toward handling experimental data. Dalton's atomic theory was consistent with the empirical laws of definite, equivalent, and multiple proportions. The basis of our current notation and of precise analysis was established by Berzelius, but confusion about atomic weight multiples, which could have been clarified early by the law of Avogadro and Gay-Lussac, would persist for more than half a century.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L20|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10467 en Lecture 21 - Berzelius to Liebig and Wöhler (1805-1832) The most prominent chemist in the generation following Lavoisier was Berzelius in Sweden. Together with Gay-Lussac in Paris and Davy in London, he discovered new elements, and improved atomic weights and combustion analysis for organic compounds. Invention of electrolysis led not only to new elements but also to the theory of dualism, with elements being held together by electrostatic attraction. Wöhler's report on the synthesis of urea revealed isomerism but also persistent naiveté about treating quantitative data. In their collaborative investigation of oil of bitter almonds Wöhler and Liebig extended dualism to organic chemistry via the radical theory.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L21|Professor McBride's web resources for CHEM 125 (Fall 2008)]]n
10468 en Lecture 22 - Radical and Type Theories (1832-1850) Work by Wöhler and Liebig on benzaldehyde inspired a general theory of organic chemistry focusing on so-called radicals, collections of atoms which appeared to behave as elements and persist unchanged through organic reactions. Liebig's French rival, Dumas, temporarily advocated radicals, but converted to the competing theory of types which could accommodate substitution reactions. These decades teach more about the psychology, sociology, and short-sightedness of leading chemists than about fundamental chemistry, but both theories survive in competing schemes of modern organic nomenclature. The HOMO-LUMO mechanism of addition to alkenes and the SOMO mechanism of free-radical chain reactions are introduced.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L22|Professor McBride's web resources for CHEM 125 (Fall 2008)]]n
10469 en Lecture 23 - Valence Theory and Constitutional Structure (1858) Youthful chemists Couper and Kekulé replaced radical and type theories with a new approach involving atomic valence and molecular structure, and based on the tetravalence and self-linking of carbon. Valence structures offered the first explanation for isomerism, and led to the invention of nomenclature, notation, and molecular models closely related to those in use today.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L23|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10470 en Lecture 24 - Determining Chemical Structure by Isomer Counting (1869) Half a century before direct experimental observation became possible, most structures of organic molecules were assigned by inspired guessing based on plausibility. But Wilhelm Körner developed a strictly logical system for proving the structure of benzene and its derivatives based on isomer counting and chemical transformation. His proof that the six hydrogen positions in benzene are equivalent is the outstanding example of this chemical logic but was widely ignored because, in Palermo, he was far from the seats of chemical authority.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nn[[http://webspace.yale.edu/chem125_oyc/#L24|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10471 en Lecture 25 - Models in 3D Space (1869-1877); Optical Isomers Despite cautions from their conservative elders, young chemists like Paternó and van't Hoff began interpreting molecular graphs in terms of the arrangement of a molecule's atoms in 3-dimensional space. Benzene was one such case, but still more significant was the prediction, based on puzzling isomerism involving "optical activity," that molecules could be "chiral," that is, right- or left-handed. Louis Pasteur effected the first artificial separation of racemic acid into tartaric acid and its mirror-image.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L25|Professor McBride's web resources for CHEM 125 (Fall 2008)]]n
10472 en Lecture 26 - Van't Hoff's Tetrahedral Carbon and Chirality With his tetrahedral carbon models van't Hoff explained the mysteries of known optical isomers possessing stereogenic centers and predicted the existence of chiral allenes, a class of molecules that would not be observed for another sixty-one years. Symmetry operations that involve inverting an odd number of coordinate axes interconvert mirror-images. Like printed words, only a small fraction of molecules are achiral. Verbal and pictorial notation for stereochemistry are discussed.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L26|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10473 en Lecture 27 - Communicating Molecular Structure in Diagrams and Words It is important that chemists agree on notation and nomenclature in order to communicate molecular constitution and configuration. It is best when a diagram is as faithful as possible to the 3-dimensional shape of a molecule, but the conventional Fischer projection, which has been indispensable in understanding sugar configurations for over a century, involves highly distorted bonds. Ambiguity in diagrams or words has led to multibillion-dollar patent disputes involving popular drugs. International agreements provide descriptive, unambiguous, unique, systematic "IUPAC" names that are reasonably convenient for most organic molecules of modest molecular weight.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L27|Professor McBride's web resources for CHEM 125 (Fall 2008)]]n
10474 en Lecture 28 - Stereochemical Nomenclature; Racemization and Resolution Determination of the actual atomic arrangement in tartaric acid in 1949 motivated a change in stereochemical nomenclature from Fischer's 1891 genealogical convention (D, L) to the CIP scheme (R, S) based on conventional group priorities. Configurational isomers can be interconverted by racemization and epimerization. Pure enantiomers can be separated from racemic mixtures by resolution schemes based on selective crystallization of conglomerates or temporary formation of diastereomers.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L28|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10475 en Lecture 29 - Preparing Single Enantiomers and the Mechanism of Optical Rotation Within a lecture on biological resolution, the synthesis of single enantiomers, and the naming and 3D visualization of omeprazole, Professor Laurence Barron of the University of Glasgow delivers a guest lecture on the subject of how chiral molecules rotate polarized light. Mixing wave functions by coordinated application of light's perpendicular electric and magnetic fields shifts electrons along a helix that can be right- or left-handed, but so many mixings are involved, and their magnitudes are so subtle, that predicting net optical rotation in practical cases is rarely simple.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L29|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10476 en Lecture 30 - Esomeprazole as an Example of Drug Testing and Usage The chemical mode of action of omeprazole is expected to be insensitive to its stereochemistry, making clinical trials of the proposed virtues of a chiral switch crucial. Design of the clinical trials is discussed in the context of marketing. Otolaryngologist Dr. Dianne Duffey provides a clinician's perspective on the testing and marketing of pharmaceuticals, on the FDA approval process, on clinical trial system, on off-label uses, and on individual and institutional responsibility for evaluating pharmaceuticals.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L30|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10477 en Lecture 31 - Preparing Single Enantiomers and Conformational Energy After mentioning some legal implications of chirality, the discussion of configuration concludes using esomeprazole as an example of three general methods for producing single enantiomers. Conformational isomerism is more subtle because isomers differ only by rotation about single bonds, which requires careful physico-chemical consideration of energies and their relation to equilibrium and rate constants. Conformations have their own notation and nomenclature. Curiously, the barrier to rotation about the C-C bond of ethane was established by measuring its heat capacity.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L31|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10478 en Lecture 32 - Stereotopicity and Baeyer Strain Theory Why ethane has a rotational barrier is still debatable. Analyzing conformational and configurational stereotopicity relationships among constitutionally equivalent groups reveals a subtle discrimination in enzyme reactions. When Baeyer suggested strain-induced reactivity due to distorting bond angles away from those in an ideal tetrahedron, he assumed that the cyclohexane ring is flat. He was soon corrected by clever Sachse, but Sachse's weakness in rhetoric led to a quarter-century of confusion.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L32|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10479 en Lecture 33 - Conformational Energy and Molecular Mechanics Understanding conformational relationships makes it easy to draw idealized chair structures for cyclohexane and to visualize axial-equatorial interconversion. After quantitative consideration of the conformational energies of ethane, propane, and butane, cyclohexane is used to illustrate the utility of molecular mechanics as an alternative to quantum mechanics for estimating such energies. To give useful accuracy this empirical scheme requires thousands of arbitrary parameters. Unlike quantum mechanics, it assigns strain to specific sources such as bond stretching, bending, and twisting, and van der Waals repulsion or attraction.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L33|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10480 en Lecture 34 - Sharpless Oxidation Catalysts and the Conformation of Cycloalkanes Professor Barry Sharpless of Scripps describes the Nobel-prizewinning development of titanium-based catalysts for stereoselective oxidation, the mechanism of their reactions, and their use in preparing esomeprazole. Conformational energy of cyclic alkanes illustrates the use of molecular mechanics.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L34|Professor McBride's web resources for CHEM 125 (Fall 2008)]]n
10481 en Lecture 35 - Understanding Molecular Structure and Energy through Standard Bonds Although molecular mechanics is imperfect, it is useful for discussing molecular structure and energy in terms of standard covalent bonds. Analysis of the Cambridge Structural Database shows that predicting bond distances to within 1% required detailed categorization of bond types. Early attempts to predict heats of combustion in terms of composition proved adequate for physiology, but not for chemistry. Group- or bond-additivity schemes are useful for understanding heats of formation, especially when corrected for strain. Heat of atomization is the natural target for bond energy schemes, but experimental measurement requires spectroscopic determination of the heat of atomization of elements in their standard states.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L35|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10482 en Lecture 36 - Bond Energies, the Boltzmann Factor and Entropy After discussing the classic determination of the heat of atomization of graphite by Chupka and Inghram, the values of bond dissociation energies, and the utility of average bond energies, the lecture focuses on understanding equilibrium and rate processes through statistical mechanics. The Boltzmann factor favors minimal energy in order to provide the largest number of different arrangements of "bits" of energy. The slippery concept of disorder is illustrated using Couette flow. Entropy favors "disordered arrangements" because there are more of them than there are of recognizable ordered arrangements.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L36|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
10586 en IAS Video 
10851 en Morphology and Characteristics of the Advanced Electro-Ceramics Remarkable progress of the electronics components enabled the tremendous development of the recent electronics devices. In particular, the electro-ceramics has played an important role in cooperate with IC in the circuits. The major subjects for the components have been functionality, miniaturization, volumetric efficiency, reliability, safety materials and lower price.nnSeveral research works on morphology designing of the ceramics has been done to develop the new components with higher performance. The functionality of the electronics devices has been developed using specific morphologies of the ceramics such as bulk (3-Dimention), plate (2-D), fiber(1-D) and particle (0-D). Device designing has been remarkably progressed in cooperate with ceramic materials and process technologies.nnMultilayer is a most popular technology to design the ceramics devices consist of dielectric, piezo-electric, magnetic, semiconductor and insulator ceramics. How we can prepare the thinner dielectric layer than 0.3mm is a most important work for the ceramic capacitor engineering. Miniaturized 3-D module circuits are realized by embedding the passive components into the multiplayer ceramic and polymer substrates.nnMorphology control of the ceramic structure such as core-shell and arrangements of crystal axis has been developed to improve the electric performances. To enhance the piezoelectricity of the non-lead ferroelectric materials, textured grain orientation and ultra high magnetic field application are known as useful process methods.nnSelf-assembling of the nano-particles is a challenging technology to build-up the super lattice structure and tailoring the more functional materials and devices. Preparation and characterization of the ceramic nano particles are presented.
10855 en Anatomy of Financial Crisis Much of the recent financial crisis originates from the common practice of financial firms of making investments with large sums of borrowed money (leverage). The collateral for these borrowed funds is usually put up in the form of financial assets, which are far from being ’solid’ values. The dependence of the value of collateral on asset prices is often the heart of a credit crisis. In a simple agent based model we study an ’ecology’ of financial players such as un-informed noise traders, informed funds, banks, and investors to hedge funds. This model economy allows to identify the effects of leverage on the stability of the financial system. It becomes possible to understand how minor random fluctuations can trigger a financial crisis, eventually leading to the collapse of the entire system. The main message is that novel means of monitoring of a specific collection of financial indicators could be used to foresee the likelihood for the development of crisis and meltdown.
10856 en Space Research in Europe Europe had joined forces more than three decades ago, creating the European Space Agency (ESA). Today the commercially most successful launcher is european, and our space research and technology is cutting edge. Our spacecraft observe the earth, study the Universe and orbit other planets.nnThese achievements build on a coherent approach, in which ESA plays a major role. Two major scientific satellites have been launched this year: Herschel, the largest telescope yet launched by mankind, and Planck, which is looking back at the dawn of time. Examples of recent results will be presented, and a view of future missions will be given. The lecture will cast some light onto the structure and working of ESA, and its interactions with european industry and research institutions.
10862 en Recommendations for modified binder usage in pavement 
10875 en Lecture 1: Welcome to CS106A So welcome to CS106A. If you don't think you should be in CS106A, you think younshould be somewhere different, now is probably a good time to go, not that I wouldndiscourage anyone from taking this class. I think we'll have a lovely time in here. But thisnclass is CS106A or E70A, so if you're, like, "Wait. I thought I was in E70A," you're fine.nThey're the same class; it's the same thing. No worries, okay? ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture01.pdf|Programming Methodology - Lecture 01]]
10876 en Lecture 2: Handout Information Alrighty, welcome back to CS106A. If you're stuck in thenback, just come on down, have a seat. Originally, I thought maybe we would havenslightly fewer people today than last time, but that appears not to be the case. So whilenwe're waiting, everyone loves babies, so I decided to put – that's Karel, the Robot, thenearly days. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture02.pdf|Programming Methodology - Lecture 02]]
10877 en Lecture 3: Karel and Java Couple quick announcements before we dive into things.nThere's one handout, which, hopefully, you should have gotten. It will contain the Karelnexample we did in class last time, the steeple chase, as well as some more examples thatnwe're gonna go over this time. But I encourage you to actually pay attention to what wendo in class rather than sort of looking at it on the handout, because one thing that's alwaysntrue about programs is once you see the solution to a program, it's easy to lull yourselfninto thinking oh, I could have done that, it seems so easy. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture03.pdf|Programming Methodology - Lecture 03]]
10878 en Lecture 4: The History of Computing So welcome back. Now we’re officially started with thenclass. I hope you had a good weekend. I was just asking people before class what kind ofnstuff they did this weekend. So if anyone wants to, come early. We’ll just engage innrandom conversation. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture04.pdf|Programming Methodology - Lecture 04]]
10879 en Lecture 5: Variables All righty. Let’s go ahead and get started. A couple ofnquick announcements before we start today – so hopefully you’re all busy working awaynon Karel and life is good. Just quick poll – how many people have actually finished Karelnalready? Oh, yeah. I won’t ask how many people have not yet downloaded Eclipse. Therenare no handouts today. Getting’ a little breather – no handouts. Don’t worry; you’ll getnsome more of that next time. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture05.pdf|Programming Methodology - Lecture 05]]
10880 en Lecture 6: readInt() and readDouble() All righty. So I want to take a quick pain pole before we start. So let’s actually dive intonthe real sort of meaningful things. What the pain pole really is – remember I asked you tonthink about how much time it actually took you to do the assignment? So total it up overnall the Karel problems; how many total hours; think about it, it took you to actually do thenassignment. Right? And we’re just going to go through and do a quick show of hands. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture06.pdf|Programming Methodology - Lecture 06]]
10881 en Lecture 7: The Loop and a Half Problem All right. So one real quick point before we dive into the main meat of the lecture, it’snjust a clarification on something we did last time called The Cast. So remember last timenwhere we had a cast, which was this thing that allowed us to say treat this one data point,nor this one data item, this one variable as a different type for one particular operation. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture07.pdf|Programming Methodology - Lecture 07]]
10882 en Lecture 8: Information Hiding Alrighty. Welcome back to yet another day of CS106a.nCouple quick announcements before we start – so first announcement, there is onenhandout which I’m not sure if it’s here yet, but it will be here momentarily if it’s not herennow. You can pick it up on the way out. I think Ben might have just been delayed on thenway in. So there is one handout; hopefully, you can pick it up on the way out if youndidn’t see it back there now. It’s already posted online as well. So if you don’t get it innclass, you can get it online, or you can get it – there’s the Handout Hangout as we like tonrefer to it, which is a bunch of file folders on the first floor of Gates where there’snhardcopies of all the handouts that get left over from class. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture08.pdf|Programming Methodology - Lecture 08]]
10883 en Lecture 9: Strings So a couple of quick announcements: One is that there's two handouts, one on codingnstyle, and one on the use of variables. I'd encourage you to read both of them becausenthey are both extremely critical concepts in this class. There are some things that – it'snlike, "Yeah, it's not so important." These two are really important so please make sure tonread the handout. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture09.pdf|Programming Methodology - Lecture 09]]
10884 en Lecture 10: Importance of Private Variables Okay, I would just, even at this point, just email text.nMight be easier. I think we need to get started.nLet’s go ahead and get started. Couple quick announcements before we start. So one ofnthem is that there are three handouts in the back, including your next assignment. Andnyour next assignment’s a little game called Breakout. How many people have ever heardnof a game called Breakout? ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture10.pdf|Programming Methodology - Lecture 10]]
10885 en Lecture 11: The GImage Class A couple quick announcements before we dive into things. There's two handouts today.nThere's a whole bunch of coding examples. There's coding examples up the wazoo.nThey're sort of like – I gave you a bunch of code that might just be useful for you to looknout for Breakout, just because it's good times. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture11.pdf|Programming Methodology - Lecture 11]]
10886 en Lecture 12: Enumeration So welcome back to yet another fun filled, exciting day ofnCS106A. A couple quick announcements before we start. There’s actually no handoutsnfor today, and you’re like if there’s no handouts for today, why are there two handouts innthe back? If you already picked up the two handouts from last class, you might want tondouble check to make sure you don’t already have them, but if you don’t have them, feelnfree to pick them up. We just don’t want to cut down any more trees than we need to, sonif you accidentally picked them up, you can put them back at the end of the class, passnthem to a friend, whatever you’d like to do. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture12.pdf|Programming Methodology - Lecture 12]]
10887 en Lecture 13: String Processing And now, since we're getting to the middle of the quarter, it’s that time for the mid-termnto be coming up. So, in fact, the mid-term is next week. It’s a week from Tuesday. Inknow. The quarters go by so quickly. If you have a conflict with the mid-term, and bynconflict, I mean an unmovable academic conflict. ...nnSee the whole transcript at [http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture13.pdf|Programming Methodology - Lecture 13]]
10888 en Lecture 14: Memory The other two handouts are the practice midterm and practice midterm solutions. So thosenwill also be here at the end of class. That will give you a whole bunch of details about thenmidterm and what the midterm actually covers, but it will also give you examples of realnexam problems that have been given in the past. So you can work on them. Do it in thentime, you know, time sort of setting so you can see what you’re slow on and what you’renfast on. But it’s kind of in flavor, very similar, to what the real exam’s gonna be in termsnof what it covers, the kind of complexity of the problems, et cetera. So you can get thatnafter class. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture14.pdf|Programming Methodology - Lecture 14]]
10889 en Lecture 15: Pointer Recap But a few announcements before we delve into things today. The handouts from last time,nif you didn’t get the handouts from last time, namely, especially the practice midterm andnsolutions to the practice midterm as well as assignment No. 4, if you didn’t get those,nthey’re available in the back today. If you already got them, you don’t need to pick upnadditional copies. There’s no additional handouts for today, but there are just copies ofnthe ones from last week. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture15.pdf|Programming Methodology - Lecture 15]]
10890 en Lecture 16: Array So it’s the day before the midterm. This is kind of like, you know, the calm before – wellnI shouldn’t even say the calm before the storm. It’s a probably a lot more calm for menthan it is for you. But hopefully you’re getting prepared for the midterm, so a fewnannouncements before we start. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture16.pdf|Programming Methodology - Lecture 16]]
10891 en Lecture 17: Multi-dimensional Arrays All right. So welcome back to yet another fun fillednexciting day of cs106a. A couple quick announcements before we start. First of which,nthere is one handout, which is kind of a quick reference for you on ArrayLists. It’s sort ofnall the ArrayLists you need to know for the hangman assignment part three. If younhaven’t already done it, now you have a quickie reference for it. If you’ve already donenit, you don’t need the quickie reference, but presumably, you saw everything you needednalready in the textbook or what we did last time in class. ...nnSee the whole transcript at [http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture17.pdf|Programming Methodology - Lecture 17]]
10892 en Lecture 18: A Wrap Up of Multi-dimensional Arrays You – one of the handouts also solutions for the midterm, and so you can – if you gotnanything wrong you can compare your answers with the solutions. One thing to also keepnin mind is that the actual solutions we were looking for are shorter than the solutions thatnI give you. The solutions that I give you have comments. For one of the problems Inactually gave you two different ways of doing it just so you see different approaches. Butnyou weren't expected to actually write that much code. All we wanted was sort of thencode without the comments, which is actually pretty slim if you consider how much codenthere is there without comments. But you'll get those back after class. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture18.pdf|Programming Methodology - Lecture 18]]
10893 en Lecture 19: An Interface Howdy. So welcome back to yet another fun filled,nexciting day in CS106A. I don’t know if there’s any days actually we started where Indidn’t say that. I don’t know. Someday, I should go back and watch the video. Butnthey’re all fun filled and exciting, aren’t they? So it’s not like false advertising. There’snno handouts today. A little breather after the four handouts you got last time. And anothernquick announcement, if you didn’t pick up your midterm already, you can pick it up.nThey’re along the back wall over there in alphabetical order, so hopefully you can pick itnup if you haven’t already gotten yours. If you’re an SITN student and you’re worryingnabout where you can get your midterm, it will be sent back to you through the SITNncourier, unless you come into class and you picked it up, in which case it won’t be sentnback to you because then you already have it. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture19.pdf|Programming Methodology - Lecture 19]]
10894 en Lecture 20: GUI So to give you an appropriate incentive for this, what we're going to do is when you turnnthem in, Ben and I are going to look through them. We're going to take a first pass andntake a sub-set of them that we think are the best. Then we'll have the section leaders innthe class collectively vote as to what the winners are in both of the categories. You get, atnmost, one entry. Your entry, you don't have to designate for a category. We'll look atnevery entry as being entered in both categories. You just get one entry. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture20.pdf|Programming Methodology - Lecture 20]]
10895 en Lecture 21: Review of Interactors and Listeners There's two handouts today. One handout's your section handout for this week. Sectionsnare still happening this week. The other handout is some code on interactors that you sawnlast time as well as some additional code that we'll go over today on interactors. Othernannouncement, computer science career panel. So just a quick show of hands, how manynpeople are thinking about computers science as a major? ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture21.pdf|Programming Methodology - Lecture 21]]
10896 en Lecture 22: Overview of NameSurfer - The Next Assignment Howdy! So welcome back to yet another fun-filled,nexciting day of CS106a. We’re getting close to that Thanksgiving recess, which is alwaysna good time. In the days of yore, it used not be a whole week. It used to be you got likenone or two days off. You got like Thursday and Friday, which means you would havengotten only one day off from this class and now you get a whole week to mellow in stylenor catch up on all your other work as the case may be. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture22.pdf|Programming Methodology - Lecture 22]]
10897 en Lecture 23: Introduction to Lecture's material - Searching You may be wondering who I am. I would have thought before we got back mid-quarternevaluations that you stood a chance of recognizing me as the TA of your class, but thencomment of more than half of the people who responded to the question of how is Benndoing was, “I don’t know Ben. I’ve never interacted with Ben. So I assume he’s doing angreat job.” ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture23.pdf|Programming Methodology - Lecture 23]]
10898 en Lecture 24: Principles of Good Software Engineering for Managing Large Amounts of Data So a couple quick announcements before we get into things. One is there is one handout,nwhich is your section handout for this week. And kind of one of the themes of this weeknis bigness. In some sense writing bigger programs, bigger data structures, that’s the wholendeal. And we’ll kind of talk about that as we go along. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture24.pdf|Programming Methodology - Lecture 24]]
10899 en Lecture 25: Defining a Social Network for Our Purposes So welcome back to another fun filled, exciting day ofncs106a. A couple quick announcements before we start. First announcement, there is twonhandouts. One of those handouts, which we'll spend some time talking about today, isnyour last assignment for the class, which is assignment No. 7. It's worth noting, we sort ofntalked about it the very first day of class when we talked about late days. The very firstnhandout says no late days can be used on assignment No. 7. So that’s important tonremember. No late days can be used for assignment No. 7 because it's due the last day ofnthe class. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture25.pdf|Programming Methodology - Lecture 25]]
10900 en Lecture 26: Introduction to the Standard Java Libraries The graphics context, for those of you who are doing it, is due today. Just wondering,nquick show of hands, how many people entered the graphics contest. Wow. Not as manynas I would've thought. There could be a couple people who are at home, even if you don'tnwin of getting 100 on the final in a random drawing. So that's a good sign. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture26.pdf|Programming Methodology - Lecture 26]]
10901 en Lecture 27: Life After CS106A So welcome back. Wow. That’s a little loud. To our lastnweek of cs106a. Of course, it is another fun filled exciting day despite it being our lastnweek. We’re getting down to the end. We have class today, there’s class on Wednesday,nthere’s no class on Friday. So next time will be our last day. But a few announcements.nThere’s actually just a load of announcements because we’re so close to the end of thenquarter. First announcement, there’s one handout, which is your section handout for thisnweek. There are still sections this week, so despite the fact that we don’t have class onnFriday, still go to your sections this week. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture27.pdf|Programming Methodology - Lecture 27]]
10902 en Lecture 28: The Graphics Contest Winners So there’s two handouts today, the last two handouts except for the final exam which arena practice final and the solutions for the practice final. The problems that are on that arenactually taken from – most of them. Some of them were written just for that, but most ofnthem were taken from actual final exams in the past. So just like the midterm, it shouldngive you a chance to get a notion of what kind of questions we would ask, what sort ofntopical coverage there would be, the level of difficulty, and I’ve left the blank pages outnof the exam just to save trees, but we would have space in the exam for you to actuallyntake in. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/icspmcs106a/transcripts/ProgrammingMethodology-Lecture28.pdf|Programming Methodology - Lecture 28]]
10958 en Lecture 1: Course Overview In introduction to robotics, we are going to really covernthe foundations of robotics. That is, we are going to look at mathematical models thatnrepresent robotic systems in many different ways. In fact, you just saw a simulation of anhumanoid robotic system that we are controlling at the same time. If you think about anmodel that you are going to use for the simulation, you need to represent the kinematicsnof the system. You need also to be able to actuate the system by going to the motors andnfinding the right torques to make the robot move..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture01.pdf|Programming Introduction to Robotics - Lecture 01]]
10959 en Lecture 2: Spatial Descriptions //"Then obviously when we determine the location of a link we need to be able to transformnthat description to the next link or to describe the position and orientation of the endeffectornin our previously link so we need really to handle transformations. Then we neednto discuss how we represent the position and orientation. There are many different waysnthrough which we can describe a position or an orientation, and we will discuss a fewndifferent representations..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture02.pdf|Introduction to robotics - Lecture 02]]
10962 en Lecture 5: Summary - Frame Attachment //"The moving style of Gibbons, shown in this video, isncalled brachiation. The brachiation robot is a dynamically mobile robot modeled on thenGibbon. It moves from branch to branch swinging its body like them. The brachiationnrobot, which we have developed, has two arms and no body. The total length is one meternand the total weight is 4.8 kilograms. The arms and grippers are actuated with DC motorsnthrough harmonic drive gears. This is the movement without actuation. At first, the robotndoesn’t know how to move at all..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture05.pdf|Introduction to robotics - Lecture 05]]
10963 en Lecture 6: Instantaneous Kinematics //"Polypod is a reconfigurable module robot. It’s made up of two types of modules callednsegments and nods. Segments are two degree freedom modules with two motors, forcenand position sensing and a microcomputer on board. Nods are [inaudible] shapednhousings for batteries. Segments may be mounted parallel to each other or they may benmounted perpendicular to each other. Modules may also attach on any face of a nod.nSimple locomotion gaits are statically stable gaits that move along a straight line..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture06.pdf|Introduction to robotics - Lecture 06]]
10964 en Lecture 7: Jacobian - Explicit Form //"Future robots are going to work in your houses and hospitals with humans. Toshiba hasndeveloped a beach ball volley playing robot as demonstration of such a human friendlynrobot technology. We consider that it is essential to interact with robots using everydaynwords such as, let’s play volleyball. For the everyday word commands to work, the robotnneeds to measure the target’s relative position with respect to the robot position to knownthe mechanics and procedures of the tasks, and to have a good database for thenenvironment and the target..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture07.pdf|Introduction robotics - Lecture 07]]
10965 en Lecture 8: Scheinman Arm - Demo //"At [inaudible] we have developed and tested automatic parallel parking and pulling outnmaneuvers on an experimental electric car. The car can be driven manually or movenautonomously with automatic steering and velocity control. It is equipped with variousnsensors, including sonars, to monitor its surroundings..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture08.pdf|Introduction to robotics - Lecture 08]]
10966 en Lecture 9: Intro - Guest Lecturer: Gregory Hager //"Okay. Let’s get started. So today it’s really a greatnopportunity for all of us to have a guest lecturer. One of the leaders in robotics vision. AnGregory Hager from John Hopkins who will be giving this guest lecture. On Monday, Inwanted to mention that on Wednesday we have the mid-term in class. Tonight andntomorrow we have the review sessions, so I think everyone has signed on for thosensessions. And next Wednesday the lecture will be given by a former Ph.D. student fromnStanford University, Krasimir Kolarov who will be giving the lecture on trajectories andninverse kinematics. So welcome back..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture09.pdf|Introduction to robotics - Lecture 09]]
10967 en Lecture 10: Guest Lecturer: Krasimir Kolarov //"Good afternoon. My name is Krasimir Kolarov. I amngoing to be teaching the lecture today and also the co-author of the notes for the course.nSo if you have any complaints, direct it to me. If you have any praises, direct it tonOussama. I did my [inaudible] here at Stanford about 16 years ago. So I was in yournshoes, and I’ve been kinda doing a few lectures as well as some of the classes completelynsince. I’m not working in the robotics area right now, but I’m staying pretty current innthat..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture10.pdf|Introduction to robotics - Lecture 10]]
10968 en Lecture 11: Joint Space Dynamics //"At the second tier is the Ranger. Rangers are larger robots used to transport, deploy, andncoordinate the Scouts. Scouts are wholly original robots with cylindrical bodies 40nmillimeters in diameter and 110 millimeters in length. The Scout carries a sensor payloadnused to relay environmental information to other robots. The most common Scoutnpayload is a small video camera, but other payloads, such as microphones, are also used.nVideo data is broadcast to other systems via an analog RF transmitter. Scoutsncommunicate with other robots using an RF data link..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture11.pdf|Introduction to robotics - Lecture 11]]
10969 en Lecture 12: Lagrange Equations //"Autonomous mobile robots have become a key technology for unmanned planetarynmissions. To cope with the rough terrain encountered on most of the planets of interest,nnew locomotion concepts for rovers and micro-rovers have to be developed andninvestigated. In this video sequence, we present an innovative off-road rover able tonpassively overcome unstructured obstacles of up to two times its wheel diameter. Using anrhombus configuration, this rover has one wheel mounted on a fork in the front, onenwheel in the rear, and two bogies on each side..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture12.pdf|Introduction to robotics - Lecture 12]]
10970 en Lecture 13: Control - Overview //"Okay. So who’s interested in juggling? Well, those whonare interested in juggling could try it next quarter in Experimental Robotics. In fact, a lotnof the projects in Experimental Robotics involve dynamic skills, throwing a ball into anbasket, playing ping-pong, or whatever. So juggling is quite challenging, actually. Well,njuggling requires control and here we are. So this is a little bit of a concept that we arengoing to see over the discussions on control. And the concept is instead of really thinkingnabout the robot as a programmable machine where you need to find all the join motionsncorresponding to your task. So you want to move to some location and you want to benable to reach that location with some orientation of your vector..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture13.pdf|Introduction to robotics - Lecture 13]]
10971 en Lecture 14: PD Control //"Well, yeah, sometimes you, I mean, a human – tactilensensing is amazing. So you have the static information, so if you grab something, now thenwhole surface is in contact, and you can determine the shape, right? So what does it meannin term of, like, designing a tactile sensor, just if you think about the static case?"//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture14.pdf|Introduction to robotics - Lecture 14]]
10972 en Lecture 15: Manipulator Control //"Okay. So you can imagine, maybe, a sort of resistive orncapacitive sensor that will deflect a little bit and give you that information. How many ofnthose you would need? You need, sort of, an array, right? So how large, like, let’s say thisnis the end of factor. I’m trying to see if you did that problem – you’re going to have a lotnof information here, and you need to take it back, and you have a lot of wires; you have anmatrix, and you’re going to have a lot of, basically, information to transmit. So, thendesign of tactile sensors being this problem of how we can put enough sensors, and hownwe can extract this information and take it back. So these guys came up with anninteresting idea; here it is..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture15.pdf|Introduction to robotics - Lecture 15]]
10973 en Lecture 16: Compliance //"So this video is about a very important aspect of robotics, which isncompliant motion. You see, the sponge is pushing up, and you see no reflection onnthe sponge, right? That means there is no force applied. Here, we are coming to ansurface that is unknown, and the robot is sliding over the surface. So it's makingncontact at different points, even if we remove the whole object. Now here is a wavynsurface that is being followed just by saying press down and move to the right,ncleaning a window without breaking it. It's very important..."//nnSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture16.pdf|Introduction to robotics - Lecture 16]]
10974 en Lecture 1: Previous Knowledge Recommended (Matlab) We are on the air. Okay. Welcome, one and all. And as it saidnon the TV when you were walking in, but just to make sure everybody knows, this isnEE261, The Fourier Transform and its Applications, Fourier Transforms et al., Fourier.nAnd my name is Brad Osgood. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture01.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 01]]
10975 en Lecture 2: Periodicity; How Sine And Cosine Can Be Used To Model More Complex Functions Second – our second main – our third main thing would be the office hours for the TA’snhave been set. Information is available on the course website under the link of coursenstaff. You’ll see on the left-hand side there’s a link to course staff, and our individualnoffice hours have been set, and they will start on Monday, October 1st. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture02.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 02]]
10976 en Lecture 3: Summary Of Previous Lecture (Analyzing General Periodic Phenomena As A Sum Of Simple Periodic Phenomena) So last time, I say we took the first step in analyzing general periodic phenomena via thensum, so several combination, a linear combination of simple building blocks, simplenperiodic phenomena. So let me remind you what we did because it’s very important thatnyou realize what we did and what we didn’t do. We said suppose that you can write anperiodic signal in a certain form what has to happen. So we start off by saying F of T is angiven periodic function, periodic signal. Function, signal same thing. And just to bendefinite we took it to have period one, all right? And the question is can it be representednin terms of others and suppose it can be represented in terms of other simple signals ofnperiod one, namely the complex exponential. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture03.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 03]]
10977 en Lecture 4: Wrapping Up Fourier Series; Making Sense Of Infinite Sums And Convergence It doesn’t work on anything else, except – use the Mac, thenword from over there is you have to use Safari, which is the one that comes with it. And Indon’t know about other ones. Anybody else have issues with this? I can find out and I cannpost an announcement, I suppose. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture04.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 04]]
10978 en Lecture 5: Continued Discussion Of Fourier Series And The Heat Equation But if you find yourself wanting to fill in those last minute comments, I would say that –nthe policy is the homework should be due by 5:00 on Wednesday, and you can turn it innto the magic filing cabinet. Across from my office – my office is 271 Packard, and there’sna little hallway sort of across from there, and there are several gray filing cabinets, one ofnwhich has my name and the course number on it. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture05.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 05]]
10979 en Lecture 6: Correction To Heat Equation Discussion First thing I want to say is that I made a little mistake last time in lecture. It was gentlynpointed out to me. When I was talking about the heat equation; the floorshow was fine,nthe discussion was fine, but then I said this thing about as T tens to infinity, thentemperature tens to zero. That wasn’t right. I forgot about the zero – because somebodynsaid look, the fusion man, you don’t lose anything, it just diffuses. So it’s not right to saynthe temperature changes to zero, I was thinking of while it was escaping to the universenor something like that. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture06.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 06]]
10980 en Lecture 7: Review Of Fourier Transform (And Inverse) Definitions So, again, f of t is a signal and the Fourier Transform or function, same thing, the FouriernTransform, I use this notation. I want to comment about that, again, in just a second.nIntegral from my infinity – infinity of either the -2p I ST, F of T, VT and the inversenFourier Transform looks very similar except for a change in sign in the exponential. Sonthe inverse Fourier Transform of – I use a different function, although it doesn’t matter.nWe’re gonna go from -8 to 8 of either the +2p I ST, G of S, DS, okay? ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture07.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 07]]
10981 en Lecture 8: Effect On Fourier Transform Of Shifting A Signal So that's what we're gonna do today, we're going down more the second path, includingnan extremely important operation. So we're gonna have three big items today, each ofnwhich are important in themselves and come up all the time. One is delays, what to donwith a Fourier transform when the signal is delayed. One, a formula for what happens tonthe Fourier transform under a stretch, and finally, a very general operation, which wenhave now seen a couple times in different forms, but today we're gonna see them today innthe context of the Fourier transform in its full glory, so to speak, and that is convolution. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture08.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 08]]
10982 en Lecture 9: Continuing Convolution: Review Of The Formula Now, in this picture you see not much. So you see a couple of examples. You see angenerally periodic phenomenon, but you see a lot of jaggedness in there, you see a lot ofnjaggedness in the picture. So, like I said, the horizontal scale is time, I think it's a periodnof months, and the vertical scale is whatever it is. And you certainly see a periodicnphenomenon here, but it's noisy or it's jagged. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture09.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 09]]
10984 en Lecture 10: Central Limit Theorem And Convolution; Main Idea All right. Big day today. We're going to talk about – we're going to do our finalnapplication of convolution. I suppose I shouldn't say "final application of convolution”nbecause it is the kind of operation that comes up repeatedly throughout the course. Butnsort of as the – as a last treatment of the kind of areas we've been talking about, I wannantalk about application convolution to the central limit theorem. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture10.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 10]]
10985 en Lecture 11: Correction To The End Of The CLT Proof There’s a single function, P of X, which describes how each one of them is distributed.nSo it’s a distribution for each. And then I formed P of N of X was the distribution for thensum scaled by square root of N. So it’s the average – excuse me. There was somenassumption we made on the Xs on normalation, that is. We assume they had mean zeronand we assume they had standard deviation or variance one, and then if you form thensum, the mean of the sum is zero but the standard deviation or the variant centerndeviation of the sum is the square root of N, so it’s scaled by the square root of N. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture11.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 11]]
10986 en Lecture 12: Cop Story They're infinitely differentiable and any derivative decays faster than any power of X. Inwill write that down. First of all, Phi of X is infinitely differentiable, so as smooth as youncould want, has as many derivatives as you could want and more, differentiable andnsecondly that, as I said, any derivative decreases faster than any power of X. For any Mnand N greater than or equal to zero, X to the ND N DX to the [inaudible] derivative ofnPhi of X [inaudible] also tends to zero as X tends to plus or minus infinity. Those twonproperties. This is the M. M and N are independent here, so this says – there's nothingnmysterious here. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture12.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 12]]
10987 en Lecture 13: Setting Up The Fourier Transform Of A Distribution On the air. Oh, what these people miss before the cameranstarts rolling. Okay. I sent out a note yesterday, or over the weekend about the mid-termncoming up. So, the midterm is coming up a week from Wednesday, and without goingninto too much detail right now, we’re going to have three sessions – it’s a 90-minutenexam outside of class, so we’ll have class that day. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture13.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 13]]
10988 en Lecture 14: Derivative Of A Distribution Anyway, I'll post it on the website and make the announcement next time. I'll say a littlenbit more detail about the exam. So when you're signing up there, it's just so we can have ansense of how many people are going to be in which slot. You're not signing your lifenaway or anything, but I'm figuring that between one of those slots, from 2:00 p.m. to 3:30np.m., from 4:00 p.m. to 5:30 p.m. and from 6:00 p.m. to 7:30 p.m., should be able toncover most everybody. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture14.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 14]]
10989 en Lecture 15: Application Of The Fourier Transform: Diffraction: Setup Now again, having said that, it’s also true that you can’t avoid computation completely,nso I wanna try to make a balance. We will provide for you, and it is already posted, hasnbeen for a while, the formula sheet. That’s a formula sheet for the entire course, so we’lln– I’ll make copies of that and bring it to the exam. That has all sorts of helpful, usefulnformulas on it. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture15.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 15]]
10990 en Lecture 16: More On Results From Last Lecture (Diffraction Patterns And The Fourier Transforms) I want to talk a little bit more about diffraction actually. And as a way of actually makingna transition to our next topic, this may seem a little odd way – our next topic is samplingnan interpolation. And going from diffraction to sampling interpolation may seem like anlittle odd of way going but it’s – there’s an interesting connection here that I want tonexploit. The topic itself that I – the general areas of diffraction, and in particular what Inwant to talk about today, is interesting in itself and it does make actually for a nice link,nso I want to talk about the problem of crystallography. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture16.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 16]]
10991 en Lecture 17: Review Of Main Properties Of The Shah Function They are, in some sense, flip sides of each other and we’ll see that very strongly todaynbecause convulsion and multiplication are sort of swapped back and forth by taking thenFourier Transform of the Inverse Fourier Transform for you. Okay. So in fact, there’snactually sort of two sides of the same coin. The final property of the Shaw function, thenremarkable property that falls from this [inaudible] formula, the fact about the integers isnthe Fourier Transform property. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture17.pdf|The Fourier Transform and its ApplicationsnCo - Lecture 17]]
10992 en Lecture 18: Review Of Sampling And Interpolation Results All right. I want to spend a little more time today [Audio breaks up] theorem – sampling,nand interpolation, and some of the phenomena that was associated with it. I'll remind younwhat the setup was from last time. And I'll almost carry out the proof again – or at leastnI'll give you the setup for the proof again because as I said many times in this – last time,nand as I will say again today, for me the sampling [Audio breaks up] the derivation of then– the sampling theorem, the formula is identical with a proof of the sampling formula. Inthink the two are so closely related that to understand one you really have to think innterms of the other. All right. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture18.pdf|The Fourier Transform and its Applications - Lecture 18]]
10993 en Lecture 19: Aliasing Demonstration With Music So according to the way we do things then, the bandwidth or the spectrum of – a slice ofnspectrum of music would go roughly up to say 20,000 hertz and then down to minusn20,000, so the frequency would be – and beyond that it’s essentially zero. At least as farnas you’re concerned it’s zero. You can’t hear anything. So if that’s a picture of thenspectrum of a slice of music, then it’s between minus 20,000 and 20,000, so the way wenwrite things, that would be P over 2. The bandwidth is 20,000, so P is about 40,000,nwhich means that if you wanna sample and reconstruct music, you should do it roughly atna rate of 40,000 hertz. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture19.pdf|The Fourier Transform and its Applications - Lecture 19]]
10994 en Lecture 20: Review: Definition Of The DFT And it’s defined by its nth component, so the nth component of the Fourier transform isnthe sum from say N equals zero to N minus one of the Nth component of F times E to thenminus two pi I N M over N. All right? So everything is defined here in terms of thenindices in the exponential, and these are the values of the discrete function at the indexnpoints, F of zero, F of one, F of two and so on. That’s the definition. They say you don’tnsee at all the fact that in our derivation this came from starting with a continuous signal,nsampling it, sampling the Fourier transform, and then somehow ultimately leading to thisndefinition. Here it’s just as an operation on one discrete signal producing another discretensignal. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture20.pdf|The Fourier Transform and its Applications - Lecture 20]]
10995 en Lecture 21: Review Of Basic DFT Definitions It looked like this. You have a Discrete signal. So I'm using both the signal notation andnvector notation here, and I'll continue to do that, sort of mix the two up, because I thinknthey're both useful. So the idea is you have either an N-tuple of numbers or a Discretensignal whose value at the nth point is just the value here, FM. Okay? Oops, that doesn'tnlook right. That's not much of a statement. So you can either consider it as a Discretensignal who's defined on the integers or the integers from zero to N minus one, or you cannthink of it as an N-tuple or as a vector. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture21.pdf|The Fourier Transform and its Applications - Lecture 21]]
10996 en Lecture 22: FFT Algorithm: Setup: DFT Matrix Notation Now, what this means in particular, I have to give you a little caution on this, is thatnthere’s sort of one more general topic on this fast Fourier transform that I wouldnordinarily talk about and that is convolution, circular convolution, so I’m not going to donthat. I will leave that up to you to read. There’s not much. There wouldn’t be much to donother than talk about the formulas and basic applications. The applications we’ll do in thencontext of linear systems coming up, so you’ll see some of that later. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture22.pdf|The Fourier Transform and its Applications - Lecture 22]]
10997 en Lecture 23: Linear Systems: Basic Definitions The 21st century – I say this as a sweeping bold statement, but I stand by it. The 21stncentury may be the century of non-linearity. We don’t know yet, but non-linear problemsnare becoming increasingly more trackable because of computational techniques. One ofnthe reasons why linear problems were studied so extensively and were so useful isnbecause a lot can be done sort of theoretically even if you couldn’t compute. And then, ofncourse, later on when computational techniques – computational power was there, thennthey became even more – they were able to be exploited even more. What I wanna get tonis the connection between the Fourier Transform and linear systems, and that’s gonna benprimarily along the lines – so we definitely wanna see how the Fourier Transform appliesnto linear systems, again in a fairly limited way. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture23.pdf|The Fourier Transform and its Applications - Lecture 23]]
10998 en Lecture 24: Review Of Last Lecture: Discrete V. Continuous Linear Systems The matrices are different. You get a matrix by choosing a basis of the space of inputs,nand then you express the matrix in terms of what happens to the bases. And I’m notngonna go through this because I’m assuming that you’ve seen this in linear algebra,nalthough you may not have thought about it quite in these terms. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture24.pdf|The Fourier Transform and its Applications - Lecture 24]]
10999 en Lecture 25: Review Of Last Lecture: LTI Systems And Convolution So if W of X is equal to LV of X, then you actually get W of X is [inaudible] for minusninfinity of H of XY, Z of Y, DY. All right. So once again, the output of the system isnobtained by taking input of the system and integrating that against the impulse response.nIt’s a very satisfactory result. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture25.pdf|The Fourier Transform and its Applications - Lecture 25]]
11000 en Lecture 26: Approaching The Higher Dimensional Fourier Transform For what is an image, after all? What is a mathematical description of an image? Well, atnleast not a two-dimensional image. At least mathematically, it's given by a function ofntwo variables, say X1 and X2. Function F of X1, X2, where X1 and X2 are varying overnsome part of the X1, X2 plane. At each point, what the function prescribes is thenintensity. I'm thinking about black and white images here. So you think F of X1 and X2nas a range of numbers from zero to one, from black to white. So you think of F of X1, X2nas the intensity from black to white, say, at the point X1, X2. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture26.pdf|The Fourier Transform and its Applications - Lecture 26]]
11001 en Lecture 27: Higher Dimensional Fourier Transforms- Review The point [inaudible] getting to know your higher-dimensional Fourier transform. As Insaid, you know it. You have to convince yourself of that. There are differences, and I'llnhighlight some of them a little bit today but even more so next time. Again, the point ofnusing the notation that we've used and the approach that we're using is to make thenhigher-dimensional transform look as much like the one-dimensional transform asnpossible. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture27.pdf|The Fourier Transform and its Applications - Lecture 27]]
11002 en Lecture 28: Shift Theorem In Higher Dimensions If you make a shift by B then that corresponds to E to the minus 2 pie ISB, that’s thenphase shift times the Fourier Transform of the original function. All right. That’s easynresult. That’s one of the very first results that we proved when we were talking aboutngeneral properties of the Fourier Transform and it follows, like many other formulas, justnby making a change of variable in the interval that defines it. All right. Interval defines anFourier Transform. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture28.pdf|The Fourier Transform and its Applications - Lecture 28]]
11003 en Lecture 29: Shahs So it said this: it said the Fourier transform of F of AX – all right, so you change thenvariables X by a matrix A, a nonsingular matrix A – is one over the determinate of Antimes the Fourier transform of F evaluated at A inverse transpose at the frequencynvariable C. Okay? It’s a very interesting formula. We derived it last time, and it’sncomplicated. It’s more complicated than the one-dimensional stretch case, but it includesnthe one-dimensional stretch case, but what you don’t see in one dimensions is this newnphenomenon as I say that reciprocal somehow means inverse transpose. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture29.pdf|The Fourier Transform and its Applications - Lecture 29]]
11004 en Lecture 30: Tips For Filling Out Evals It was introduced – I actually knew the history of this a little bit more thoroughly, and Incannot recall it now. It was certainly not introduced in the context of X-ray tomographynor anything else. It was introduced for purely mathematical reasons, for interestingngeometric reasons. The idea was to sort of study the geometry of a region by knowingnintegrals of sections through it just as a purely mathematical question. I don’t think therenwere any practical implications that were anticipated or attempted certainly at the time itnwas introduced. So our question is – so you know all these values. All right? You knownall these values, and the question is can you invert the transform? Can you find mu, givennthat you know all the values of its transform? ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoftaee261/transcripts/TheFourierTransformAndItsApplications-Lecture30.pdf|The Fourier Transform and its Applications - Lecture 30]]
11005 en Lecture 1: Overview Of Linear Dynamical Systems And I'll start actually just with some — I'll cover some of the mechanics of the class andnthen we'll start in. Today is just gonna be sort of a fun lecture, so it's not representative ofnthe class. So by the end of the — you'll leave thinking, "Well, it was interesting, but itnwas kind of, like, content-free." Anyway, trust me it's not representative of the quarter. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture01.pdf|Introduction to Linear Dynamical Systems - Lecture 01]]
11006 en Lecture 2: Linear Functions (Continued) The second announcement I wanted to make is for contacting me or the TAs. Please usenthe staff email address. It’s on the webpage. That goes to all of us, and that’s a very goodnway – that way we can see which emails have been responded to and which have not.nPlease do not email the TAs’ personal email addresses. That means that other people innthe teaching staff haven’t seen your email. Please use the staff address. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture02.pdf|Introduction to Linear Dynamical Systems - Lecture 02]]
11007 en Lecture 3: Linearization (Continued) We’ll get more into detail. We’ll see this example will come up several times during thencourse. So here you have X and Y, two variables unknown coordinates in the plane andnwe have a bunch of beacons at locations PIQI, so these are X and Y coordinates of thenbeacons. And what we measure is a range. And a range, because the beacons can onlynmeasure range, ranges to this point, it could be of course, be the other way around, thatnthe point can measure its distance to the range. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture03.pdf|Introduction to Linear Dynamical Systems - Lecture 03]]
11008 en Lecture 4: Nullspace Of A Matrix(Continued) Okay, the next is that we have gotten more than just a handful of sort of requests ornsuggestions to do something like a linear algebra or matrix review session. That’s fornpeople who either never had a detailed course on this or have successfully repressed thenmemories of the course they may have taken. So for those people, the TA’s and I havenbeen talking about it, and we’ll probably do something. It won’t be a formal session. Itnmight simply be one of the office hours. One block of office hours will simply be devotednto this topic. And we would of course announce that by email and on the website. So thatnwould sort of be the idea. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture04.pdf|Introduction to Linear Dynamical Systems - Lecture 04]]
11009 en Lecture 5: Orthonormal Set Of Vectors Let’s continue with orthonormal sets of vectors. Does anyone have any questions aboutnlast time? If not, we’ll continue. Our topic is, of course, orthonormal sets of vectors. So anset of vectors is orthonormal. So I have K vectors in RN. They’re orthonormal if they’rennormalized. That’s an attribute of the vectors separately and then mutually orthogonal,nand that’s an attribute of the set of vectors, and it’s orthonormal if both. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture05.pdf|Introduction to Linear Dynamical Systems - Lecture 05]]
11010 en Lecture 6: Least-Squares So we're gonna talk about least squares. It's something you've probably seen in a couplenof different contexts, and it concerns overdetermined linear equations. So we have a setnof over determined linear equations. Now, here we have y=ax, where a is we'll makenstrictly skinny. It's overdetermined because you have more equations than unknowns.nAnd, of course, unless y is in the range of a, which if you pick y randomly, and rm is annevent of probability zero, you can't solve y=ax. So one method to approximately solveny=ax, and it's very important to emphasize here we're not actually solving y=ax, is tonchoose x to minimize the norm of this residual. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture06.pdf|Introduction to Linear Dynamical Systems - Lecture 06]]
11011 en Lecture 7: Least-Squares Polynomial Fitting So the question there is given a and b, how do you find out [inaudible] x such that xnequals b? How do you find – if there is such an x, how do you find one? And this willnexplain it. And it's connected to all the stuff we've been doing, so. So now that's a – that'snofficial announced. A few parts here we haven't covered yet, but we will in the nextnweek, or even we'll hit it today. I'm not sure. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture07.pdf|Introduction to Linear Dynamical Systems - Lecture 07]]
11012 en Lecture 8: Multi-Objective Least-Squares So we started looking at that last time. As a thought experiment, we did the following.nWe simply took ever XNRN and we evaluated J1 and J2, the two objectives. You wantnboth small. For every X, we put a point. All the shaded region shows you pairs as we’venwritten it, J2, J1, which are achievable, and then the clear area here are pairs that are notnachievable. We talked about this last time. We talked about the following idea, that if ancertain X corresponds to this point, then basically, all the Xs corresponding that map intonwhat is lower and to the left – these are actually points that are unambiguously better thannthis one. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture08.pdf|Introduction to Linear Dynamical Systems - Lecture 08]]
11013 en Lecture 9: Least-Norm Solution So least norm solution. As I said last time, this is something like the dual of least squaresnapproximate solution. So in least norm solution we’re studying the equation AX=Y. Butnin this case, A is fat. And we’re assuming it’s full rank, so that means you have Mnequations that can strain a variable X. But you have fewer equations and unknowns, so itnmeans you have extra degrees of freedom. What that means is that AX=Y actually hasnlots of solutions. There are lots of solutions. It means the null space of A is more than justna zero vector. In fact, it’s exactly N minus M dimensional, the null space. So there’s a lotnof freedom in choosing X. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture09.pdf|Introduction to Linear Dynamical Systems - Lecture 09]]
11014 en Lecture 10: Examples Of Autonomous Linear Dynamical Systems It’s – this is a buildup of x two coming from the decaying of x one or – actually, I said itnwrong. It’s a buildup of species b, because that’s a byproduct of the decay of species one,nand this is actually then – is actually the decay of x two, of – well, whatever it is, it’s thendecay of x two because some of species b is turning into species c. And the final one isnthis, which is x three dot equals k two times x two, meaning that species c, here, onlyncomes from the decay of species two. That’s this, and this – that’s this bottom row.nOkay? ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture10.pdf|Introduction to Linear Dynamical Systems - Lecture 10]]
11015 en Lecture 11: Solution Via Laplace Transform And Matrix Exponential We posted the solutions, literally minutes ago. This is in Packard for pickup. We postednthe solutions a few minutes before the lecture. And I’ll tell you, let me just say a fewnthings about it. Oh, I should say that usually the way that everything gets schedules outnand everything, it had to do with a shift in schedule. By tradition, we return the midtermsnthe day after the grade change option is – deadline has finished, that’s the tradition. Thisntime however, because the new schedule, we’re actually able to give you gradednmidterms beforehand. That doesn’t apply to the vast majority of people but there mightnbe a handful of people who, I don't know, whatever, decide they want to change theirngrading option. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture11.pdf|Introduction to Linear Dynamical Systems - Lecture 11]]nn
11016 en Lecture 12: Time Transfer Property So here's what it tells you. It says that to get this state at time Tau plus T from the state atntime Tau – first of all they're linearly related. That alone is – well, it's not unexpected, butnthey're linearly related. They're related by an end-by-end matrix that maps one to thenother. And that matrix is simply E to the TA. So E to the TA is a time propagator. Itnpropagates X dot equals AX forward T seconds in time. If T is negative it actually runsntime backwards, and reconstructs what the state was some seconds ago. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture12.pdf|Introduction to Linear Dynamical Systems - Lecture 12]]
11017 en Lecture 13: Markov Chain (Example) If I put a vector in front of all ones – that’s a row vector multiplied by P – I getnthis row vector here. This is the matrix way of saying that the column sums of P are allnone. So this also if you look at it – if you like, I could put a lambda in there and saynlambda is one. This basically says that P – that the vector of all ones is a left eigenvectornof P, associated with eigenvalue lambda equals one. It tells you in particular P has anneigenvalue of one. But if it has eigenvalue of one, it also has a right eigenvectornassociated with lambda equals one. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture13.pdf|Introduction to Linear Dynamical Systems - Lecture 13]]
11018 en Lecture 14: Jordan Canonical Form Okay. Now, we looked at various things involving Jordan Form, and the real questionnwas, what does it mean? I think we saw some of that from a dynamic point of view. Wencan get some of that by looking at the exponential of a Jordan Block. So the exponentialnof T times a Jordan Block looks like this. You get the familiar, E to the T lambda. So thatnyou'd expect. That's the eigenvalue. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture14.pdf|Introduction to Linear Dynamical Systems - Lecture 14]]
11019 en Lecture 15: DC Or Static Gain Matrix So what this describes is it actually describes the system, what relates the inputs to thenoutputs under static conditions. That is exactly what this does, so if you have staticnconditions, that means u, y, and x are all constant. Then of course, you have x dot and innthat case is zero; x is constant. See if zero = A + Bu, y = Cx + Du and if you eliminate xnfrom these equations by solving for x = minus A inverse Bu here and you plug that innhere, you get this. Okay? So this is assuming A is invertible here, so this is what itndescribes. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture15.pdf|Introduction to Linear Dynamical Systems - Lecture 15]]
11020 en Lecture 16: RC Circuit (Example) So again, you’ll have to sorta relearn – I mean, it’s not ancomplete relearning, but you’ll have to relearn what it means that way. Now, you might –nyou can just as well assume that A is A transpose. In other words, if you have A34 andnA43, these are the two contributions from I equals three, J equals four and I equals four, Jnequals three. You can see that these numbers are the same. So I can pull them out andnmake it A34 plus A43. And I might as well replace both of those with the average of thentwo. It doesn’t change anything. So in matrix language, you write it this way. You saynthat X transpose AX – and let’s do a quick calculations this first. Let’s take X transposenAX, and that is a scaler. That’s a scaler. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture16.pdf|Introduction to Linear Dynamical Systems - Lecture 16]]
11021 en Lecture 17: Gain Of A Matrix In A Direction Now there are cases when it doesn’t vary with direction. You’ve seen one. One is when Anis orthogonal. So if A is square and its columns are orthonormal, then Y equals AX, thennorm of Y is the norm of X, the gain is one in all directions. So watch, you’re going tonget is the complete picture today, on that. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture17.pdf|Introduction to Linear Dynamical Systems - Lecture 17]]
11022 en Lecture 18: Sensitivity Of Linear Equations To Data Error Getting closer. Well, we'll just not worry about it. It's still twisted, but that's okay. Sonwe'll look at what happens when Y varies. Of course, if Y varies a little bit, then X willnvary a little bit, and the change in X will be A inverse delta Y. Last time, I think, Inpointed this out, but if you have a matrix, which is invertible, nonsingular, but where theninverse is huge – and of course this is exactly what you'd get if you had a matrix whichnwas, for example, singular, and then you perturbed it slightly to make it nonsingular. Younwill have a matrix that's now nonsingular, but it’s inverse if going to be huge. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture18.pdf|Introduction to Linear Dynamical Systems - Lecture 18]]
11023 en Lecture 19: Reachability So you could say hi, I’m here in Rio and we’rengonna talk about the singular value decomposition or just something like that, but wenhaven’t actually approached SCPD to see if they can pull that off, but I do want to do thatnsometime. Anyway, this afternoon is a tape ahead. Please come, statically. So as long asnsome of you come. My guess is that some people will come anyway. All right. Anynquestions about last time or administrative stuff? ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture19.pdf|Introduction to Linear Dynamical Systems - Lecture 19]]
11024 en Lecture 20: Continuous-Time Reachability And then I wanna reserve some time at the end of this class tonjust say a few things at the very highest levels about what the class is, how all this stuffnfits together, and all this sort of stuff, so that’s what we’ll do. So continuous timenreachability we looked at last time, which in fact was I guess for the people here in realntime was this morning, and we looked at what’s the reachability subspace for ancontinuous time system, so that’s the set of all points you can hit in T seconds startingnfrom zero in a continuous time system. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsoeldsee263/transcripts/IntroToLinearDynamicalSystems-Lecture20.pdf|Introduction to Linear Dynamical Systems - Lecture 20]]
11044 en Lecture 1: Course Logistics I guess we’re on. I guess I’ll stay in character and saynwelcome to 364b and now I’ll explain why that’s staying in character. I believe we’renmaking history here. This is, as far as I know, the world’s first tape behind. Do we knownyet if that’s correct? Okay. We’re gonna have SCPD tell us. I have a thumbs up. So we’renmaking history here. This is a dramatic enactment of the events of the first lecture ofn364b. So let me explain a little bit of the background. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture01.pdf|Convex Optimization II - Lecture 01]]
11045 en Lecture 2: Recap: Subgradients Okay, let's see – and a couple other announcements. I presume people will be coming,nwandering in in the next ten minutes; that's perfectly okay because if you're not on this –nif you weren’t actually registered or on the access list or something like that, there'd be –nyou'd be scratching your head reading this sign over at the other room about right now,nand I'd say about ten minutes later we'll have a bunch of people coming in. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture02.pdf|Convex Optimization II - Lecture 02]]
11046 en Lecture 3: Convergence Proof Okay, if there’s no questions about last time, then I think we’ll just jump innand start in on subgradient methods. So far subgradient methods, we look at the – I mean,nsubgradient method is embarrassingly simple, right, it’s – you make a step in the negativenin anegative, I’ll call the negative, but the correct English would be an anegativensubgradient direction. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture03.pdf|Convex Optimization II - Lecture 03]]
11047 en Lecture 4: Project Subgradient For Dual Problem Sure, we’re going to need strong duality holds if it werenstrictly feasible. We’d have Slater’s condition and strong duality would hold. That givesnyou zero duality gap and I guess if you don’t have that, then you can’t solve this at all,nbecause the optimal values aren’t even the same. So let’s assume that. There’s more,nactually, to it than just that. What the sledgehammer condition is is this. What you’ll neednis that when you find lambda*, what you want is that the Lagrangian at lambda* shouldnhave a unique minimizer in x. If it does, then that x is actually x* up here. Okay? Sonthat’s the condition. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture04.pdf|Convex Optimization II - Lecture 04]]
11048 en Lecture 5: Stochastic Programming By the way, if fi of x omega is less than or equal to some – if that constraint representsnsomething like a resource usage, it means this is the expected over-utilization. Somethingnlike that. It can be all sorts of things. If it is a timing constraint, saying something has tonbe done by a certain amount, this is called the expected tardiness. If your basic inequalitynsaid this job has to be finished by this time, that’s the expected tardiness. That worksnbecause the plus function is non-decreasing and convex and so on. Now another onenwould be this, you could take the expected value of the maximum of all the constraints,nand this is the expected worst violation. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture05.pdf|Convex Optimization II - Lecture 05]]
11049 en Lecture 6: Addendum: Hit-And-Run CG Algorithm There’s got to be an opening paragraph. Innfact, we were even thinking of defining a formal XML scheme or whatever for it. But it’sngot to be a paragraph that has nothing but English and says kinda what the context of thenproblem is. So for example, I’m doing air traffic control or I’m blending covariantnmatrices from disparate sources or something like that. And says a little bit about whatnthe challenges are and what you’ll consider. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture06.pdf|Convex Optimization II - Lecture 06]]
11050 en Lecture 7: Example: Piecewise Linear Minimization Doesn’t really matter what it is. You find the analytic center of the polyhedron, which isnto say more precisely, you find the analytic center of the linear inequalities that representnthe polyhedron. And you query the cutting-plane oracle at that point. If that point is innyour target set, you quit. Otherwise, you add, you append this new cutting-plane to thenset. Of course, what happens now is your point X (K+1) is not in P (K+1) by definition.nWell, sorry. It might be in it if it’s a neutral cut, but it’s on the boundary. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture07.pdf|Convex Optimization II - Lecture 07]]
11051 en Lecture 8: Recap: Ellipsoid Method No. We can do this bisection now. It’s an integer. Well, Inguess that’s not given, but it’s an integer and it’s more than zero and less than two. Sonone person showed up. So by the way, we’ve already graded their project – actually, wenwrote the code for him. We assigned his grade on access. So it was very, very sad, really.nSo let me just remind everyone that attendance is required, even though we’re on videonnow. The flip side is this, there’s a lot of other people watching this other places. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture08.pdf|Convex Optimization II - Lecture 08]]
11052 en Lecture 9: Comments: Latex Typesetting Style I guess we’ll start. I guess the most obvious thing, I guess most of you have figured outnby now is we have looked through these preliminary project proposals, and they’renfloating around so make sure you get yours. Do not throw those away because we didn’tncopy them and we wanna make sure there’s progress, so when you resubmit them, wenwant to look at the old one again too; and then judge whether enough progress was madento justify our even looking at it again. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture09.pdf|Convex Optimization II - Lecture 09]]
11053 en Lecture 10: Decomposition Applications Today we’re gonna do – last time we finished up a bunch ofnabstract stuff about decomposition, but today we’re just going to look at two or a handfulnof applications in details to see how decomposition actually works, or where it’s actuallynapplied. So the first one we’re gonna do is rate control in a network. Actually, this is sortnof a big topic right now, so if you were to look at this stuff, you would, if you were to gonto Google or something, you’d find zillions of papers on this topic. Is that bounce in yourn– what’s that? ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture10.pdf|Convex Optimization II - Lecture 10]]
11054 en Lecture 11: Sequential Convex Programming The good news is that we can actually pull this off, I think, in one day. So we’re going toncome back later. There’ll be several other lectures on problems that are not convex andnvarious methods. There’s going to be a problem on reluxations. We’re going to have anwhole study of L1 type methods for sparse solutions. Those will come later. But this isnreally our first foray outside of convex optimization. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture11.pdf|Convex Optimization II - Lecture 11]]
11055 en Lecture 12: Recap: 'Difference Of Convex' Programming Let's see, a couple of other things. Oh, you should watch for email, and maybe the webnsite, because we may tape ahead next -Tuesday's lecture tomorrow some time. So – andnwe would. You'll hear about that. Okay. So, I can't think of any other administrativenthings. I guess we posted a new homework, but I guess most of you know that. Anynquestions about last time? Because if not, what we're gonna do is we're gonna finish upnthis topic of Heuristics based on convex optimization for solving the non-convexnproblem. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture12.pdf|Convex Optimization II - Lecture 12]]
11056 en Lecture 13: Recap: Conjugate Gradient Method So we’re looking at solving symmetric positive definite systems of equations and thisnwould come up in Newton’s method, it comes up in, you know, interior point methods,nleast squares, all these sorts of things. And last time we talked about, I mean, the CGnMethod the basic idea is it’s a method which solves Ax=b where A is positive definite.nAnd – but it does so in a different way. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture13.pdf|Convex Optimization II - Lecture 13]]
11057 en Lecture 14: Methods (Truncated Newton Method) So this is what we’ll do. The problem was scaled in such a way that we could pull off anCholesky factorization. I think the Cholesky factor had something like 30 millionnnonzeros. So it’ll take some time to do both the Cholesky factorization and also to do thenbackward and forward substitution. So but direct is possible. All we have to do with thisnproblem is scale it by a factor of ten and direct becomes kind of out of the question, sonthen at least on a little standard machine. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture14.pdf|Convex Optimization II - Lecture 14]]
11058 en Lecture 15: Recap: Example: Minimum Cardinality Problem All right, I think this means we are on. There is no good waynin this room to know if you are – when the lecture starts. Okay, well, we are down to anskeleton crew here, mostly because it’s too hot outside. So we’ll continue with L_1nmethods today. So last time we saw the basic idea. The most – the simplest idea is this. Ifnyou want to minimize the cardinality of X, find the sparsest vector X that’s in a convexnset, the simplest heuristic – and actually, today, we’ll see lots of variations on it that arenmore sophisticated. But the simplest one, by far, is simply to minimize the one norm of Xnsubject to X and Z. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture15.pdf|Convex Optimization II - Lecture 15]]
11059 en Lecture 16: Model Predictive Control Today we’re jumping around a different order of topics, but this is maybe, I think, thennext topic that some people are working on, it’s obviously too late for them for theirnprojects, but we can at least cover the material and for people who are doing this at leastnit’ll make a lot of sense. For other people, it’s actually very, very good stuff to knownabout. It’s widely, widely used. So it’s called Model Predictive Control. In fact, I’ve beennreading a lot about it the last couple of days. To sit through very long airplane flights,nread a couple more books on it. It has got tons of different names, all different. Basicallynall the different areas doing this don’t know about the others. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture16.pdf|Convex Optimization II - Lecture 16]]
11060 en Lecture 17: Stochastic Model Predictive Control So what this is is that the next state depends on actually two things – well, three thingsnreally. It depends on the current state, so that’s this. It depends on your action, and itndepends on this random variable. Actually, at this point, it’s not yet a random variable,nbut it’s something – the idea is it will soon be a random variable – and it depends onnsomething that you don’t know fully. That’s W of t. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture17.pdf|Convex Optimization II - Lecture 17]]
11061 en Lecture 18: Announcements But you’re not gonna give up globalness – globality? You’re gonna give up neithernglobalness nor globality but you are gonna give up speed. So what’s gonna happen isnthese are gonna be methods that are slow but they don’t lie. They will produce a – at anynpoint you can stop them and exactly like a convex optimization method they will have anlower bound and they will have an upper bound. ...nnSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture18.pdf|Convex Optimization II - Lecture 18]]
11086 en Group summary 
11110 en Immunization Hesitancy: A Rising Tide that Challenges the Public Health Societal support for traditional childhood immunization is changing. Increasingly, parents are renegotiating recommended immunization schedules with pediatricians. Marcuse, also associate medical director at Seattle Children's Hospital, discusses this hesitancy and the potential consequences for disease prevention. In this videotaped lecture, he also addresses balancing parental rights with protecting public health. This lecture was part of the Howard A. Schneiderman Memorial Bioethics Lecture Series, which began in 1990 with an endowment from Schneiderman, the third biological sciences school dean. The series brings renowned experts to UCI to speak about the social and ethical implications of advances in biology and medicine.
11153 en Multi-Label Prediction via Compressed Sensing We consider multi-label prediction problems with large output spaces under the assumption of output sparsity - that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.
11155 en An Additive Latent Feature Model for Transparent Object Recognition Existing methods for recognition of object instances and categories based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization. The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance. We model transparent local patch appearance using an additive model of latent factors: background factors due to scene content, and factors which capture a local edge energy distribution characteristic of the refraction. We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into 'transparent visual words' according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment.
11156 en Semi-Supervised Learning in Gigantic Image Collections With the advent of the Internet it is now possible to collect hundreds of millions of images. These images come with varying degrees of label information. "Clean labels'' can be manually obtained on a small fraction, "noisy labels'' may be extracted automatically from surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the number of images, making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes. In this paper we show how to utilize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images. Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. We combine this with a label sharing framework obtained from Wordnet to propagate label information to classes lacking manual annotations. Our algorithm enables us to apply semi-supervised learning to a database of 80 million images with 74 thousand classes.
11157 en Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non-stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.
11158 en Locality-Sensitive Binary Codes from Shift-Invariant Kernels This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections, such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g., a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme, and report favorable experimental performance as compared to a recent state-of-the-art method, spectral hashing.
11159 en Discriminative Network Models of Schizophrenia Schizophrenia is a complex psychiatric disorder that has eluded a characterization in terms of local abnormalities of brain activity, and is hypothesized to affect the collective, "emergent'' working of the brain. We propose a novel data-driven approach to capture emergent features using functional brain networks [Eguiluzet al] extracted from fMRI data, and demonstrate its advantage over traditional region-of-interest (ROI) and local, task-specific linear activation analyzes. Our results suggest that schizophrenia is indeed associated with disruption of global, emergent brain properties related to its functioning as a network, which cannot be explained by alteration of local activation patterns. Moreover, further exploitation of interactions by sparse Markov Random Field classifiers shows clear gain over linear methods, such as Gaussian Naive Bayes and SVM, allowing to reach 86% accuracy (over 50% baseline - random guess), which is quite remarkable given that it is based on a single fMRI experiment using a simple auditory task.
11161 en Explaining Human Multiple Object Tracking as Resource-Constrained Approximate Inference in a Dynamic Probabilistic Model Multiple object tracking is a task commonly used to investigate the architecture of human visual attention. Human participants show a distinctive pattern of successes and failures in tracking experiments that is often attributed to limits on an object system, a tracking module, or other specialized cognitive structures. Here we use a computational analysis of the task of object tracking to ask which human failures arise from cognitive limitations and which are consequences of inevitable perceptual uncertainty in the tracking task. We find that many human performance phenomena, measured through novel behavioral experiments, are naturally produced by the operation of our ideal observer model (a Rao-Blackwelized particle filter). The tradeoff between the speed and number of objects being tracked, however, can only arise from the allocation of a flexible cognitive resource, which can be formalized as either memory or attention.
11162 en Optimizing Multi-Class Spatio-Spectral Filters via Bayes Error Estimation for EEG Classification The method of common spatio-spectral patterns (CSSPs) is an extension of common spatial patterns (CSPs) by utilizing the technique of delay embedding to alleviate the adverse effects of noises and artifacts on the electroencephalogram (EEG) classification. Although the CSSPs method has shown to be more powerful than the CSPs method in the EEG classification, this method is only suitable for two-class EEG classification problems. In this paper, we generalize the two-class CSSPs method to multi-class cases. To this end, we first develop a novel theory of multi-class Bayes error estimation and then present the multi-class CSSPs (MCSSPs) method based on this Bayes error theoretical framework. By minimizing the estimated closed-form Bayes error, we obtain the optimal spatio-spectral filters of MCSSPs. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on the data set of BCI competition 2005. The experimental results show that our method significantly outperforms the previous multi-class CSPs (MCSPs) methods in the EEG classification.
11163 en Functional Network Reorganization In Motor Cortex Can Be Explained by Reward-Modulated Hebbian Learning The control of neuroprosthetic devices from the activity of motor cortex neurons benefits from learning effects where the function of these neurons is adapted to the control task. It was recently shown that tuning properties of neurons in monkey motor cortex are adapted selectively in order to compensate for an erroneous interpretation of their activity. In particular, it was shown that the tuning curves of those neurons whose preferred directions had been misinterpreted changed more than those of other neurons. In this article, we show that the experimentally observed self-tuning properties of the system can be explained on the basis of a simple learning rule. This learning rule utilizes neuronal noise for exploration and performs Hebbian weight updates that are modulated by a global reward signal. In contrast to most previously proposed reward-modulated Hebbian learning rules, this rule does not require extraneous knowledge about what is noise and what is signal. The learning rule is able to optimize the performance of the model system within biologically realistic periods of time and under high noise levels. When the neuronal noise is fitted to experimental data, the model produces learning effects similar to those found in monkey experiments.
11164 en Know Thy Neighbour: A Normative Theory of Synaptic Depression Synapses exhibit an extraordinary degree of short-term malleability, with release probabilities and effective synaptic strengths changing markedly over multiple timescales. From the perspective of a fixed computational operation in a network, this seems like a most unacceptable degree of added noise. We suggest an alternative theory according to which short term synaptic plasticity plays a normatively-justifiable role. This theory starts from the commonplace observation that the spiking of a neuron is an incomplete, digital, report of the analog quantity that contains all the critical information, namely its membrane potential. We suggest that one key task for a synapse is to solve the inverse problem of estimating the pre-synaptic membrane potential from the spikes it receives and prior expectations, as in a recursive filter. We show that short-term synaptic depression has canonical dynamics which closely resemble those required for optimal estimation, and that it indeed supports high quality estimation. Under this account, the local postsynaptic potential and the level of synaptic resources track the (scaled) mean and variance of the estimated presynaptic membrane potential. We make experimentally testable predictions for how the statistics of subthreshold membrane potential fluctuations and the form of spiking non-linearity should be related to the properties of short-term plasticity in any particular cell type.
11165 en Efficient Learning using Forward-Backward Splitting We describe, analyze, and experiment with a new framework for empirical lossnminimization with regularization. Our algorithmic framework alternates betweenntwo phases. On each iteration we first perform an unconstrained gradient descentnstep. We then cast and solve an instantaneous optimization problem that trades offnminimization of a regularization term while keeping close proximity to the resultnof the first phase. This yields a simple yet effective algorithm for both batch penalizednrisk minimization and online learning. Furthermore, the two phase approachnenables sparse solutions when used in conjunction with regularization functionsnthat promote sparsity, such as ?1. We derive concrete and very simple algorithmsnfor minimization of loss functions with ?1, ?2, ?2n2, and ?∞ regularization. Wenalso show how to construct efficient algorithms for mixed-norm ?1/?q regularization.nWe further extend the algorithms and give efficient implementations for verynhigh-dimensional data with sparsity. We demonstrate the potential of the proposednframework in experiments with synthetic and natural datasets.
11166 en On Stochastic and Worst-case Models for Investing In practice, most investing is done assuming a probabilistic model of stock price returns known as the Geometric Brownian Motion (GBM). While it is often an acceptable approximation, the GBM model is not always valid empirically. This motivates a worst-case approach to investing, called universal portfolio management, where the objective is to maximize wealth relative to the wealth earned by the best fixed portfolio in hindsight. In this paper we tie the two approaches, and design an investment strategy which is universal in the worst-case, and yet capable of exploiting the mostly valid GBM model. Our method is based on new and improved regret bounds for online convex optimization with exp-concave loss functions.
11167 en Bootstrapping from Game Tree Search In this paper we introduce a new algorithm for updating the parameters of a heuristic evaluation function, by updating the heuristic towards the values computed by an alpha-beta search. Our algorithm differs from previous approaches to learning from search, such as Samuel's checkers player and the TD-Leaf algorithm, in two key ways. First, we update all nodes in the search tree, rather than a single node. Second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation function. We implemented our algorithm in a chess program Meep, using a linear heuristic function. After initialising its weight vector to small random values, Meep was able to learn high quality weights from self-play alone. When tested online against human opponents, Meep played at a master level, the best performance of any chess program with a heuristic learned entirely from self-play.
11168 en Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions Embeddings of probability measures into reproducing kernel Hilbert spaces have been proposed as a straightforward and practical means of representing and comparing probabilities. In particular, the distance between embeddings (the maximum mean discrepancy, or MMD) has several key advantages over many classical metrics on distributions, namely easy computability, fast convergence and low bias of finite sample estimates. An important requirement of the embedding RKHS is that it be characteristic: in this case, the MMD between two distributions is zero if and only if the distributions coincide. Three new results on the MMD are introduced in the present study. First, it is established that MMD corresponds to the optimal risk of a kernel classifier, thus forming a natural link between the distance between distributions and their ease of classification. An important consequence is that a kernel must be characteristic to guarantee classifiability between distributions in the RKHS. Second, the class of characteristic kernels is broadened to incorporate all bounded, continuous strictly positive definite kernels: these include non-translation invariant kernels and kernels on non-compact domains. Third, a generalization of the MMD is proposed for families of kernels, as the supremum over MMDs on a class of kernels (for instance the Gaussian kernels with different bandwidths). This extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate. This generalization is reasonable, given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding kernel classifier. The generalized MMD is shown to have consistent finite sample estimates, and its performance is demonstrated on a homogeneity testing example.
11169 en An LP View of the M-Best MAP Problem We consider the problem of finding the M assignments with maximum probability in a probabilistic graphical model. We show how this problem can be formulated as a linear program (LP) on a particular polytope. We prove that, for tree graphs (and junction trees in general), this polytope has a particularly simple form and differs from the marginal polytope in a single inequality constraint. We use this characterization to provide an approximation scheme for non-tree graphs, by using the set of spanning trees over such graphs. The method we present puts the M-best inference problem in the context of LP relaxations, which have recently received considerable attention and have proven useful in solving difficult inference problems. We show empirically that our method often finds the provably exact M best configurations for problems of high tree width.
11170 en Fast Subtree Kernels on Graphs In this article, we propose fast subtree kernels on graphs. On graphs with n nodesnand m edges and maximum degree d, these kernels comparing subtrees of heightnh can be computed in O(mh), whereas the classic subtree kernel by Ramon &nGärtner scales as O(n24dh). Key to this efficiency is the observation that thenWeisfeiler-Lehman test of isomorphism from graph theory elegantly computes ansubtree kernel as a byproduct. Our fast subtree kernels can deal with labeledngraphs, scale up easily to large graphs and outperform state-of-the-art graph kernelsnon several classification benchmark datasets in terms of accuracy and runtime.
11171 en Sharing Features among Dynamical Systems with Beta Processes We propose a Bayesian nonparametric approach to relating multiple time series via a set of latent, dynamical behaviors. Using a beta process prior, we allow data-driven selection of the size of this set, as well as the pattern with which behaviors are shared among time series. Via the Indian buffet process representation of the beta process' predictive distributions, we develop an exact Markov chain Monte Carlo inference method. In particular, our approach uses the sum-product algorithm to efficiently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals. We validate our sampling algorithm using several synthetic datasets, and also demonstrate promising unsupervised segmentation of visual motion capture data.
11172 en Reading Tea Leaves: How Humans Interpret Topic Models Probabilistic topic models are a commonly used tool for analyzing text data, where the latent topic representation is used to perform qualitative evaluation of models and guide corpus exploration. Practitioners typically assume that the latent space is semantically meaningful, but this important property has lacked a quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may actually infer less semantically meaningful topics.
11173 en Time-rescaling Methods for the Estimation and Assessment of Non-Poisson Neural Encoding Models Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically, these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (B) a modulated non-Poisson renewal process (e.g., inhomogeneous gamma process). Here we show that the two approaches can be combined, resulting in a {\it conditional renewal} (CR) model for neural spike trains. This model captures both real and rescaled-time effects, and can be fit by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model, the log-likelihood is concave in the linear filter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g. gamma with $\kappa \neq1$), suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover, we show that goodness-of-fit tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data.
11217 en Introduction: Presentations of Different Views on Clustering by the Workshop Organizers 
11220 en What Is a Cluster: Perspectives from Game Theory Instead of insisting on the idea of determiningna partition of the input data, and hence obtaining the clusters as a by-product of the partitioningnprocess, in this presentation I propose to reverse the terms of the problem and attempt instead to derivena rigorous formulation of the very notion of a cluster. Clearly, the conceptual question “what isna cluster?” is as hopeless, in its full generality, as is its companion “what is an optimal clustering?”nwhich has dominated the literature in the past few decades, both being two sides of the same coin.nAn attempt to answer the former question, however, besides shedding fresh light into the nature ofnthe clustering problem, would allow us, as a consequence, to naturally overcome the major limitationsnof the partitional approach alluded to above, and to deal with more general problems where,ne.g., clusters may overlap and clutter elements may get unassigned, thereby hopefully reducing thengap between theory and practice.nnIn our endeavor to provide an answer to the question raised above, we found that game theorynoffers a very elegant and general perspective that serves well our purposes. Hence, in the second,nconstructive part of the presentation I will describe a game-theoretic framework for clustering [21,n25, 22] which has found applications in fields as diverse as computer vision and bioinformatics.nThe starting point is the elementary observation that a “cluster” may be informally defined as anmaximally coherent set of data items, i.e., as a subset of the input data C which satisfies both anninternal criterion (all elements belonging to C should be highly similar to each other) and an externalnone (no larger cluster should contain C as a proper subset). We then formulate the clustering problemnas a non-cooperative clustering game. Within this context, the notion of a cluster turns out to benequivalent to a classical equilibrium concept from (evolutionary) game theory, as the latter reflectsnboth the internal and external cluster conditions mentioned above.
11221 en Clustering with Prior Information In summary, we have demonstrated analytically that any small (but finite) amount of semi–nsupervision suppresses the phase transition in cluster detectability for the planted–bisection model,nby shifting the detection threshold to its lowest possible value. For graphs where the links withinnand across the clusters have different weights, we found that semi–supervision leads to a detectionnthreshold that depends on ?. Furthermore, if J < K, then for ? ? 0+, the detection threshold convergesnto a value lower (better) from the one obtained via balancing within–cluster and inter–clusternweights. This suggests that for weighted graphs a small [but generic] semi-supervising can be employednfor defining the very clustering structure. This definition is non-trivial, since it performsnbetter than the weight-balancing definition. Note also that for weighted graphs the very notion ofnthe detection threshold is not clear a priori, in contrast to unweighted networks, where the onlynpossible definition goes via the connectivity balance ? = ?. To illustrate this unclarity, considerna node connected to one cluster via few heavy links, and to another cluster via many light links.nTo which cluster this node should belong in principle? Our (speculative) answer is that the properncluster assignment in this case can be defined via semi-supervising.
11222 en Finding a Better k: A Psychophysical Investigation of Clustering Finding the number of groups in a data set, k, is an important problem in thenfield of unsupervised machine learning with applications across many scientificndomains. The problem is difficult however, because it is ambiguous and hierarchical,nand current techniques for finding k often produce unsatisfying results.nHumans are adept at navigating ambiguous and hierarchical situations, and thisnpaper measures human performance on the problem of finding k across a widenvariety of data sets. We find that humans employ multiple strategies for choosingnk, often simultaneously, and the number of possible interpretations of even simplendata sets with very few (N < 20) samples can be quite high. In addition, twonleading machine learning algorithms are compared to the human results.
11223 en Single Data, Multiple Clusterings There has been extensive research in the clustering community on formalizing thendefinition of the quality of a given data clustering. However, is it possible to measurenthe quality of a clustering unless human judgment is taken into consideration?nThe notion of quality is subjective: for example, given the task of clustering a setnof movie reviews, some users might want to cluster them according to sentiment,nwhile others might want to cluster them according to genre. If the clustering algorithmnis passive (i.e., it does not have the ability to produce multiple clusteringsnby actively taking user intent into account), it is hard to justify the algorithm tonbe qualitatively best across different domains. There has been a recent surge ofninterest in quantifying how clusterable a dataset is [2]. Can we similarly definenmulti-clusterability? In this paper, we present a (really) simple active clusteringnarchitecture that can help understand the multi-clusterability of a dataset.
11224 en Empricial Study of Cluster Evaluation Metrics A wide range of abstract characteristics of partitions have been proposed for clusternevaluation. We empirically evaluated the performance of these metrics for flowncytometry data and found that the set-matching metrics perform closest to human.nnClustering is an increasingly popular module in data processing applications. Many clustering algorithmsnhave been developed and many more are anticipated to emerge in the future. Thus, methodsnfor assessing the performance of a clustering algorithms are in great demand. Such methods assessnthe performance of a clustering algorithm by computing a quality score of the solution against anground truth partition, usually designed by a human expert. A wide range of these criterion havenbeen proposed [9]. Evaluating clustering algorithms heavily relies on the chosen quality score however,nit is not practical to study the performance of these metrics in a domain-independent way [3].nIn this paper we aim to empirically evaluate the available metrics to find the best metric for comparingnclustering solutions against ground truth partitions for flow cytometry (FCM) applications.nThis work was motivated in part by the challenges we faced in choosing the best clustering comparisonnmetric for the FlowCap project. FlowCap is an international open project designed to providenan objective way to compare and evaluate FCM data clustering methods, and also to establishnguidance about appropriate use and application of these methods (for more information visitnhttp://flowcap.flowsite.org/).
11225 en Clustering Applications at Yahoo! 
11226 en Some Ideas for Formalizing Clustering Despite being one of the most commonly used tools for unsupervised exploratoryndata analisys and despite its and extensive literature very little is known about thentheoretical foundations of clustering methods. We have been working on variousnmathematical approaches which allow the extension of earlier results in this areanbased on ideas from topology and metric geometry. We will give an overview atnthe workshop.
11227 en Characterization of Linkage Based Clustering There are a wide variety of clustering algorithms that, when run on the same data, often producenvery different clusterings. Yet there is no principled method to guide the selection of a clusteringnalgorithm. The choice of an appropriate clustering is, of course, task dependent. As such, we mustnrely on domain knowledge. The challenge is to communicate such knowledge between the domainnexpert and the algorithm designer. One approach to providing guidance to clustering users in thenselection of a clustering algorithm is to identify important properties that a user may want an algorithmnto satisfy, and determine which algorithms satisfy each of these properties. Clustering usersncan then utilize prior knowledge to determine the properties that make sense for their application.nnUltimately, there would be a sufficiently rich set of properties that would provide detailed enoughnguidelines for a wide variety of clustering users. For a property to be useful, a user needs to benable to easily determine the desirability of the property. Such a description of clustering algorithmsnwould yield principled guidelines for clustering algorithm selection by answering a series of simplenquestions. Bosagh Zadeh and Ben-David [1] make progress in this direction by providing a setnof abstract properties that characterize single linkage. In this work, we give another result in thensame direction by characterizing a family of clustering algorithms. These are initial steps toward thenambitious program of developing broad guidelines for clustering algorithm selection.nnLinkage-based clustering is one of the most commonly-used and widely-studied clusteringnparadigms. We provide a surprisingly simple set of properties that uniquely identify linkage-basednclustering algorithms. Our characterization highlights how linkage-based algorithms compare tonother clustering algorithms.nnCombining previously proposed properties with our newly proposed ones, we show how these propertiesnpartition the space of commonly-used clustering algorithms. Specifically, we show which ofnthese properties are satisfied by common linkage-based, centroid-based, and spectral clustering algorithms.nWe hope that this analysis, as well as our characterization of linkage-based clustering,nwill provide useful guidelines for users in selecting clustering algorithms.
11228 en Information Theoretic Model Selection in Clustering Model selection in clustering requires (i) to specify a clustering principle and (ii)nto decide an appropriate number of clusters depending on the noise level in thendata. We advocate an information theoretic perspective where the uncertainty innthe data set induces an uncertainty in the solution space of clusterings. A clusteringnmodel, which can tolerate a higher level of noise in the data than competingnmodels, is considered to be superior provided that the clustering solutionnis equally informative. This tradeoff between informativeness and robustness isnused as a model selection criterion. The request that solutions should generalizenfrom one data set to an equally probable second data set gives rise to a new notionnof structure induced information.
11229 en PAC-Bayesian Approach to Formulation of Clustering Objectives Clustering is a widely used tool for exploratory data analysis. However, thentheoretical understanding of clustering is very limited. We still do not have anwell-founded answer to the seemingly simple question of “how many clusters arenpresent in the data?”, and furthermore a formal comparison of clusterings basednon different optimization objectives is far beyond our abilities. The lack of goodntheoretical support gives rise to multiple heuristics that confuse the practitionersnand stall development of the field.nWe suggest that the ill-posed nature of clustering problems is caused by the factnthat clustering is often taken out of its subsequent application context. We arguenthat one does not cluster the data just for the sake of clustering it, but rather tonfacilitate the solution of some higher level task. By evaluation of the clustering’sncontribution to the solution of the higher level task it is possible to compare differentnclusterings, even those obtained by different optimization objectives. In thenpreceding work it was shown that such an approach can be applied to evaluationnand design of co-clustering solutions. Here we suggest that this approach can benextended to other settings, where clustering is applied.
11230 en Planning under Uncertainty Using Distributions over Posteriors Modern control theory has provided a large number of tools for dealing with probabilistic systems. However, most of these tools solve for local policies; there are relatively few tools for solving for complex plans that, for instance, gather information. In contrast, the planning community has provided ways to compute plans that handle complex probabilistic uncertainty, but these often don't work for large or continuous problems. Recently, our group has developed techniques for planners that can efficiently search for complex plans in probabilistic domains by taking advantage of local solutions provided by feedback and open-loop controllers, and predicting a distribution over the posteriors. This approach of planning over distributions of posteriors can incorporate a surprisingly wide variety of sensor models and objective functions. I will show some results in a couple of domains including helicopter flight in GPS-denied environments.
11231 en GP-BayesFilters: Gaussian Process Regression for Bayesian Filtering Bayes filters recursively estimate the state of dynamical systems from streams of sensor data. Key components of each Bayes filter are probabilistic prediction and observation models. In robotics, these models are typically based on parametric descriptions of the physical process generating the data. In this talk I will show how non-parametric Gaussian process prediction and observation models can be integrated into different versions of Bayes filters, namely particle filters and extended and unscented Kalman filters. The resulting GP-BayesFilters can have several advantages over standard filters. Most importantly, GP-BayesFilters do not require an accurate, parametric model of the system. Given enough training data, they enable improved tracking accuracy compared to parametric models, and they degrade gracefully with increased model uncertainty. We extend Gaussian Process Latent Variable Models to train GP-BayesFilters from partially or fully unlabeled training data. The techniques are evaluated in the context of visual tracking of a micro blimp and IMU-based tracking of a slotcar.
11232 en Imitation Learning and Purposeful Prediction: Probabilistic and Non-probabilistic Methods Programming robot behavior remains a challenging task. While it is often easy to abstractly define or even demonstrate a desired behavior, designing a controller that embodies the same behavior is difficult, time consuming, and ultimately expensive. The machine learning paradigm offers the promise of enabling "programming by demonstration" for developing high-performance robotic systems. Unfortunately, many "behavioral cloning" approaches that utilize the classical tools of supervised learning (e.g. decision trees, neural networks, or support vector machines) do not fit the needs of modern robotic systems. Classical statistics and supervised machine learning exist in a vacuum: predictions made by these algorithms are explicitly assumed to not affect the world in which they operate. nIn practice, robotic systems are often built atop sophisticated planning algorithms that efficiently reason far into the future; consequently, ignoring these planning algorithms in lieu of a supervised learning approach often leads to myopic and poor-quality robot performance. While planning algorithms have shown success in many real-world applications ranging from legged locomotion to outdoor unstructured navigation, such algorithms rely on fully specified cost functions that map sensor readings and environment models to quantifiable costs. Such cost functions are usually manually designed and programmed. Recently, our group has developed a set of techniques that learn these functions from human demonstration. These algorithms apply an Inverse Optimal Control (IOC) approach to find a cost function for which planned behavior mimics an expert's demonstration. nI'll discuss these methodologies, both probabilistic and otherwise, for imitation learning. I'll focus on the Principle of Causal Maximum Entropy that generalizes the classical Maximum Entropy Principle, widely used in many fields including physics, statistics, and computer vision, to problems of decision making and control. This generalization enables MaxEnt to apply to a new class of problems including Inverse Optimal Control and activity forecasting. This approach further elucidates the intimate connections between probabilistic inference and optimal control. nI'll consider case studies in activity forecasting of drivers and pedestrians as well as the imitation learning of robotic locomotion and rough-terrain navigation. These case-studies highlight key challenges in applying the algorithms in practical settings that utilize state-of-the-art planners and are constrained by efficiency requirements and imperfect expert demonstration.
11233 en Probabilistic Control in Human Computer Interaction Continuous interaction with computers can be treated as a control problem subject to various sources of uncertainty. We present examples of interaction based on multiple noisy sensors (capacitive sensing, location- and bearing sensing and EEG), in domains which rely on inference about user intention, and where the use of particle filters can improve performance. We use the "H-metaphor" for automated, flexibly handover of level of autonomy in control, as a function of the certainty of control actions from the user, in an analogous fashion to 'loosening the reins' when horse-riding. Integration of the inference mechanisms with probabilistic feedback designs can have a significant effect on behaviour, and some examples are presented. (Joint work with John Williamson, Simon Rogers and Steven Strachan).
11234 en Estimating the Sources of Motor Errors Motor adaptation is usually defined as the process by which our nervous system produces accurate movements while the properties of our bodies and our environment continuously change. Many experimental and theoretical studies have characterized this process by assuming that the nervous system uses internal models to compensate for motor errors. Here we extend these approaches and construct a probabilistic model that not only compensates for motor errors but estimates the sources of these errors. These estimates dictate how the nervous system should generalize. For example, estimated changes of limb properties will affect movements across the workspace but not movements with the other limb. We extend previous studies in that area to account for temporal and context effects. This extended model explains aspects of savings along with aspects of generalization.
11235 en Linear Bellman Equations: Theory and Applications I will provide a brief overview of a class stochastic optimal control problems recently developed by our group as well as by Bert Kappen's group. This problem class is quite general and yet has a number of unique properties, including linearity of the exponentially-transformed (Hamilton-Jacobi) Bellman equation, duality with Bayesian inference, convexity of the inverse optimal control problem, compositionality of optimal control laws, path-integral representation of the exponentially-transformed value function. I will then focus on function approximation methods that exploit the linearity of the Bellman equation, and illustrate how such methods scale to high-dimensional continuous dynamical systems. Computing the weights for a fixed set of basis functions can be done very efficiently by solving a large but sparse linear problem. This enables us to work with hundreds of millions of (localized) bases. Still, the volume of a high-dimensional state space is too large to be filled with localized bases, forcing us to consider adaptive methods for positioning and shaping those bases. Several such methods will be compared.
11236 en KL Control Theory and Decision Making under Uncertainty KL control theory consists of a class of control problems for which the control computation can be solved as a graphical model inference problem. In this talk, we show how to apply this theory in the context of a delayed choice task and for collaborating agents. We first introduce the KL control framework. Then we show that in a delayed reward task when the future is uncertain it is optimal to delay the timing of your decision. We show preliminary results on human subjects that confirm this prediction. Subsequently, we discuss two player games, such as the stag-hunt game, where collaboration can improve or worsten as a result of recursive reasoning about the opponents actions. The Nash equilibria appear as local minima of the optimal cost to go, but may disappear when monetary gain decreases. This behaviour is in agreement with experimental findings in humans.
11237 en Linear Bellman Combination for Simulation of Human Motion Simulation of natural human motion is challenging because the relevant system dynamics is high-dimensional, underactuated—no direct control over global position and orientation—and non-smooth—frequent and intermittent ground contacts. In order to succeed, control policy must look ahead to determine stabilizing actions and it must optimize to generate lifelike motion. In this talk, we will review recently developed control systems that yield high-quality agile movements for three-dimensional human simulations. Creating such controllers requires intensive computer optimization and reveals a need for reusing as many control policies as possible. We will answer this problem partially with an efficient combination that creates a new optimal control policy by reusing a set of optimal controls for related tasks. It remains to be seen if the same approach can also be applied to control systems needed to generate lifelike human motion.
11239 en Probabilistic Design: Promises and Prospects The Fully Probabilistic Design (FPD) suggests a probabilistic description of the closed control loop behaviour as well as desired closed-loop behaviour. The optimal control strategy is selected as the minimiser of the Kullback-Leibler divergence of these distributions. The approach yields: (i) an explicit minimiser with the evaluation reduced to a conceptually feasible solution of integral equations; (ii) a randomised optimal strategy; (iii) a proper subset of FPDs formed via standard Bayesian designs; (iv) uncertain knowledge, multiple control goals, and optimisation constrains be expressed in the common probabilistic language. It implies: (i) an easier approximation of the dynamic programming counterpart; (ii) the optimal strategy is naturally explorative; (iii) the goals-expressing ideal distribution can be, even recursively, tailored to the observed closed-loop behavior; (iv) an opportunity to automatically harmonise knowledge and goals within a flat cooperation structure of decentralised task. An importance of the last point has been confirmed by a huge amount of societal/industrial problems that cannot be governed in a centralised way. The anticipated decentralised solution based on the FPD may concern either a number of interacting, locally independent elements, which have their local goals, but have to collaborate to reach a common group goal (e.g. cooperative robots, multi-agent systems, etc.); or a set of independent elements with own goals that need to coordinate their activities (e.g. transportation). The talk will recall the basic properties of FPD and discusses the promises of an exploitation of the FPD potential.
11240 en Approximate Inference Control Approximate Inference Control (AICO) is a method for solving Stochastic Optimal Control (SOC) problems. The general idea is to think of control as the problem of computing a posterior over trajectories and control signals conditioned on constraints and goals. Since exact inference is infeasible in realistic scenarios, the key for high-speed planning and control algorithms is the choice of approximations. In this talk I will introduce to the general approach, discuss its intimate relations to DDP and the current research on Kalman's duality, and discuss the approximations that we use to get towards real-time planning in high-dimensional robotic systems. I will also mention recent work on using Expectation Propagation and truncated Gaussians for inference under hard constraints and limits as they typically arise in robotics (collision and joint limit constraints).
11241 en Inference for PCFGs and Adaptor Grammars This talk describes the procedures we've developed for adaptor grammar inference. Adaptor grammars are a non-parametric extension to PCFGs that can be used to describe a variety of phonological and morphological language learning tasks. We start by reviewing an MCMC sampler for Probabilistic Context-Free Grammars that serves as the basis for adaptor grammar inference, and then explain how samples from a PCFG whose rules depend on the other sampled trees can be used as a proposal distribution in an MCMC procedure for estimating adaptor grammars. Finally we describe several optimizations that dramatically speed inference of complex adaptor grammars.
11242 en Learning to Disambiguate Natural Language Using World Knowledge We present a general framework and learning algorithm for the task of conceptnlabeling: each word in a given sentence has to be tagged with the unique physicalnentity (e.g. person, object or location) or abstract concept it refers to. Our methodnallows both world knowledge and linguistic information to be used during learningnand prediction. We show experimentally that we can handle natural language andnlearn to use world knowledge to resolve ambiguities in language, such as wordnsenses or coreference, without the use of hand-crafted rules or features.
11243 en Language Modeling with Tree Substitution Grammars We show that a tree substitution grammar (TSG) induced with a collapsed Gibbsnsampler results in lower perplexity on test data than both a standard context-freengrammar and other heuristically trained TSGs, suggesting that it is better suited tonlanguage modeling. Training a more complicated bilexical parsing model acrossnTSG derivations shows further (though nuanced) improvement. We conduct analysisnand point to future areas of research using TSGs as language models.
11244 en A Preliminary Evaluation of Word Representations for Named-Entity Recognition We use different word representations as word features for a named-entity recognitionn(NER) system with a linear model. This work is part of a larger empiricalnsurvey, evaluating different word representations on different NLP tasks. We evaluatenBrown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnihn& Hinton, 2009) embeddings of words. All three representations improve accuracynon NER, with the Brown clusters providing a larger improvement than thentwo embeddings, and the HLBL embeddings more than the Collobert and Westonn(2008) embeddings. We also discuss some of the practical issues in using embeddingsnas features. Brown clusters are simpler than embeddings because theynrequire less hyperparameter tuning.
11245 en Learnable Representations for Natural Language The Chomsky hierarchy was explicitly intended to represent the hypotheses from distributional learning algorithms; yet these standard representations are well known to be hard to learn, even under quite benign learning paradigms, because of the computationally complexity of inferring rich hidden structures like trees.nnThere is a lot of interest in unsupervised learning of natural language -- current approaches (e.g. Klein and Manning, Johnson's Adaptor Grammars) use modifications of existing models such as tree or dependency structures together with sophisticated statistical models in order to recover structures that are as close as possible to gold standard manual annotations.nnThis tutorial will cover a different approach: recent algorithms for the unsupervised learning of representations of natural language based on distributional learning (Clark & Eyraud 2007; Clark, Eyraud and Habrard, 2008; Clark 2009). This research direction involves abandoning the standard models and designing new representation classes for formal languages that are richly structured but where the structure is not hidden but based on observable structures of the language -- the syntactic monoid or a lattice derived from that monoid. These representation classes are as a result easy to learn.nnWe will look briefly at algorithms for learning deterministic automata, and then move on to algorithms for learning context free and context sensitive languages. These algorithms explicitly model the distribution of substrings of the language: they are efficient (polynomial update time) and provably correct for a class of languages that includes all regular languages, many context free languages and a few context sensitive languages. This class may be rich enough to represent natural language syntax.
11246 en Learning Languages and Rational Kernels This talk will discuss several topics related to learning automata and learning languages with rationalnkernels.
11247 en Sparsity in Grammar Induction 
11249 en Where's What? - Towards Semantic Mapping of Urban Environments The availability of continuous streams of data from multiple modalities covering the same workspace has long been recognised as a privilege by robotics researchers. Data fusion has a successful track record in the field leading to the by now routine generation of high-quality large scale metric and topological maps of unstructured environments. With this success, however, comes the realisation that prominent applications in robotics -- such as action selection and human machine interaction -- require information beyond mere metric or topological representations. As a result, researchers throughout the community are becoming increasingly interested in adding higher-order, semantic information to the maps obtained. In this context, the availability of a rich set of data from complimentary modalities once again comes into its own. In this talk we provide a snapshot of ongoing work aiming to enrich standard metric or topological maps as provided by a mobile robot with higher-order semantic information. Environmental cues are considered for classification at different scales. The first stage considers local scene properties using a probabilistic bag-of-words classifier. The second stage incorporates contextual information across a given scene (spatial context) and across several consecutive scenes (temporal context) via a Markov Random Field (MRF). Our approach is driven by data from an onboard camera and 3D laser scanner and uses a combination of visual and geometric features. We demonstrate the virtue of considering such spatial and temporal context during the classification task and analyse the performance of our technique on data gathered over 17 km of track through a city.
11250 en A Bayesian Approach to Occupancy Mapping with Uncertain Inputs This work addresses the problem of occupancy mapping with uncertain measurements taken from none or more mobile robots. Appropriate modeling of sensor and localisation uncertainty is critical nto obtaining consistent and robust maps which may subsequently be used in planning and motion ncontrol. n
11251 en Domain Adaptation for Mobile Robot Navigation An important challenge in outdoor mobile robotic perception is maintaining terrain classiﬁcation nperformance throughout the extremely variable conditions that we may wish a robot to operate nunder. Outdoor robots operate in a series of “environments” that consist of diverse terrain, nvegetation, weather, and lighting conditions. A physical robot does not randomly jump between nenvironments; typically it will operate for long stretches of time in one particular environment, nmaking it advantageous to adapt the robot’s performance to its current environment.
11252 en Learning CRF Models from Drill Rig Sensors for Autonomous Mining This paper investigates an approach that combines ensemble methods with graphical models to analyse multiple sensor measurements in the context of mine automation. Drill sensor measurements used for drilling automation have the potential to provide an estimate of the subsurface geological properties of the rocks nbeing drilled. A Boosting algorithm is used as a local classiﬁer mapping drill measurements to corresponding geological categories. A Conditional Random Field nthen uses this local information in conjunction with neighbouring measurements nto jointly reason about their categories. Model parameters are learned from training data by maximizing the pseudo-likelihood. The probability distribution of nclassiﬁed borehole sections is calculated using belief propagation. We present experimental results of applying the method to classify rock types from sensor data ncollected from a semi-autonomous drill rig at an iron ore mine in Australia.
11253 en Poster Spotlights 
11254 en Multi-Task Learning with Gaussian Processes with Applications to Robot Inverse Dynamics I will discuss multi-task learning, and a number of ways in which transfer between tasks can take place, mainly in a co-kriging (or Gaussian process) framework. I will then go into more detail on multi-task Gaussian process learning of robot inverse dynamics (joint work with Kian Ming Chai, Stefan Klanke, Sethu Vijayakumar).
11255 en Multitask Learning Using Nonparametrically Learned Predictor Subspaces Given several related learning tasks, we propose a nonparametric Bayesian learning model that captures task relatedness by assuming that the task parameters (i.e., nweight vectors) share a latent subspace. More speciﬁcally, the intrinsic dimensionality of this subspace is not assumed to be known a priori. We use an inﬁnite nlatent feature model - the Indian Buffet Process - to automatically infer this number. We also propose extensions of this model where the subspace learning can nincorporate (labeled, and additionally unlabeled if available) examples, or the task nparameters share a mixture of subspaces, instead of sharing a single subspace. The nlatter property can allow learning nonlinear manifold structure underlying the task nparameters, and can also help in preventing negative transfer from outlier tasks.
11256 en Bayesian Localized Multiple Kernel Learning Many problems in machine learning involve datasets that are comprised of multiple views. The nseparate views can be deﬁned over a single input (e.g., multiple image feature types), or from multiple information sources (e.g., audio and video). In this context, each view can provide a redundant nindication of the underlying class or event of interest, useful for classiﬁcation.
11257 en Multi-Way, Multi-View Learning We extend multi-way, multivariate ANOVA-type analysis to cases where one ncovariate is the view, with features of each view coming from different, high- ndimensional domains. The different views are assumed to be connected by having npaired samples; this is common in our main application, biological experiments nintegrating data from different sources. Such experiments typically also include a ncontrolled multi-way experimental setup where disease status, medical treatment ngroups, gender and time of the measurement are usual covariates. We introduce na multi-way latent variable model for this new task, by extending the generative model of Bayesian canonical correlation analysis (CCA) both to take multi-way ncovariate information into account as population priors, and by reducing the dimensionality by an integrated factor analysis that assumes the features to come in correlated groups.
11258 en Information Theoretic Kernel Integration In this paper we consider a novel information-theoretic approach to multiple kernel learning based on minimising a Kullback-Leibler (KL) divergence between nthe output kernel matrix and the input kernel matrix. There are two formula- ntions which we refer to as MKLdiv-dc and MKLdiv-conv. We propose to solve nMKLdiv-dc by a difference of convex (DC) programming method and MKLdiv- nconv by a projected gradient descent algorithm. The effectiveness of the proposed napproaches is evaluated on a benchmark dataset for protein fold recognition and a nyeast protein function prediction problem.
11259 en Discussion and Future Directions 
11260 en Chordal Sparsity in Semidefinite Programming and Machine Learning Chordal graphs play a fundamental role in algorithms for sparse matrix factorization, graphical models, and matrix completion problems. In matrix optimization chordal sparsity patterns can be exploited in fast algorithms for evaluating the logarithmic barrier function of the cone of positive definite matrices with a given sparsity pattern and of the corresponding dual cone. We will give a survey of chordal sparse matrix methods and discuss two applications in more detail: linear optimization with sparse matrix cone constraints, and the approximate solution of dense quadratic programs arising in support vector machine training.
11261 en A Pathwise Algorithm for Covariance Selection Covariance selection seeks to estimate a covariance matrix by maximum likelihoodnwhile restricting the number of nonzero inverse covariance matrix coefficients.nA single penalty parameter usually controls the tradeoff between log likelihoodnand sparsity in the inverse matrix. We describe an efficient algorithm forncomputing a full regularization path of solutions to this problem.
11262 en Active Set Algorithm for Structured Sparsity-Inducing Norm We consider the empirical risk minimization problem for linear supervised learning,nwith regularization by structured sparsity-inducing norms. These are definednas sums of Euclidean norms on certain subsets of variables, extending the usualn?1-norm and the group ?1-norm by allowing the subsets to overlap. This leads tona specific set of allowed nonzero patterns for the solutions of such problems. Wenfirst explore the relationship between the groups defining the norm and the resultingnnonzero patterns. In particular, we show how geometrical information aboutnthe variables can be encoded by our regularization. We finally present an activenset algorithm to efficiently solve the corresponding minimization problem.
11263 en On Recent Trends in Extremely Large-Scale Convex Optimization In the talk, we focus on algorithms for solving well-structured large-scale convex programs in the case where huge problem's sizes prevent processing it by polynomial time algorithms and thus make computationally cheap first order optimization methods the methods of choice. We overview significant recent progress in utilizing problem's structure within the first order framework, with emphasis on algorithms with dimension-independent (and optimal in the large-scale case) iteration complexity being the target accuracy. We then discuss the possibility to further accelerate the first order algorithms by randomization, specifically, by passing from expensive in the extremely large scale case precise deterministic first order oracles to their computationally cheap stochastic counterparts. Applications to be discussed include SVM's, minimization, testing sensing matrices for "goodness" in the Compressed Sensing context, low-dimensional approximation of high-dimensional samples, and some others.
11264 en Tree Based Ensemble Models Regularization by Convex Optimization Tree based ensemble methods can be seen as a way to learn a kernel from a samplenof input-output pairs. This paper proposes a regularization framework to incorporatennon-standard information not used in the kernel learning algorithm, so as tontake advantage of incomplete information about output values and/or of some priorninformation about the problem at hand. To this end a generic convex optimizationnproblem is formulated which is first customized into a manifold regularizationnapproach for semi-supervised learning, then as a way to exploit censored outputnvalues, and finally as a generic way to exploit prior information about the problem.
11265 en On the Convergence of the Convex-Concave Procedure The concave-convex procedure (CCCP) is a majorization-minimization algorithmnthat solves d.c. (difference of convex functions) programs as a sequence of convexnprograms. In machine learning, CCCP is extensively used in many learning algorithmsnlike sparse support vector machines (SVMs), transductive SVMs, sparsenprincipal component analysis, etc. Though widely used in many applications, thenconvergence behavior of CCCP has not gotten a lot of specific attention. In thisnpaper, we provide a rigorous analysis of the convergence of CCCP by addressingnthese questions:n*(i) When does CCCP find a local minimum or a stationary pointnof the d.c. program under consideration?n*(ii) When does the sequence generatednby CCCP converge?nnWe also present an open problem on the issue of localnconvergence of CCCP.
11266 en SINCO - An Efficient Greedy Method for Learning Sparse INverse COvariance Matrix Herein, we propose a simple greedy algorithm (SINCO) for solving this optimization problem.nSINCO solves the primal problem (unlike its predecessors such as COVSEL [10] and glasso [4]),nusing coordinate ascent, in a greedy manner, thus naturally preserving the sparsity of the solution.nAs demonstrated by our empirical results, SINCO has better capability in reducing the false-positivenerror rate (while maintaining similar true positive rate when networks are sufficiently sparse) thannglasso [4], because of its greedy incremental nature.
11267 en Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparse Learning We analyze the convergence behaviour of a recently proposed algorithm for sparsenlearning called Dual Augmented Lagrangian (DAL). We theoretically analyzenunder some conditions that DAL converges super-linearly in a non-asymptoticnand global sense. We experimentally confirm our analysis in a large scale ?1-regularized logistic regression problem and compare the efficiency of DAL algorithmnto existing algorithms.
11296 en The End of the Ubicomp World is Near, My Friend We live in the golden age of ubiquitous computing. Many elements of Weiser’s bold vision are today commonplace in the lives of billions of people. Without even thinking about it, we today routinely search the web, reach out to people with our mobile internet devices, and find places and things with positioning technologies. Together the myriad devices and services that make up the internet form an unprecedented ubicomp platform of simply tremendous possibilities. We have seen many wonderful ubicomp systems, and research shows us that more are heading our way. The bad news is, greater failures are heading our way, too.nnEach new generation of systems brings added functionality, which inevitably means added complexity somewhere in the systems. This, in turn, creates numerous new failure modes with each generation. Moreover, increasing connectivity brings novel ways to propagate the failures into other systems. We see around us major shortcomings in terms of usability, interoperability, and security, and worse can be expected, should your cup be half empty.nnThis talk will explore some of the factors that shape the ubicomp field – breakneck speeds of innovation, unstoppable technology development, maturing of services and information economies, among others – and discuss why such developments may have undesirable consequences. On a positive note, this talk will also identify other promising factors that may ultimately render our cup more than half full.
11297 en The Art of Mobility Screen cultures today are dominated by narrative and its modes of framing. The advent of “Pervasive” or “Ubiquitous” media such as mobile smartphones with GPS sensing means that new dispersed forms of narrative interaction are now possible for the public. The convergence of mobile technologies and ubiquitous computing are creating a world where information-rich environments may be mapped directly onto urban topologies. Dispersed forms of interaction raise intriguing new questions about the nature of narrative and communication, particularly in relation to modes of audience’s participation and reception.nnThis new and experimental work, so far undertaken in the arena of interactive public art or spatialised interaction through mobile technologies, is in pressing need of exploration, definition, and documentation. Emergent technologies of interaction and the changing nature of public interactive engagement present a radical challenge to Western narrative and its vehicles and traditions. Boundaries between established forms (i.e., games and cinema) are thrown into question and the very concept of creative authorship becomes problematic. Whilst other emerging technologies are already redefining existing forms of screen?based exhibition and reception (interactive television and digital cinema), they still tie down the audience in relation to the screen. Locative technology blurs the borders between physical and virtual space, leading to the redefinition of the concept of the virtual from that of simulation to that of augmentation.nnThis poses a series of questions around changing concepts of space and place for a wide range of traditional disciplines, ranging from Anthropology, Art and Architecture, Computer Studies, Cultural and Media Studies, Fashion to Graphic design. The talk will be illustrated by examples from Rieser's recent practice, including The Third Woman interactive mobile film.
11299 en Introduction to Programming Applications for Mobile Devices Mobile phones are increasingly capable devices. These devices are also increasingly friendly towards developers and it is now straightforward to write your own programs. The objective of the Tutorial is to provide hands on experience with creating applications for mobile devices. This will be achieved through an example of a location aware application for three mobile platforms: Google Android, Windows Mobile, and Apple iPhone platform. Participants will be guided through the development on the Android platform with detailed descriptions on how to achieve similar objectives using Windows Mobile and Apple iPhone.nnThe format will be designed around attendees actively working through the example application on their own computers.nnRequirements:nBasic knowledge of the Java programming language will be of benefit but we will aim to make the content accessible to everyone. For active participation, please bring a laptop and install a copy of the Android SDK. Here is the list of system requirements .nnThe instructors will provide assistence with installing the Android SDK 12:30 to 13:00, prior to the tutorial session.
11300 en Security and Privacy: Is It only a Matter of Time before a Massive Loss of Personal Data or Identity theft Happens on a Smart Mobile Platform? We have a growing number of smart platforms that are becoming established, each with its own market place for applications (Blackberry RIM, Nokia Ovi, Apple iPhone, Google g2, Vodafone 360) but we don’t have a security architecture that actually makes sense in terms of protecting end users against all the attacks that are common place on the Internet today. Securing the potentially massive amount of interactions using mobile devices is difficult because, typically, there will be no a priori shared information such as passwords, addresses, or PIN codes between the phone, its user, and the service they want to use.nnAdditionally, mobile devices often lack powerful user interfaces to support classical authentication methods. Personal content is indeed private but with emerging mobile payment and ticketing solutions, and the socialising of contact information, personal information is becoming even more highly sensitive. It’s only a matter of time before a massive loss of personal data, or identity theft happens on one of these platforms (or more than one) and the economic and technical fallout will be quite serious.
11301 en One Minute Madness 1. //Using OWL To Provide Content Mobility in Augmented Reality// - **Babar Chaudary**\\nn2. //Construction of User Scenarios with Machinima Technique// - **[[http://videolectures.net/arto_puikkonen/|Arto Puikkonen]]** (Presenter)\\nn3. //Semantic Information Interoperability in Smart Spaces// - **[[http://videolectures.net/antti_evesti/|Antti Evesti]]** (Presenter)\\nn4. //Customizable Real-time Delivery of Flash Video to iPhones// - **[[http://videolectures.net/francis_t_marchese/|Francis T. Marchese]]** (Presenter)\\nn5. //1000 Cell Phones// - **[[http://videolectures.net/david_carroll/|David Carroll]]** (Presenter)\\nn6. //weConnect - Supporting Close Relationships through Mobile Broadcasting// - **[[http://videolectures.net/jamie_costello/|Jamie Costello]]** (Presenter)\\nn7. //mGuide - Rich Information Capture and Sharing in Mobile Contexts// - **[[http://videolectures.net/jamie_costello/|Jamie Costello]]** (Presenter)\\nn8. //SpARC - Supplementary Assistance for Rowing Coaching// - **[[http://videolectures.net/simon_fothergill/|Simon Fothergill]]** (Presenter)\\nn9. //Augmented Reality Explorer// - **[[http://videolectures.net/arto_puikkonen/|Arto Puikkonen]]** (Presenter)\\nn10. //Mind-Controlled Educational Computer Games// - **[[http://videolectures.net/ian_glasscock/|Ian Glasscock]]** (Presenter)\\
11302 en Mind-Controlled Educational Computer Games 
11303 en Augmented Reality Explorer 
11304 en SpARC - Supplementary Assistance for Rowing Coaching This system gathers data and provides real-time and post-session feedback to athletes on aspects of their technique by measuring and analysing the kinetics of their performance. The motion of the ergometer handle, both relative to the erg and the seat are used along with the forces applied through the handle and each foot. Ideal performances can be programmed in and a score of similarity between this and a current performance is calculated every stroke to motivate consistency in the athlete's technique.nnThe data collected is also used to evaluate analysis algorithms to provide more sophisticated feedback to athletes.
11305 en weConnect - Supporting Close Relationships through Mobile Broadcasting Building on the Internet (IP) platforms available on mobile devices and personal computers (PCs), we implemented a service called weConnect that facilitates broadcast of content via dedicated and personal media channels. weConnect includes simple tools for integrating content into media 'mixes' and provides 'content viewers' for ubiquitous access to weConnect channels via mobile, desktop, and other IP enabled devices. In this paper we present an exploratory user study based on deployment of the weConnect service among individuals in close relationships. The study focuses on the user perception and experience with the 'always-on' channels, delivering personalized content. We started with images, text, and animations as familiar and easily accessible media. We observed a high level of reciprocity in creating and exchanging expressive content and a need for persistence, reuse, and notification of content delivery. The users voiced their enthusiasm for receiving personalized media across mobile and desktop devices. The always-on nature of the weConnect channels raised new requirements for the service design to assist content producers with creating streams of personalized media efficiently and to enable recipients to view the content flexibly.
11306 en mGuide - Rich Information Capture and Sharing in Mobile Contexts mGuide is a Web service and mobile application that incorporates location-based awareness into the user communication.nnIt assists users in meeting at a particular location by enabling mobile users to exchange images and messages and to see each other’s progress on a map.nJourneys The user can record a journey through images, voice messages, and location displayed on maps.nnThese journeys can be shared with others in real time or in retrospect, both through PCs and on the mobile devices.ndClone dClone enables a user to maintain contact with another person through subtle, unobtrusive communications.nThe sender can compose a display of personal information from images, voice or text messages, and general online information such as local time or weather information.nThe recipient can choose to have the information displayed on a home screen at all times to be continuously connected and aware of the sender’s experience.
11307 en Semantic Information Interoperability in Smart Spaces Link on YouTube.com [[http://www.youtube.com/watch?v=EU9alk9t7dA|Semantic Information Interoperability in Smart Spaces]]
11308 en Customizable Real-time Delivery of Flash Video to iPhones 
11309 en 1000 Cell Phones Emerging out of an institutional collaboration between Parsons The New School for Design (New YorknCity) and the Academy of Arts and Design at Tsinghua University (Beijing), the mobile medianinstallation, “1000 Cell Phones,” exposes the invisible conversations that constantly occur betweennthe networked devices we carry throughout our nomadic urban daily life. The installation consists ofnmultiple displays that playfully visualize and animate discovered Bluetooth devices within its situatednspace. Devices are represented as abstract discs in dimensional screen space, colored byntranscribing the devices’ unique identifiers to distinctive tone and hue values. This simple butnevocative effect emphasizes how an ID number expressed as a one-of-a-kind color not only makesnvisible a distinguishing feature of our portable networked device, but also reshapes its obfuscatedntechnical datum into an aesthetic and coherent design object. It asks us if this machine identifiernexpresses our persona and personality, perhaps without our knowledge and complete understandingnof the implications. In addition, discovered device names animate across the screens, emphasizingnthe transient nature of the tracking devices we carry, unwittingly broadcasting a unique identifier fornanyone or anything willing to listen. By installing the work in a social space, such as a café or lobby,n“1000 Cell Phones” captures the unseen dialogue between mobile phones and laptops broadcastingntheir Bluetooth identities while owners lurk and socialize. When participants realize how their devicennames render on the displays, they often engage in the intervention by altering their device settings tonaffect the textual content on the visualization. In these moments, the conversations between theninvisible and visible, technical and aesthetic, surveillance and dissemination, machines and people allnbecome intertwined in a simple but enjoyable expression.
11310 en Interview with Pertti Huuskonen 
11311 en Interview with Martin Rieser 
11312 en Interview with Alan Blackwell 
11318 en Lecture 37 - Potential Energy Surfaces, Transition State Theory and Reaction Mechanism Overview:nnAfter discussing the statistical basis of the law of mass action, the lecture turns to developing a framework for understanding reaction rates. A potential energy surface that associates energy with polyatomic geometry can be realized physically for a linear, triatomic system, but it is more practical to use collective energies for starting material, transition state, and product, together with Eyring theory, to predict rates. Free-radical chain halogenation provides examples of predicting reaction equilibria and rates from bond dissociation energies. The lecture concludes with a summary of the semester's topics from the perspective of physical-organic chemistry.nnProblem sets/Reading assignment:nnReading assignments, problem sets, PowerPoint presentations, and other resources for this lecture can be accessed from Professor McBride's on-campus course website, which was developed for his Fall 2008 students. Please see Resources section below.nnResources:nn[[http://webspace.yale.edu/chem125_oyc/#L37|Professor McBride's web resources for CHEM 125 (Fall 2008)]]
11335 en Topological Data Analysis Computational topology is a relatively new field between mathematics and computer science which, on one hand, uses concepts and methods from topology to formalize and solve problems in computer science and, on the other hand, designs algorithms for computing complex invariants of algebraic topology. In this talk we will present several topological approaches to data and image analysis.
11339 en Parallel Exact Inference on Multi-Core Processors Exact inference in Bayesian networks is a fundamental AI technique that has numerous applications including medical diagnosis, consumer help desk, pattern recognition, credit assessment, data mining, genetics, and others. Inference is NP-hard and in many applications real time performance is required. In this talk we show task and data parallel techniques to achieve scalable performance on general purpose multi-core and heterogeneous multi-core architectures. We develop collaborative schedulers to dynamically map the junction tree tasks leading to highly optimized implementations. We design lock-free structures to reduce thread coordination overheads in scheduling, while balancing the load across the threads. For the Cell BE, we develop a light weight centralized scheduler that coordinates the activities of the synergistic processing elements (SPEs). Our scheduler is further optimized to run on throughput oriented architectures such as SUN Niagara processors. We demonstrate scalable and efficient implementations using Pthreads for a wide class of Bayesian networks with various topologies, clique widths, and number of states of random variables. Our implementations show improved performance compared with OpenMP and complier based optimizations.
11340 en Parallel Online Learning A fundamental limit on the speed of training and prediction is imposed by bandwidth: there is a finite amount of data that a computer can access in a fixed amount of time. Somewhat surprisingly, we can build an online learning algorithm fully capable of hitting this limit. I will discuss approaches for breaking the bandwidth limit, including empirical results.
11341 en Probabilistic Machine Learning in Computational Advertising In the past years online advertising has grown at least an order of magnitude faster than advertising on all other media. This talk focuses on advertising on search engines, where accurate predictions of the probability that a user clicks on an advertisement crucially benefit all three parties involved: the user, the advertiser, and the search engine. We present a Bayesian probabilistic classification model that has the ability to learn from terabytes of web usage data. The model explicitly represents uncertainty allowing for fully probabilistic predictions: 2 positives out of 10 instances or 200 out of 1000 both give an average of 20%, but in the first case the uncertainty about the prediction should be larger. We also present a scheme for approximate parallel inference that allows efficient training of the algorithm on a distributed data architecture.
11342 en Parallel Topic Models 
11343 en Scalable Learning in Computer Vision Computer vision is a challenging application area of machine learning. Recent work has shown that large training sets may yield higher performance in vision tasks like object detection. We overview our work in object detection using a scalable, distributed training system capable of training on more than 100 million examples in just a few hours. We also briefly describe recent work with deep learning algorithms that may allow us to apply these architectures to large datasets as well.
11344 en Hadoop-ML: An Infrastructure for the Rapid Implementation of Parallel Reusable Analytics Hadoop is an open-source implementation of Google's Map-Reduce programming model. Over the past few years, it has evolved into a popular platform for parallelization in industry and academia. Furthermore, trends suggest that Hadoop will likely be the analytics platform of choice on forthcoming Cloud-based systems. Unfortunately, implementing parallel machine learning/data mining (ML/DM) algorithms on Hadoop is complex and time consuming. To address this challenge, we present Hadoop-ML, an infrastructure to facilitate the implementation of parallel ML/DM algorithms on Hadoop. Hadoop-ML has been designed to allow for the specification of both task-parallel and data-parallel ML/DM algorithms. Furthermore, it supports the composition of parallel ML/DM algorithms using both serial as well as parallel building blocks -- this allows one to write reusable parallel code. The proposed abstraction eases the implementation process by requiring the user to only specify computations and their dependencies, without worrying about scheduling, data management, and communication. As a consequence, the codes are portable in that the user never needs to write Hadoop-specific code. This potentially allows one to leverage future parallelization platforms without rewriting one's code.
11345 en Recreational Activities and Discussion An informal tutorial on the Vowpal Wabbit algorithm.
11346 en Large-Scale Machine Learning: The Problems, Algorithms, and Challenges To seed discussion, I will attempt to organize research efforts in large-scale machine learning by looking at common computational problems across all of machine learning, and the challenges of creating efficient parallel algorithms for them. I'll begin by identifying four common types of computational bottlenecks that occur across all of machine learning, or prototype algorithmic problems: N-body problems, graph operations, linear algebra, and optimization. Within each category, I'll discuss what we can or cannot learn from the existing body of work in scientific computing, highlight a few of the most successful and recent specific serial algorithms that have been developed for concreteness, and discuss what makes them easy or hard to parallelize. I'll synthesize some of these observations to obtain a list of desiderata for parallel machine learning algorithms research and software toolkits.
11347 en 1 Billion Instances, 1 Thousand Machines and 3.5 Hours Training conditional maximum entropy models on massive data sets requires significant computational resources, but by distributing the computation, training time can be significant reduced. Recent theoretical results have demonstrated conditional maximum entropy models trained by weight mixtures of independently trained models converge at the same rate as traditional distributed schemes, but significantly faster. This efficiency is achieved primarily by reducing network communication costs, a cost not usually considered but actually quite crucial.
11348 en FPGA-based MapReduce Framework for Machine Learning Machine learning algorithms are becoming increasingly important in our daily life. However, training on very large scale datasets is usually very slow. FPGA is a reconfigurable platform that can achieve high parallelism and data throughput. Many works have been done on accelerating machine learning algorithms on FPGA. In this paper, we adapt Google's MapReduce model to FPGA by realizing an on-chip MapReduce framework for machine learning algorithms. A processor scheduler is implemented for the maximum computation resource utilization and load balancing. In accordance with the characteristics of many machine learning algorithms, a common data access scheme is carefully designed to maximize data throughput for large scale dataset. This framework hides the task control, synchronization and communication away from designers to shorten development cycles. In a case study of RankBoost acceleration, up to 31.8x speedup is achieved versus CPU-based design, which is comparable with a fully manually designed version. We also discuss the implementations of two other machine learning algorithms, SVM and PageRank, to demonstrate the capability of the framework.
11349 en Large-Scale Graph-based Transductive Inference We consider the issue of scalability of graph-based semi-supervised learning (SSL) algorithms. In this context, we propose a fast graph node ordering algorithm that improves parallel spatial locality by being cache cognizant. This approach allows for a linear speedup on a shared-memory parallel machine to be achievable, and thus means that graph-based SSL can scale to very large data sets. We use the above algorithm an a multi-threaded implementation to solve a SSL problem on a 120 million node graph in a reasonable amount of time.
11350 en Splash Belief Propagation: Efficient Parallelization Through Asynchronous Scheduling In this work we focus on approximate parallel inference in loopy graphical models using loopy belief propagation. We demonstrate that the natural, fully synchronous parallelization of belief propagation is highly inefficient. By bounding the achievable parallel performance of loopy belief propagation on chain graphical models we develop a theoretical understanding of the parallel limitations of belief propagation. We then introduce Splash belief propagation, a parallel asynchronous approach which achieves the optimal bounds and demonstrates linear to super-linear scaling on large graphical models. Finally we discuss how these ideas may be generalized to parallel iterative graph algorithms in the context of our new GraphLab framework.
11362 en Lecture 1: Goals of the course; what is computation; introduction to data types, operators, and variables 
11363 en Lecture 2: Operators and operands; statements; branching, conditionals, and iteration 
11364 en Lecture 3: Common code patterns: iterative programs 
11365 en Lecture 4: Decomposition and abstraction through functions; introduction to recursion 
11366 en Lecture 5: Floating point numbers, successive refinement, finding roots 
11367 en Lecture 6: Bisection methods, Newton/Raphson, introduction to lists 
11368 en Lecture 7: Lists and mutability, dictionaries, pseudocode, introduction to efficiency 
11369 en Lecture 8: Complexity; log, linear, quadratic, exponential algorithms 
11370 en Lecture 9: Binary search, bubble and selection sorts 
11371 en Lecture 10: Divide and conquer methods, merge sort, exceptions 
11372 en Lecture 11: Testing and debugging 
11373 en Lecture 12: More about debugging, knapsack problem, introduction to dynamic programming 
11374 en Lecture 13: Dynamic programming: overlapping subproblems, optimal substructure 
11375 en Lecture 14: Analysis of knapsack problem, introduction to object-oriented programming 
11376 en Lecture 15: Abstract data types, classes and methods 
11377 en Lecture 16: Encapsulation, inheritance, shadowing 
11378 en Lecture 17: Computational models: random walk simulation 
11379 en Lecture 18: Presenting simulation results, Pylab, plotting 
11380 en Lecture 19: Biased random walks, distributions 
11381 en Lecture 20: Monte Carlo simulations, estimating 
11382 en Lecture 21: Validating simulation results, curve fitting, linear regression 
11383 en Lecture 22: Normal, uniform, and exponential distributions; misuse of statistics 
11384 en Lecture 23: Stock market simulation 
11385 en Lecture 24: Course overview; what do computer scientists do? 
11469 en Lecture 1: Prokofiev's Visions Fugitives, student presentations and composition workshop 
11470 en Lecture 1 : Running Clinic with Danny Abshire 
11552 en Biomine search engine for probabilistic graphs Biomine is a search engine prototype under development. It can be usednto find biological entities that are (indirectly) related to given querynentities, as well as to display and evaluate the relations. Biomine isnbased on an integrated index to a number of public biological databases.nThe representation is a probabilistic graph, where nodes correspond tonbiological entities (typically a record in a biological database) andnedges to their relationships (typically a cross-reference betweenndatabase records). Edges are annotated with probabilities that reflectnthe strength or the reliability of the relation. I will discuss researchnproblems and challenges for search in such graphs.n
11565 en Ensembles for predicting structured outputs In many real-world domains, such as bioinformatics (functionalngenomics), text classification and image annotation, the goal is tonpredict a complex output. For example, in functional genomics, thengoal is to predict the function of a gene, while the set of functionsncan be organized as tree (FunCat) or graph (GO ontology).nIn this talk, we present an approach for predicting structured outputsnusing ensembles of trees. The proposed approach is scalable to largendatasets, different types of outputs and it is applicable to widenrange of domains. First, we describe the types of structured outputsnthat we typically encounter, and then we explain the base classifiersn- predictive clustering trees (PCTs). Next, we discuss the ensemblenmethods that we extended (bagging and random forests) to deal withnstructured outputs and accordingly adapted the voting schemes.nAfterwards, we present experimental evaluation of the proposednapproach on wide range of real-world domains. At the end, we presentnan application of the proposed approach in functional genomics andnshow that our approach is competitive with state-of-the-art approaches.
11566 en Italy in Space Italy has been making important investments in space since 1960s. The first European country, the third in the world, to launch scientific satellites, Italy was one of the founding countries of the European Space Agency (1975) and now it is the third contributor to ESA (European Space Agency). Established in 1988, ASI is a Governmental Agency for promotion, development and diffusion of scientific and technological research in the fields of space and aerospace, as scientific observation and exploration of the universe, earth observation, telecommunications and navigation, microgravity and education. Important is the contribution to the International space station, where a relevant part of the habitable volume of the orbiting platform has been mainly built in Italy.
11570 en Polarization: Light Waves, Rainbows, and Cheap Sunglasses In this lecture taped before a live audience of elementary and middle school students and their families, MIT Physics Professor Walter Lewin explains polarization, and demonstrates properties of light in rainbows, smoke and the sky. He answers the perennial question, "why is the sky blue?" and creates a red sunset in the laboratory.nn;**Link to** - **[[http://mitworld.mit.edu/video/74|Lecture´s Homepage]]**n;**Host** - **[[http://web.mit.edu/museum/|MIT Museum]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/23|Family Adventures in Science and Technology Program]]**
11571 en The Sounds of Music Have you ever wondered about the annoying hum your car makes at a certain speed on a particular stretch of highway? Or why a flute’s notes are higher than a trombone’s? Walter Lewin uses rubber hose, wooden boxes with holes, metal plates and an assortment of other home-made instruments to demonstrate how objects produce sound. It all boils down to how something vibrates -- pushing air out in all directions.nnLewin illustrates the shape of sounds, taking a rope tethered at one end, shaking it up and down at different speeds and producing specific wave shapes. These shapes are the rope’s resonant frequencies, or harmonics. It’s the same for a bowed violin, where the oscillations of the strings generate a set of harmonics, producing the notes we hear -- the faster the oscillations, the higher the tones. Lewin invites children from the audience to produce sounds with their musical instruments, and shows the amplitude and frequency of the tones. Later he demonstrates destructive resonances: video of a bridge that twists so violently that it collapses, and then, live in the laboratory, the shattering of a wine glass with progressively louder and higher tones. In this event where physics meets performance art, Lewin provides surprises throughout.nn;**Link to** - **[[http://mitworld.mit.edu/video/168|Lecture´s Homepage]]**n;**Host** - **[[http://web.mit.edu/museum/|MIT Museum]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/23|Family Adventures in Science and Technology Program]]**
11572 en The Wonders of Electricity and Magnetism The inimitable Walter Lewin gives a literally hair-raising performance in this MIT Museum lecture/demonstration for learners young and old. He unveils the real meaning behind words and things most of us use everyday without reflecting on what marvels they really represent.nnHere are some of the mysteries exhibited, explored, and explained in this talk: How can you make two perfectly normal balloons zoom apart from each other? What happens when you connect a 12-volt light bulb to a 110-volt outlet? If you toss a handful of confetti onto a comb, why does some of it stick and some of it fly away? What’s the best way to make sure your flashlight will work the next time you really need it? (If you guessed putting in new batteries, go to the back of the class.)nnLewin is at his electrifying best when working with children from the audience. He gives a 12-year-old girl the worst hair day of her life, and offers a young boy 10 cents for 10 hours of backbreaking labor. But Lewin reaches a new high (low?) when he repeatedly beats one of his young assistants with a swatch of cat fur. Lewin doesn’t exempt himself from the torture, though: he even makes a serious attempt to electrocute himself with a 150,000-volt Van der Graaf generator.nnLewin indulges the armchair physicist who’s mathematically challenged, by covering all the basics of electricity and magnetism while introducing just one equation. If you’re still undecided, check out some of the unique special effects – sparks, flashes, smashes, and more –pinpointed in the Video Index. Keep watching, and you will find out why Walter Lewin was recently honored with MIT’s Everett Moore Baker Memorial Award for Excellence in Undergraduate Teaching.nnWith the addition of this video to MIT World, Lewin has a total of 100 lectures available on line--94 at OpenCourseWare and 6 at MIT World.nn;**Link to** - **[[http://mitworld.mit.edu/video/319|Lecture´s Homepage]]**n;**Host** - **[[http://web.mit.edu/museum/|MIT Museum]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/23|Family Adventures in Science and Technology Program]]**
11647 en Bridging the Structured Un-Structured Gap 
11654 en Online Advertising: Using Theory and Data to Optimize Marketplaces 
11656 en Leveraging Temporal Dynamics of Document Content in Relevance Ranking Many web documents are dynamic, with content changing in varying amounts at varying frequencies. However, current document search algorithms have a static view of the document content, with only a single version of the document in the index at any point in time. In this paper, we present the first published analysis of using the temporal dynamics of document content to improve relevance ranking. We show that there is a strong relationship between the amount and frequency of content change and relevance. We develop a novel probabilistic document ranking algorithm that allows differential weighting of terms based on their temporal characteristics. By leveraging such content dynamics we show significant performance improvements for navigational queries.
11657 en Towards Recency Ranking in Web Search In web search, recency ranking refers to ranking documents by relevance which takes freshness into account. In this paper, we propose a retrieval system which automatically detects and responds to recency sensitive queries. The system detects recency sensitive queries using a high precision classifier. The system responds to recency sensitive queries by using a machine learned ranking model trained for such queries. We use multiple recency features to provide temporal evidence which effectively represents document recency. Furthermore, we propose several training methodologies important for training recency sensitive rankers. Finally, we develop new evaluation metrics for recency sensitive queries. Our experiments demonstrate the efficacy of the proposed approaches.
11658 en Ranking Mechanisms in Twitter-Like Forums We study the problem of designing a mechanism to rank items in forums by making use of the user reviews such as thumb and star ratings. We compare mechanisms where forum users rate individual posts and also mechanisms where the user is asked to perform a pairwise comparison and state which one is better. The main metric used to evaluate a mechanism is the ranking accuracy vs the cost of reviews, where the cost is measured as the average number of reviews used per post. We show that for many reasonable probability models, there is no thumb (or star) based ranking mechanism that can produce approximately accurate rankings with bounded number of reviews per item. On the other hand we provide a review mechanism based on pairwise comparisons which achieves approximate rankings with bounded cost. We have implemented a system, shout velocity [5], which is a twitter-like forum but items (i.e., tweets in Twitter) are rated by using comparisons. For each new item the user who posts the item is required to compare two previous entries. This ensures that over a sequence of n posts, we get at least n comparisons requiring one review per item on average. Our mechanism uses this sequence of comparisons to obtain a ranking estimate. It ensures that every item is reviewed at least once and winning entries are reviewed more often to obtain better estimates of top items.
11659 en Learning Concept Importance Using a Weighted Dependence Model Modeling query concepts through term dependencies has been shown to have a significant positive effect on retrieval performance, especially for tasks such as web search, where relevance at high ranks is particularly critical. Most previous work, however, treats all concepts as equally important, an assumption that often does not hold, especially for longer, more complex queries. In this paper, we show that one of the most effective existing term dependence models can be naturally extended by assigning weights to concepts. We demonstrate that the weighted dependence model can be trained using existing learning-to-rank techniques, even with a relatively small number of training queries. Our study compares the effectiveness of both endogenous (collection- based) and exogenous (based on external sources) features for determining concept importance. To test the weighted dependence model, we perform experiments on both publicly available TREC corpora and a proprietary web corpus. Our experimental results indicate that our model consistently and significantly outperforms both the standard bag-of-words model and the unweighted term dependence model, and that combining endogenous and exogenous features generally results in the best retrieval effectiveness.
11660 en Query Reformulation Using Anchor Text Query reformulation techniques based on query logs have been studied as a method of capturing user intent and improving retrieval effectiveness. The evaluation of these techniques has primarily, however, focused on proprietary query logs and selected samples of queries. In this paper, we suggest that anchor text, which is readily available, can be an effective substitute for a query log and study the effectiveness of a range of query reformulation techniques (including log-based stemming, substitution, and expansion) using standard TREC collections. Our results show that log- based query reformulation techniques are indeed effective with standard collections, but expansion is a much safer form of query modification than word substitution. We also show that using anchor text as a simulated query log is as least as effective as a real log for these techniques.
11664 en Tagging Human Knowledge A fundamental premise of tagging systems is that regular users can organize large collections for browsing and other tasks using uncontrolled vocabularies. Until now, that premise has remained relatively unexamined. Using library data, we test the tagging approach to organizing a collection. We find that tagging systems have three major large scale organizational features: consistency, quality, and completeness. In addition to testing these features, we present results suggesting that users produce tags similar to the topics designed by experts, that paid tagging can effectively supplement tags in a tagging system, and that information integration may be possible across tagging systems.
11665 en Precomputing Search Features for Fast and Accurate Query Classification Query intent classification is crucial for web search and advertising. It is known to be challenging because web queries contain less than three words on average, and so provide little signal to base classification decisions on. At the same time, the vocabulary used in search queries is vast: thus, classifiers based on word-occurrence have to deal with a very sparse feature space, and often require large amounts of training data. Prior efforts to address the issue of feature sparseness augmented the feature space using features computed from the results obtained by issuing the query to be classified against a web search engine. However, these approaches induce high latency, making them unacceptable in practice.nIn this paper, we propose a new class of features that realizes the benefit of search-based features without high latency. These leverage co-occurrence between the query keywords and tags applied to documents in search results, resulting in a significant boost to web query classification accuracy. By precomputing the tag incidence for a suitably chosen set of keyword-combinations, we are able to generate the features online with low latency and memory requirements. We evaluate the accuracy of our approach using a large corpus of real web queries in the context of commercial search.
11666 en I Tag, You Tag: Translating Tags for Advanced User Models Collaborative tagging services (folksonomies) have been among the stars of the Web 2.0 era. They allow their users to label diverse resources with freely chosen keywords (tags). Our studies of two real-world folksonomies unveil that individual users develop highly personalized vocabularies of tags. While these meet individual needs and preferences, the considerable differences between personal tag vocabularies (personomies) impede services such as social search or customized tag recommendation. In this paper, we introduce a novel user-centric tag model that allows us to derive mappings between personal tag vocabularies and the corresponding folksonomies. Using these mappings, we can infer the meaning of user-assigned tags and can predict choices of tags a user may want to assign to new items. Furthermore, our translational approach helps in reducing common problems related to tag ambiguity, synonymous tags, or multilingualism. We evaluate the applicability of our method in tag recommendation and tag-based social search. Extensive experiments show that our translational model improves the prediction accuracy in both scenarios.
11667 en Pairwise Interaction Tensor Factorization for Personalized Tag Recommendation Tagging plays an important role in many recent websites. Recommender systems can help to suggest a user the tags he might want to use for tagging a specific item. Factorization models based on the Tucker Decomposition (TD) model have been shown to provide high quality tag recommendations outperforming other approaches like PageRank, FolkRank, collaborative filtering, etc. The problem with TD models is the cubic core tensor resulting in a cubic runtime in the factorization dimension for prediction and learning.nnIn this paper, we present the factorization model PITF (Pairwise Interaction Tensor Factorization) which is a special case of the TD model with linear runtime both for learning and prediction. PITF explicitly models the pairwise interactions between users, items and tags. The model is learned with an adaption of the Bayesian personalized ranking (BPR) criterion which originally has been introduced for item recommendation. Empirically, we show on real world datasets that this model outperforms TD largely in run- time and even can achieve better prediction quality. Besides our lab experiments, PITF has also won the ECML/PKDD Discovery Challenge 2009 for graph-based tag recommendation.
11668 en fLDA: Matrix Factorization through Latent Dirichlet Allocation We propose fLDA, a novel matrix factorization method to predict ratings in recommender system applications where a “bag-of-words” representation for item meta-data is natu- ral. Such scenarios are commonplace in web applications like content recommendation, ad targeting and web search where items are articles, ads and web pages respectively. Because of data sparseness, regularization is key to good predictive accuracy. Our method works by regularizing both user and item factors simultaneously through user features and the bag of words associated with each item. Specifically, each word in an item is associated with a discrete latent factor often referred to as the topic of the word; item topics are obtained by averaging topics across all words in an item. Then, user rating on an item is modeled as user’s affinity to the item’s topics where user affinity to topics (user factors) and topic assignments to words in items (item factors) are learned jointly in a supervised fashion. To avoid overfitting, user and item factors are regularized through Gaussian linear regression and Latent Dirichlet Allocation (LDA) priors respectively.nnWe show our model is accurate, interpretable and handles both cold-start and warm-start scenarios seamlessly through a single model. The efficacy of our method is illustrated on benchmark datasets and a new dataset from Yahoo! Buzz where fLDA provides superior predictive accuracy in cold-start scenarios and is comparable to state-of- the-art methods in warm-start scenarios. As a by-product, fLDA also identifies interesting topics that explains user- item interactions. Our method also generalizes a recently proposed technique called supervised LDA (sLDA) to col- laborative filtering applications. While sLDA estimates item topic vectors in a supervised fashion for a single regression, fLDA incorporates multiple regressions (one for each user) in estimating the item factors.
11697 en Coupled Semi-Supervised Learning for Information Extraction We consider the problem of semi-supervised learning to extract categories (e.g., academic fields, athletes) and relations (e.g., PlaysSport (athlete, sport)) from web pages, starting with a handful of labeled training examples of each category or relation, plus hundreds of millions of unlabeled web documents. Semi-supervised training using only a few labeled examples is typically unreliable because the learning task is underconstrained. This paper pursues the thesis that much greater accuracy can be achieved by further constraining the learning task, by coupling the semi-supervised training of many extractors for different categories and relations. We characterize several ways in which the training of category and relation extractors can be coupled, and present experimental results demonstrating significantly improved accuracy as a result.
11698 en Data-oriented Content Query System: Searching for Data into Text on the Web As the Web provides rich data embedded in the immense contents inside pages, we witness many ad-hoc efforts for exploiting fine granularity information across Web text, such as Web information extraction, typed-entity search, and question answering. To unify and generalize these efforts, this paper proposes a general search system - Data-oriented Content Query System (DoCQS) - to search directly into document contents for finding relevant values of desired data types. Motivated by the current limitations, we start by distilling the essential capabilities needed by such content querying. The capabilities call for a conceptually relational model, upon which we design a powerful Content Query Language (CQL). For efficient processing, we design novel index structures and query processing algorithms. We evaluate our proposal over two concrete domains of realistic Web corpora, demonstrating that our query language is rather flexible and expressive, and our query processing is efficient with reasonable index overhead.
11699 en Corroborating Information from Disagreeing Views We consider a set of views stating possibly conflicting facts. Negative facts in the views may come, e.g., from functional dependencies in the underlying database schema. We want to predict the truth values of the facts. Beyond simple methods such as voting (typically rather accurate), we explore techniques based on “corroboration”, i.e., taking into account trust in the views. We introduce three fix-point algorithms corresponding to different levels of complexity of an underlying probabilistic model. They all estimate both truth values of facts and trust in the views. We present experimental studies on synthetic and real-world data. This analysis illustrates how and in which context these methods improve corroboration results over baseline methods. We believe that corroboration can serve in a wide range of applications such as source selection in the semantic Web, data quality assessment or semantic annotation cleaning in social networks. This work sets the bases for a wide range of techniques for solving these more complex problems.
11720 en GeoFolk: Latent Spatial Semantics in Web 2.0 Social Media We describe an approach for multi-modal characterization of social media by combining text features (e.g. tags as a prominent example of short, unstructured text labels) with spatial knowledge (e.g. geotags and coordinates of images and videos). Our model-based framework GeoFolk combines these two aspects in order to construct better algorithms for content management, retrieval, and sharing. The approach is based on multi-modal Bayesian models which allow us to integrate spatial semantics of social media in a well-formed, probabilistic manner. We systematically evaluate the solution on a subset of Flickr data, in characteristic scenarios of tag recommendation, content classification, and clustering. Experimental results show that our method outperforms baseline techniques that are based on one of the aspects alone. The approach described in this contribution can also be used in other domains such as Geoweb retrieval.
11721 en Learning Influence Probabilities in Social Networks Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance.
11722 en You Are Who You Know: Inferring User Profiles in Online Social Networks Online social networks are now a popular way for users to connect, express themselves, and share content. Users in today’s online social networks often post a profile, consisting of attributes like geographic location, interests, and schools attended. Such profile information is used on the sites as a basis for grouping users, for sharing content, and for suggesting users who may benefit from interaction. However, in practice, not all users provide these attributes.nnIn this paper, we ask the question: given attributes for some fraction of the users in an online social network, can we infer the attributes of the remaining users? In other words, can the attributes of users, in combination with the social network graph, be used to predict the attributes of another user in the network? To answer this question, we gather fine-grained data from two social networks and try to infer user profile attributes. We find that users with common attributes are more likely to be friends and often form dense communities, and we propose a method of inferring user attributes that is inspired by previous approaches to detecting communities in social networks. Our results show that certain user attributes can be inferred with high accuracy when given information on as little as 20% of the users.
11723 en TwitterRank: Finding Topic-sensitive Influential Twitterers This paper focuses on the problem of identifying influential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called “following”, in which each user can choose who she wants to “follow” to receive tweets from without requiring the latter to give permission first. In a dataset prepared for this study, it is observed that (1) 72.4% of the users in Twitter follow more than 80% of their followers, and (2) 80.5% of the users have 80% of users they are following follow them back. Our study reveals that the presence of “reciprocity” can be explained by phenomenon of homophily [14]. Based on this finding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the influence of users in Twitter. TwitterRank measures the influence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.
11724 en Folks in Folksonomies: Social Link Prediction from Shared Metadata Web 2.0 applications have attracted a considerable amount of attention because their open-ended nature allows users to create light-weight semantic scaffolding to organize and share content. To date, the interplay of the social and semantic components of social media has been only partially explored. Here we focus on Flickr and Last.fm, two social media systems in which we can relate the tagging activity of the users with an explicit representation of their social network. We show that a substantial level of local lexical and topical alignment is observable among users who lie close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local alignment between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar topical interests are more likely to be friends, and therefore semantic similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on the Last.fm data set, confirming that the social network constructed from semantic similarity captures actual friendship more accurately than Last.fm’s suggestions based on listening patterns.
11725 en A Novel Click Model and Its Applications to Online Advertising Recent advances in click model have positioned it as an attractive method for representing user preferences in web search and online advertising. Yet, most of the existing works focus on training the click model for individual queries, and cannot accurately model the tail queries due to the lack of training data. Simultaneously, most of the existing works consider the query, url and position, neglecting some other important attributes in click log data, such as the local time. Obviously, the click through rate is different between daytime and midnight.\\ In this paper, we propose a novel click model based on Bayesian network, which is capable of modeling the tail queries because it builds the click model on attribute values, with those values being shared across queries. We called our work General Click Model (GCM) as we found that most of the existing works can be special cases of GCM by assigning different parameters. Experimental results on a large- scale commercial advertisement dataset show that GCM can significantly and consistently lead to better results as compared to the state-of-the-art works.
11726 en Adaptive Weighing Designs for Keyword Value Computation Attributing a dollar value to a keyword is an essential part of running any profitable search engine advertising campaign. When an advertiser has complete control over the interaction with and monetization of each user arriving on a given keyword, the value of that term can be accurately tracked. However, in many instances, the advertiser may monetize arrivals indirectly through one or more third parties. In such cases, it is typical for the third party to provide only coarse-grained reporting: rather than report each monetization event, users are aggregated into larger channels and the third party reports aggregate information such as total daily revenue for each channel. Examples of third parties that use channels include Amazon and Google AdSense.nnIn such scenarios, the number of channels is generally much smaller than the number of keywords whose value per click (VPC) we wish to learn. However, the advertiser has flexibility as to how to assign keywords to channels over time. We introduce the channelization problem: how do we adaptively assign keywords to channels over the course of multiple days to quickly obtain accurate VPC estimates of all keywords? We relate this problem to classical results in weighing design, devise new adaptive algorithms for this problem, and quantify the performance of these algorithms experimentally. Our results demonstrate that adaptive weighing designs that exploit statistics of term frequency, variability in VPCs across keywords, and flexible channel assignments over time provide the best estimators of keyword VPCs.
11727 en Translating Webpages into Bidphrases for Advertising One of the most prevalent online advertising methods is textual advertising. To produce a textual ad, an advertiser must craft a short creative (the text of the ad) linking to a landing page, which describes the product or service being promoted. Furthermore, the advertiser must associate the creative to a set of manually chosen bid phrases representing those Web search queries that should trigger the ad. For efficiency, given a landing page, the bid phrases are often chosen first, and then for each bid phrase the creative is produced using a template. Nevertheless, an ad campaign (e.g., for a large retailer) might involve thousands of landing pages and tens or hundreds of thousands of bid phrases, hence the entire process is very laborious.nnOur study aims towards the automatic construction of online ad campaigns: given a landing page, we propose several algorithmic methods to generate bid phrases suitable for the given input. Such phrases must be both relevant (that is, reflect the content of the page) and well-formed (that is, likely to be used as queries to a Web search engine). To this end, we use a two phase approach. First, candidate bid phrases are generated by a number of methods, including a (monolingual) translation model capable of generating phrases not contained within the text of the input as well as previously "unseen" phrases. Second, the candidates are ranked in a probabilistic framework using both the translation model, which favors relevant phrases, as well as a bid phrase language model, which favors well-formed phrases.nnEmpirical evaluation based on a real-life corpus of advertiser-created landing pages and associated bid phrases confirms the value of our approach, which successfully re-generates many of the human-crafted bid phrases and performs significantly better than a pure text extraction method.
11728 en Personalized Click Prediction in Sponsored Search Sponsored search is a multi-billion dollar business that generates most of the revenue for search engines. Predicting the probability that users click on ads is crucial to sponsored search because the prediction is used to influence ranking, filtering, placement, and pricing of ads. Ad ranking, filtering and placement have a direct impact on the user experience, as users expect the most useful ads to rank high and be placed in a prominent position on the page. Pricing impacts the advertisers’ return on their investment and revenue for the search engine. The objective of this paper is to present a framework for the personalization of click models in sponsored search. We develop user-specific and demographic-based features that reflect the click behavior of individuals and groups. The features are based on observations of search and click behaviors of a large number of users of a commercial search engine. We add these features to a baseline non-personalized click model and perform experiments on offline test sets derived from user logs as well as on live traffic. Our results demonstrate that the personalized models significantly improve the accuracy of click prediction.
11729 en Improving Ad Relevance in Sponsored Search We describe a machine learning approach for predicting sponsored search ad relevance. Our baseline model incorporates basic features of text overlap and we then extend the model to learn from past user clicks on advertisements. We present a novel approach using translation models to learn user click propensity from sparse click logs.nnOur relevance predictions are then applied to multiple sponsored search applications in both offline editorial evaluations and live online user tests. The predicted relevance score is used to improve the quality of the search page in three areas: filtering low quality ads, more accurate ranking for ads, and optimized page placement of ads to reduce prominent placement of low relevance ads. We show significant gains across all three tasks.
11758 en Grammar is to meaning as the law is to good behaviour 
11759 en Optimizing Word Sketches for a large-scale lexicographic project 
11768 en Besedna postaja: pogovor s pisateljem in prevajalcem Yuyutsujem RD Sharma Gost tokratne postaje je bil nepalski pesnik, pisatelj in prevajalec Yuyutsu RD Sharma. O njegovi poeziji se je s pesnikom pogovarjal Evald Flisar, prevajalec zbirke pesmi Jezero Fewa in konj (Sodobnost, 2008). Yuyutsu Ramdass Sharma se je rodil v Nakodarju v Pand?abu. ?olal se je v Batalu in na univerzi v Rad?astanu, kjer je spoznal ameri?kega pesnika Davida Raya. To sre?anje je bilo klju?no za njegovo literarno ustvarjanje. Izdal je deset pesni?kih zbirk in napisal roman. Sicer anglist, specialist za sodobno ameri?ko poezijo, se je v preteklosti ukvarjal tudi z igralstvom in akademsko kariero, med drugim je u?il na univerzi v Katmanduju. Leta 1995 se je v celoti posvetil prevajanju in pisanju. Poezijo, ?lanke in kolumne objavlja v ?asopisih ter revijah in bere svojo poezijo povsod po svetu. Je tudi urednik za leposlovje pri zalo?bi Nirala. V angle??ino aktivno prevaja sodobne nepalske pesnike, nabralo se je ?e ve? antologij, ki jih je sestavil in uredil, med drugimi Roaring Recitals: Five Nepali Poets, ki je bila nominirana za laskavo priznanje azijske knjige leta 2001. Njegova poezija, o kateri kritiki, tako doma?i kot tuji, govorijo v superlativih, je prevedena v nem??ino, franco??ino, italijan??ino, hebrej??ino in nizozem??ino. ?ivi v Katmanduju in New Delhiju.
11782 en Experimental Techniques and Data for Systems Biology A major challenge in biology is to unravel the organisation and interactions of cellular networks that enable complex life processes. The underlying complexity arises from dynamic interactions among large numbers of cellular constituents, such as genes, proteins and metabolites. The classical biochemistry and molecular biology approach has successfully identified most of the components and some interactions, but using such reductionist approach it is not possible to comprehend how system properties emerge. To understand how cells function, comprehensive and quantitative data on components and interactions are required on one hand, coupled with rigorous data integration and modelling on the other. The aim of this seminar is to explain the principles of experimental techniques for study of cell processes at the level of gene expression, protein composition and metabolite profile, in addition to studies of interactions between different components. The structure of data as well as reliability of the data obtained by different techiques will be discussed.
11875 en Living with Catastrophic Terrorism: Can Science and Technology Make the U.S. Safer? After the terrorists attack of September 11, three Academies-the National Academy of Sciences, the National Academy of Engineering, and the Institute of Medicine-sponsored a major study of the role that science and technology might play in countering the threat of catastrophic terrorism in the United States. This study involved a committee of 24 experts, co-chaired by Lewis Branscomb and Richard Klausner, and was supported by 95 others on specialized panels.nnThe 400-page report was presented to Congress and to Governor Ridge, President Bush's choice for Director of Homeland Security in June 2002. It was published by the National Academies Press under the title "Making America Safer: The Role of Science and Technology in Countering Terrorism."nnThis lecture summarizes the output of this project, addresses its influence on legislation for a Department of Homeland Security, and points to the areas of public policy that require the most urgent attention. Professor Branscomb also presents his own expanded views on some issues in the report.nn;**Link to** - **[[http://mitworld.mit.edu/video/48|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/13|Brunel Lecture Series on Complex Systems]]**
11876 en The Columbia Tragedy: System Level Issues for Engineering Among the “tragedy of errors” that doomed the space shuttle Columbia, perhaps the most damning were NASA’s organizational blunders. Sheila Widnall served on the board investigating Columbia’s destruction in February, 2003, and she can describe the technical failures that led, moment by moment, to the ghastly trail of debris across the western United States. But the investigation board traced the roots of this disaster to NASA’s “culture of invincibility,” years in the making. Well-intentioned people, Widnall states, became desensitized to deviations from the norm. NASA managers treated repeated anomalies -- such as foam smashing into shuttle tiles on take off -- as “maintenance turnaround events.”nnFoam striking protective tiles on the leading edge of Columbia’s wing led to the horrors of re-entry: gases in excess of 5000 degrees F entered through a possibly 10-inch-wide breech in the wing, melting sensors and internal structure, sending the shuttle out of control. The failures that led to this moment, are both engineering system failures, and human communication failures.nnWidnall and the investigation board recommend independent safety oversight for shuttle flights; NASA leadership that heeds minority points of view and doesn’t let scheduling or budget pressures define space missions; and routine inclusion of engineers who have the right to address both technological and operational issues of a flight.nn;**Link to** - **[[http://mitworld.mit.edu/video/171|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/13|Brunel Lecture Series on Complex Systems]]**
11877 en Engineering Engineering Systems The top engineering achievements of the 20th century, from the automobile and airplane to telephone, radio, TV and computer, all constitute “complex technical systems,” says Tom Magnanti. It is certain that the next century’s top engineering challenges, such as “reconciling the inevitable growth in world-wide energy demand with potential environmental costs,” will involve complex solutions, too. Will engineering systems (ES) as a discipline play a critical role in educating engineers to respond successfully to these challenges? Magnanti takes up the slippery issue of what constitutes ES as a field. He examines earlier MIT curricula, such as Systems Design and Management, and the 4M Conceptual model (Mine, Model, Manipulate, Measure), for ways to think about his topic. “Is ES a single discipline the way sociology and psychology are?,” he ponders. He applies different architectural constructs to engineering systems: Should ES be viewed as an intersection of engineering, management and social sciences and thus a subset of each; or as borrowing components from technology, economics, human resources, design, and thus comprising “all of engineering plus everything else.” Networking may prove central to all ES work, Magnanti says, whether ES is a single discipline or a “field that has a core focus … and draws upon multiple disciplines.nnn;**Link to** - **[[http://mitworld.mit.edu/video/233|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/13|Brunel Lecture Series on Complex Systems]]**
11878 en The 21st Century is about Engineering, Systems and Society What’s “big, complex and hairy,” and requires the efforts of an impassioned, interdisciplinary team to tackle? The answer, says A. Richard Newton, is the “one-off problem” – the kind of sprawling social, scientific and engineering puzzle that increasingly challenges contemporary society. Think about the conundrum of affordable healthcare, or emergency preparedness. How do you address such enormous issues? Newton’s answer is CITRIS, the Center for Information Technology Research in the Interest of Society. This partnership among academia, government and industry specializes in attacking problems critical to the quality of life, and whose solutions require “societal-scale information systems.” Current CITRIS research includes designing information technology for the energy-deprived developing world. This demands, says Newton, a “complete rethinking of the architecture of information and communication systems.” Work so far points toward wireless technologies in remote villages, with communication antennas flying atop balloons anchored by cables. And on the home front: Newton points out that one-third of the total national outlay on healthcare derives from lab tests -- about 500 billion dollars a year. Could we reduce the tab by coming up with new kinds of testing that don’t require a visit to the office, and whose results could be more efficiently communicated to healthcare providers, Newton wonders. Cracking any of these problems requires an understanding of information systems, and benefits enormously from “passionate individuals” pulling together around a shared vision – “like the moon shot.”n;**Link to** - **[[http://mitworld.mit.edu/video/310|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/13|Brunel Lecture Series on Complex Systems]]**
11879 en Educating Engineers for 2020 and Beyond Though two years departed from the MIT President’s office, Charles Vest has lost none of his zeal for issues of education and training. Says Vest, "I envy the next generation of engineering students. This is without question the most exciting period of human history in science, technology and engineering."nnHe cites exponential advances in knowledge, instrumentation, communication and computational capabilities, which have "created mind-boggling possibilities," cutting across traditional boundaries and blurring distinctions between science and engineering. At the same time, globalization is changing how engineers train and work, as well as how nation's resources are directed. "The entire nature of the innovation ecosystem and business enterprise is changing dramatically in ways we do not yet fully understand," says Vest. These dizzying changes require an accelerated commitment to engineering research and education, and compel research institutions simultaneously to advance the frontiers of fundamental science and technology, and to address the most important problems that face the world.nnVest perceives two key frontiers of engineering: the intersection of physical, life and information sciences -- so-called bio, nano, info-- "which offers stunning, unexplored possibilities;" and the macro world of energy, food, manufacturing, communications, which presents "daunting challenges of the future."nnThe kind of students Vest hopes will explore these new frontiers should reflect a diverse society, write and communicate well, think about ethics and social responsibility, conceive and operate systems of great complexity within a framework of sustainable development and be prepared to live and work as global citizens. It's a "tall order," admits Vest, but "there are men and women every day here who seem to be able to do all these things and more."nnTo prepare this new generation, engineering schools should focus on creating an environment that provides inspiration. In the long run, offering "exciting, creative adventures, rigorous, demanding and empowering milieus is more important than specifying details of the curriculum," says Vest. Students are "driven by passion, curiosity, engagement and dreams." Give them opportunities to discover and do – to participate in research teams, perform challenging work in industry, gain professional experience in other countries. Vest says, "We must ensure the best and brightest become engineers of 2020 and beyond. We can't afford to fail."nn;**Link to** - **[[http://mitworld.mit.edu/video/409|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/13|Brunel Lecture Series on Complex Systems]]**
11880 en Process Improvement in the Rarefied Environment of Academic Medicine If, as Paul Levy says, “medicine for the most part remains a cottage industry,” then how can you impose system-wide improvements -- especially if you’re presiding over an academic hospital, where the culture rewards brilliant, independent, free-thinking doctors?nnThis has been Levy’s challenge since 2002, when he took over an ailing Beth Israel Deaconess Medical Center. The result of a merger between two hospitals in the late ‘90s , BIDMC immediately fell into a “downward spiral,” recounts Levy. Doctors and nurses left, and losses grew to nearly $70 million a year. The hospital burned up $200 million of a $500 million endowment.nnWhen he arrived, Levy recognized that the hospital’s problems had less to do with medicine than with management and organization. For instance, it took 100 days for a bill to go out after the actual service was performed, and bills were often inaccurate, based on a doctor’s hand-scribbled note.nnLevy set to work enhancing the hospital’s routines, such as providing an electronic billing system with pull-down menus. He met with demoralized nurses to address their concerns, and succeeded in reversing the 15% turnover rate. Then, says Levy, “we started focusing on what really matters: how well we’re taking care of people, how often are we hurting and killing people and what to do to stop.”nnHospitals, he notes, “are very dangerous places,” with “bugs floating around and mistakes being made.” One common problem at BIDMC, ventilator associated pneumonia, had a 30% mortality rate. The fixes were simple --raising beds, better oral hygiene, hand-washing --but accomplishing them required systemic compliance.nnLevy identified doctors who could lead colleagues in the new practices. He attached protractors to beds so nurses could raise them by precisely 45 degrees. “Lots of low-tech solutions must be institutionalized,” says Levy. Mortality due to this pneumonia dropped, and Levy figures the hospital saves 96 lives per year, or $12 million in expenses.nnBy shadowing nurses and other staff, Levy’s discovered that individuals often find workarounds to problems, but aren’t aware that others might benefit from their solutions. Levy set up a blog to post these solutions and focus the organization as a whole on areas of concern. Supporting good performance, sharing clinical results such as “how many people we hurt and kill” stimulates people in a hospital to do better, he believes. Public exposure goes a long way in helping academic medical staff to understand they must be “held accountable for their actions particularly when it comes to harm.”nn;**Link to** - **[[http://mitworld.mit.edu/video/504|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/13|Brunel Lecture Series on Complex Systems]]**n;**Event Co-Sponsor** - **[[http://mitworld.mit.edu/host/view/37|Harvard-MIT Division of Health Sciences and Technology]]**
11881 en From IT to Cleantech: New Sources of Innovation Imagine a response to oil dependence and climate change that offers people around the world a new and improved version of the car, premised on redesigning infrastructure top to bottom with green technology in a way that recharges ailing national economies. Applying both an entrepreneurial spirit and a systems engineering approach, Shai Agassi has devised just such a visionary plan for cracking these vexing global challenges.nnA recent World Economic Forum asked participants how to make the world a better place by 2020. Agassi felt an engineer’s compulsion to respond. He describes a process “like a fractal problem…opening up a cascade of questions.” First came the notion of running a country without oil. He seized on, then dismissed, the idea of bio- and hydrogen-based fuels. He then experienced the seminal insight that “you need to go down from molecules to electrons if you want to change the world.”nnThis realization meant addressing both economic and engineering problems. He’d need to offer consumers not a vehicle limited to two seats, three wheels and 28 mph speeds -- but one that could go faster than gas cars, with all the requisite bells and whistles. To move his plan along, he also determined to use available electric car battery engineering. This raised significant issues of convenience: where to recharge and how frequently. Agassi envisioned charging docks in parking lots and home garages. He devised a simple battery replacement method.nnThen came the issue of affordability, which Agassi solved by applying a familiar business model, though not one associated with cars: cell phone minutes. Sell consumers an electric car with a subscription for miles: the longer the subscription, the greater the discount (or rebate check). In Europe, Agassi notes, where gas costs $7 to $8 a gallon, a five-year subscription pretty much gets you “a free electric car.”nnThe model’s complexity and infrastructure requirements imply government backing, which Agassi has already secured. In Denmark there’s a 180% tax on gasoline, and gas-powered sedans costs 60 thousand euros while electrics go for 20 thousand. North Sea windmills will provide clean electricity for charge stations. Israel’s building a desert solar field to “drive every car,” and a smart grid to monitor battery charging. The U.S. is hosting pilot programs in Hawaii and the Bay Area.nnHis is not a plan to phase in gradually: The time is now, he says. “We must do the right, moral thing,” to contend with climate change and brutal oil regimes, and “to create the biggest expansion in U.S. history.”n;**Link to** - **[[http://mitworld.mit.edu/video/642|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/13|Brunel Lecture Series on Complex Systems]]**
11882 en Liberty by Design Recalling a lecture he gave at MIT in 2005, Alan Davidson returns to the questions of the impact of public policy on the way technology is evolving in the Internet space.nnInstead of viewing it as a lawyer for a public policy interest group—his previous role—he now approaches it from his new perspective as a public policy advisor to Google's engineering design group, counseling them on how to build products and run a business. He encourages his fellow engineers "to think broadly […] about their role in the world […], to be more than bench-tied engineers and more involved in the deep social debates of the time."nnThese questions remain: What are the big issues facing the Internet and, specifically, Google; and what are the lessons that have been learned? The old Conventional Wisdom said censorship could not be stopped, only contained. But Davidson believes that in the last 10-15 years there has been a backlash from governments, large institutions, and influential economic actors. The new Conventional Wisdom is: "You don't think we can regulate the Internet? Watch us!"nnAs the Internet revolution advances in processing and storage power, in the ability to network with anyone anywhere in the world, and with device mobility—PCs vs. remote devices, Davidson uses a half a dozen real-life examples from his experience at Google to illustrate their win-win solutions. Using screenshots, he describes the products and shows how the engineers and policy makers worked together to create solutions dealing with privacy, copyright/intellectual property, and censorship issues.nnWhile the solutions lie in building products that address these issues, the challenges lie in raising the issues proactively in the engineering process or by being able to influence regulation and business decisions. The engineers build the product, the public policy experts advise, but they work as partners. By balancing matters of individual freedoms against government and economic interests, Davidson is certain that choices made today will define the kind of Internet we will have in 10 years.n;**Link to** - **[[http://mitworld.mit.edu/video/732|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/13|Brunel Lecture Series on Complex Systems]]**
11883 en Building Resilient Infrastructure to Combat Terrorism: Lessons from September 11th Building Resilient Infrastructure to Combat Terrorism: Lessons from September 11th nn;**Link to** - **[[http://mitworld.mit.edu/video/49|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/14|The Technology and Policy Program Homeland Security Technology and Policy Seminar]]**n;**Event Co-Sponsors**n;**[[http://mitworld.mit.edu/host/view/29|MIT Technology and Policy Program]]**n;**[[http://mitworld.mit.edu/host/view/30|Engineering Systems Division]]**
11884 en Volvo's Environmental Business Strategy With the global harmonization of legislation, rapid technical developments, and an increasing number of customers with environmental awareness, opportunities open for the transport-related industry to contribute to a sustainable society, while maintaining favorable profitability. As one of the world's leading transportation companies in the commercial field, the Volvo Group seeks to play an important role in achieving these objectives.nn;**Link to** - **[[http://mitworld.mit.edu/video/60|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/129|Laboratory for Energy and the Environment]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/17|The Wallenberg Lecture on Sustainability and the Environment at MIT]]**
11997 en Opening Address for the NIPS WS on the Generative and Discriminative Learning Interface 
11998 en Generative and Discriminative Models in Statistical Parsing Since the earliest work on statistical parsing, a constant theme has been the development of discriminative and generative models with complementary strengths. In this work I’ll give a brief history of discriminative and generative models in statistical parsing, focusing on strengths and weaknesses of the various models. I’ll start with early work on discriminative history-based models (in particular, the SPATTER parser), moving through early discriminative and generative models based on lexicalized (dependency) representations, through to recent work on conditional-random-field based models. Finally, I’ll describe research on semi-supervised approaches that combine discriminative and generative models.
11999 en Generative and Discriminative Latent Variable Grammars Latent variable grammars take an observed (coarse) treebank and induce more fine-grained grammar categories, that are better suited for modeling the syntax of natural languages. Estimation can be done in a generative or a discriminative framework, and results in the best published parsing accuracies over a wide range of syntactically divergent languages and domains. In this paper we highlight the commonalities and the differences between the two learning paradigms and speculate that a hybrid approach might outperform either respectively.
12000 en Discriminative and Generative Views of Binary Experiments We consider Binary experiments (supervised learning problems where there are two different labels) and explore formal relationships between two views of them, which we call “generative” and “discriminative”. The discriminative perspective involves an expected loss. The generative perspective (in our sense) involves the distances between class-conditional distributions. We extend known results to the class of all proper losses (scoring rules) and all f-divergences as distances between distributions. We also sketch how one can derive the SVM and MMD algorithms from the generative perspective.
12001 en Multi-Task Discriminative Estimation for Generative Models and Probabilities Maximum entropy discrimination is a method for estimating distributions such that they meet classification constraints and perform accurate prediction. These distributions are over parameters of a classifier, for instance, log-linear prediction models or log-likelihood ratios of generative models. Many of the resulting optimization problems are convex programs and sometimes just simple quadratic programs. In multi-task settings, several discrimination constraints are available from many tasks which potentially produce even better discrimination. This advantage manifests itself if some parameter tying is involved, for instance, via multi-task sparsity assumptions. Using new variational bounds, it is possible to implement the multitask variants as (sequential) quadratic programs or sequential versions of the independent discrimination problems. In these settings, it is possible to show that multi-task discrimination requires no more than a constant increase in computation over independent single-task discrimination.
12002 en Generative and Discriminative Image Models Creating a good probabilistic model for images is a challenging task, due to the large variability in natural images. For general photographs, an ideal generative model would have to cope with scene layout, occlusion, variability in object appearance, variability in object position and 3D rotation and illumination effects like shading and shadows. The formidable challenges in creating such a model have led many researchers to pursue discriminative models, which instead use image features that are largely invariant to many of these sources of variability. In this talk, I will compare both approaches and describe some strengths and weaknesses of each and suggest some directions in which the best aspects of both can be combined.
12003 en Learning Feature Hierarchies by Learning Deep Generative Models In this paper we present several ideas based on learning deep generative models from high-dimensional, richly structured sensory input. We will exploit the following two key properties: First, we show that deep generative models can be learned efficiently from large amounts of unlabeled data. Second, they can be discriminatively fine-tuned using the standard backpropagation algorithm. Our results reveal that the learned high-level feature representations capture a lot of structure in the unlabeled input data, which is useful for subsequent discriminative tasks, such as classification or regression, even though these tasks are unknown when the deep generative model is being trained.
12004 en Why Does Unsupervised Pre-training Help Deep Discriminant Learning? Recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase, with a generative model. Even though these new algorithms have enabled training deep models fine-tuned with a discriminant criterion, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: why does unsupervised pre-training work and why does it work so well? Answering these questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of unsupervised pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that are better in terms of the underlying data distribution; the evidence from these results supports an unusual regularization explanation for the effect of pre-training.
12005 en Unsupervised Learning by Discriminating Data from Artificial Noise Noise-contrastive estimation is a new estimation principle that we have developed for parameterized statistical models. The idea is to train a classifier to discriminate between the observed data and some artificially generated noise, using the model log-density function in a logistic regression function. It can be proven that this leads to a consistent (convergent) estimator of the parameters. The method is shown to directly work for models where the density function does not integrate to unity (unnormalized models). The normalization constant (partition function) can be estimated like any other parameter. We compare the method with other methods that can be used to estimate unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the correct normalization is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images.
12027 en Migratory Narratives: Why Some Stories Replicate Across Media, Cultures, Historical Eras True stories and their fictional spin-offs -- especially bloody ones -- occupy an enduring spot in western culture. Thomas Pettitt’s specialty, the “murdered sweetheart” tale, emerged from medieval times to seize hold of the public imagination in England and Scandinavia over several centuries. The story, involving a seduced girl, her murder by a lover, and the lover’s death, stems from some long-lost actual case. Publishers cranked out ballads based on this story, with helpfully lurid woodcut illustrations. In this “highly successful genre,” says Pettitt, “marketing strategies” distilled the “shocking and juicy story” down to the bare bones. “I sometimes wonder if the weapon of choice was a knife because it rhymes conveniently with wife,” muses Pettitt.nnThe sinking of the Titanic sparked a media frenzy all too familiar these days: reporters rowed out to meet survivors, so they could wire their newspapers first. Richard Howells takes stock of this tragedy and its media manipulation over time. First the Edwardians “celebrated the heroism, triumph, Anglo-Saxon pluck and courage” of the voyagers, with newsreels (including one a month after the tragedy), postcards, sheet music and records. Later, fiction films exploited the story as a fable about the emerging middle class. In our own times, with the epic James Cameron film and assorted merchandise including Titanic software, and beer, Howells sees the Titanic as an “allegory for decline, disaster, decadence and doom …and finally as kitsch-entertainment.” As a modern myth, the Titanic has become “a multimedia narrative.”nnJanet Staiger finds lots of reasons for storytelling, from the anthropological to the psychoanalytical. But she emphasizes “economic explanations: the standardization of stories for a capitalist purpose.” We know that a murdered sweetheart ballad “will be a seller,” so it can be premarketed and mass-produced. Some stories get yoked to particular characters, and others can wander more freely across formulas. Staiger compares Barbie and Cinderella, stuck in their plot lines, to Batman, who can show up in detective, adventure, parody or melodrama form. The “ability to sell figures separate from a formula enhances their capacity for capitalization,” says Staiger. nn;**Link to** - **[[http://mitworld.mit.edu/video/281|Lecture´s Homepage]]**n;**Lecture Host** - **[[http://mitworld.mit.edu/host/view/69|MIT Communications Forum]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/75|Media in Transition 4: The Work of Stories]]**
12028 en Why Are Stories Violent? Note: Due to copyright restrictions, this video does not include the film clips screened by Professor Sandler. His presentation includes sufficient description and context to make the argument clear.nnYou wouldn’t ordinarily expect to find Euripides, Snow White, Bruno Bettelheim, and Rambo discussed at the same event. But they share the limelight in this session. Is violence an intrinsic part of human art and experience? A device exploited by cynical producers to lure consumers? A threat to healthy child development, or a natural means of teaching and learning? There is no consensus here, but plenty of provocative examples and scholarly insight. David Thorburn sets the stage, evoking the relentless tradition of violence in the Western literary canon from the eras of the Bible and Greek tragedy.nnKevin Sandler, who dubs himself a “political media economist,” fills out the contemporary end of the continuum, with an analysis of ratings handed out by the film industry board. Using “Eyes Wide Shut” and “Collateral Damage” as evidence, Sandler argues that major studios have positioned violence as an entertainment vehicle safely within the “cultural function” we expect movies to fulfill.nnWhat about Snow White’s poisoned apple, or the cruelties Willy Wonka inflicts on chocolate factory visitors? Literary scholar Maria Tatar lays out three possible functions of violence in stories for children: stimulating their imagination through surreal depictions of “what might be;” teaching them how to behave through fear; and giving them a therapeutic outlet for primitive emotions. A series of lively questioners try to penetrate what Tatar calls the “mystery of cultural effects,” speculating on the psychological, social, and political consequences of so much violence in children’s media.nn;**Link to** - **[[http://mitworld.mit.edu/video/282|Lecture´s Homepage]]**n;**Lecture Host** - **[[http://mitworld.mit.edu/host/view/69|MIT Communications Forum]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/75|Media in Transition 4: The Work of Stories]]**
12029 en Narratives of Science Robert Kanigel poses the central question of this panel: “The storytelling express is leaving the station. Do we want to jump aboard, or under some circumstances, stay where we are?” Science writing has matured as a discipline and genre, and for many writers, this means telling a story with what Kanigel describes as “a narrative arc: a cannon propelling you through a text, because of readers’ eagerness to know what’s happening next.” This implies some kind of linear movement, whether the writer focuses on “the smallest atomic unit” or a larger canvas. But Kanigel wonders if “there are circumstances when we don’t want stories.”nnThomas Levenson responds, “You can go a long way down the path of understanding science as a human activity without getting a story.” He offers the example of a writer who keeps a diary of a year spent in a laboratory -- what Levenson describes as “science travelogues.” But from his early experiences as a journalist, Levenson has found that “Science produces a certain kind of knowledge, but the activity of science takes shape within and is shaped by the world beyond science.” So he brings an historical, interpretative method to bear on his subjects, including Einstein. Says Levenson, “You get to make meaning out of the story you want rather than asking people to extract meaning out of a happenstance of facts.”nnAlan Lightman ponders the role of science in novels, theatre and film. He offers several examples of “gripping and suspenseful” discussions of science within narratives, such as Michael Ondaatje’s novel, The English Patient and Michael Frayn’s play, Copenhagen. These authors avoid the didactic with a “motor that drives us through discussion.” But the very popular genre of science biography proves trickier to propel successfully. “In science, it’s more of a challenge to intertwine work with life because life deals with the inanimate,” says Lightman. nn;**Link to** - **[[http://mitworld.mit.edu/video/284|Lecture´s Homepage]]**n;**Lecture Host** - **[[http://mitworld.mit.edu/host/view/69|MIT Communications Forum]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/75|Media in Transition 4: The Work of Stories]]**
12064 en Introduction - Excerpt 
12065 en Lecture 1: Ch. 1. Syllabus and Introduction 
12069 en Triangular Numbers (Part I) Elementary explanation of triangular numbers and Gauss demonstration for the sum of the first 100 natural numbers.
12070 en Triangular Numbers (Part II) Using Gauss Idea to find the sum 1+2 + ... +n. Arithmetic progressions an obtaining a general formula for the sum of an arithmetic progression.
12071 en Triangular Numbers (Part III) Recursive Relation for triangular numbers. Finding a solution to the recursive equation and another solution to the Recursive equation.
12072 en It's all Greek to me! Sigma notation Sigma Notation. Tetrahedral numbers. Pyramidal Numbers. Some relations between them.
12073 en Summation Telescoping property We explain the summation telescoping property and apply it to finding two summations.
12074 en 1,2,3, .. infinity. Mathematical Induction Explain the Method of Mathematical Induction. Francesco Maurolico, Pascal and John Wallis. Applying the method of Induction to prove the sum of odd numbers is a square.
12075 en Mathematical Induction (Part II) Prove Inequality using the Method of Mathematical Induction.
12076 en Mathematical Induction (Part III) Recursive Relation for triangular numbers. Finding a solution to the recursive equation and another solution to the Recursive equation.
12077 en The well ordering principle Proving The Well Ordering Principle is equivalent to The Principle of Mathematical Induction.
12078 en Weaving numbers Vedic multiplication or weaving multiplication. Fibonacci's Sieve or Lattice Multiplication. John Napier's Bones multiplication.n
12079 en Are you having phun yet? Introduction to Phun. The new entertaining and extreme fun educational computer program. Using Phun to explain Math.
12080 en Dimension 2 Hipparchus explains how two numbers can describe the position of a point on a sphere. He then explains stereographic projection: how can one draw a picture of the Earth on a piece of paper?
12081 en Dimension three M. C. Escher tells the adventures of two-dimensional creatures who are trying to imagine three-dimensional objects.?
12082 en The fourth dimension Mathematician Ludwig Schläfli talks to us about objects in the fourth dimension and shows us a procession of regular polyhedra in dimension 4, strange objects with 24, 120 and even 600 faces!
12083 en The fourth dimension continued Mathematician Ludwig Schläfli talks to us about objects in the fourth dimension and shows us a procession of regular polyhedra in dimension 4, strange objects with 24, 120 and even 600 faces!
12084 en Complex Numbers Mathematician Adrien Douady explains complex numbers. The square root of negative numbers is explained in simple terms. Transforming the plane, deforming pictures, creating fractal images.
12085 en Complex Numbers Continued Mathematician Adrien Douady explains complex numbers. The square root of negative numbers is explained in simple terms. Transforming the plane, deforming pictures, creating fractal images.
12086 en Fibration The mathematician Heinz Hopf describes his "fibration". Using complex numbers he builds beautiful arrangements of circles in space.
12087 en Fibration Continued The mathematician Heinz Hopf describes his "fibration". Using complex numbers he builds beautiful arrangements of circles in space.
12088 en Proof Mathematician Bernhard Riemann explains the importance of proofs in mathematics. He proves a theorem on stereographic projection.
12089 en Probabilities and Language Models 
12090 en Probabilistic Methods for Classification 
12091 en Machine Learning - Finding Patterns in the World 
12092 en Text Summarization 
12093 en Machine translation 
12094 en Machine translation 
12095 en Speech Perception Engineering 
12096 en Machine Recognition of Speech 
12097 en Introduction to Speech Processing 
12098 en Collection Fusion 
12099 en Intro to Information Retrieval 
12100 en Data-Intensive Text Processing with MapReduce 
12101 en Acquisition of Lexical Knowledge from N Grams 
12102 en Practical Linguistically Motivated Parsing 
12103 en UBM based Acoustic Modeling for ASR 
12104 en Lecture 3: Ch. 1. Bond Polarity, Formal Charge, Lewis Structures 
12105 en Lecture 4: Ch. 1. Resonance Structures, Skeletal Structures, Bond Length 
12106 en Lecture 5: Ch. 1. Orbital Models of Structure and Bonding 
12107 en Lecture 6: Ch. 1. Molecular Geometry. Ch. 2. Acids and Bases 
12108 en Lecture 7: Ch. 2. Acid Strength and Equilibria 
12109 en Lecture 8: Ch. 2. Lewis Acids, Lewis Bases, and Organic Reaction Mechanisms 
12110 en Lecture 9: Ch. 3. Organic Compounds and Functional Groups 
12111 en Lecture 10: Ch. 3. Hydrocarbons, Alcohols, Amines 
12112 en Lecture 11: Ch. 3. Carbonyl Compounds, Intermolecular Forces 
12113 en Lecture 12: Ch. 4. Introduction to Alkanes 
12114 en Lecture 13: Ch. 4. Nomenclature of Alkanes, Conformations of Ethane 
12115 en Lecture 14: Ch. 4. Conformations of Butane and Cycloalkanes 
12117 en Lecture 16: Ch. 5. Introduction to Stereochemistry 
12118 en Lecture 17: Ch. 5. Assigning Tetrahedral Stereogenic Centers 
12119 en Lecture 18: Ch. 5. Meso Compounds 
12120 en Lecture 19: Ch. 5. Properties of Chiral Compounds 
12121 en Lecture 20: Ch. 6. Introduction to Understanding Organic Reactions 
12122 en Lecture 21: Ch. 6. Energy Diagrams, Transition States, and Reaction Rates 
12123 en Lecture 22: Ch. 6. Energetics of Reactions 
12124 en Lecture 23: Ch. 7. Introduction to Alkyl Halides and Nuceophilic Substitution 
12125 en Lecture 24: Ch. 7. Mechanistic and Stereochemical Aspects of SN2  Reactions 
12126 en Lecture 27: Ch. 8. Elimination Reactions. Introduction to E2 Reactions 
12127 en Lecture 25: Ch. 7. Mechanistic and Stereochemical Aspects of SN1  Reactions 
12128 en Lecture 26: Ch. 7. Properties of Electrophiles, Nucleophiles, and Leaving Groups 
12129 en Lecture 28: Ch. 8. Regiochemical and Stereochemical Course of E2 Reactions 
12130 en Lecture 29: Ch. 8. E1 Reactions. Comparison of SN1, SN2, E1, and E2 Reactions 
12148 en From the Mega to the Nano: Computer Modeling in Engineering and Sciences / Od mega do nano:ra?unalni?ko modeliranje v in?enirstvu in znanosti Computer Modeling in Engineering & Sciences (CMES) is a multi-disciplinary enabling- methodology fornan integrated process & product simulation & design of devices, which span the mega to nano lengthscales,nand which operate at equally diverse time-scales. CMES is an engine for global economic growth,nplays a vital role in reducing the product-development time & cost, and in assessing the longevity, lifecycle-ncosts, and failure prevention in various devices. This lecture is a brief overview of the past, present,nand the future of CMES, from the speaker’s perspective.nn----nnRa?unalni?ko modeliranje v in?enirstvu in znanosti (ang. Computer Modeling in Engineering and Sciences, CMES) je multidisciplinarna metodologija za integrirano procesno in produktno simuliranje ter na?rtovanje naprav, ki delujejo na dol?inskih skalah od mega do nano in na enako raznolikih ?asovnih skalah. CMES je lahko gonilo globalne ekonomske rasti in igra klju?no vlogo pri zmanj?evanju razvojnega ?asa in stro?kov ter pri oceni ?ivljenjske dobe, stro?kov vzdr?evanja in prepre?evanja okvar raznovrstnih naprav. Predavanje je kratek pregled preteklosti, sedanjosti in prihodnosti CMES iz predavateljeve perspektive.
12212 en Automatic Annotation of Images using Ensembles of Trees for Hierarchical Multi-label Classification This research presents a large scale system for detection of visual nconcepts and annotation of images. The system is composed of two parts: nfeature extraction and classification/ annotation. The feature nextraction part provides global and local descriptions of the images in nthe form of numerical vectors. Using these numerical descriptions, we ntrain a classifier, a predictive clustering tree (PCT), to produce nannotations for unseen images. PCTs are able to handle target concepts nthat are organized in a hierarchy, i.e., perform hierarchical nmulti-label classification. To improve the classification performance, nwe construct ensembles (bags and random forests) of PCTs.nnnWe evaluate our system on two different databases: IRMA database which ncontains medical images and the image database from the ImageCLEF@ICPR n2010 photo annotation task which contains general images. The extensive nexperiments conducted on the benchmark databases show that our system nhas very high predictive performance and can be easily scaled to large namounts of visual concepts and data. In addition, our approach is very ngeneral: it can be easily extended with new feature extraction methods, nand it can thus be easily applied to other domains, types of images and nother classification schemes. Furthermore, it can handle arbitrarily nsized hierarchies organized as trees or directed acyclic graphs.
12247 en A Translation from Logic to English with Dynamic Semantics We present a procedure for translating standard predicate logic into English. The procedure generates both referring expressions and non-referring expressions, including both referential and bound variable anaphora. Non-referring expressions correspond to short-term discoursenreferents [1], which present a special set of challenges for a natural language generation system: (i) they have limited ‘lifespans’ and (ii) the determiner with which they are introduced (every, some, any, no) is sensitive to the logical context. Our system addresses these challenges usingndynamically updated information states.
12251 en Data, information, design and traffic injuries / Podatki, informacije, oblikovanje in prometne po?kodbe **English:** The nature of “information design” is not determined by what one does (signage, forms, documents, etc.) but how one does it. The users are at the centre of information design, with all their differences, possibilities, needs and wants. Data can be absorbing, but information design is not about data: is about people. To reach people we have to know them.nn  Data without context is not information. Information is difference that makes a difference. To make a difference there is a need for significance, and significance comes from comparison: comparison between one thing and another, between one thing and its context, or between one thing and its consequences.nn  The presentation will outline the roles of information and persuasion in communication design. To provide an example of the possible importance of the problems that design should address, it will present the social and economic costs of traffic injuries, discussing how to turn data into information; how to contextualize information so that its significance can be perceived; and how significance is indispensable when there is a need to promote action and change.nn  The central objective of the paper is to contribute to the lecture series’ main topic: the relevance of information design for things that matter in society.nn**Slovensko:**n»Informacijskega oblikovanja« ne dolo?a to, kar nekdo po?ne (ozna?evanje, formularji, dokumenti itd.), temve? kako to po?ne. V sredi??u informacijskega oblikovanja so uporabniki z vsemi razlikami, mo?nostmi, potrebami in zahtevami. Podatki so lahko mikavni, vendar informacijsko oblikovanje ni namenjeno podatkom, temve? ljudem. ?e ?elimo ljudi dose?i, jih moramo poznati.nn  Podatki brez konteksta niso informacije. Informacije so tisto, kar naredi potrebni preskok. Za to pa je potreben pomen, ki izvira iz primerjave med razli?nimi stvarmi, med eno stvarjo in njenim kontekstom ali med eno stvarjo in njenimi posledicami.nn  Predavatelj bo predstavil vlogo informacij in prepri?evanja v oblikovanju komunikacije. Kot primer pomembnosti problemov, ki bi se jih oblikovanje lahko lotilo, bodo prikazani dru?beni in gospodarski stro?ki prometnih nesre?, ki bodo pokazali, kako je mogo?e podatke spremeniti v informacije; kako informacije postaviti v kontekst, da lahko dojamemo njihov pomen; ter kako nepogre?ljiv je pomen, ko se pojavi potreba po spodbujanju delovanja in sprememb.nn  Namen predavanja je predvsem prispevati h glavni temi serije predavanj: k pomenu informacijskega oblikovanja na dru?beno pomembnih podro?jih.
12270 en From Microscopy Images to Models of Cellular Processes The advance of fluorescent tagging and of confocal microscopy is allowing biologists to image biochemical processes at a level of detail that was unimaginable just a few years ago. However, as the analysis of these images is done mostly by hand, there is a severe bottleneck in transforming these images into useful quantitative data that can be used to evaluate mathematical models.nOne of the inherent challenges involved in automating this transformation is that image data is highly variable. This requires a recalibration of the image processing algorithms for each experiment. We use machine learning methods to enable the experimentalist to calibrate the image processing methods without having any knowledge of how these methods work. This, we believe, will allow the rapid integration of computer vision methods with confocal microscopy and open the way to the development of quantitative spatial models of cellular processes.nFor more information, see then[[http://seed.ucsd.edu/~yfreund/NewHomePage/Applications/Biomedical_Imaging.html| Bio-medical image analysis page]]
12271 en Computational advertising: business models, technologies and issues (CoAd) Internet advertising revenues in the United States totaled $21 billion for 2007, up 25 percent versus 2006 revenues of $16.9 billion (according to the Interactive Advertising Bureau); this represents approximately half the worldwide revenue from online advertising. Fueled by these growth rates and the desire to provide added incentives and opportunities for both advertisers and publishers, alternative business models to online advertising are been developed. This tutorial will review the main business models of online advertising including: the pay-per-impression model (CPM); and the pay-per-click model (CPC); a relative new comer, the pay-per-action model (CPA), where an action could be a product purchase, a site visit, a customer lead, or an email signup; and dynamic CPM (dCPM) which optimizes a campaign towards the sites and site sections that perform best for the advertiser.nThis tutorial will also discuss in detail the technology being leveraged to automatically target ads within these business models; this largely derives from the fields of machine learning (e.g., logistic regression, online learning), statistics (e.g., binomial maximum likelihood), information retrieval (vector space model, BM25), optimization theory (linear and quadratic programming), economics (auction mechanisms, game theory). Challenges such as click fraud (the spam of online advertising), deception, privacy and other open issues will also be discussed. Web 2.0 applications such as social networks, and video/photo-sharing pose new challenges for online advertising. These will also be discussed.
12272 en Enterprise and Desktop search (EDS) The Enterprise and Desktop Search problems recently received a considerable amount of attention from academia, mainly due to the increasing demand in industrial solutions supporting various search tasks in intranets. While challenges arising in intranet search are not entirely new comparing to those that web community has faced for years, advanced web search technologies are often unable to address them properly. In this course we give research prospective on distinctive features of both Enterprise and Desktop Search, typical search scenarios, existing ranking techniques and algorithms. First lecture gives a general introduction, reviews existing systems and outlines typical research challenges. In our next lecture we plan to summarize advanced ranking algorithms and personalization methods utilizing implicit and explicit feedback from users. Third lecture provides an overview of exploratory search methods, including search result clustering/ categorization, faceted search, as well as related techniques stimulating interaction with a user. Later, we discuss latest developments in expert/people search, for example, graph-based and language model based methods. Last lecture covers various aspects of Desktop search: state-of-the-art research prototypes, advanced real-world applications and recent break-through ideas like just-in-time retrieval and task context detection. The course is wrapped up with discussion on open problems and research directions in Enterprise and Desktop search.
12273 en Information Retrieval Modeling (IRM) There is no such thing as a dominating model or theory of information retrieval, unlike the situation in for instance the area of databases where the relational model is the dominating database model. In information retrieval, some models work for some applications, whereas others work for other applications. For instance, vector space models are well-suited for similarity search and relevance feedback in many (also non-textual) situations if a good weighting function is available; the probabilistic retrieval model or naive Bayes model might be a good choice if examples of relevant and nonrelevant documents are available; Google's Pagerank model is often used in situations that need modelling of more of less static relations between documents; region models have been designed to search in structured text; and language models are helpful in situations that require models of language similarity or document priors; In this tutorial, I carefully describe all these models by exlpaining the consequences of modelling assumptions. I address approaches based on statistical language models in great depth. After the course, students are able to choose a model of information retrieval that is adequate in new situations, and to apply the model in practical situations.
12275 en Modeling Web Searcher Behavior and Interactions Hundreds of millions of users search the web daily, clicking on the results, submitting and refining queries and otherwise interacting with the search engines. The vast amount of information generated as a by-product of these interactions can be mined to dramatically improve the effectiveness of web search, and information access in general.nThis course will survey the research in modeling user behavior in web search, and how this information can improve web search effectiveness. The emphasis will be on learning and analyzing the appropriate data mining and machine learning techniques for the user behavior and interaction data, and on the integration of the behavioral models into the search engine operation.
12277 en Persistence-based Clustering Clustering is a classical problem which looks for important segmentsnin an unstructured data set. In general, this is an ill-posednproblem. A common approach is to consider the data set as a sample ofnan unknown probability distribution function on some underlyingnspace. Clustering then becomes a problem of understanding thenbehaviour of the distribution function.nnIn this talk, I will introduce persistence-based clustering. Undernsome mild assumptions, the algorithm comes with a variety of strongntheoretical guarantees. In particular, it provably approximates thenstructure of the underlying distribution function even when underlyingnspace is only approximately known. The approach is based heavily onnpersistent homology (also refered to as topological persistence), anrelatively recent development in the area of computationalntopology. It is precisely this framework which makes many of thenproofs possible. The talk will include a general introduction tonpersistence so no prior knowledge is expected. On the practical side,nthe algorithm is efficient, both in memory size and running time, sonit can handle large, high dimensional data sets quickly. Finally, itnprovides visual feedback in addition to the clusters, something whichnis particularly useful when the data sets cannot be visualized.
12305 en Welcome presentation/ Computer security/ Workshop presentation 
12306 en Introduction to CERN 
12307 en Introduction to Particle Physics (for non particle physicists) These lectures are an introduction to the ideas of particle physics, aimed at students and teachers with little or on knowledge of the subject. They form a broad basis that will be developed in more detail by the subsequent lecturers in the school. nnPrerequisite knowledge Basic physics: No prior knowledge of particle physics needed. For a cheap pocket sized guide see "Particle Physics - A Very Short Introduction" by Frank Close, published by Oxford University Press n
12308 en Installation, Commissioning and Startup of Atlas & CMS Experiments 
12309 en Particle Detectors This lecture will serve as an introduction to particle detectors and detection techniques.nIn the first lecture, a historic overview of particle detector development will be given. In the second lecture, some basic techniques and concepts for particle detection will be discussed. In the third lecture, the interaction of particles with matter, the basis of particle detection, will be presented.nThe fourth and fifth lectures will discuss different detector types used for particle tracking, energy measurement and particle identification.
12310 en Introduction to Root 
12311 en Accelerators 1a) Introduction and motivationn1b) History and accelerator typesn2) Transverse beam dynamicsn3a) Longitudinal beam dynamicsn3b) Figure of merit of a synchrotron/collidern3c) Beam controln4) Main limiting factorsn5) Technical challengesnnPrerequisite knowledge: Previous knowledge of accelerators is not required.n
12312 en Antimatter in the Lab 
12313 en Data Acquisition Systems 
12318 en Future Colliders 
12321 en Superconducting Magnets 
12325 en Future Linear Colliders 
12330 en Introduction to Medical Physics 
12331 en The future of energy and climate The talk will review some of the basic facts about the history and present status of the use of energy and its climatic consequences. It is clear that the world will have to change its way of energy production, the sooner the better. Because of the difficulty of storing electric energy, by far the best energy source for the future is thermal solar from the deserts, with overnight thermal storage. I will give some description of the present status of the technologies involved and end up with a pilot project for Europe and North Africa.
12332 en Visible and invisible in modern Physics 
12357 en Machine Learning Biological Network Models In this talk we survey work being conducted at the Centre fornIntegrative Systems Biology at Imperial College on the use ofnmachine learning to build models of biochemical pathways.nWithin the area of Systems Biology these models providengraph-based descriptions of bio-molecular interactions whichndescribe cellular activities such as gene regulation, metabolismnand transcription. One of the key advantages of the approach taken,nInductive Logic Programming, is the availability of background knowledgenon existing known biochemical networks from publicly availablenresources such as KEGG and Biocyc. The topic has clear societal impactnowing to its application in Biology and Medicine. Moreover, objectndescriptions in this domain have an inherently relational structure in thenform of spatial and temporal interactions of the molecules involved.nThe relationships include biochemical reactions in which one setnof metabolites is transformed to another mediated by the involvementnof an enzyme. Existing genomic information is very incompletenconcerning the functions and even the existence of genes andnmetabolites, leading to the necessity of techniques such asnlogical abduction to introduce novel functions and inventnnew objects. Moreover, the development of active learningnalgorithms has allowed automatic suggestion of new experimentsnto test novel hypotheses. The approach thus provides supportnfor the overall scientific cycle of hypothesis generation andnexperimental testing.
12360 en The European Space Agency 
12361 en ESA future projects and missions opportunity for Slovenia 
12362 en Workshop for PECS Slovene users 
12364 en Discussion 
12365 en Knowledge transfer to and from clinical research 
12377 en Compassion - The Art of Happiness One great question underlies our experience, whether we think about it consciously or not: What is the purpose of life? The Fourteenth Dalai Lama has considered this question and would like to share his thoughts in the hope that they may be of direct, practical benefit to those who read them.n n"I believe that the purpose of life is to be happy. From the moment of birth, every human being wants happiness and does not want suffering. Neither social conditioning nor education nor ideology affect this. From the very core of our being, we simply desire contentment. I don't know whether the universe, with its countless galaxies, stars and planets, has a deeper meaning or not, but at the very least, it is clear that we humans who live on this earth face the task of making a happy life for ourselves. Therefore, it is important to discover what will bring about the greatest degree of happiness."
12428 en VideoLectures Promo VideoLectures.NET is a free and open access educational video lectures repository. The lectures are given by distinguished scholars and scientists at the most important and prominent events like conferences, summer schools, workshops and science promotional events from many fields of Science. The portal is aimed at promoting science, exchanging ideas and fostering knowledge sharing by providing high quality didactic contents not only to a scientific community but also to a general public. All lectures, accompanying documents, information and links are systematically selected and classified through the editorial process taking into account also users' comments.nnThe training materials are being developed within the FP5, FP6, and FP7 European Framework Programs, where the web based portal VideoLectures.NET is being used as an educational platform for several EU funded research projects such as PASCAL NoE, ECOLEAD NoE, SEKT IP and different organizations among others MIT OpenCourseWare and CERN. The range of countries involved and languages used varies from Europe, USA, Taiwan, Australia, Ukraine, Russia and Brazil.nnThe portal is becoming a major reference video training material repository and dissemination channel for academic researchers all around the world. Following the ideas to network with other similar initiatives, new frameworks and plans are being prepared aiming at boosting up an e-science video reference network combining universities and research institutes that provides a qualitative stream of scientific and training programs.nnFor that purpose we will continue to film and provide services for major world scientific conferences and extend the coverage also to non- technical and natural science disciplines like Fine Arts, Humanities, Social studies and Law.
12436 en Visual information about medicines for patients: designing for Don Quixote? / Vizualne informacije o zdravilih za bolnike: oblikovanje za Don Kihota? **English:** \\nSituation: Information about medicines\\nIt is very hard to handle medicines properly without visual information. Leaflets, packaging, websites and pharmacist-labels provide patients with a plethora of texts and images that aim to inform about dosage, correct way of taking, side effects, warnings and storage. Unfortunately, this visual information does not seem to result in ‘effective instructions and warnings’ and frequently leads to confusion and anxiety. Medical errors, persistently increasing costs of medicines, and ineffective use are seen as unavoidable and part of this system.nnIssue: Four questionable assumptions\\nThe main practical task is to convince patients to take medicines appropriately and effectively. This is problematic. The reason might be that some of the assumptions that underlie the legal framework for the development of visual information for patients are incorrect. The first assumption is that patients are helped by standardization and by strictly prescribing the information that is required. The second assumption is that patients are not the main influential factor when information about medicines is considered. The third assumption is that ‘medicines’ and ‘information’ must be regulated by governmental authorities in the same manner. The fourth assumption underestimates the ability of patients to recognize and interpret information about medicines. These four assumptions lead to a view that it is only necessary to ‘protect patients’ against ‘incorrect and incomplete’ information. The result is a profusion of guidelines and regulations about visual information.nnConsequences: we are solving the wrong problems\\nThe current development process of information about medicines diverts from the tried and tested processes of developing appropriate arguments that could convince patients. There are discrepancies in the selection of the contents, the structure of the argument, and the style in which the argument is presented. The result is that the interpretation of visual information about medicines is unnecessarily difficult.nnApproach: Developing alternative prototypes\\nAn analysis shows that the four assumptions about ‘effective communication’ related to information about medicines are malignant. These assumptions hamper the development of appropriate visual arguments that might support patients to handle medicines appropriately. Fighting these ‘windmills’ requires substantial efforts without benefitting patients.nnThe development of alternative genres that could provide patients with reliable and understandable information about medicines is essential. Novel prototypes that show what clear and understandable information really looks like are urgently required.nn**Slovensko:**\\nStanje: Informacije o zdravilih\\nBrez vizualnih informacij je zelo te?ko pravilno ravnati z zdravili. Zgibanke, embala?a, spletne strani in lekarni?ke nalepke bolnikom ponujajo razli?na besedila in podobe, da bi jih informirali o odmerku, pravilnem jemanju zdravil, stranskih u?inkih, opozorilih in hranjenju zdravil. ?al te vizualne informacije ne tvorijo »u?inkovitih navodil in opozoril«, temve? so pogosto vzrok za zmedo in preplah. Zdravni?ke napake, rasto?i stro?ki zdravil in neu?inkovita uporaba se zdijo neizogibni del tega sistema.nnZadeva: ?tiri vpra?ljive predpostavke\\nGlavna prakti?na naloga je prepri?ati bolnike, naj ustrezno in u?inkovito jemljejo zdravila. To je problemati?no. Razlog za to so po vsej verjetnosti napa?ne predpostavke, ki so temelj pravnega okvira razvoja vizualnih informacij za bolnike. Prva predpostavka je, da sta standardizacija in strogo dolo?anje potrebnih informacij v pomo? bolnikom. Druga predpostavka je, da bolniki niso glavni vplivni dejavnik pri informacijah o zdravilih. Tretja predpostavka je, da morajo vladni organi enako nadzorovati »zdravila« in »informacije«. ?etrta predpostavka pa je podcenjevanje sposobnosti bolnikov, da prepoznajo in interpretirajo informacije o zdravilih. Te ?tiri predpostavke vodijo k mnenju, da je treba samo »za??ititi bolnike« pred »napa?nimi in nepopolnimi« informacijami. Posledica tega je mno?ica navodil in predpisov o vizualnih informacijah.nnPosledice: Re?ujemo napa?ne probleme\\nTrenutni razvojni proces informacij o zdravilih te?e stran od preizku?enih procesov razvijanja ustreznih argumentov, ki bi lahko prepri?ali bolnike. Obstajajo odstopanja v izbiri vsebine ter v strukturi in slogu argumentiranja. Posledica je nepotrebno zapletena interpretacija vizualnih informacij o zdravilih.nnPristop: Razvijanje alternativnih prototipov\\nAnalize ka?ejo, da so glavni krivec ?tiri predpostavke o »u?inkoviti komunikaciji«. Te predpostavke ovirajo razvoj ustreznih vizualnih argumentov, ki bi bolnike lahko spodbudili k pravilnem ravnanju z zdravili. Boj proti tem »mlinom na veter« zahteva precej?en napor, kar pa ne koristi bolnikom.nnNujno je treba razvijati alternativne oblike, ki bi bolnikom ponudili zanesljive in razumljive podatke o zdravilih. Nujno potrebujemo nove prototipe, ki bi pokazali, kak?ne so jasne in razumljive informacije.
12444 en An Introduction to Project Halo In this talk, I will describe Project Halo, a large-scale research program in artificial intelligence sponsored by Paul Allen’s Vulcan Inc.. Project Halo has three parts: 1) an expert system called AURA for knowledge formulating and question answering in science; 2) an advanced reasoning system called SILK for default and higher-order inference; and 3) a social semantic web platform called Semantic MediaWiki+ (SMW+) for effectively building the large basic knowledge base that is necessary for the other two parts to work. I will briefly describe the first two of these parts, but I will spend most of my time describing Project Halo’s work in on SMW+.  SMW+ combines semantic web technology and wiki-based social mechanisms, and marries the structure and flexibility of a database to the crowd sourcing power of a wiki. I will show several examples of SMW+, and conclude by describing Vulcan’s Ultrapedia prototype, in which we show what Wikipedia might look like if it was built with a semantic wiki.n
12617 en Optical Wireless Technologies for Broadband Communications Today's information exchange depends on the transmission of data, voice and multimedia across the telecommunication networks. Optical wireless broadband communication represents one of the most promising approaches for addressing the emerging broadband access market. It offers low start-up and operational costs, high security, rapid deployment, high fibre-like bandwidths where no licence is needed. On the other side optical wireless links can be severely affected by fog and atmospheric turbulence. However, the optical wireless communications, also refered to as free space optical communications, will play a significant role in future information superhighways.nThe research group Optikom at TU Graz deals with the research and development of optical communications with special emphasis on Free Space Optics (FSO). It participates in several national and international projects, including EU Network of Excellence SatNEx, several ESA projects, and COST IC0802 action related to channel modelling for FSO systems. The group is currently investigating how to increase the reliability of FSO systems by new hybrid RF-FSO system design, considering different weather and atmospheric conditions, new modulation and coding schemes and different diversity techniques.nn
12689 en Welcome 
12690 en Forensic Statistics: Where are We and Where are We Going? I will discuss the present situation of Forensic statistics. Rapid developments in forensic science are putting statistics and probability more and more into the court-room lime-light, often with apalling results. Why is this and where should we go? Standard Bayesian and standard frequentist statistics are based on the wrong paradigms. Forensic statisticians have to learn from the learning community. But in forensic statistics, N=1. How can we learn?
12691 en Approximate Bayesian Computation: What, Why and How? Approximate Bayesian Computation (ABC) arose in response to the difficulty of simulating observations from posterior distributions determined by intractable likelihoods. The method exploits the fact that while likelihoods may be impossible to compute in complex probability models, it is often easy to simulate observations from them. ABC in its simplest form proceeds as follows: (i) simulate a parameter from the prior; (ii) simulate observations from the model with this parameter; (iii) accept the parameter if the simulated observations are close enough to the observed data. The magic, and the source of potential disasters, is in step (iii). This talk will outline what we know (and don't!) about ABC and illustrate the methods with applications to the fossil record and stem cell biology.
12693 en Boosted optimization for network classification In this paper we propose a new classification algorithm designed for application on complex networks motivated by algorithmic similarities between boosting learning and message passing. We consider a network classifier as a logistic regression where the variables define the nodes and the interaction effects define the edges. From this definition we represent the problem as a factor graph of local exponential loss functions. Using the factor graph representation it is possible to interpret the network classifier as an ensemble of individual node classifiers. We then combine ideas from boosted learning with network optimization algorithms to define two novel algorithms, Boosted Expectation Propagation (BEP) and Boosted Message Passing (BMP). These algorithms optimize the global network classifier performance by locally weighting each node classifier by the error of the surrounding network structure. We compare the performance of BEP and BMP to logistic regression as well state of the art penalized logistic regression models on simulated grid structured networks. The results show that using local boosting to optimize the performance of a network classifier increases classification performance and is especially powerful in cases when the whole network structure must be considered for accurate classification.
12694 en Detecting weak but hierarchically-structured patterns in networks The ability to detect weak distributed activation patterns in networks is critical to several applications, such as identifying the onset of anomalous activity or incipient congestion in the Internet, or faint traces of a biochemical spread by a sensor network. This is a challenging problem since weak distributed patterns can be invisible in per node statistics as well as a global network-wide aggregate. Most prior work considers situations in which the activation/non-activation of each node is statistically independent, but this is unrealistic in many problems. In this paper, we consider structured patterns arising from statistical dependencies in the activation process. Our contributions are three-fold. First, we propose a sparsifying transform that succinctly represents structured activation patterns that conform to a hierarchical dependency graph. Second, we establish that the proposed transform facilitates detection of very weak activation patterns that cannot be detected with existing methods. Third, we show that the structure of the hierarchical dependency graph governing the activation process, and hence the network transform, can be learnt from very few (logarithmic in network size) independent snapshots of network activity.
12695 en Function class complexity and cluster structure with applications to transduction We relate function class complexity to structure in the function domain. This facilitates risk analysis relative to cluster structure in the input space which is particularly effective in semi-supervised learning. In particular we quantify the complexity of function classes defined over a graph in terms of the graph structure.
12696 en Multiclass-multilabel classification with more labels than examples We discuss multiclass-multilabel classification problems in which the set of possible labels is extremely large. Most existing multiclass-multilabel learning algorithms expect to observe a reasonably large sample from each class, and fail if they receive only a handful of examples with a given label. We propose and analyze the following two-stage approach: first use an arbitrary (perhaps heuristic) classification algorithm to construct an initial classifier, then apply a simple but principled method to augment this classifier by removing harmful labels from its output. A careful theoretical analysis allows us to justify our approach under some reasonable conditions (such as label sparsity and power-law distribution of label frequencies), even when the training set does not provide a statistically accurate representation of most classes. Surprisingly, our theoretical analysis continues to hold even when the number of classes exceeds the sample size. We demonstrate the merits of our approach on the ambitious task of categorizing the entire web using the 1.5 million categories defined on Wikipedia.
12697 en Empirical Bernstein boosting Concentration inequalities that incorporate variance information (such as Bernstein's or Bennett's inequality) are often significantly tighter than counterparts (such as Hoeffding's inequality) that disregard variance. Nevertheless, many state of the art machine learning algorithms for classification problems like AdaBoost and support vector machines (SVMs) extensively use Hoeffding's inequalities to justify empirical risk minimization and its variants. This article proposes a novel boosting algorithm based on a recently introduced principle--sample variance penalization--which is motivated from an empirical version of Bernstein's inequality. This framework leads to an efficient algorithm that is as easy to implement as AdaBoost while producing a strict generalization. Experiments on a large number of datasets show significant performance gains over AdaBoost. This paper shows that sample variance penalization could be a viable alternative to empirical risk minimization.
12698 en Sufficient covariates and linear propensity analysis Working within the decision-theoretic framework for causal inference, we study the properties of "sufficient covariates", which support causal inference from observational data, and possibilities for their reduction. In particular we illustrate the role of a propensity variable by means of a simple model, and explain why such a reduction typically does not increase (and may reduce) estimation efficiency.
12699 en Dirichlet process mixtures of generalised linear models We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLMs), a new method of nonparametric regression that accommodates continuous and categorical inputs, models a response variable locally by a generalized linear model. We give conditions for the existence and asymptotic unbiasedness of the DP-GLM regression mean function estimate; we then give a practical example for when those conditions hold. We evaluate DP-GLM on several data sets, comparing it to modern methods of nonparametric regression including regression trees and Gaussian processes.
12700 en Bayesian Gaussian process latent variable model We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.
12701 en Factored 3-way restricted Boltzmann machines for modeling natural images Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the "tiny images" data set. Even better features are obtained by then using standard binary RBM's to learn a deeper model.
12702 en Learning the structure of deep sparse graphical models Deep belief networks are a powerful way to model complex probability distributions. However, it is difficult to learn the structure of a belief network, particularly one with hidden units. The Indian buffet process has been used as a nonparametric Bayesian prior on the structure of a directed belief network with a single infinitely wide hidden layer. Here, we introduce the cascading Indian buffet process (CIBP), which provides a prior on the structure of a layered, directed belief network that is unbounded in both depth and width, yet allows tractable inference. We use the CIBP prior with the nonlinear Gaussian belief network framework to allow each unit to vary its behavior between discrete and continuous representations. We use Markov chain Monte Carlo for inference in this model and explore the structures learned on image data.
12703 en Solving the uncapacitated facility location problem using message passing algorithms The Uncapacitated Facility Location Problem (UFLP) is one of the most widely studied discrete location problems, whose applications arise in a variety of settings. We tackle the UFLP using probabilistic inference in a graphical model - an approach that has received little attention in the past. We show that the fixed points of max-product linear programming (MPLP), a convexified version of the max-product algorithm, can be used to construct a solution with a 3-approximation guarantee for metric UFLP instances. In addition, we characterize some scenarios under which the MPLP solution is guaranteed to be globally optimal. We evaluate the performance of both max-sum and MPLP empirically on metric and non-metric problems, demonstrating the advantages of the 3-approximation construction and algorithm applicability to non-metric instances.
12704 en Dense message passing for sparse principal component analysis We describe a novel inference algorithm for sparse Bayesian PCA with a zero-norm prior on the model parameters. Bayesian inference is very challenging in probabilistic models of this type. MCMC procedures are too slow to be practical in a very high-dimensional setting and standard mean-field variational Bayes algorithms are ineffective. We adopt a dense message passing algorithm similar to algorithms developed in the statistical physics community and previously applied to inference problems in coding and sparse classification. The algorithm achieves near-optimal performance on synthetic data for which a statistical mechanics theory of optimal learning can be derived. We also study two gene expression datasets used in previous studies of sparse PCA. We find our method performs better than one published algorithm and comparably to a second.
12705 en Focused belief propagation for query-specific inference With the increasing popularity of large-scale probabilistic graphical models, even "lightweight" approximate inference methods are becoming infeasible. Fortunately, often large parts of the model are of no immediate interest to the end user. Given the variable that the user actually cares about, we show how to quantify edge importance in graphical models and to significantly speed up inference by focusing computation on important parts of the model. Our algorithm empirically demonstrates convergence speedup by multiple times over state of the art
12706 en Exploiting feature covariance in high-dimensional online learning Some online algorithms for linear classification model the uncertainty in their weights over the course of learning. Modeling the full covariance structure of the weights can provide a significant advantage for classification. However, for high-dimensional, large-scale data, even though there may be many second-order feature interactions, it is computationally infeasible to maintain this covariance structure. To extend second-order methods to high-dimensional data, we develop low-rank approximations of the covariance structure. We evaluate our approach on both synthetic and real-world data sets using the confidence-weighted online learning framework. We show improvements over diagonal covariance matrices for both low and high-dimensional data.
12707 en REGO: Rank-based estimation of Renyi information using Euclidean graph optimization We propose a new method for a non-parametric estimation of Renyi and Shannon information for a multivariate distribution using a corresponding copula, a multivariate distribution over normalized ranks of the data. As the information of the distribution is the same as the negative entropy of its copula, our method estimates this information by solving a Euclidean graph optimization problem on the empirical estimate of the distribution's copula. Owing to the properties of the copula, we show that the resulting estimator of Renyi information is strongly consistent and robust. Further, we demonstrate its applicability in the image registration in addition to simulated experiments.
12708 en Coherent inference on optimal play in game trees Round-based games are an instance of discrete planning problems. Some of the best contemporary game tree search algorithms use random roll-outs as data. Relying on a good policy, they learn on-policy values by propagating information upwards in the tree, but not between sibling nodes. Here, we present a generative model and a corresponding approximate message passing scheme for inference on the optimal, off-policy value of nodes in smooth AND/OR trees, given random roll-outs. The crucial insight is that the distribution of values in game trees is not completely arbitrary. We define a generative model of the on-policy values using a latent score for each state, representing the value under the random roll-out policy. Inference on the values under the optimal policy separates into an inductive, pre-data step and a deductive, post-data part. Both can be solved approximately with Expectation Propagation, allowing off-policy value inference for any node in the (exponentially big) tree in linear time.
12709 en Nonlinear functional regression: A functional RKHS approach This paper deals with functional regression, in which the input attributes as well as the response are functions. To deal with this problem, we develop a functional reproducing kernel Hilbert space approach; here, a kernel is an operator acting on a function and yielding a function. We demonstrate basic properties of these functional RKHS, as well as a representer theorem for this setting; we investigate the construction of kernels; we provide some experimental insight.
12710 en On the relation between universality, characteristic kernels and RKHS embedding of measures Universal kernels have been shown to play an important role in the achievability of the Bayes risk by many kernel-based algorithms that include binary classification, regression, etc. In this paper, we propose a notion of universality that generalizes the notions introduced by Steinwart and Micchelli et al. and study the necessary and sufficient conditions for a kernel to be universal. We show that all these notions of universality are closely linked to the injective embedding of a certain class of Borel measures into a reproducing kernel Hilbert space (RKHS). By exploiting this relation between universality and the embedding of Borel measures into an RKHS, we establish the relation between universal and characteristic kernels. The latter have been proposed in the context of the RKHS embedding of probability measures, used in statistical applications like homogeneity testing, independence testing, etc.
12711 en On combining graph-based variance reduction schemes In this paper, we consider two variance reduction schemes that exploit the structure of the primal graph of the graphical model: Rao-Blackwellised w-cutset sampling and AND/OR sampling. We show that the two schemes are orthogonal and can be combined to further reduce the variance. Our combination yields a new family of estimators which trade time and space with variance. We demonstrate experimentally that the new estimators are superior, often yielding an order of magnitude improvement over previous schemes on several benchmarks.
12712 en Convex structure learning in log-linear models beyond pairwise potentials Previous work has examined structure learning in log-linear models with L1-regularization, largely focusing on the case of pairwise potentials. In this work we consider the case of models with potentials of arbitrary order, but that satisfy a hierarchical constraint. We enforce the hierarchical constraint using group L1-regularization with overlapping groups, and an active set method that enforces hierarchical inclusion allows us to tractably consider the exponential number of higher-order potentials. We use a spectral projected gradient method as a sub-routine for solving the overlapping group L1-regularization problem, and make use of a sparse version of Dykstra's algorithm to compute the projection. Our experiments indicate that this model gives equal or better test set likelihood compared to previous models.
12713 en Modeling annotator expertise: Learning when everybody knows a bit of something Supervised learning from multiple labeling sources is an increasingly important problem in machine learning and data mining. This paper develops a probabilistic approach to this problem when annotators may be unreliable (labels are noisy), but also their expertise varies depending on the data they observe (annotators may have knowledge about different parts of the input space). That is, an annotator may not be consistently accurate (or inaccurate) across the task domain. The presented approach produces classification and annotator models that allow us to provide estimates of the true labels and annotator variable expertise. We provide an analysis of the proposed model under various scenarios and show experimentally that annotator expertise can indeed vary in real tasks and that the presented approach provides clear advantages over previously introduced multi-annotator methods, which only consider general annotator characteristics.
12714 en Fluid dynamics models for low rank discriminant analysis We consider the problem of reducing the dimensionality of labeled data for classification. Unfortunately, the optimal approach of finding the low-dimensional projection with minimal Bayes classification error is intractable, so most standard algorithms optimize a tractable heuristic function in the projected subspace. Here, we investigate a physics-based model where we consider the labeled data as interacting fluid distributions. We derive the forces arising in the fluids from information theoretic potential functions, and consider appropriate low rank constraints on the resulting acceleration and velocity flow fields. We show how to apply the Gauss principle of least constraint in fluids to obtain tractable solutions for low rank projections. Our fluid dynamic approach is demonstrated to better approximate the Bayes optimal solution on Gaussian systems, including infinite dimensional Gaussian processes.
12716 en Half transductive ranking We study the standard retrieval task of ranking a fixed set of items given a previously unseen query and pose it as the half transductive ranking problem. The task is transductive as the set of items is fixed. Transductive representations (where the vector representation of each example is learned) allow the generation of highly nonlinear embeddings that capture object relationships without relying on a specific choice of features, and require only relatively simple optimization. Unfortunately, they have no direct out-of-sample extension. Inductive approaches on the other hand allow for the representation of unknown queries. We describe algorithms for this setting which have the advantages of both transductive and inductive approaches, and can be applied in unsupervised (either reconstruction-based or graph-based) and supervised ranking setups. We show empirically that our methods give strong performance on all three tasks.
12761 en Welcome Speech 
12762 en Building Structured Web Databases: A Midterm Report from the Cimple Project 
12763 en Timely Knowledge 
12764 en DBToaster: Aggressive Compilation Techniques for Online Aggregation 
12765 en PrDB: Increasing the Representational Power and Scaling Reasoning in Probabilistic Databases 
12766 en MCMC Inference Inside the DB for Extraction, Resolution, Alignment, Provenance and Queries 
12767 en Table Search 
12768 en WWT: A system for query-driven relation extraction from the semi-structured web 
12769 en Query-Driven Integration: The Q System 
12770 en Worth its Weight in Gold or Yet Another Resource — A Comparative Study of Wiktionary, OpenThesaurus and GermaNet In this paper, we analyze the topology and the content of anrange of lexical semantic resources for the German language constructedneither in a controlled (GermaNet), semi-controlled (OpenThesaurus), orncollaborative, i.e. community-based, manner (Wiktionary). For the firstntime, the comparison of the corresponding resources is performed at thenword sense level. For this purpose, the word senses of terms are automaticallyndisambiguated in Wiktionary and the content of all resourcesnis converted to a uniform representation. We show that the resources’ntopology is well comparable as they share the small world property andncontain a comparable number of entries, although differences in theirnconnectivity exist. Our study of content related properties reveals thatnthe German Wiktionary has a different distribution of word senses andncontains more polysemous entries than both other resources. We identifynthat each resource contains the highest number of a particular type ofnsemantic relation. We finally increase the number of relations in Wiktionarynby considering symmetric and inverse relations that have beennfound to be usually absent in this resource.
12771 en Combined Structured and Keyword-Based Search in Textually Enriched Entity-Relationship Graphs We present a novel method to combine simple, nexible, keyword-based search with expressive structured queries. We assume thatnan entity-relationship graph is given wherensome of the nodes are linked to unstructuredntext documents. The aim of our approachnis to efficiently search for relevant entities ornfacts about entities. Using several examples,nwe demonstrate the new types of queryingnthat can be realized by our approach.
12772 en Aligning Sense Inventories in Wikipedia and Wordnet In this paper, we study the alignment ofnWikipedia articles and WordNet synsets. Therefore,nwe propose a method to convert Wikipedianto a sense inventory. We show that an alignednsense inventory of both resources has two majornbenefits: the coverage of senses can be increasednand enhanced information about aligned sensesncan be obtained. Our study and conclusions arenbased on human annotations of sense alignments.
12773 en ProbaMap: a scalable tool for discovering probabilistic mappings between taxonomies In this paper, we investigate a principlednapproach for defining and discovering probabilisticnmappings between two taxonomies.nFirst, we compare two ways of modelingnprobabilistic mappings which are compatiblenwith the logical constraints declared inneach taxonomy. Then we describe a generatenand test algorithm (called ProbaMap)nwhich minimizes the number of calls to thenprobability estimator for determining thosenmappings whose probability exceeds a certainnthreshold. Finally, we provide an experimentalnanalysis of this approach.
12774 en Entity Disambiguation using Relations extracted from Wikipedia We present an approach for the disambiguation of textual mentions of ambiguous names:ndisambiguation means here the identificationnof the true entity denoted by a name phrasenappearing in a query context through its assignment to the corresponding Wikipedia article. If this article does not exist, we assign this query to a default entity. Ambiguity of names is a major problem in information retrieval and causes uncertainty innthe assignment of name phrases to existingnknowledge base entries. We propose a kernelnclassifier to approach this problem and compare two Wikipedia structures to construct anrich feature space. The first approach reliesnon Wikipedia categories, the second on relations constructed from Wikipedia's hypernlink structure.nWe evaluate both approaches on the Germannversion of Wikipedia and show that both outperform a baseline approach using simple cosine similarity.
12775 en Meaning Propagation 
12776 en The Web’s Many Models 
12777 en Finding Frequent and Interesting Triples in Text 
12778 en Mining Commonsense Knowledge From Personal Stories in Internet Weblogs Recent advances in automated knowledge basenconstruction have created new opportunities tonaddress one of the hardest challenges in ArtificialnIntelligence: automated commonsense reasoning.nIn this paper, we describe our recent efforts innmining commonsense knowledge from thenpersonal stories that people write about theirnlives in their Internet weblogs. We summarizenthree preliminary investigations that involve thenapplication of statistical natural languagenprocessing techniques to corpora of millions ofnweblog stories, and outline our current approachnto solving a number of outstanding technicalnchallenges.
12779 en Automatic Extraction of Human Activity Knowledge from Method-Describing Web Articles Knowledge on daily human activities in various domains is invaluable for many customized user services that can benefit from context-awareness or activity predictions. Past approaches to constructing a knowledge base of this kind have been domain-specific and not scalable. A recent attempt to extract activities of daily living (ADL) from Web resources deal with activities and objects involved in achieving them but not the sequence of actions in an activity. This paper describes an approach to automatically extracting human activity knowledge from Web articles that describe methods for performing tasks in a variety of domains. The target knowledge base is comprised of activity goals, actions, and ingredients, which are extracted with syntactic pattern-based and probabilistic machine learning based methods. The result is evaluated for accuracy and coverage against some baselines.
12780 en Robust Web Extraction, A Principled Approach On script-generated web sites, many documents share common HTML treenstructure, allowing wrappers to effectively extract information ofninterest. Of course, the scripts and thus the tree structure evolvenover time, causing wrappers to break repeatedly, and resulting in anhigh cost of maintaining wrappers. In this paper, we explore a novelnapproach: we use temporal snapshots of web pages to develop antree-edit model of HTML, and use this model to improve wrappernconstruction. We view the changes to the tree structure asnsuppositions of a series of edit operations: deleting nodes, insertingnnodes and substituting labels of nodes. The tree structures evolve bynchoosing these edit operations stochastically.nnOur model is attractive in that the probability that a source tree hasnevolved into a target tree can be estimated efficiently -- innquadratic time in the size of the trees -- making it a potentiallynuseful tool for a variety of tree-evolution problems. We give annalgorithm to learn the probabilistic model from training examplesnconsisting of pairs of trees, and apply this algorithm to collectionsnof web-page snapshots to derive HTML-specific tree edit models.nFinally, we describe a novel wrapper-construction framework that takesnthe tree-edit model into account, and compare the quality of resultingnwrappers to that of traditional wrappers on synthetic and real HTMLndocument examples.
12781 en Reinforcement Learning for Structured Data Labeling 
12821 en Designing Online Communities from Theory Online communities are the fastest-growing portion of the Internet and provide members with information, social support, and entertainment. While a minority, such as Wikipedia, MySpace, Facebook and the Apache Server project are highly successful, many others fail. To be successful, online communities must overcome challenges common in almost all groups, organizations and voluntary associations – solving problems of start-up, recruitment, socialization, commitment, contribution, coordination and regulation of behavior. The social sciences can tell us a lot about how to create thriving online communities. Social science theories can inform choices about how to get a community started, integrate newcomers, encourage commitment, regulate behavior when there are conflicts, motivate contributions, and coordinate those contributions to maximize benefits for the community. nnThis talk focuses on ways to build members’ commitment to online communities, based on theories of social identity and interpersonal bonds. It provides an overview of the relevant theory, describes results of a 6-month field experiment in which an existing site was redesigned based on principles derived from social identity and interpersonal-bond theories, and describes the results of an agent-based model that examines how different approaches to moderating the content in a group influence social identity and interpersonal bonds. n
12822 en Behavioral Experiments in Strategic Networks For four years now, we have been conductingn“medium-scale” experiments in how humannsubjects behave in strategic and economic settingsnmediated by an underlying social networknstructure. We have explored a wide rangenof networks inspired by generative modelsnfrom the literature, and a diverse set of collectivenstrategic problems, including biased voting,ngraph coloring, consensus, and networkedntrading. These experiments have yielded anwealth of both specific findings and emergingngeneral themes about how populations of humannsubjects interact in strategic networks.nKearns will review these findings and themes,nwith an emphasis on the many more questionsnthey raise than answer.nMichael Kearns is a professor of computernand information science at the University ofnPennsylvania, where he holds the National CenternChair in Resource Management and Technology.nHe is the founding director of Penn Engineering’snnew Market and Social Systems Engineeringn(MKSE) program. Kearns hasnsecondary appointments in the Statistics andnOp erations and Information Managementn(OPIM) departments of the Wharton School,nand is an affiliated faculty member of Penn’snApplied Math and Computational Science graduatenprogram. Kearns also serves as an advisornto Yodle, kaChing, Invite Media, and Kwedit.nHis research interests include topics in machinenlearning, algorithmic game theory, social networks,ncomputational finance, and artificial intelligence.nMost recently, he has been conductingnhuman-subject experiments on strategic andneconomic interaction in social networks. Kearnsnreceived his B.S in mathematics and computernscience from the University of California atnBerkeley in 1985, and his Ph.D. in computernscience from Harvard University in 1989. Henhas served as the program chair of NIPS, AAAI,nCOLT, and ACM EC. He is a member of thenNIPS Foundation and the steering committeenfor the Snowbird Conference on Learning, andnserves on the editorial board of The MIT Pressnseries on adaptive computation and machinenlearning.
12823 en Got Facebook? Investigating What's Social About Social Media Watkins has been researching young people’snmedia behaviors for more than ten years andnhe teaches in the departments of Radio - Television -nFilm and Sociology and the Center fornAfrican and African American Studies. His recentnbook, The Young and the Digital: Whatnthe Migration to Social Network Sites, Games,nand Anytime, Anywhere Media Means for OurnFuture, explores young people’s dynamic engagementnwith social media, online games, mobilenphones, and communities like Facebooknand MySpace. A few short years ago Facebooknwas widely viewed as a site for college studentsnwho used the platform primarily for socialnpurposes - making new friends, stalkingneach other, and posting pictures from a weekendnof partying and drinking. Watkins’ talk willnshow how young people’s participation innFacebook is actually more complex than thenpopular images and myths suggests. Drawingnfrom both survey data and in-depth interviewsnwith current college students and recent collegengraduates the talk considers how the usenof Facebook evolves across the young life-cycle.nThe talk also discusses the question: what’snsocial about social media?
12824 en Words as reflections of psychological state Pennebaker’s research explores the links betweenntraumatic experiences, expressive writing,nnatural language use, and physical andnmental health. Across numerous real-world contexts,nhis studies have demonstrated that thenwords people use serve as powerful reflectionsnof their personality and social worlds. For example,nhis analyses of over 1,000 people whonwrote blogs in the weeks before and afternSeptember 11 terrorist attacks showed that thenjournal entries revealed pronounced psychologicalnchanges in response to the attacks. His talknwill address the fact that most language-basedncomputer programs analyze content-heavynwords (such as nouns and regular verbs) andnthrow out “junk” words (such as pronouns,nprepositions, and articles) to understand people’snthinking, buying, searching, and other behaviors.nAccording to Pennebaker’s work, analysesnof junk words can yield important insightsninto the social and psychological processes ofnpeople across cultures and languages. He willntalk about his recent studies, which point to thenrole of junk words in identifying personality,ndepression, status, honesty, group cohesiveness,nand other individual and group behaviors.
12825 en Examining Online and Offline Communication Processes in Online Dating and Social Network Sites Ellison’s research has been examining the waysnin which new information and communicationntechnologies shape social processes, and vicenversa. Her recent research has focused on thensocial capital implications of social network sitenuse and issues of self-presentation, relationshipnformation and maintenance, and impressionnformation in online contexts. Ellison’s talk willnpresent research examining how people usenthe Internet to engage in self-presentation, formnand maintain relationships, and garner socialncapital benefits. The talk will focus on two newnstreams of research: one examining the ways innwhich people use Facebook to connect withnothers and access social support and the othernexploring online dating participants’ perceptionsnregarding acceptable and unacceptablenmisrepresentation in online dating profiles.
12826 en Sequential Influence Models in Social Networks The spread of influence among individuals in a social network can be naturally modeled in a probabilistic framework, but it is challenging to reason about differences between various models as well as to relate these models to actual social network data. Here we consider two of the most fundamental definitions of influence, one based on a small set of "snapshot'' observations of a social network and the other based on detailed temporal dynamics. The former is particularly useful because large-scale social network data sets are often available only in snapshots or crawls. The latter however provides a more detailed process model of how influence spreads. We study the relationship between these two ways of measuring influence, in particular establishing how to infer the more detailed temporal measure from the more readily observable snapshot measure. We validate our analysis using the history of social interactions on Wikipedia; the result is the first large-scale study to exhibit a direct relationship between snapshot and temporal models of social influence.n
12827 en Who Acquires Friends Through Social Media and Why? “Rich Get Richer” versus “Seek and Ye Shall Find” There is an ongoing debate, not just among academics but in popular culture, about whether social media can expand people’s social networks, and whether online friends can be “real” friends. The debate refuses to die. This paper addresses this question subjectively, from the point of view of the user, and examines the predictors of acquiring new friends through social media use. This is a multi-method study with quantitative (n=617) and qualitative sections. Some previous studies have found a “rich-get-richer” effect where people who are socially active offline appear to benefit most from online interactions. This paper is based on the idea that whether online friends can be real friends may be subject to a self-fulfilling prophecy: those who do not believe that online friendships can be real friendships are not likely to make such connections. I compare the “Rich Get Richer” and “Seek and Ye Shall Find” models by examining relationships between the amount of offline socializing, amount of online social activity and the belief in online friendships. I also qualitatively examine reasons cited by respondents as to why online sociality is or is not a plausible route to meaningful friendship. The results show strong support for one of the earliest theories of computer-mediated-communication: hyperpersonal interaction. It appears that a sizable portion of the young generation finds online interaction positive because it is perceived to concentrate on the conversation itself, rather than on appearances, and is seen as freer of social judgments. African-Americans are significantly more likely to meet new friends online. There also appear to be underlying personality traits, apart from traditional demographic variables, that divide the population in terms of their attitude towards online sociality.
12828 en To Be a Star Is Not Only Metaphoric: From Popularity to Social Linkage The emergence of online platforms allowing to mix self publishing activities and social networking offers new possibilities for building online reputation and visibility. In this paper we present a method to analyze the online popularity that takes into consideration both the success of the published content and the social network topology. First, we adapt the Kohonen self organizing maps in order to cluster the users of online platforms depending on their audience and authority characteristics. Then, we perform a detailed analysis of the manner nodes are organized in the social network. Finally, we study the relationship between the network local structure around each node and the corresponding user’s popularity. We apply this method to the MySpace music social network. We observe that the most popular artists are centers of star shaped social structures and that it exists a fraction of artists who are involved in community and social activity dynamics independently of their popularity. This method based on a learning algorithm and on network analysis appears to be a robust and intuitive technique for a rich description of the online behavior.
12829 en ePluribus: Ethnicity on Social Networks We propose an approach to determine the ethnic breakdown of a population based solely on people's names and data provided by the U.S. Census Bureau. We demonstrate that our approach is able to predict the ethnicities of individuals as well as the ethnicity of an entire population better than natural alternatives. We apply our technique to the population of U.S. Facebook users and uncover the demographic characteristics of ethnicities and how they relate. We also discover that while Facebook has always been diverse, diversity has increased over time leading to a population that today looks very similar to the overall U.S. population. We also find that different ethnic groups relate to one another in an assortative manner, and that these groups have different profiles across demographics, beliefs, and usage of site features.n
12830 en The Social Dynamics of Economic Activity in a Virtual World This paper examines social structures underlying economic activity in Second Life (SL), a massively multiplayer virtual world that allows users to create and trade virtual objects and commodities. We find that users conduct many of their transactions both within their social networks and within groups. Using frequency of chat as a proxy of tie strength, we observe that free items are more likely to be exchanged as the strength of the tie increases. Social ties particularly play a significant role in paid transactions for sellers with a moderately sized customer base. We further find that sellers enjoying repeat business are likely to be selling to niche markets, because their customers tend to be contained in a smaller number of groups. But while social structure and interaction can help explain a seller's revenues and repeat business, they provide little information in the forecasting a seller's future performance. Our quantitative analysis is complemented by a novel method of visualizing the transaction activity of a seller, including revenue, customer base growth, and repeat business.
12831 en Your Brain on Facebook: Neuropsychological Associations with Social Versus other Media We measured individuals’ mental associations between four types of media (books, television, social/Facebook, and general informational web pages) and relevant concepts (Addictive, Story, Interesting, Frivolous, Personal, and Useful) using three different measurements: a Likert scale questionnaire, a speeded Yes/No judgment task, and electrical brain activity. The three measures were designed to capture associations at different levels of mental processing, from very automatic (electrical brain activity) to conscious and reasoned (questionnaire). At more conscious levels of cognitive processing, Facebook was seen as interesting, addictive, and highly personal. Results for the electrical brain activity measure show that Facebook tells less of a story and, surprisingly, is less personal than other forms of media. We discuss differences in results across the three measures and how our findings can inform the design of future social media systems.n
12832 en Towards Social Causality: An Analysis of Interpersonal Relations in Online Blogs and Forums In this paper we present encouraging preliminary results into the problem of social causality (causal reasoning used by intelligent agents in a social environment) in online social interactions based on a model of reciprocity. At every level, social relationships are guided by the shared understanding that most actions call for appropriate reactions, and that inappropriate reactions require management. Thus, we present an analysis of interpersonal relationships in English reciprocal contexts. Specifically, we rely here on a large and recently built database of 10,882 reciprocal relation instances in online media. The resource is analyzed along a set of novel and important dimensions: symmetry, affective value, gender}, and {\em intentionality of action which are highly interconnected. At a larger level, we automatically generate {\em chains of causal relations} between verbs indicating interpersonal relationships. Statistics along these dimensions give insights into people's behavior, judgments, and thus their social interactions.
12833 en How Does the Data Sampling Strategy Impact the Discovery of Information Diffusion in Social Media? Platforms such as Twitter have provided researchers with ample opportunities to analytically study social phenomena. There are however, significant computational challenges due to the enormous rate of production of new information: researchers are therefore, often forced to analyze a judiciously selected “sample” of the data. Like other social media phenomena, information diffusion is a social process–it is affected by user context, and topic, in addition to the graph topology. This paper studies the impact of different attribute and topology based sampling strategies on the discovery of an important social media phenomena–information diffusion.nnWe examine several widely-adopted sampling methods that select nodes based on attribute (random, location, and activity) and topology (forest fire) as well as study the impact of attribute based seed selection on topology based sampling. Then we develop a series of metrics for evaluating the quality of the sample, based on user activity (e.g. volume, number of seeds), topological (e.g. reach, spread) and temporal characteristics (e.g. rate). We additionally correlate the diffusion volume metric with two external variables–search and news trends. Our experiments reveal that for small sample sizes (30%), a sample that incorporates both topology and user context (e.g. location, activity) can improve on naive methods by a significant margin of ~15-20%.
12834 en Photo Tagging Over Time: A Longitudinal Study of the Role of Attention, Network Density, and Motivations Along with the growth in artifact sharing in online communities such as Flickr, YouTube, and Facebook comes the demand for adding descriptive meta-information, or tags. Tags help individuals to organize and communicate the content and context of their work for themselves and for others. This longitudinal study draws on research in social psychology, network theory and online communities to explain tagging over time. Our findings suggest that tagging increases as a contributor receives attention from others in the community. Further, we find that the more a user's network neighbors are connected to each other directly, the less the focal user will tend to tag his photos. However, density interacts with attention such that those who are surrounded by a dense ego network respond more to attention than others whose ego networks are sparsely interconnected. Unexpectedly, we find no direct correlation between tagging and the individual motivations of enjoyment and commitment. While commitment is not directly associated with tagging, there is an interaction effect such that the effect of commitment on tagging is positive for users with low-density ego networks and negative when a user is surrounded by a high-density network. Directions for future research as well as implications for theory and practice are discussed.
12835 en Microblogging Inside and Outside the Workplace Microblogging has recently generated a lot of research interest. Yet very little is known about how corporate employees use microblogging tools. This study examined microblogging in the workplace by conducting a content analysis comparing posts from individuals who were using an internal proprietary tool and Twitter simultaneously. In both settings, posts that provided information or were directed to others were more common than posts on status. Within these categories, it was more frequent to provide information externally than internally but more common to ask questions either through broadcast or directed posts internally than externally. Qualitative interviews explored users’ motivations regarding microblogging behavior. The paper concludes with a discussion of the implications of microblogging for business use.
12836 en Measuring User Influence on Twitter: The Million Follower Fallacy Directed links in social media could represent anything from intimate friendships to common interests, or even a passion for breaking news or celebrity gossip. Such directed links determine the flow of information and hence indicate a user's influence on others — a concept that is crucial in sociology and viral marketing. In this paper, using a large amount of data collected from Twitter, we present an in-depth comparison of three measures of influence: indegree, retweets, and mentions. Based on these measures, we investigate the dynamics of user influence across topics and time. We make several interesting observations. First, popular users who have high indegree are not necessarily influential in terms of spawning retweets or mentions. Second, most influential users can hold significant influence over a variety of topics. Third, influence is not gained spontaneously or accidentally, but through concerted effort such as limiting tweets to a single topic. We believe that these findings provide new insights for viral marketing and suggest that topological measures such as indegree alone reveals very little about the influence of a user.
12837 en The Directed Closure Process in Information Networks with an Analysis of Link Formation on Twitter It has often been taken as a working assumption that directed links in information networks are frequently formed by "short-cutting'' a two-step path between the source and the destination — a kind of implicit "link copying'' analogous to the process of triadic closure in social networks. Despite the role of this assumption in theoretical models such as preferential attachment, it has received very little direct empirical investigation. Here we develop a formalization and methodology for studying this type of directed closure process, and we provide evidence for its important role in the formation of links on Twitter. We then analyze a sequence of models designed to capture the structural phenomena related to directed closure that we observe in the Twitter data.
12838 en Characterizing Microblogs with Topic Models As microblogging grows in popularity, services like Twitter are coming to support information gathering needs above and beyond their traditional roles as social networks. But most users’ interaction with Twitter is still primarily focused on their social graphs, forcing the often inappropriate conflation of “people I follow” with “stuff I want to read.” We characterize some information needs that the current Twitter interface fails to support, and argue for better representations of content for solving these challenges. We present a scalable implementation of a partially supervised learning model (Labeled LDA) that maps the content of the Twitter feed into dimensions. These dimensions correspond roughly to substance, style, status, and social characteristics of posts. We characterize users and tweets using this model, and present results on two information consumption oriented tasks.
12839 en Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment Twitter is a microblogging website where users read and write millions of short messages on a variety of topics every day. This study uses the context of the German federal election to investigate whether Twitter is used as a forum for political deliberation and whether online messages on Twitter validly mirror offline political sentiment. Using LIWC text analysis software, we conducted a content-analysis of over 100,000 messages containing a reference to either a political party or a politician. Our results show that Twitter is indeed used extensively for political deliberation. We find that the mere number of messages mentioning a party reflects the election result. Moreover, joint mentions of two parties are in line with real world political ties and coalitions. An analysis of the tweets’ political sentiment demonstrates close correspondence to the parties' and politicians’ political positions indicating that the content of Twitter messages plausibly reflects the offline political landscape. We discuss the use of microblogging message content as a valid indicator of political sentiment and derive suggestions for further research.
12840 en From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series We connect measures of public opinion measured from polls with sentiment measured from text. We analyze several surveys on consumer conﬁdence and political opinion over the 2008 to 2009 period, and ﬁnd they correlate to sentiment word frequencies in contemporaneous Twitter messages. While our results vary across datasets, in several cases the correlations are as high as 80%, and capture important large-scale trends. The results highlight the potential of text streams as a substitute and supplement for traditional polling. consumer conﬁdence and political opinion, and can also pre- dict future movements in the polls. We ﬁnd that temporal smoothing is a critically important issue to support a successful model.
12841 en Information Contagion: an Empirical Study of the Spread of News on Digg and Twitter Social Networks Social networks have emerged as a critical factor in information dissemination, search, marketing, expertise and influence discovery, and potentially an important tool for mobilizing people. Social media has made social networks ubiquitous, and also given researchers access to massive quantities of data for empirical analysis. These data sets offer a rich source of evidence for studying dynamics of individual and group behavior, the structure of networks and global patterns of the flow of information on them. However, in most previous studies, the structure of the underlying networks was not directly visible but had to be inferred from the flow of information from one individual to another. As a result, we do not yet understand dynamics of information spread on networks or how the structure of the network affects it. We address this gap by analyzing data from two popular social news sites. Specifically, we extract social networks of active users on Digg and Twitter, and track how interest in news stories spreads among them. We show that social networks play a crucial role in the spread of information on these sites, and that network structure affects dynamics of information flow.
12842 en Tweeting from the Town Square: Measuring Geographic Local Networks This paper examines tweets about two geographically local events - a shooting and a building collapse - that took place in Wichita, Kansas and Atlanta, Georgia, respectively. Most Internet research has focused on examining ways the Internet can connect people across long distances, yet there are benefits to being connected to others who are nearby. People in close geographic proximity can provide real-time information and eyewitness updates for one another about events of local interest. We first show a relationship between structural properties in the Twitter network and geographic properties in the physical world. We then describe the role of mainstream news in disseminating local information. Last, we present a poll of 164 users’ information seeking practices. We conclude with practical and theoretical implications for sharing information in local communities.
12843 en StepGreen.org: Increasing Energy Saving Behaviors via Social Networks Decades of research have explored factors that can influence green behavior. However, much less is known about how technology in general, and social technologies in particular can motivate people to participate in green activities. In this paper we describe the goals, design and evaluation of StepGreen.org, a site intended to promote energy-saving behaviors. We present the results of a field study, during which participants chose to engage in different actions and reported when they had completed them. Our results suggest that motivating factors like public commitment and competition are effective, and better leveraging these factors will likely lead to even greater appeal and effectiveness. Our contribution is to create an understanding of the impact of different decisions on the success of StepGreen.org that can benefit the designers of other multifaceted, online systems for behavior change.
12844 en Governance in Social Media: A Case Study of the Wikipedia Promotion Process Social media sites are often guided by a core group of committed users engaged in various forms of {governance. A crucial aspect of this type of governance is deliberation, in which such a group reaches decisions on issues of importance to the site. Despite its crucial — though subtle — role in how a number of prominent social media sites function, there has been relatively little investigation of the deliberative aspects of social media governance. Here we explore this issue, investigating a particular deliberative process that is extensive, public, and recorded: the promotion of Wikipedia admins, which is determined by elections that engage committed members of the Wikipedia community. We find that the group decision-making at the heart of this process exhibits several fundamental forms of relative assessment. First we observe that the chance that a voter will support a candidate is strongly dependent on the relationship between characteristics of the voter and the candidate. Second we investigate how both individual voter decisions and overall election outcomes can be based on models that take into account the sequential, public nature of the voting.
12846 en Responses to Remixing on a Social Media Sharing Website In this paper we describe the ways participants of the Scratch online community, primarily young people, engage in remixing of each others' shared animations, games, and interactive projects. In particular, we try to answer the following questions: How do users respond to remixing in a social media environment where remixing is explicitly permitted? What qualities of originators and their projects correspond to a higher likelihood of plagiarism accusations? Is there a connection between plagiarism complaints and similarities between a remix and the work it is based on? Our findings indicate that users have a very wide range of reactions to remixing and that as many users react positively as accuse remixers of plagiarism. We test several hypotheses that might explain the high number of plagiarism accusations related to original project complexity, cumulative remixing, originators' integration into remixing practice, and remixee-remixer project similarity, and find support for the first and last explanations.
12847 en ICWSM — A Great Catchy Name: Semi-Supervised Recognition of Sarcastic Sentences in Online Product Reviews Sarcasm is a sophisticated form of speech act widely used in online communities. Automatic recognition of sarcasm is, however, a novel task. Sarcasm recognition could contribute to the performance of review summarization and ranking systems. This paper presents SASI, a novel Semi-supervised Algorithm for Sarcasm Identification that recognizes sarcastic sentences in product reviews. SASI has two stages: semi-supervised pattern acquisition, and sarcasm classification. We experimented on a data set of about 66000 Amazon reviews for various books and products. Using a gold standard in which each sentence was tagged by 3 annotators, we obtained precision of 77% and recall of 83.1% for identifying sarcastic sentences. We found some strong features that characterize sarcastic utterances. However, a combination of more subtle pattern-based features proved more promising in identifying the various facets of sarcasm. We also speculate on the motivation for using sarcasm in online communities and social networks.
12849 en Star Quality: Aggregating Reviews to Rank Products and Merchants Given a set of reviews of products or merchants from a wide range of authors and several reviews websites, how can we measure the true quality of the product or merchant? How do we remove the bias of individual authors or sources? How do we compare reviews obtained from different websites, where ratings may be on different scales (1-5 stars, A/B/C, etc.)? How do we filter out unreliable reviews to use only the ones with "star quality"? Taking into account these considerations, we analyze data sets from a variety of different reviews sites (the first paper, to our knowledge, to do this). These data sets include 8 million product reviews and 1.5 million merchant reviews. We explore statistic - and heuristic - based models for estimating the true quality of a product or merchant, and compare the performance of these estimators on the task of ranking pairs of objects. We also apply the same models to the task of using Netflix ratings data to rank pairs of movies, and discover that the performance of the different models is surprisingly similar on this data set.
12851 en Inverse Problem Solutions in Heat Transfer: Applications in Development of New Manufacturing Technologies for Ceramics and Glass, and Studies of Advanced Materials Thermophysical Properties The radiative and conductive heat transfer (RCHT) dominates in a lot of modern machines and manufacturing technologies. At Bauman MSTU since the end of 1970’s there have been worked out methods, algorithms and software package “CAR” (Conduction and Radiation) for solving direct problems (DP) and inverse problems (IP) of RCHT in composite materials and constructions. Numerical methods of solving DP, extreme statements of IP, numerical optimization of residual functionals and iterative regularization methods form the theoretical base of the package “CAR” approaches. The package “CAR” was used for researches on project reusable launch vehicles “Buran”, “Hopper”, large space frame “SOFORA”, large deployable space antenna NPO EGS. For today the software package “CAR” is the base for the current studies of advanced materials for aerospace applications and in development of new manufacturing technologies.nIn the lecture two branches of RCHT IP application are presented. The first branch connected with studies of advanced materials thermophysical properties. The statements are formulated as coefficient IP. Short description of experimental technique and some features of combined heat transfer in porous semitransparent material are given. The second branch connected with designing new manufacturing technologies for ceramics and glass. The statements are formulated as boundary IP and assume the determination of time variation of incident heat flux on the glass surface. Mathematical modelling have been performed in the frame of development three heat treatment technologies as infrared annealing, thermal polishing and sintering of glassceramic. n
12933 en Welcome 
12934 en Haitian Creole: Developing MT for a Low Data Language 
12935 en Translation Technology for Fighting World Poverty 
12936 en Recent Progresses on MT and Language Information Processing of CCLIE & BIT 
12937 en TAUS Data Association: News and Roadmap 
12938 en META-NET: Towards the Multilingual Europe Technology Alliance 
12939 en Evaluation, Localization and Open Source Tools in EuroMatrixPlus 
12940 en Questions and discussion 
12941 en Community Translation, what is it, when to use it, how to manage it 
12942 en Welocalize Open Source efforts and MT integration 
12943 en LetsMT! – Towards Cloud-Based Service for MT Generation 
12944 en Maltese 
12945 en Latvian 
12946 en Dutch 
12947 en Short statements 
12948 en Asia Online Technology Platform Overview & Vision 
12949 en Free/Open-Source Machine Translation: The Apertium Platform 
12950 en Flexible and efficient management of translation quality 
12951 en The Evolution of the European Machine Translation Programme at the EPO 
12952 en New Developments on MT at the European Commission 
12953 en 2010: The Industrialisation of MT 
12954 en Language Change and Linguistic Diversity – Future Challenges for MT 
12955 en Closing 
12996 en Obama's Imperialist Policies Following a New York Peace Action Benefit viewing of Karen Malpede's new play, Prophesy, Noam Chomsky criticizes Obama's rightwing policies, war making, medical care, coziness with commercial interests . He warns of the coming war in Kandahar and Israel's possible attack on Iran that could go nuclear. In the Q & A, moderated by Karen, Chomsky comments on BP's Gulf Oil disaster, the likely next financial crisis, the Free Gaza Flotilla incident, urges Guantanamo being returned to Cuba and tortured detainees either being tried or released.
12997 en Interview with Noam Chomsky Question n.1: nDo you currently see an elephant room of cognitive science, just like you named one 50 years ago—I guess that's a reference to my critique of radical behaviorism—something that needs addressing that gets too little attention?nnQuestion n.2: nWhat are some of your criticisms of today's anarchist movement? How to be as effective as possible is something many anarchists overlook, and you're perhaps the most prolific voice on this topic, so your thoughts would be very influential.nnnQuestion n.3: nAs far as we favor a stateless society in the long run, it would be a mistake to work for the elimination—I've said that it would be a mistake to work for the elimination of the state in the short run, and we should be trying to strengthen the state, 'cause it's needed on the check of power of large corporations. Yet the tendency of a lot of anarchist research—my own, too—is to show that the power of large corporations derives from state privilege, and governments tend to get captured by concentrated private interests. That would seem to imply that the likely beneficiaries of a more powerful state is going to be the same corporate elite we're trying to oppose. So if business both derives from the state and is so good at capturing the state, why isn't abolishing the state a better strategy for defeating business power than enhancing the state's power would be?nnQuestion n.4:nI guess the question that comes to mind that just grows out of these comments is there's a very large number of people who are committed sincerely and rightly to the kind of long-term objectives that anarchists have always tried to uphold. And the question is: why can't we get together and decide on—and instead of, you know, condemning one another for not doing things exactly the way we do, why can't we try to formulate concrete proposals which combine two properties?
13231 en Welcome and opening remarks 
13232 en Liquid crystal nematic configurations on thin films We shall be concerned with the influence of surface geometry on orientational ordering in liquid crystal thin films. We will focus on universal mechanisms, such as merging and splitting of topological defects, reshaping of defect cores, positioning of defects and defect induced global system transformations. All phenomena are analyzed in the frame of 2D or 3D Landau-de Gennes theory in terms of a tensorial order parameter. For the cases studied we discuss future research trends and possible applications. We first consider possible defect structures and merging of defects in thin nematic shells of various geometries using a 2D approach [1], which has been recently developed by the Pavia group. It is well known [2] that the equilibrium texture of a spherical film exhibits four defects of winding number 1/2 sitting at the vertices of a regular tetrahedron to maximize their mutual distance. By varying the film geometry, the position of the defects and their structure could be altered. Among others things, we show that for sufficiently prolate ellipsoidal shells the four defects merge pairwise at the poles leaving only two +1 defects. We next consider external field induced transformations of defect cores, positioning of defects, and defect induced or suppressed surface transitions in 3D. For this purpose we consider a hybrid cylindrical cell where we topologically stabilize a boojum in the center of the upper confining plate [3]. If an external electric field is applied, the boojum could be either stretched towards the cell interior or expelled from the cell, depending on the sign of the field anisotropy constant. The stretching of the boojum is also accompanied by a substantial widening of the defect core. Depending on conditions, the presence of the boojum could either substantially increase or depress an external field driven surface biaxial transition.nFinally we consider temperature driven surface wetting and dewetting transitions in a thin film. We show that for appropriate conditions the temperature difference between the surface and bulk phase transition could be anomalously increased with respect to already reported related phenomena.n
13233 en Nearly Isostatic Periodic Lattices In 1864, James Clerk Maxwell showed that a system of N point particles in d-dimensions is mechanically stable only if the number, z, of two-point contacts between particles exceeds z_c = 2d. Systems with z=z_c are isostatic. Recent work confirms that randomly packed spheres are isostatic at the point J where the volume fraction \phi reaches the critical value \phi_c necessary to support shear and that the mechanics of this isostatic state determine behavior at volume fractions above \phi_c. Infinite square and kagome lattices with nearest neighbor springs are isostatic, but their finite counterparts have N^{1/2} ``floppy" modes of zero frequency. This talk will discuss the mechanical properties and phonon spectrum of nearly isostatic versions of these lattices in which next-nearest-neighbor springs with a variable spring constant are added either homogeneously or randomly. In particular, it will show that these lattices exhibit characteristic lengths that diverge as 1/(z-z_c) and frequencies that vanish as (z-z_c) in agreement with general arguments by the Chicago group. The shear elastic modulus depends on the geometry of the isostatic network and is not universal. Response near z=z_c in the random case is highly nonaffine. This talk will also discuss an isostatic variant of the kagome lattice that has a vanishing bulk modulus and a negative Poisson ratio and whose floppy isostatic modes are not found in the phonon spectrum with periodic boundary conditions. Finally, if time permits, the application of some of these ideas to networks of semi-flexible polymers will be discussed.
13234 en Optically- and electrically-controlled topological defect architectures in confined chiral liquid crystals Liquid crystal (LC) defects have recently attracted a great deal of interest due to their ability of pre-defining the symmetry of colloidal interactions in nematics and enabling the existence of distinct thermodynamic phases such as the twist grain boundary and blue phases. In confined nematic and cholesteric fluids, defects appear as a result of temperature quenching, symmetry-breaking phase transitions, mechanical stresses, etc. These defects commonly annihilate to minimize the free energy and are hard to be controlled or utilized for applications in a reliable way. This lecture will discuss the facile optical creation and multistable optical and electrical switching of 3D localized defect configurations in confined chiral nematic LCs. By use of focused Laguerre-Gaussian vortex laser beams with different optical phase singularities, we generate topological LC defect architectures containing both point and line singularities bound to each other by the director twist and forming stable or metastable configurations. While being generated, the defect architectures are probed in 3D by multimodal labeling-free nonlinear optical imaging that incorporates simultaneous study with three-photon and two-photon excitation self-fluorescence microscopy, second harmonic generation microscopy, and broadband coherent anti-Stokes Raman scattering polarizing microscopy, in addition to the conventional fluorescence confocal polarizing microscopy. In chiral nematic LCs confined into homeotropic cells, the laser-generated topological defects embed the localized 3D twist into the uniform background of confinement-untwisted director field, forming localized chiro-elastic particle-like excitations - dubbed “torons” [1] - that interact with each other via short-range repulsive elasticity-mediated interactions. In the field-controlled cells with the in-plane director field, one observes formation of dipolar structures of twist-bound defects composed of the torons and umbilics. Similar to the elastic dipoles formed by colloidal particles accompanied by hyperbolic point defects, the toron-umbilical pairs interact with each other via long-range elastic interactions and form dipolar chains. At high densities of the toron-umbilical pairs, these chains self-organize into super-lattices with toron-umbilical pairs bound into antiferroelectric-like two-dimensional crystals of dipolar colloidal chains. We demonstrate that the periodic lattices of twist-bound defects can be used as optically- and electrically- controlled diffraction gratings, for optical data storage, as well as in the design of all-optical information displays. This work was supported by the Renewable and Sustainable Energy Initiative and Innovation Initiative Seed Grant Programs of University of Colorado, International Institute for Complex Adaptive Matter, and by the NSF grants DMR0645461, DMR0820579, and DMR0847782.
13235 en The physics of liquid crystals under confinement: Porous media, networks, and the future Studies of physical systems in solid porous hosts date back several decades, for example, to studies of the helium superfluid transition in jewelers rouge. Research of the superfluid properties in porous media kept center stage until the nineties where they had to share their supremacy with confined liquid crystal studies (those of us working in liquid crystals would like to believe that). Point in case, from an applied perspective, a liquid crystal display is arguably one the better known confined physical systems. Their operation strongly depends on the interaction between the liquid crystal molecules and the host solid surfaces. From a fundamental point of view, liquid crystals imbedded in solid porous materials or with dispersed nanoparticles are incredibly rich systems that allow the study of a variety of physical phenomena including probing effects on different order phase transitions. These phenomena include dramatic changes at phase transitions and in particular, size-dependent critical exponents; surface-induced liquid crystal alignment; bilayer-by-bilayer smectic growth; complete, quasi-complete and partial wetting, and molecular configurational transitions, among others. It should be mentioned that much of the above experimental work heavily relied on the theoretical contributions of S. Zumer and his co-workers. Specifically, some of these effects were born out by NMR and ac calorimetry studies with cyanobiphenyl liquid crystals embedded in the well-defined cylindrical cavities of aluminum oxide Anopore or polycarbonate Nucleopore membranes; in the randomly interconnected networks of pores like in Vycor and Aerogel glasses; in the Millipore filter cellulose voids or by dispersing spherical silica nanometer particles, Aerosil, in the liquid crystal; and of course, the original liquid crystals in polymer networks. After reviewing some of the aforementioned studies, we will touch upon where future studies might be. These may range from confining networks formed from metallic nanoparticles or nanowires to photovoltaics to drug delivery to understanding trans-to-cis photo-isomerization transition in porous media. *These studies would not have been possible without the many contributions of B. Zalar and S. Zumer at the University of Ljubljana, Slovenia, and G. Iannacchione, T. Jin, S. Qian, H. Zeng, and G. Crawford at Kent State University.n
13236 en Recent advances on nematic order reconstruction Investigations on Pi-cell, a sandwich cell with the director rotating by 180°, are demonstrating the possibility to obtain nematic transitions between two textures with different topologies, for instance between an untwisted state and a Pi-twisted one. Fast topological changes can be observed by means of suitable electric fields or by micro/nano-confinement in the nematic bulk or close to a boundary surface. A strong frustration, in fact, as in the case of a strong local distortion, can induce transient and/or local biaxial states inside a nematic calamitic phase and the resultant nematic dynamics demands a full Landau-de Gennes Q-tensor description. In this work, we present Q-tensor numerical models capable to describe the nematic order dynamics in full details, comparing theoretical results with experiments. We will also discuss as nematic order reconstruction close to a boundary surface with strong or infinite anchoring conditions can provide transitions equivalent to anchoring breaking.
13237 en The power of Poincare: Elucidating the hidden symmetries in focal conic domains Focal conic domains are typically the ``smoking gun'' by which smectic liquid crystalline phases are identified. The geometry of the equally-spaced smectic layers is highly generic but, at the same time, difficult to work with. In this talk we develop an approach to the study of focal sets in smectics which exploits a hidden Poincare symmetry revealed only by viewing the smectic layers as projections from one-higher dimension. We use this perspective to shed light upon several classic focal conic textures, including the concentric cyclides of Dupin, polygonal textures and tilt-grain boundaries.
13238 en NMR of Demixing and Phase Separation in Liquid Crystals Demixing is a well-known phenomenon present in multi-component liquids and polymer blends, conventionally modeled by the Flory-Huggins free energy approach. In liquid crystals, it can result in a wide coexistence range of nematic and isotropic phases [1]. Determination of the respective temperature-concentration phase diagram is often a rather tedious task, due to many varying parameters: the relative volumes of the phases, the nematic order parameters of the individual mesogens in the nematic phase, as well as the concentrations of components in the nematic and isotropic phases. We demonstrate that deuteron NMR is a particularly useful experimental tool for the study of molecular segregation in nematics, providing for a simultaneous measurement of all the above parameters, both in bulk and confined systems, e.g. in liquid crystalline microemulsions, in doped liquid single-crystal elastomers, and in photoisomerizable nematics.
13239 en Confinement effects in two dimensions on freely suspended liquid crystal films We are studying the interactions and dynamics of islands, textures, and topological defects onnfreely suspended smectic C liquid crystal films. Freely suspended smectic films are quantizednin thickness in units of single smectic layers and Islands studied here are thicker stacks ofnlayers on a thinner background film. We will report on the following:na) Diffusion of islands on smectic A films, revealing the distinct effects of hydrodynamicndrag from liquid crystal flow and from airflow.nb) Interactions of islands on smectic C films, exploring the effects of chirality on thencollective behavior of islands and their companion topological defects.nc) Dynamic behavior of buckminsterfullerene nanospheres in freely suspended films.nd) Effects of polarization on textures and defects in two dimensions, where a dramaticntransformation in textures is found to accompany the increase of polarization, a changenwhich can be produced by increase of the enentiomeric excess (ee) of the LC. Thenimages below show an example, where the ee = 0% textures are smooth with largenorientation fluctuations, the ee = 100% textures exhibit sharp discontinuities.
13245 en Carbon nanotube based liquid crystals Carbon nanotubes are promising anisotropic particles for a variety of applications such as strong and lightweight composites, sensors, electronic devices, conductive inks, substrates for tissue engineering, etc. The dispersion behavior and spatial ordering of carbon nanotubes are critical to optimize the properties of nanotube based materials. Various approaches are currently explored to achieve diverse structures such as macroscopic alignment, percolated isotropic networks, solid or liquid crystalline states. We present in this talk the phase behavior of nanotube suspensions stabilized by surfactants or amphiphilic polymers. Those systems can form nematic liquid crystals. Nevertheless, achieving large values of the order parameter as well as large mono-domains remains challenging. We will discuss in particular the effect of processing conditions of nanotube based liquid crystals. Another approach for aligning carbon nanotubes consists in dispersing the particles in a liquid crystalline medium. Carbon nanotubes, because of their small dimensions, don’t create distortions of the liquid crystal host (“weak anchoring” regime). But they still align in response to the surface energy anisotropy. We will show some examples of such materials made of nanotubes embedded in liquid crystals. Routes for further improvements and future applications will be discussed.
13246 en Monte Carlo Simulations of Platelets Colloidal suspensions of platelets are known, experimentally, to show a variety of phenomena reflecting the interplay of their Oseen-Frank elastic constants and surface tension effects [1, 2]. We have previously determined the Frank elastic constants of thin hard platelets by Monte Carlo simulation [3] and have recently extended this work to non-zero thickness [4], in both cases comparing with theoretical predictions. As expected, the bend elastic constant K3 is at least an order of magnitude smaller than the other two; the results are in reasonable agreement with experiment. We have also examined the phases of thin hard platelets confined between hard flat walls, observing surface-induced ordering and capillary condensation of the nematic phase.
13247 en Effect of strong confinement on defect structures of cholesteric blue phases Cholesteric blue phases (BPs) [1] found in highly chiral liquid crystals are interesting examples of three-dimensional ordered structures in soft materials. They appear as a result of frustrations between local preference of a double-twist configuration over a single twist, and a global topological constraint that a double-twist configuration cannot fill the whole space. Though the structures of bulk BPs with cubic symmetry are now well established, it is not yet understood how the anchoring of confining surfaces affects the order of structures of BPs. Here we discuss the structures of topological defects in a strongly confined chiral liquid crystal [2].nOur study is based on numerical calculations using a Landau–de Gennes theory, in which the orientational order is described by a second-rank tensor. Defect structures in a planar cell imposing homeotropic anchoring at the surfaces do not resemble those of bulk BPs and can indeed be thermodynamically stable when the cell thickness is of the order of the dimension of the unit cell of bulk BPs. These novel defect structures can be regarded as a consequence of a different frustration between the local preferred structure of a regular array of disclinations, and the constraint imposed by the anchoring of confining surfaces. Our results indicate that there are still possibilities for unknown ordered structures in liquid crystals arising from more complex frustrations.nWe acknowledge financial support by Slovenian Research Agency (ARRS research program P1- 0099 and project J1-2335) and KAKENHI (Grant-in-Aid for Scientific Research) on Priority Area “Soft Matter Physics” from the Ministry of Education, Culture, Sports, Science and Technology of Japan.
13248 en Nematic Fluctuations as a Probe the Properties of Liquid Crystal Elastomers In recent experiments[1] we have shown that the director fluctuations in nematic liquid crystal elastomers (LCE), as measured by dynamic light scattering, are a sensitive probe of the properties of LCE under strain, in particular with respect to the semisoft behavior. Semisoft elastic response originates in a symmetry breaking internal strain that was built into the material as a result of the procedure to obtain a monodomain sample. This strain is coupled to the nematic director, locking it into the orientation direction. It makes the relaxation rate of the director finite and essentially independent of the wave-vector. An applied external strain perpendicular to the orientation direction at a critical value, corresponding to the onset of the semisoft plateau, effectively cancels the anisotropy field acting on the director so that the relaxation rate at zero wave -vector goes to zero, reflecting the fact that in an ideal LCE the nematic director fluctuations are the Goldstone mode of the system. Our dynamic light scattering measurements of the relaxation rate of the nematic fluctuations as a function of strain an temperature are in complete accordance with the theory semisoft elasticity of Warner and Terentjev and combined with mechanical measurements allow us to determine all the parameters of the theory.nPhotosensitive LCE, doped with azobenzene derivatives undergoing trans to cis isomerization, are of particular interest. Our experiments on pure LCE have shown that the semisoftness parameter that measures the internal strain is proportional to the nematic order parameter. In photosensitive LCE under UV illumination the semisoftness parameter cannot be determined from the stress-strain curves due to strong inhomogeneity of the cis isomer concentration. So we exploited the fact that the director fluctuations are coupled to the anisotropy field and measured the change in the nematic order parameter as a function of UV illumination by measuring the relaxation rate of the director fluctuations with dynamic light scattering. We have also written holographic diffraction grating in photosensitive LCE and determined the depth of the layer that underwent cis-trans isomerization by measuring the angular width of the diffraction peak around the Bragg angle.
13249 en Liquid crystal colloids: functionalization of inclusions and continuum Liquid crystal colloids combine orientational anisotropy of liquid crystalline continuum phase and discrete ordering of dispersed colloidal inclusions [1,2]. The long-ranged orientational ordering of liquid crystals generates structural forces between the inclusions which allows for assembly of colloidal composites to be used as advanced optical elements. Fully, the applicability and means for optimisation of optical structures in liquid crystal colloids is revealed by the variety of possibilities for functionalizing the materials - both inclusions and the continuum phase.nHere, we present routes for functionalization of colloidal particles and continuum liquid crystal. For particles, the role of shape, surface structure, and Janus [3] and higher Janus surface anchoring profile is shown. For bulk medium, the role of chirality and material activity is demonstrated. Novel 3D flow profiles are shown in active liquid crystal cells. Combining confinement of colloidal particles and active material flow microfluidic elements could be assembled.
13250 en Nematic Colloid as a topological playground Particles dispersed in a nematic yield complex defect structures. Beside structures where defects are localized close to particles also structures where disclination lines entangle two or more particles are found. Here we focus on dimmers and show that a simple geometrical description based on local differences can be used to classify configurations and describe restructurings among possible forms. The linking number of a disclination loop, which tells how the director field of a defect line twists when following a disclination, help us to distinguish between topologically different classes. Simple ring defects have zero linking number while entangled states exhibit fractional linking numbers.
13251 en Round table 
